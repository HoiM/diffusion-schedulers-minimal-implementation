/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 1/100 Iteration 1/234: loss=0.501746 lr=0.000020 grad_norm=0.177404
Epoch 1/100 Iteration 2/234: loss=0.499202 lr=0.000020 grad_norm=0.132572
Epoch 1/100 Iteration 3/234: loss=0.499725 lr=0.000020 grad_norm=0.149580
Epoch 1/100 Iteration 4/234: loss=0.500277 lr=0.000020 grad_norm=0.242728
Epoch 1/100 Iteration 5/234: loss=0.499037 lr=0.000020 grad_norm=0.265426
Epoch 1/100 Iteration 6/234: loss=0.499980 lr=0.000020 grad_norm=0.330085
Epoch 1/100 Iteration 7/234: loss=0.500179 lr=0.000020 grad_norm=0.366317
Epoch 1/100 Iteration 8/234: loss=0.499224 lr=0.000020 grad_norm=0.571722
Epoch 1/100 Iteration 9/234: loss=0.499407 lr=0.000020 grad_norm=0.504185
Epoch 1/100 Iteration 10/234: loss=0.496033 lr=0.000020 grad_norm=0.535578
Epoch 1/100 Iteration 11/234: loss=0.497352 lr=0.000020 grad_norm=0.584944
Epoch 1/100 Iteration 12/234: loss=0.497064 lr=0.000020 grad_norm=0.591421
Epoch 1/100 Iteration 13/234: loss=0.497460 lr=0.000020 grad_norm=0.658475
Epoch 1/100 Iteration 14/234: loss=0.496214 lr=0.000020 grad_norm=0.724792
Epoch 1/100 Iteration 15/234: loss=0.496014 lr=0.000020 grad_norm=0.775956
Epoch 1/100 Iteration 16/234: loss=0.494236 lr=0.000020 grad_norm=0.755331
Epoch 1/100 Iteration 17/234: loss=0.494693 lr=0.000020 grad_norm=0.678282
Epoch 1/100 Iteration 18/234: loss=0.495469 lr=0.000020 grad_norm=0.819686
Epoch 1/100 Iteration 19/234: loss=0.491551 lr=0.000020 grad_norm=0.920295
Epoch 1/100 Iteration 20/234: loss=0.492875 lr=0.000020 grad_norm=0.765978
Epoch 1/100 Iteration 21/234: loss=0.491280 lr=0.000020 grad_norm=0.807615
Epoch 1/100 Iteration 22/234: loss=0.494338 lr=0.000020 grad_norm=0.734392
Epoch 1/100 Iteration 23/234: loss=0.488427 lr=0.000020 grad_norm=1.048004
Epoch 1/100 Iteration 24/234: loss=0.492674 lr=0.000020 grad_norm=0.882722
Epoch 1/100 Iteration 25/234: loss=0.486449 lr=0.000020 grad_norm=1.069714
Epoch 1/100 Iteration 26/234: loss=0.489917 lr=0.000020 grad_norm=0.943495
Epoch 1/100 Iteration 27/234: loss=0.488289 lr=0.000020 grad_norm=0.855292
Epoch 1/100 Iteration 28/234: loss=0.487598 lr=0.000020 grad_norm=0.976633
Epoch 1/100 Iteration 29/234: loss=0.485811 lr=0.000020 grad_norm=1.090118
Epoch 1/100 Iteration 30/234: loss=0.482906 lr=0.000020 grad_norm=1.102739
Epoch 1/100 Iteration 31/234: loss=0.483510 lr=0.000020 grad_norm=1.173980
Epoch 1/100 Iteration 32/234: loss=0.481738 lr=0.000020 grad_norm=1.176004
Epoch 1/100 Iteration 33/234: loss=0.480947 lr=0.000020 grad_norm=1.066526
Epoch 1/100 Iteration 34/234: loss=0.479608 lr=0.000020 grad_norm=1.168273
Epoch 1/100 Iteration 35/234: loss=0.478375 lr=0.000020 grad_norm=1.139587
Epoch 1/100 Iteration 36/234: loss=0.480328 lr=0.000020 grad_norm=0.938491
Epoch 1/100 Iteration 37/234: loss=0.476375 lr=0.000020 grad_norm=1.203002
Epoch 1/100 Iteration 38/234: loss=0.473989 lr=0.000020 grad_norm=1.209162
Epoch 1/100 Iteration 39/234: loss=0.477363 lr=0.000020 grad_norm=1.442600
Epoch 1/100 Iteration 40/234: loss=0.474011 lr=0.000020 grad_norm=1.541292
Epoch 1/100 Iteration 41/234: loss=0.472972 lr=0.000020 grad_norm=1.248963
Epoch 1/100 Iteration 42/234: loss=0.476132 lr=0.000020 grad_norm=1.207376
Epoch 1/100 Iteration 43/234: loss=0.474338 lr=0.000020 grad_norm=1.128690
Epoch 1/100 Iteration 44/234: loss=0.473579 lr=0.000020 grad_norm=1.114216
Epoch 1/100 Iteration 45/234: loss=0.468732 lr=0.000020 grad_norm=1.170787
Epoch 1/100 Iteration 46/234: loss=0.470324 lr=0.000020 grad_norm=1.179473
Epoch 1/100 Iteration 47/234: loss=0.463264 lr=0.000020 grad_norm=1.220958
Epoch 1/100 Iteration 48/234: loss=0.466136 lr=0.000020 grad_norm=1.183855
Epoch 1/100 Iteration 49/234: loss=0.464529 lr=0.000020 grad_norm=1.186757
Epoch 1/100 Iteration 50/234: loss=0.462346 lr=0.000020 grad_norm=1.240343
Epoch 1/100 Iteration 51/234: loss=0.464061 lr=0.000020 grad_norm=1.285982
Epoch 1/100 Iteration 52/234: loss=0.454279 lr=0.000020 grad_norm=1.556884
Epoch 1/100 Iteration 53/234: loss=0.462131 lr=0.000020 grad_norm=1.431994
Epoch 1/100 Iteration 54/234: loss=0.456905 lr=0.000020 grad_norm=1.316729
Epoch 1/100 Iteration 55/234: loss=0.460111 lr=0.000020 grad_norm=1.254096
Epoch 1/100 Iteration 56/234: loss=0.456140 lr=0.000020 grad_norm=1.256433
Epoch 1/100 Iteration 57/234: loss=0.458132 lr=0.000020 grad_norm=1.112637
Epoch 1/100 Iteration 58/234: loss=0.447744 lr=0.000020 grad_norm=1.375909
Epoch 1/100 Iteration 59/234: loss=0.457450 lr=0.000020 grad_norm=1.109046
Epoch 1/100 Iteration 60/234: loss=0.456148 lr=0.000020 grad_norm=1.111002
Epoch 1/100 Iteration 61/234: loss=0.454398 lr=0.000020 grad_norm=1.131799
Epoch 1/100 Iteration 62/234: loss=0.450444 lr=0.000020 grad_norm=1.200075
Epoch 1/100 Iteration 63/234: loss=0.450427 lr=0.000020 grad_norm=1.158568
Epoch 1/100 Iteration 64/234: loss=0.452447 lr=0.000020 grad_norm=1.050440
Epoch 1/100 Iteration 65/234: loss=0.445612 lr=0.000020 grad_norm=1.165083
Epoch 1/100 Iteration 66/234: loss=0.448503 lr=0.000020 grad_norm=1.066600
Epoch 1/100 Iteration 67/234: loss=0.449992 lr=0.000020 grad_norm=1.078848
Epoch 1/100 Iteration 68/234: loss=0.444943 lr=0.000020 grad_norm=1.164644
Epoch 1/100 Iteration 69/234: loss=0.438768 lr=0.000020 grad_norm=1.230126
Epoch 1/100 Iteration 70/234: loss=0.435921 lr=0.000020 grad_norm=1.345065
Epoch 1/100 Iteration 71/234: loss=0.445177 lr=0.000020 grad_norm=1.294884
Epoch 1/100 Iteration 72/234: loss=0.446871 lr=0.000020 grad_norm=1.300409
Epoch 1/100 Iteration 73/234: loss=0.437755 lr=0.000020 grad_norm=1.227936
Epoch 1/100 Iteration 74/234: loss=0.437620 lr=0.000020 grad_norm=1.241282
Epoch 1/100 Iteration 75/234: loss=0.429492 lr=0.000020 grad_norm=1.329418
Epoch 1/100 Iteration 76/234: loss=0.436700 lr=0.000020 grad_norm=1.151046
Epoch 1/100 Iteration 77/234: loss=0.431681 lr=0.000020 grad_norm=1.244885
Epoch 1/100 Iteration 78/234: loss=0.432960 lr=0.000020 grad_norm=1.301002
Epoch 1/100 Iteration 79/234: loss=0.433761 lr=0.000020 grad_norm=1.218737
Epoch 1/100 Iteration 80/234: loss=0.438148 lr=0.000020 grad_norm=1.141791
Epoch 1/100 Iteration 81/234: loss=0.419917 lr=0.000020 grad_norm=1.358036
Epoch 1/100 Iteration 82/234: loss=0.441882 lr=0.000020 grad_norm=1.260332
Epoch 1/100 Iteration 83/234: loss=0.426666 lr=0.000020 grad_norm=1.271050
Epoch 1/100 Iteration 84/234: loss=0.426099 lr=0.000020 grad_norm=1.140912
Epoch 1/100 Iteration 85/234: loss=0.424855 lr=0.000020 grad_norm=1.185739
Epoch 1/100 Iteration 86/234: loss=0.419165 lr=0.000020 grad_norm=1.232614
Epoch 1/100 Iteration 87/234: loss=0.424105 lr=0.000020 grad_norm=1.067391
Epoch 1/100 Iteration 88/234: loss=0.424057 lr=0.000020 grad_norm=1.095156
Epoch 1/100 Iteration 89/234: loss=0.422630 lr=0.000020 grad_norm=1.153080
Epoch 1/100 Iteration 90/234: loss=0.414653 lr=0.000020 grad_norm=1.151339
Epoch 1/100 Iteration 91/234: loss=0.423939 lr=0.000020 grad_norm=1.105903
Epoch 1/100 Iteration 92/234: loss=0.430245 lr=0.000020 grad_norm=1.017333
Epoch 1/100 Iteration 93/234: loss=0.415811 lr=0.000020 grad_norm=1.089788
Epoch 1/100 Iteration 94/234: loss=0.413038 lr=0.000020 grad_norm=1.131578
Epoch 1/100 Iteration 95/234: loss=0.422110 lr=0.000020 grad_norm=0.986498
Epoch 1/100 Iteration 96/234: loss=0.418997 lr=0.000020 grad_norm=1.015660
Epoch 1/100 Iteration 97/234: loss=0.411530 lr=0.000020 grad_norm=1.091322
Epoch 1/100 Iteration 98/234: loss=0.412579 lr=0.000020 grad_norm=1.055929
Epoch 1/100 Iteration 99/234: loss=0.416175 lr=0.000020 grad_norm=1.004761
Epoch 1/100 Iteration 100/234: loss=0.414344 lr=0.000020 grad_norm=0.990182
Epoch 1/100 Iteration 101/234: loss=0.420446 lr=0.000020 grad_norm=0.943793
Epoch 1/100 Iteration 102/234: loss=0.405733 lr=0.000020 grad_norm=1.142049
Epoch 1/100 Iteration 103/234: loss=0.410054 lr=0.000020 grad_norm=1.238249
Epoch 1/100 Iteration 104/234: loss=0.394486 lr=0.000020 grad_norm=1.653929
Epoch 1/100 Iteration 105/234: loss=0.413316 lr=0.000020 grad_norm=1.617719
Epoch 1/100 Iteration 106/234: loss=0.406722 lr=0.000020 grad_norm=1.092927
Epoch 1/100 Iteration 107/234: loss=0.417089 lr=0.000020 grad_norm=1.225372
Epoch 1/100 Iteration 108/234: loss=0.404725 lr=0.000020 grad_norm=1.204756
Epoch 1/100 Iteration 109/234: loss=0.408433 lr=0.000020 grad_norm=1.083248
Epoch 1/100 Iteration 110/234: loss=0.408051 lr=0.000020 grad_norm=1.144109
Epoch 1/100 Iteration 111/234: loss=0.407824 lr=0.000020 grad_norm=1.074943
Epoch 1/100 Iteration 112/234: loss=0.412407 lr=0.000020 grad_norm=0.983975
Epoch 1/100 Iteration 113/234: loss=0.410455 lr=0.000020 grad_norm=0.960891
Epoch 1/100 Iteration 114/234: loss=0.406218 lr=0.000020 grad_norm=1.006315
Epoch 1/100 Iteration 115/234: loss=0.394422 lr=0.000020 grad_norm=1.046325
Epoch 1/100 Iteration 116/234: loss=0.412580 lr=0.000020 grad_norm=0.964922
Epoch 1/100 Iteration 117/234: loss=0.407896 lr=0.000020 grad_norm=0.955137
Epoch 1/100 Iteration 118/234: loss=0.402756 lr=0.000020 grad_norm=0.976391
Epoch 1/100 Iteration 119/234: loss=0.387974 lr=0.000020 grad_norm=1.103399
Epoch 1/100 Iteration 120/234: loss=0.397040 lr=0.000020 grad_norm=1.076475
Epoch 1/100 Iteration 121/234: loss=0.398253 lr=0.000020 grad_norm=1.055612
Epoch 1/100 Iteration 122/234: loss=0.395764 lr=0.000020 grad_norm=1.080056
Epoch 1/100 Iteration 123/234: loss=0.383158 lr=0.000020 grad_norm=1.121472
Epoch 1/100 Iteration 124/234: loss=0.391766 lr=0.000020 grad_norm=1.070478
Epoch 1/100 Iteration 125/234: loss=0.391702 lr=0.000020 grad_norm=1.052091
Epoch 1/100 Iteration 126/234: loss=0.391421 lr=0.000020 grad_norm=1.070432
Epoch 1/100 Iteration 127/234: loss=0.397805 lr=0.000020 grad_norm=0.962088
Epoch 1/100 Iteration 128/234: loss=0.405062 lr=0.000020 grad_norm=0.921473
Epoch 1/100 Iteration 129/234: loss=0.380740 lr=0.000020 grad_norm=1.125287
Epoch 1/100 Iteration 130/234: loss=0.390161 lr=0.000020 grad_norm=1.102108
Epoch 1/100 Iteration 131/234: loss=0.395466 lr=0.000020 grad_norm=1.033171
Epoch 1/100 Iteration 132/234: loss=0.400774 lr=0.000020 grad_norm=0.953717
Epoch 1/100 Iteration 133/234: loss=0.388394 lr=0.000020 grad_norm=1.027151
Epoch 1/100 Iteration 134/234: loss=0.391825 lr=0.000020 grad_norm=1.010089
Epoch 1/100 Iteration 135/234: loss=0.382304 lr=0.000020 grad_norm=1.052305
Epoch 1/100 Iteration 136/234: loss=0.387172 lr=0.000020 grad_norm=1.045898
Epoch 1/100 Iteration 137/234: loss=0.392052 lr=0.000020 grad_norm=0.989186
Epoch 1/100 Iteration 138/234: loss=0.392050 lr=0.000020 grad_norm=0.869579
Epoch 1/100 Iteration 139/234: loss=0.381031 lr=0.000020 grad_norm=0.934028
Epoch 1/100 Iteration 140/234: loss=0.394603 lr=0.000020 grad_norm=0.940537
Epoch 1/100 Iteration 141/234: loss=0.384574 lr=0.000020 grad_norm=0.985078
Epoch 1/100 Iteration 142/234: loss=0.391021 lr=0.000020 grad_norm=0.915397
Epoch 1/100 Iteration 143/234: loss=0.374560 lr=0.000020 grad_norm=1.054343
Epoch 1/100 Iteration 144/234: loss=0.377061 lr=0.000020 grad_norm=1.279079
Epoch 1/100 Iteration 145/234: loss=0.376041 lr=0.000020 grad_norm=1.689801
Epoch 1/100 Iteration 146/234: loss=0.373958 lr=0.000020 grad_norm=1.822811
Epoch 1/100 Iteration 147/234: loss=0.375929 lr=0.000020 grad_norm=1.267963
Epoch 1/100 Iteration 148/234: loss=0.387120 lr=0.000020 grad_norm=1.065036
Epoch 1/100 Iteration 149/234: loss=0.371602 lr=0.000020 grad_norm=1.109686
Epoch 1/100 Iteration 150/234: loss=0.380509 lr=0.000020 grad_norm=1.112105
Epoch 1/100 Iteration 151/234: loss=0.359236 lr=0.000020 grad_norm=1.030249
Epoch 1/100 Iteration 152/234: loss=0.383234 lr=0.000020 grad_norm=1.040659
Epoch 1/100 Iteration 153/234: loss=0.376586 lr=0.000020 grad_norm=0.925036
Epoch 1/100 Iteration 154/234: loss=0.359997 lr=0.000020 grad_norm=1.073054
Epoch 1/100 Iteration 155/234: loss=0.358423 lr=0.000020 grad_norm=1.087971
Epoch 1/100 Iteration 156/234: loss=0.372785 lr=0.000020 grad_norm=1.121516
Epoch 1/100 Iteration 157/234: loss=0.351722 lr=0.000020 grad_norm=1.155463
Epoch 1/100 Iteration 158/234: loss=0.376921 lr=0.000020 grad_norm=1.044299
Epoch 1/100 Iteration 159/234: loss=0.373198 lr=0.000020 grad_norm=0.981909
Epoch 1/100 Iteration 160/234: loss=0.377594 lr=0.000020 grad_norm=0.888908
Epoch 1/100 Iteration 161/234: loss=0.361952 lr=0.000020 grad_norm=0.965579
Epoch 1/100 Iteration 162/234: loss=0.358254 lr=0.000020 grad_norm=0.957216
Epoch 1/100 Iteration 163/234: loss=0.381867 lr=0.000020 grad_norm=0.781598
Epoch 1/100 Iteration 164/234: loss=0.378901 lr=0.000020 grad_norm=0.846850
Epoch 1/100 Iteration 165/234: loss=0.375301 lr=0.000020 grad_norm=0.849152
Epoch 1/100 Iteration 166/234: loss=0.372156 lr=0.000020 grad_norm=0.852669
Epoch 1/100 Iteration 167/234: loss=0.361464 lr=0.000020 grad_norm=0.866348
Epoch 1/100 Iteration 168/234: loss=0.349760 lr=0.000020 grad_norm=0.926122
Epoch 1/100 Iteration 169/234: loss=0.361582 lr=0.000020 grad_norm=0.852313
Epoch 1/100 Iteration 170/234: loss=0.358272 lr=0.000020 grad_norm=0.955875
Epoch 1/100 Iteration 171/234: loss=0.346275 lr=0.000020 grad_norm=1.005800
Epoch 1/100 Iteration 172/234: loss=0.353134 lr=0.000020 grad_norm=1.065517
Epoch 1/100 Iteration 173/234: loss=0.353349 lr=0.000020 grad_norm=1.249330
Epoch 1/100 Iteration 174/234: loss=0.362362 lr=0.000020 grad_norm=1.382831
Epoch 1/100 Iteration 175/234: loss=0.373505 lr=0.000020 grad_norm=1.239984
Epoch 1/100 Iteration 176/234: loss=0.364237 lr=0.000020 grad_norm=1.223934
Epoch 1/100 Iteration 177/234: loss=0.362108 lr=0.000020 grad_norm=1.580363
Epoch 1/100 Iteration 178/234: loss=0.338082 lr=0.000020 grad_norm=1.587983
Epoch 1/100 Iteration 179/234: loss=0.355706 lr=0.000020 grad_norm=1.353754
Epoch 1/100 Iteration 180/234: loss=0.352401 lr=0.000020 grad_norm=1.130414
Epoch 1/100 Iteration 181/234: loss=0.348883 lr=0.000020 grad_norm=1.060719
Epoch 1/100 Iteration 182/234: loss=0.369200 lr=0.000020 grad_norm=0.985118
Epoch 1/100 Iteration 183/234: loss=0.355825 lr=0.000020 grad_norm=0.980919
Epoch 1/100 Iteration 184/234: loss=0.371415 lr=0.000020 grad_norm=0.895065
Epoch 1/100 Iteration 185/234: loss=0.352391 lr=0.000020 grad_norm=0.979995
Epoch 1/100 Iteration 186/234: loss=0.358966 lr=0.000020 grad_norm=0.947162
Epoch 1/100 Iteration 187/234: loss=0.368478 lr=0.000020 grad_norm=0.830791
Epoch 1/100 Iteration 188/234: loss=0.353511 lr=0.000020 grad_norm=0.838117
Epoch 1/100 Iteration 189/234: loss=0.360176 lr=0.000020 grad_norm=0.834168
Epoch 1/100 Iteration 190/234: loss=0.343791 lr=0.000020 grad_norm=0.894783
Epoch 1/100 Iteration 191/234: loss=0.331662 lr=0.000020 grad_norm=0.901502
Epoch 1/100 Iteration 192/234: loss=0.364713 lr=0.000020 grad_norm=0.747366
Epoch 1/100 Iteration 193/234: loss=0.345857 lr=0.000020 grad_norm=0.784038
Epoch 1/100 Iteration 194/234: loss=0.340961 lr=0.000020 grad_norm=0.796652
Epoch 1/100 Iteration 195/234: loss=0.336077 lr=0.000020 grad_norm=0.855971
Epoch 1/100 Iteration 196/234: loss=0.342626 lr=0.000020 grad_norm=0.839754
Epoch 1/100 Iteration 197/234: loss=0.361460 lr=0.000020 grad_norm=0.764152
Epoch 1/100 Iteration 198/234: loss=0.347948 lr=0.000020 grad_norm=0.747740
Epoch 1/100 Iteration 199/234: loss=0.345006 lr=0.000020 grad_norm=0.834292
Epoch 1/100 Iteration 200/234: loss=0.368388 lr=0.000020 grad_norm=0.817545
Epoch 1/100 Iteration 201/234: loss=0.359329 lr=0.000020 grad_norm=0.874095
Epoch 1/100 Iteration 202/234: loss=0.353324 lr=0.000020 grad_norm=0.892507
Epoch 1/100 Iteration 203/234: loss=0.349853 lr=0.000020 grad_norm=0.927133
Epoch 1/100 Iteration 204/234: loss=0.354360 lr=0.000020 grad_norm=0.916828
Epoch 1/100 Iteration 205/234: loss=0.350015 lr=0.000020 grad_norm=0.815726
Epoch 1/100 Iteration 206/234: loss=0.349882 lr=0.000020 grad_norm=0.728133
Epoch 1/100 Iteration 207/234: loss=0.340751 lr=0.000020 grad_norm=0.849189
Epoch 1/100 Iteration 208/234: loss=0.363899 lr=0.000020 grad_norm=0.828297
Epoch 1/100 Iteration 209/234: loss=0.351097 lr=0.000020 grad_norm=0.917751
Epoch 1/100 Iteration 210/234: loss=0.359135 lr=0.000020 grad_norm=1.073869
Epoch 1/100 Iteration 211/234: loss=0.350044 lr=0.000020 grad_norm=1.284897
Epoch 1/100 Iteration 212/234: loss=0.350630 lr=0.000020 grad_norm=1.196803
Epoch 1/100 Iteration 213/234: loss=0.344953 lr=0.000020 grad_norm=0.941639
Epoch 1/100 Iteration 214/234: loss=0.359315 lr=0.000020 grad_norm=1.042645
Epoch 1/100 Iteration 215/234: loss=0.343685 lr=0.000020 grad_norm=1.101176
Epoch 1/100 Iteration 216/234: loss=0.337932 lr=0.000020 grad_norm=1.137000
Epoch 1/100 Iteration 217/234: loss=0.348010 lr=0.000020 grad_norm=1.099810
Epoch 1/100 Iteration 218/234: loss=0.340708 lr=0.000020 grad_norm=0.895211
Epoch 1/100 Iteration 219/234: loss=0.336185 lr=0.000020 grad_norm=1.061495
Epoch 1/100 Iteration 220/234: loss=0.334409 lr=0.000020 grad_norm=0.917751
Epoch 1/100 Iteration 221/234: loss=0.337706 lr=0.000020 grad_norm=0.950981
Epoch 1/100 Iteration 222/234: loss=0.330546 lr=0.000020 grad_norm=0.796609
Epoch 1/100 Iteration 223/234: loss=0.347849 lr=0.000020 grad_norm=0.760129
Epoch 1/100 Iteration 224/234: loss=0.339391 lr=0.000020 grad_norm=0.896390
Epoch 1/100 Iteration 225/234: loss=0.336184 lr=0.000020 grad_norm=0.783116
Epoch 1/100 Iteration 226/234: loss=0.339169 lr=0.000020 grad_norm=0.714599
Epoch 1/100 Iteration 227/234: loss=0.340822 lr=0.000020 grad_norm=0.665288
Epoch 1/100 Iteration 228/234: loss=0.343183 lr=0.000020 grad_norm=0.726838
Epoch 1/100 Iteration 229/234: loss=0.334065 lr=0.000020 grad_norm=0.710443
Epoch 1/100 Iteration 230/234: loss=0.331605 lr=0.000020 grad_norm=0.767381
Epoch 1/100 Iteration 231/234: loss=0.326884 lr=0.000020 grad_norm=0.786737
Epoch 1/100 Iteration 232/234: loss=0.338865 lr=0.000020 grad_norm=0.829861
Epoch 1/100 Iteration 233/234: loss=0.357988 lr=0.000020 grad_norm=0.891604
Epoch 1/100 Iteration 234/234: loss=0.333911 lr=0.000020 grad_norm=0.863942
Epoch 1/100 finished. Avg Loss: 0.408536
Epoch 2/100 Iteration 1/234: loss=0.340030 lr=0.000020 grad_norm=0.791014
Epoch 2/100 Iteration 2/234: loss=0.337276 lr=0.000020 grad_norm=0.805931
Epoch 2/100 Iteration 3/234: loss=0.327674 lr=0.000020 grad_norm=0.777401
Epoch 2/100 Iteration 4/234: loss=0.345872 lr=0.000020 grad_norm=0.934373
Epoch 2/100 Iteration 5/234: loss=0.332143 lr=0.000020 grad_norm=0.966194
Epoch 2/100 Iteration 6/234: loss=0.343220 lr=0.000020 grad_norm=0.827560
Epoch 2/100 Iteration 7/234: loss=0.325670 lr=0.000020 grad_norm=0.693280
Epoch 2/100 Iteration 8/234: loss=0.320700 lr=0.000020 grad_norm=0.840130
Epoch 2/100 Iteration 9/234: loss=0.332749 lr=0.000020 grad_norm=0.870977
Epoch 2/100 Iteration 10/234: loss=0.318941 lr=0.000020 grad_norm=0.767591
Epoch 2/100 Iteration 11/234: loss=0.343627 lr=0.000020 grad_norm=0.821862
Epoch 2/100 Iteration 12/234: loss=0.355721 lr=0.000020 grad_norm=0.797991
Epoch 2/100 Iteration 13/234: loss=0.327086 lr=0.000020 grad_norm=0.797423
Epoch 2/100 Iteration 14/234: loss=0.330866 lr=0.000020 grad_norm=0.784683
Epoch 2/100 Iteration 15/234: loss=0.348061 lr=0.000020 grad_norm=0.646338
Epoch 2/100 Iteration 16/234: loss=0.342560 lr=0.000020 grad_norm=0.672370
Epoch 2/100 Iteration 17/234: loss=0.343274 lr=0.000020 grad_norm=0.642961
Epoch 2/100 Iteration 18/234: loss=0.352074 lr=0.000020 grad_norm=0.612061
Epoch 2/100 Iteration 19/234: loss=0.328639 lr=0.000020 grad_norm=0.650270
Epoch 2/100 Iteration 20/234: loss=0.330705 lr=0.000020 grad_norm=0.621299
Epoch 2/100 Iteration 21/234: loss=0.346896 lr=0.000020 grad_norm=0.675233
Epoch 2/100 Iteration 22/234: loss=0.323744 lr=0.000020 grad_norm=0.745361
Epoch 2/100 Iteration 23/234: loss=0.332489 lr=0.000020 grad_norm=0.916666
Epoch 2/100 Iteration 24/234: loss=0.323824 lr=0.000020 grad_norm=1.179885
Epoch 2/100 Iteration 25/234: loss=0.327585 lr=0.000020 grad_norm=1.347618
Epoch 2/100 Iteration 26/234: loss=0.329440 lr=0.000020 grad_norm=1.114218
Epoch 2/100 Iteration 27/234: loss=0.337432 lr=0.000020 grad_norm=0.741793
Epoch 2/100 Iteration 28/234: loss=0.333226 lr=0.000020 grad_norm=1.012121
Epoch 2/100 Iteration 29/234: loss=0.321507 lr=0.000020 grad_norm=0.899406
Epoch 2/100 Iteration 30/234: loss=0.338584 lr=0.000020 grad_norm=0.878834
Epoch 2/100 Iteration 31/234: loss=0.329948 lr=0.000020 grad_norm=0.932515
Epoch 2/100 Iteration 32/234: loss=0.320184 lr=0.000020 grad_norm=1.095071
Epoch 2/100 Iteration 33/234: loss=0.322365 lr=0.000020 grad_norm=1.249593
Epoch 2/100 Iteration 34/234: loss=0.334139 lr=0.000020 grad_norm=1.294605
Epoch 2/100 Iteration 35/234: loss=0.324895 lr=0.000020 grad_norm=0.972019
Epoch 2/100 Iteration 36/234: loss=0.319665 lr=0.000020 grad_norm=1.138337
Epoch 2/100 Iteration 37/234: loss=0.330286 lr=0.000020 grad_norm=1.159956
Epoch 2/100 Iteration 38/234: loss=0.326479 lr=0.000020 grad_norm=1.253554
Epoch 2/100 Iteration 39/234: loss=0.310911 lr=0.000020 grad_norm=1.033315
Epoch 2/100 Iteration 40/234: loss=0.319054 lr=0.000020 grad_norm=1.018940
Epoch 2/100 Iteration 41/234: loss=0.326444 lr=0.000020 grad_norm=1.460045
Epoch 2/100 Iteration 42/234: loss=0.330115 lr=0.000020 grad_norm=1.786116
Epoch 2/100 Iteration 43/234: loss=0.341077 lr=0.000020 grad_norm=1.497180
Epoch 2/100 Iteration 44/234: loss=0.330805 lr=0.000020 grad_norm=0.934047
Epoch 2/100 Iteration 45/234: loss=0.337708 lr=0.000020 grad_norm=0.879638
Epoch 2/100 Iteration 46/234: loss=0.327300 lr=0.000020 grad_norm=0.890239
Epoch 2/100 Iteration 47/234: loss=0.318534 lr=0.000020 grad_norm=0.891363
Epoch 2/100 Iteration 48/234: loss=0.328039 lr=0.000020 grad_norm=0.777331
Epoch 2/100 Iteration 49/234: loss=0.333700 lr=0.000020 grad_norm=0.854852
Epoch 2/100 Iteration 50/234: loss=0.312550 lr=0.000020 grad_norm=0.764460
Epoch 2/100 Iteration 51/234: loss=0.313676 lr=0.000020 grad_norm=0.755243
Epoch 2/100 Iteration 52/234: loss=0.320317 lr=0.000020 grad_norm=0.850696
Epoch 2/100 Iteration 53/234: loss=0.325207 lr=0.000020 grad_norm=0.780207
Epoch 2/100 Iteration 54/234: loss=0.329066 lr=0.000020 grad_norm=0.717205
Epoch 2/100 Iteration 55/234: loss=0.323707 lr=0.000020 grad_norm=0.696759
Epoch 2/100 Iteration 56/234: loss=0.316890 lr=0.000020 grad_norm=0.740721
Epoch 2/100 Iteration 57/234: loss=0.329794 lr=0.000020 grad_norm=0.815436
Epoch 2/100 Iteration 58/234: loss=0.321766 lr=0.000020 grad_norm=0.704688
Epoch 2/100 Iteration 59/234: loss=0.313743 lr=0.000020 grad_norm=0.730635
Epoch 2/100 Iteration 60/234: loss=0.309682 lr=0.000020 grad_norm=0.655622
Epoch 2/100 Iteration 61/234: loss=0.307689 lr=0.000020 grad_norm=0.682501
Epoch 2/100 Iteration 62/234: loss=0.333509 lr=0.000020 grad_norm=0.607813
Epoch 2/100 Iteration 63/234: loss=0.324315 lr=0.000020 grad_norm=0.564482
Epoch 2/100 Iteration 64/234: loss=0.331133 lr=0.000020 grad_norm=0.562014
Epoch 2/100 Iteration 65/234: loss=0.322908 lr=0.000020 grad_norm=0.636451
Epoch 2/100 Iteration 66/234: loss=0.318844 lr=0.000020 grad_norm=0.645433
Epoch 2/100 Iteration 67/234: loss=0.315197 lr=0.000020 grad_norm=0.597588
Epoch 2/100 Iteration 68/234: loss=0.336240 lr=0.000020 grad_norm=0.630006
Epoch 2/100 Iteration 69/234: loss=0.318442 lr=0.000020 grad_norm=0.601037
Epoch 2/100 Iteration 70/234: loss=0.314744 lr=0.000020 grad_norm=0.589854
Epoch 2/100 Iteration 71/234: loss=0.324899 lr=0.000020 grad_norm=0.532283
Epoch 2/100 Iteration 72/234: loss=0.333594 lr=0.000020 grad_norm=0.533346
Epoch 2/100 Iteration 73/234: loss=0.307427 lr=0.000020 grad_norm=0.615062
Epoch 2/100 Iteration 74/234: loss=0.312604 lr=0.000020 grad_norm=0.661849
Epoch 2/100 Iteration 75/234: loss=0.329327 lr=0.000020 grad_norm=0.645602
Epoch 2/100 Iteration 76/234: loss=0.320280 lr=0.000020 grad_norm=0.627863
Epoch 2/100 Iteration 77/234: loss=0.311754 lr=0.000020 grad_norm=0.690523
Epoch 2/100 Iteration 78/234: loss=0.332655 lr=0.000020 grad_norm=0.683268
Epoch 2/100 Iteration 79/234: loss=0.323322 lr=0.000020 grad_norm=0.568934
Epoch 2/100 Iteration 80/234: loss=0.340459 lr=0.000020 grad_norm=0.628839
Epoch 2/100 Iteration 81/234: loss=0.327802 lr=0.000020 grad_norm=0.628284
Epoch 2/100 Iteration 82/234: loss=0.323577 lr=0.000020 grad_norm=0.571259
Epoch 2/100 Iteration 83/234: loss=0.313181 lr=0.000020 grad_norm=0.588767
Epoch 2/100 Iteration 84/234: loss=0.306259 lr=0.000020 grad_norm=0.635726
Epoch 2/100 Iteration 85/234: loss=0.333326 lr=0.000020 grad_norm=0.515114
Epoch 2/100 Iteration 86/234: loss=0.305778 lr=0.000020 grad_norm=0.585565
Epoch 2/100 Iteration 87/234: loss=0.307848 lr=0.000020 grad_norm=0.567231
Epoch 2/100 Iteration 88/234: loss=0.313034 lr=0.000020 grad_norm=0.634249
Epoch 2/100 Iteration 89/234: loss=0.305836 lr=0.000020 grad_norm=0.722900
Epoch 2/100 Iteration 90/234: loss=0.317101 lr=0.000020 grad_norm=0.732807
Epoch 2/100 Iteration 91/234: loss=0.321267 lr=0.000020 grad_norm=0.773092
Epoch 2/100 Iteration 92/234: loss=0.314485 lr=0.000020 grad_norm=0.755378
Epoch 2/100 Iteration 93/234: loss=0.317158 lr=0.000020 grad_norm=0.645813
Epoch 2/100 Iteration 94/234: loss=0.312692 lr=0.000020 grad_norm=0.717696
Epoch 2/100 Iteration 95/234: loss=0.307567 lr=0.000020 grad_norm=0.663802
Epoch 2/100 Iteration 96/234: loss=0.315320 lr=0.000020 grad_norm=0.643891
Epoch 2/100 Iteration 97/234: loss=0.309328 lr=0.000020 grad_norm=0.698883
Epoch 2/100 Iteration 98/234: loss=0.324833 lr=0.000020 grad_norm=0.680556
Epoch 2/100 Iteration 99/234: loss=0.317434 lr=0.000020 grad_norm=0.644070
Epoch 2/100 Iteration 100/234: loss=0.310064 lr=0.000020 grad_norm=0.768494
Epoch 2/100 Iteration 101/234: loss=0.336707 lr=0.000020 grad_norm=0.699236
Epoch 2/100 Iteration 102/234: loss=0.326326 lr=0.000020 grad_norm=0.602974
Epoch 2/100 Iteration 103/234: loss=0.323152 lr=0.000020 grad_norm=0.657300
Epoch 2/100 Iteration 104/234: loss=0.290602 lr=0.000020 grad_norm=0.781094
Epoch 2/100 Iteration 105/234: loss=0.288698 lr=0.000020 grad_norm=0.991474
Epoch 2/100 Iteration 106/234: loss=0.303942 lr=0.000020 grad_norm=1.024292
Epoch 2/100 Iteration 107/234: loss=0.314453 lr=0.000020 grad_norm=0.735328
Epoch 2/100 Iteration 108/234: loss=0.311787 lr=0.000020 grad_norm=1.010481
Epoch 2/100 Iteration 109/234: loss=0.312032 lr=0.000020 grad_norm=1.084004
Epoch 2/100 Iteration 110/234: loss=0.311643 lr=0.000020 grad_norm=0.948984
Epoch 2/100 Iteration 111/234: loss=0.318116 lr=0.000020 grad_norm=1.062764
Epoch 2/100 Iteration 112/234: loss=0.314328 lr=0.000020 grad_norm=0.960875
Epoch 2/100 Iteration 113/234: loss=0.308132 lr=0.000020 grad_norm=0.688595
Epoch 2/100 Iteration 114/234: loss=0.308311 lr=0.000020 grad_norm=0.753706
Epoch 2/100 Iteration 115/234: loss=0.288981 lr=0.000020 grad_norm=0.869723
Epoch 2/100 Iteration 116/234: loss=0.309832 lr=0.000020 grad_norm=0.915441
Epoch 2/100 Iteration 117/234: loss=0.314995 lr=0.000020 grad_norm=0.573675
Epoch 2/100 Iteration 118/234: loss=0.314814 lr=0.000020 grad_norm=0.659622
Epoch 2/100 Iteration 119/234: loss=0.318526 lr=0.000020 grad_norm=0.757329
Epoch 2/100 Iteration 120/234: loss=0.298821 lr=0.000020 grad_norm=0.758642
Epoch 2/100 Iteration 121/234: loss=0.317811 lr=0.000020 grad_norm=0.669186
Epoch 2/100 Iteration 122/234: loss=0.301774 lr=0.000020 grad_norm=0.671481
Epoch 2/100 Iteration 123/234: loss=0.314905 lr=0.000020 grad_norm=0.630694
Epoch 2/100 Iteration 124/234: loss=0.295765 lr=0.000020 grad_norm=0.684492
Epoch 2/100 Iteration 125/234: loss=0.327338 lr=0.000020 grad_norm=0.627494
Epoch 2/100 Iteration 126/234: loss=0.288869 lr=0.000020 grad_norm=0.571944
Epoch 2/100 Iteration 127/234: loss=0.314756 lr=0.000020 grad_norm=0.689714
Epoch 2/100 Iteration 128/234: loss=0.298565 lr=0.000020 grad_norm=0.639876
Epoch 2/100 Iteration 129/234: loss=0.325055 lr=0.000020 grad_norm=0.654377
Epoch 2/100 Iteration 130/234: loss=0.291046 lr=0.000020 grad_norm=0.568164
Epoch 2/100 Iteration 131/234: loss=0.316800 lr=0.000020 grad_norm=0.653922
Epoch 2/100 Iteration 132/234: loss=0.318852 lr=0.000020 grad_norm=0.599962
Epoch 2/100 Iteration 133/234: loss=0.319156 lr=0.000020 grad_norm=0.597777
Epoch 2/100 Iteration 134/234: loss=0.319827 lr=0.000020 grad_norm=0.594693
Epoch 2/100 Iteration 135/234: loss=0.299821 lr=0.000020 grad_norm=0.603504
Epoch 2/100 Iteration 136/234: loss=0.303893 lr=0.000020 grad_norm=0.609981
Epoch 2/100 Iteration 137/234: loss=0.293890 lr=0.000020 grad_norm=0.575130
Epoch 2/100 Iteration 138/234: loss=0.293349 lr=0.000020 grad_norm=0.653573
Epoch 2/100 Iteration 139/234: loss=0.304505 lr=0.000020 grad_norm=0.624858
Epoch 2/100 Iteration 140/234: loss=0.314521 lr=0.000020 grad_norm=0.543749
Epoch 2/100 Iteration 141/234: loss=0.292292 lr=0.000020 grad_norm=0.529413
Epoch 2/100 Iteration 142/234: loss=0.324677 lr=0.000020 grad_norm=0.547330
Epoch 2/100 Iteration 143/234: loss=0.330803 lr=0.000020 grad_norm=0.492915
Epoch 2/100 Iteration 144/234: loss=0.290558 lr=0.000020 grad_norm=0.596183
Epoch 2/100 Iteration 145/234: loss=0.299520 lr=0.000020 grad_norm=0.712196
Epoch 2/100 Iteration 146/234: loss=0.301511 lr=0.000020 grad_norm=0.606508
Epoch 2/100 Iteration 147/234: loss=0.304783 lr=0.000020 grad_norm=0.560019
Epoch 2/100 Iteration 148/234: loss=0.287085 lr=0.000020 grad_norm=0.645870
Epoch 2/100 Iteration 149/234: loss=0.308571 lr=0.000020 grad_norm=0.604627
Epoch 2/100 Iteration 150/234: loss=0.312736 lr=0.000020 grad_norm=0.570764
Epoch 2/100 Iteration 151/234: loss=0.316293 lr=0.000020 grad_norm=0.517560
Epoch 2/100 Iteration 152/234: loss=0.282762 lr=0.000020 grad_norm=0.597224
Epoch 2/100 Iteration 153/234: loss=0.303902 lr=0.000020 grad_norm=0.641532
Epoch 2/100 Iteration 154/234: loss=0.284876 lr=0.000020 grad_norm=0.753748
Epoch 2/100 Iteration 155/234: loss=0.308014 lr=0.000020 grad_norm=0.807411
Epoch 2/100 Iteration 156/234: loss=0.308642 lr=0.000020 grad_norm=0.775587
Epoch 2/100 Iteration 157/234: loss=0.298932 lr=0.000020 grad_norm=0.717780
Epoch 2/100 Iteration 158/234: loss=0.294058 lr=0.000020 grad_norm=0.653728
Epoch 2/100 Iteration 159/234: loss=0.304456 lr=0.000020 grad_norm=0.612074
Epoch 2/100 Iteration 160/234: loss=0.300662 lr=0.000020 grad_norm=0.734078
Epoch 2/100 Iteration 161/234: loss=0.316026 lr=0.000020 grad_norm=0.699548
Epoch 2/100 Iteration 162/234: loss=0.283026 lr=0.000020 grad_norm=0.667760
Epoch 2/100 Iteration 163/234: loss=0.285834 lr=0.000020 grad_norm=0.656683
Epoch 2/100 Iteration 164/234: loss=0.316984 lr=0.000020 grad_norm=0.741502
Epoch 2/100 Iteration 165/234: loss=0.289732 lr=0.000020 grad_norm=0.640831
Epoch 2/100 Iteration 166/234: loss=0.317340 lr=0.000020 grad_norm=0.496085
Epoch 2/100 Iteration 167/234: loss=0.307088 lr=0.000020 grad_norm=0.787581
Epoch 2/100 Iteration 168/234: loss=0.304809 lr=0.000020 grad_norm=0.723054
Epoch 2/100 Iteration 169/234: loss=0.321811 lr=0.000020 grad_norm=0.525266
Epoch 2/100 Iteration 170/234: loss=0.303874 lr=0.000020 grad_norm=0.696498
Epoch 2/100 Iteration 171/234: loss=0.301357 lr=0.000020 grad_norm=0.616686
Epoch 2/100 Iteration 172/234: loss=0.299260 lr=0.000020 grad_norm=0.561097
Epoch 2/100 Iteration 173/234: loss=0.264497 lr=0.000020 grad_norm=0.643388
Epoch 2/100 Iteration 174/234: loss=0.294744 lr=0.000020 grad_norm=0.506612
Epoch 2/100 Iteration 175/234: loss=0.298371 lr=0.000020 grad_norm=0.557192
Epoch 2/100 Iteration 176/234: loss=0.289647 lr=0.000020 grad_norm=0.594977
Epoch 2/100 Iteration 177/234: loss=0.284211 lr=0.000020 grad_norm=0.557425
Epoch 2/100 Iteration 178/234: loss=0.303085 lr=0.000020 grad_norm=0.618150
Epoch 2/100 Iteration 179/234: loss=0.298310 lr=0.000020 grad_norm=0.529975
Epoch 2/100 Iteration 180/234: loss=0.292820 lr=0.000020 grad_norm=0.547379
Epoch 2/100 Iteration 181/234: loss=0.281573 lr=0.000020 grad_norm=0.662698
Epoch 2/100 Iteration 182/234: loss=0.311322 lr=0.000020 grad_norm=0.613507
Epoch 2/100 Iteration 183/234: loss=0.287810 lr=0.000020 grad_norm=0.676558
Epoch 2/100 Iteration 184/234: loss=0.285416 lr=0.000020 grad_norm=0.643043
Epoch 2/100 Iteration 185/234: loss=0.310783 lr=0.000020 grad_norm=0.495919
Epoch 2/100 Iteration 186/234: loss=0.323004 lr=0.000020 grad_norm=0.568469
Epoch 2/100 Iteration 187/234: loss=0.297428 lr=0.000020 grad_norm=0.474433
Epoch 2/100 Iteration 188/234: loss=0.303639 lr=0.000020 grad_norm=0.669984
Epoch 2/100 Iteration 189/234: loss=0.305523 lr=0.000020 grad_norm=0.619872
Epoch 2/100 Iteration 190/234: loss=0.280880 lr=0.000020 grad_norm=0.474059
Epoch 2/100 Iteration 191/234: loss=0.293359 lr=0.000020 grad_norm=0.644745
Epoch 2/100 Iteration 192/234: loss=0.327424 lr=0.000020 grad_norm=0.742338
Epoch 2/100 Iteration 193/234: loss=0.295368 lr=0.000020 grad_norm=0.531467
Epoch 2/100 Iteration 194/234: loss=0.313035 lr=0.000020 grad_norm=0.593254
Epoch 2/100 Iteration 195/234: loss=0.298859 lr=0.000020 grad_norm=0.522360
Epoch 2/100 Iteration 196/234: loss=0.286275 lr=0.000020 grad_norm=0.554977
Epoch 2/100 Iteration 197/234: loss=0.309371 lr=0.000020 grad_norm=0.564627
Epoch 2/100 Iteration 198/234: loss=0.295272 lr=0.000020 grad_norm=0.584351
Epoch 2/100 Iteration 199/234: loss=0.307705 lr=0.000020 grad_norm=0.579896
Epoch 2/100 Iteration 200/234: loss=0.305714 lr=0.000020 grad_norm=0.559101
Epoch 2/100 Iteration 201/234: loss=0.274427 lr=0.000020 grad_norm=0.503476
Epoch 2/100 Iteration 202/234: loss=0.298651 lr=0.000020 grad_norm=0.491647
Epoch 2/100 Iteration 203/234: loss=0.278821 lr=0.000020 grad_norm=0.533659
Epoch 2/100 Iteration 204/234: loss=0.267910 lr=0.000020 grad_norm=0.521924
Epoch 2/100 Iteration 205/234: loss=0.278876 lr=0.000020 grad_norm=0.507974
Epoch 2/100 Iteration 206/234: loss=0.298344 lr=0.000020 grad_norm=0.557848
Epoch 2/100 Iteration 207/234: loss=0.278894 lr=0.000020 grad_norm=0.568298
Epoch 2/100 Iteration 208/234: loss=0.301095 lr=0.000020 grad_norm=0.555761
Epoch 2/100 Iteration 209/234: loss=0.274102 lr=0.000020 grad_norm=0.468588
Epoch 2/100 Iteration 210/234: loss=0.302815 lr=0.000020 grad_norm=0.494697
Epoch 2/100 Iteration 211/234: loss=0.296041 lr=0.000020 grad_norm=0.459855
Epoch 2/100 Iteration 212/234: loss=0.286116 lr=0.000020 grad_norm=0.604875
Epoch 2/100 Iteration 213/234: loss=0.289014 lr=0.000020 grad_norm=0.564344
Epoch 2/100 Iteration 214/234: loss=0.275159 lr=0.000020 grad_norm=0.606161
Epoch 2/100 Iteration 215/234: loss=0.302753 lr=0.000020 grad_norm=0.589697
Epoch 2/100 Iteration 216/234: loss=0.302201 lr=0.000020 grad_norm=0.592512
Epoch 2/100 Iteration 217/234: loss=0.295332 lr=0.000020 grad_norm=0.542782
Epoch 2/100 Iteration 218/234: loss=0.284658 lr=0.000020 grad_norm=0.553209
Epoch 2/100 Iteration 219/234: loss=0.269851 lr=0.000020 grad_norm=0.583572
Epoch 2/100 Iteration 220/234: loss=0.289427 lr=0.000020 grad_norm=0.661438
Epoch 2/100 Iteration 221/234: loss=0.275064 lr=0.000020 grad_norm=0.553027
Epoch 2/100 Iteration 222/234: loss=0.313357 lr=0.000020 grad_norm=0.617276
Epoch 2/100 Iteration 223/234: loss=0.305567 lr=0.000020 grad_norm=0.561385
Epoch 2/100 Iteration 224/234: loss=0.290192 lr=0.000020 grad_norm=0.531337
Epoch 2/100 Iteration 225/234: loss=0.276101 lr=0.000020 grad_norm=0.747997
Epoch 2/100 Iteration 226/234: loss=0.281058 lr=0.000020 grad_norm=0.840987
Epoch 2/100 Iteration 227/234: loss=0.279520 lr=0.000020 grad_norm=0.895087
Epoch 2/100 Iteration 228/234: loss=0.282243 lr=0.000020 grad_norm=0.934770
Epoch 2/100 Iteration 229/234: loss=0.276320 lr=0.000020 grad_norm=0.822603
Epoch 2/100 Iteration 230/234: loss=0.293866 lr=0.000020 grad_norm=0.724778
Epoch 2/100 Iteration 231/234: loss=0.303867 lr=0.000020 grad_norm=0.593887
Epoch 2/100 Iteration 232/234: loss=0.289783 lr=0.000020 grad_norm=0.544488
Epoch 2/100 Iteration 233/234: loss=0.288377 lr=0.000020 grad_norm=0.641725
Epoch 2/100 Iteration 234/234: loss=0.292459 lr=0.000020 grad_norm=0.613015
Epoch 2/100 finished. Avg Loss: 0.310887
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 3/100 Iteration 1/234: loss=0.271530 lr=0.000020 grad_norm=0.479035
Epoch 3/100 Iteration 2/234: loss=0.299666 lr=0.000020 grad_norm=0.627878
Epoch 3/100 Iteration 3/234: loss=0.271996 lr=0.000020 grad_norm=0.568793
Epoch 3/100 Iteration 4/234: loss=0.278537 lr=0.000020 grad_norm=0.536598
Epoch 3/100 Iteration 5/234: loss=0.266624 lr=0.000020 grad_norm=0.559866
Epoch 3/100 Iteration 6/234: loss=0.302405 lr=0.000020 grad_norm=0.497950
Epoch 3/100 Iteration 7/234: loss=0.286925 lr=0.000020 grad_norm=0.542269
Epoch 3/100 Iteration 8/234: loss=0.307847 lr=0.000020 grad_norm=0.604804
Epoch 3/100 Iteration 9/234: loss=0.290003 lr=0.000020 grad_norm=0.554023
Epoch 3/100 Iteration 10/234: loss=0.272704 lr=0.000020 grad_norm=0.625741
Epoch 3/100 Iteration 11/234: loss=0.286372 lr=0.000020 grad_norm=0.705338
Epoch 3/100 Iteration 12/234: loss=0.274050 lr=0.000020 grad_norm=0.649267
Epoch 3/100 Iteration 13/234: loss=0.307698 lr=0.000020 grad_norm=0.456867
Epoch 3/100 Iteration 14/234: loss=0.291865 lr=0.000020 grad_norm=0.591599
Epoch 3/100 Iteration 15/234: loss=0.255641 lr=0.000020 grad_norm=0.710507
Epoch 3/100 Iteration 16/234: loss=0.271109 lr=0.000020 grad_norm=0.670816
Epoch 3/100 Iteration 17/234: loss=0.276367 lr=0.000020 grad_norm=0.580697
Epoch 3/100 Iteration 18/234: loss=0.280851 lr=0.000020 grad_norm=0.481576
Epoch 3/100 Iteration 19/234: loss=0.276726 lr=0.000020 grad_norm=0.527061
Epoch 3/100 Iteration 20/234: loss=0.312246 lr=0.000020 grad_norm=0.627708
Epoch 3/100 Iteration 21/234: loss=0.273676 lr=0.000020 grad_norm=0.569210
Epoch 3/100 Iteration 22/234: loss=0.274478 lr=0.000020 grad_norm=0.571507
Epoch 3/100 Iteration 23/234: loss=0.282851 lr=0.000020 grad_norm=0.551210
Epoch 3/100 Iteration 24/234: loss=0.294823 lr=0.000020 grad_norm=0.507003
Epoch 3/100 Iteration 25/234: loss=0.297382 lr=0.000020 grad_norm=0.595414
Epoch 3/100 Iteration 26/234: loss=0.281087 lr=0.000020 grad_norm=0.516311
Epoch 3/100 Iteration 27/234: loss=0.286158 lr=0.000020 grad_norm=0.514889
Epoch 3/100 Iteration 28/234: loss=0.288302 lr=0.000020 grad_norm=0.554558
Epoch 3/100 Iteration 29/234: loss=0.284107 lr=0.000020 grad_norm=0.468168
Epoch 3/100 Iteration 30/234: loss=0.284717 lr=0.000020 grad_norm=0.631898
Epoch 3/100 Iteration 31/234: loss=0.291884 lr=0.000020 grad_norm=0.690734
Epoch 3/100 Iteration 32/234: loss=0.270967 lr=0.000020 grad_norm=0.494933
Epoch 3/100 Iteration 33/234: loss=0.306479 lr=0.000020 grad_norm=0.694936
Epoch 3/100 Iteration 34/234: loss=0.283058 lr=0.000020 grad_norm=0.709777
Epoch 3/100 Iteration 35/234: loss=0.276913 lr=0.000020 grad_norm=0.475655
Epoch 3/100 Iteration 36/234: loss=0.291652 lr=0.000020 grad_norm=0.616014
Epoch 3/100 Iteration 37/234: loss=0.294352 lr=0.000020 grad_norm=0.552430
Epoch 3/100 Iteration 38/234: loss=0.282690 lr=0.000020 grad_norm=0.490201
Epoch 3/100 Iteration 39/234: loss=0.292675 lr=0.000020 grad_norm=0.640592
Epoch 3/100 Iteration 40/234: loss=0.313469 lr=0.000020 grad_norm=0.604773
Epoch 3/100 Iteration 41/234: loss=0.293025 lr=0.000020 grad_norm=0.459110
Epoch 3/100 Iteration 42/234: loss=0.264704 lr=0.000020 grad_norm=0.721621
Epoch 3/100 Iteration 43/234: loss=0.269267 lr=0.000020 grad_norm=0.761719
Epoch 3/100 Iteration 44/234: loss=0.283065 lr=0.000020 grad_norm=0.552187
Epoch 3/100 Iteration 45/234: loss=0.285319 lr=0.000020 grad_norm=0.568526
Epoch 3/100 Iteration 46/234: loss=0.282031 lr=0.000020 grad_norm=0.574117
Epoch 3/100 Iteration 47/234: loss=0.284568 lr=0.000020 grad_norm=0.553500
Epoch 3/100 Iteration 48/234: loss=0.283401 lr=0.000020 grad_norm=0.616487
Epoch 3/100 Iteration 49/234: loss=0.268328 lr=0.000020 grad_norm=0.671761
Epoch 3/100 Iteration 50/234: loss=0.297233 lr=0.000020 grad_norm=0.609001
Epoch 3/100 Iteration 51/234: loss=0.286987 lr=0.000020 grad_norm=0.517425
Epoch 3/100 Iteration 52/234: loss=0.289056 lr=0.000020 grad_norm=0.489311
Epoch 3/100 Iteration 53/234: loss=0.292015 lr=0.000020 grad_norm=0.584533
Epoch 3/100 Iteration 54/234: loss=0.276429 lr=0.000020 grad_norm=0.533293
Epoch 3/100 Iteration 55/234: loss=0.284232 lr=0.000020 grad_norm=0.742350
Epoch 3/100 Iteration 56/234: loss=0.297490 lr=0.000020 grad_norm=0.734036
Epoch 3/100 Iteration 57/234: loss=0.269399 lr=0.000020 grad_norm=0.521988
Epoch 3/100 Iteration 58/234: loss=0.279161 lr=0.000020 grad_norm=0.586436
Epoch 3/100 Iteration 59/234: loss=0.271626 lr=0.000020 grad_norm=0.547912
Epoch 3/100 Iteration 60/234: loss=0.280200 lr=0.000020 grad_norm=0.691526
Epoch 3/100 Iteration 61/234: loss=0.293498 lr=0.000020 grad_norm=0.815701
Epoch 3/100 Iteration 62/234: loss=0.307001 lr=0.000020 grad_norm=0.814548
Epoch 3/100 Iteration 63/234: loss=0.277892 lr=0.000020 grad_norm=0.616553
Epoch 3/100 Iteration 64/234: loss=0.306194 lr=0.000020 grad_norm=0.684098
Epoch 3/100 Iteration 65/234: loss=0.279123 lr=0.000020 grad_norm=0.648947
Epoch 3/100 Iteration 66/234: loss=0.285370 lr=0.000020 grad_norm=0.564169
Epoch 3/100 Iteration 67/234: loss=0.284970 lr=0.000020 grad_norm=0.701520
Epoch 3/100 Iteration 68/234: loss=0.300407 lr=0.000020 grad_norm=0.664109
Epoch 3/100 Iteration 69/234: loss=0.253699 lr=0.000020 grad_norm=0.602583
Epoch 3/100 Iteration 70/234: loss=0.274186 lr=0.000020 grad_norm=0.794355
Epoch 3/100 Iteration 71/234: loss=0.288808 lr=0.000020 grad_norm=0.956662
Epoch 3/100 Iteration 72/234: loss=0.281450 lr=0.000020 grad_norm=0.825676
Epoch 3/100 Iteration 73/234: loss=0.285832 lr=0.000020 grad_norm=0.802180
Epoch 3/100 Iteration 74/234: loss=0.287179 lr=0.000020 grad_norm=0.751986
Epoch 3/100 Iteration 75/234: loss=0.268219 lr=0.000020 grad_norm=0.600418
Epoch 3/100 Iteration 76/234: loss=0.281348 lr=0.000020 grad_norm=0.744989
Epoch 3/100 Iteration 77/234: loss=0.286886 lr=0.000020 grad_norm=0.698695
Epoch 3/100 Iteration 78/234: loss=0.274098 lr=0.000020 grad_norm=0.568073
Epoch 3/100 Iteration 79/234: loss=0.265828 lr=0.000020 grad_norm=0.532184
Epoch 3/100 Iteration 80/234: loss=0.289740 lr=0.000020 grad_norm=0.541439
Epoch 3/100 Iteration 81/234: loss=0.275327 lr=0.000020 grad_norm=0.505767
Epoch 3/100 Iteration 82/234: loss=0.285164 lr=0.000020 grad_norm=0.635909
Epoch 3/100 Iteration 83/234: loss=0.297101 lr=0.000020 grad_norm=0.601838
Epoch 3/100 Iteration 84/234: loss=0.253476 lr=0.000020 grad_norm=0.532667
Epoch 3/100 Iteration 85/234: loss=0.292171 lr=0.000020 grad_norm=0.725895
Epoch 3/100 Iteration 86/234: loss=0.268503 lr=0.000020 grad_norm=0.698406
Epoch 3/100 Iteration 87/234: loss=0.268748 lr=0.000020 grad_norm=0.593083
Epoch 3/100 Iteration 88/234: loss=0.270882 lr=0.000020 grad_norm=0.634874
Epoch 3/100 Iteration 89/234: loss=0.278213 lr=0.000020 grad_norm=0.586844
Epoch 3/100 Iteration 90/234: loss=0.274495 lr=0.000020 grad_norm=0.575094
Epoch 3/100 Iteration 91/234: loss=0.264522 lr=0.000020 grad_norm=0.595617
Epoch 3/100 Iteration 92/234: loss=0.261674 lr=0.000020 grad_norm=0.642959
Epoch 3/100 Iteration 93/234: loss=0.279062 lr=0.000020 grad_norm=0.493383
Epoch 3/100 Iteration 94/234: loss=0.269961 lr=0.000020 grad_norm=0.645934
Epoch 3/100 Iteration 95/234: loss=0.269329 lr=0.000020 grad_norm=0.943274
Epoch 3/100 Iteration 96/234: loss=0.285303 lr=0.000020 grad_norm=0.783966
Epoch 3/100 Iteration 97/234: loss=0.271111 lr=0.000020 grad_norm=0.459808
Epoch 3/100 Iteration 98/234: loss=0.264005 lr=0.000020 grad_norm=0.998062
Epoch 3/100 Iteration 99/234: loss=0.298337 lr=0.000020 grad_norm=0.786169
Epoch 3/100 Iteration 100/234: loss=0.276098 lr=0.000020 grad_norm=0.608500
Epoch 3/100 Iteration 101/234: loss=0.279302 lr=0.000020 grad_norm=0.885976
Epoch 3/100 Iteration 102/234: loss=0.281210 lr=0.000020 grad_norm=0.530901
Epoch 3/100 Iteration 103/234: loss=0.245938 lr=0.000020 grad_norm=0.857897
Epoch 3/100 Iteration 104/234: loss=0.261303 lr=0.000020 grad_norm=0.960028
Epoch 3/100 Iteration 105/234: loss=0.309953 lr=0.000020 grad_norm=0.765605
Epoch 3/100 Iteration 106/234: loss=0.288292 lr=0.000020 grad_norm=0.959977
Epoch 3/100 Iteration 107/234: loss=0.275344 lr=0.000020 grad_norm=0.902438
Epoch 3/100 Iteration 108/234: loss=0.279176 lr=0.000020 grad_norm=0.591999
Epoch 3/100 Iteration 109/234: loss=0.295901 lr=0.000020 grad_norm=0.661813
Epoch 3/100 Iteration 110/234: loss=0.278326 lr=0.000020 grad_norm=0.616084
Epoch 3/100 Iteration 111/234: loss=0.291532 lr=0.000020 grad_norm=0.781823
Epoch 3/100 Iteration 112/234: loss=0.273763 lr=0.000020 grad_norm=0.574728
Epoch 3/100 Iteration 113/234: loss=0.263057 lr=0.000020 grad_norm=0.696055
Epoch 3/100 Iteration 114/234: loss=0.281197 lr=0.000020 grad_norm=0.720542
Epoch 3/100 Iteration 115/234: loss=0.263076 lr=0.000020 grad_norm=0.671019
Epoch 3/100 Iteration 116/234: loss=0.273598 lr=0.000020 grad_norm=0.655895
Epoch 3/100 Iteration 117/234: loss=0.285574 lr=0.000020 grad_norm=0.549658
Epoch 3/100 Iteration 118/234: loss=0.270673 lr=0.000020 grad_norm=0.431882
Epoch 3/100 Iteration 119/234: loss=0.286449 lr=0.000020 grad_norm=0.503837
Epoch 3/100 Iteration 120/234: loss=0.286823 lr=0.000020 grad_norm=0.537720
Epoch 3/100 Iteration 121/234: loss=0.281233 lr=0.000020 grad_norm=0.668045
Epoch 3/100 Iteration 122/234: loss=0.273793 lr=0.000020 grad_norm=0.528213
Epoch 3/100 Iteration 123/234: loss=0.281466 lr=0.000020 grad_norm=0.507128
Epoch 3/100 Iteration 124/234: loss=0.255996 lr=0.000020 grad_norm=0.527650
Epoch 3/100 Iteration 125/234: loss=0.258475 lr=0.000020 grad_norm=0.472371
Epoch 3/100 Iteration 126/234: loss=0.275901 lr=0.000020 grad_norm=0.704594
Epoch 3/100 Iteration 127/234: loss=0.268947 lr=0.000020 grad_norm=0.686015
Epoch 3/100 Iteration 128/234: loss=0.265662 lr=0.000020 grad_norm=0.539287
Epoch 3/100 Iteration 129/234: loss=0.292529 lr=0.000020 grad_norm=0.750881
Epoch 3/100 Iteration 130/234: loss=0.289173 lr=0.000020 grad_norm=0.672053
Epoch 3/100 Iteration 131/234: loss=0.260084 lr=0.000020 grad_norm=0.431467
Epoch 3/100 Iteration 132/234: loss=0.264162 lr=0.000020 grad_norm=0.731236
Epoch 3/100 Iteration 133/234: loss=0.297122 lr=0.000020 grad_norm=0.775774
Epoch 3/100 Iteration 134/234: loss=0.269408 lr=0.000020 grad_norm=0.537176
Epoch 3/100 Iteration 135/234: loss=0.279481 lr=0.000020 grad_norm=0.843974
Epoch 3/100 Iteration 136/234: loss=0.264150 lr=0.000020 grad_norm=0.968874
Epoch 3/100 Iteration 137/234: loss=0.268772 lr=0.000020 grad_norm=0.575445
Epoch 3/100 Iteration 138/234: loss=0.255197 lr=0.000020 grad_norm=0.828237
Epoch 3/100 Iteration 139/234: loss=0.260076 lr=0.000020 grad_norm=0.820414
Epoch 3/100 Iteration 140/234: loss=0.270936 lr=0.000020 grad_norm=0.703871
Epoch 3/100 Iteration 141/234: loss=0.258790 lr=0.000020 grad_norm=0.920131
Epoch 3/100 Iteration 142/234: loss=0.289826 lr=0.000020 grad_norm=0.707626
Epoch 3/100 Iteration 143/234: loss=0.288309 lr=0.000020 grad_norm=0.658411
Epoch 3/100 Iteration 144/234: loss=0.256245 lr=0.000020 grad_norm=0.817854
Epoch 3/100 Iteration 145/234: loss=0.266937 lr=0.000020 grad_norm=0.969106
Epoch 3/100 Iteration 146/234: loss=0.278261 lr=0.000020 grad_norm=0.747550
Epoch 3/100 Iteration 147/234: loss=0.240337 lr=0.000020 grad_norm=0.793510
Epoch 3/100 Iteration 148/234: loss=0.269265 lr=0.000020 grad_norm=0.710967
Epoch 3/100 Iteration 149/234: loss=0.278658 lr=0.000020 grad_norm=0.770751
Epoch 3/100 Iteration 150/234: loss=0.274122 lr=0.000020 grad_norm=0.699565
Epoch 3/100 Iteration 151/234: loss=0.261590 lr=0.000020 grad_norm=0.538816
Epoch 3/100 Iteration 152/234: loss=0.264457 lr=0.000020 grad_norm=0.639323
Epoch 3/100 Iteration 153/234: loss=0.269183 lr=0.000020 grad_norm=0.627473
Epoch 3/100 Iteration 154/234: loss=0.270553 lr=0.000020 grad_norm=0.493231
Epoch 3/100 Iteration 155/234: loss=0.256343 lr=0.000020 grad_norm=0.500493
Epoch 3/100 Iteration 156/234: loss=0.278257 lr=0.000020 grad_norm=0.488860
Epoch 3/100 Iteration 157/234: loss=0.275111 lr=0.000020 grad_norm=0.530932
Epoch 3/100 Iteration 158/234: loss=0.250619 lr=0.000020 grad_norm=0.428648
Epoch 3/100 Iteration 159/234: loss=0.263815 lr=0.000020 grad_norm=0.479810
Epoch 3/100 Iteration 160/234: loss=0.253181 lr=0.000020 grad_norm=0.486491
Epoch 3/100 Iteration 161/234: loss=0.280662 lr=0.000020 grad_norm=0.459104
Epoch 3/100 Iteration 162/234: loss=0.261254 lr=0.000020 grad_norm=0.419501
Epoch 3/100 Iteration 163/234: loss=0.260536 lr=0.000020 grad_norm=0.522523
Epoch 3/100 Iteration 164/234: loss=0.279236 lr=0.000020 grad_norm=0.445667
Epoch 3/100 Iteration 165/234: loss=0.291380 lr=0.000020 grad_norm=0.401325
Epoch 3/100 Iteration 166/234: loss=0.281430 lr=0.000020 grad_norm=0.433525
Epoch 3/100 Iteration 167/234: loss=0.264082 lr=0.000020 grad_norm=0.496708
Epoch 3/100 Iteration 168/234: loss=0.277786 lr=0.000020 grad_norm=0.367802
Epoch 3/100 Iteration 169/234: loss=0.254683 lr=0.000020 grad_norm=0.370746
Epoch 3/100 Iteration 170/234: loss=0.268061 lr=0.000020 grad_norm=0.376519
Epoch 3/100 Iteration 171/234: loss=0.265195 lr=0.000020 grad_norm=0.373897
Epoch 3/100 Iteration 172/234: loss=0.267991 lr=0.000020 grad_norm=0.478436
Epoch 3/100 Iteration 173/234: loss=0.258419 lr=0.000020 grad_norm=0.521850
Epoch 3/100 Iteration 174/234: loss=0.284757 lr=0.000020 grad_norm=0.387786
Epoch 3/100 Iteration 175/234: loss=0.270964 lr=0.000020 grad_norm=0.620715
Epoch 3/100 Iteration 176/234: loss=0.256729 lr=0.000020 grad_norm=0.850767
Epoch 3/100 Iteration 177/234: loss=0.270826 lr=0.000020 grad_norm=0.635206
Epoch 3/100 Iteration 178/234: loss=0.269061 lr=0.000020 grad_norm=0.611446
Epoch 3/100 Iteration 179/234: loss=0.290048 lr=0.000020 grad_norm=0.479545
Epoch 3/100 Iteration 180/234: loss=0.274523 lr=0.000020 grad_norm=0.417949
Epoch 3/100 Iteration 181/234: loss=0.277333 lr=0.000020 grad_norm=0.619208
Epoch 3/100 Iteration 182/234: loss=0.254007 lr=0.000020 grad_norm=0.469311
Epoch 3/100 Iteration 183/234: loss=0.270910 lr=0.000020 grad_norm=0.430316
Epoch 3/100 Iteration 184/234: loss=0.283061 lr=0.000020 grad_norm=0.691874
Epoch 3/100 Iteration 185/234: loss=0.262194 lr=0.000020 grad_norm=0.655254
Epoch 3/100 Iteration 186/234: loss=0.261330 lr=0.000020 grad_norm=0.437024
Epoch 3/100 Iteration 187/234: loss=0.269100 lr=0.000020 grad_norm=0.733368
Epoch 3/100 Iteration 188/234: loss=0.273221 lr=0.000020 grad_norm=0.571235
Epoch 3/100 Iteration 189/234: loss=0.265245 lr=0.000020 grad_norm=0.492218
Epoch 3/100 Iteration 190/234: loss=0.279537 lr=0.000020 grad_norm=0.824696
Epoch 3/100 Iteration 191/234: loss=0.266699 lr=0.000020 grad_norm=0.575723
Epoch 3/100 Iteration 192/234: loss=0.272729 lr=0.000020 grad_norm=0.443688
Epoch 3/100 Iteration 193/234: loss=0.265317 lr=0.000020 grad_norm=0.687068
Epoch 3/100 Iteration 194/234: loss=0.247414 lr=0.000020 grad_norm=0.630111
Epoch 3/100 Iteration 195/234: loss=0.263038 lr=0.000020 grad_norm=0.481333
Epoch 3/100 Iteration 196/234: loss=0.264130 lr=0.000020 grad_norm=0.521417
Epoch 3/100 Iteration 197/234: loss=0.271214 lr=0.000020 grad_norm=0.503408
Epoch 3/100 Iteration 198/234: loss=0.270584 lr=0.000020 grad_norm=0.592467
Epoch 3/100 Iteration 199/234: loss=0.268132 lr=0.000020 grad_norm=0.521975
Epoch 3/100 Iteration 200/234: loss=0.272833 lr=0.000020 grad_norm=0.494218
Epoch 3/100 Iteration 201/234: loss=0.270213 lr=0.000020 grad_norm=0.504552
Epoch 3/100 Iteration 202/234: loss=0.248883 lr=0.000020 grad_norm=0.512463
Epoch 3/100 Iteration 203/234: loss=0.242869 lr=0.000020 grad_norm=0.593026
Epoch 3/100 Iteration 204/234: loss=0.259163 lr=0.000020 grad_norm=0.584342
Epoch 3/100 Iteration 205/234: loss=0.271619 lr=0.000020 grad_norm=0.527704
Epoch 3/100 Iteration 206/234: loss=0.278042 lr=0.000020 grad_norm=0.467022
Epoch 3/100 Iteration 207/234: loss=0.261152 lr=0.000020 grad_norm=0.540160
Epoch 3/100 Iteration 208/234: loss=0.242353 lr=0.000020 grad_norm=0.570559
Epoch 3/100 Iteration 209/234: loss=0.267234 lr=0.000020 grad_norm=0.557547
Epoch 3/100 Iteration 210/234: loss=0.265432 lr=0.000020 grad_norm=0.507566
Epoch 3/100 Iteration 211/234: loss=0.266252 lr=0.000020 grad_norm=0.519130
Epoch 3/100 Iteration 212/234: loss=0.270505 lr=0.000020 grad_norm=0.618259
Epoch 3/100 Iteration 213/234: loss=0.257606 lr=0.000020 grad_norm=0.551969
Epoch 3/100 Iteration 214/234: loss=0.257892 lr=0.000020 grad_norm=0.530835
Epoch 3/100 Iteration 215/234: loss=0.276310 lr=0.000020 grad_norm=0.462290
Epoch 3/100 Iteration 216/234: loss=0.245837 lr=0.000020 grad_norm=0.412810
Epoch 3/100 Iteration 217/234: loss=0.248099 lr=0.000020 grad_norm=0.545496
Epoch 3/100 Iteration 218/234: loss=0.265134 lr=0.000020 grad_norm=0.756913
Epoch 3/100 Iteration 219/234: loss=0.263103 lr=0.000020 grad_norm=0.756014
Epoch 3/100 Iteration 220/234: loss=0.249413 lr=0.000020 grad_norm=0.599523
Epoch 3/100 Iteration 221/234: loss=0.245569 lr=0.000020 grad_norm=0.494937
Epoch 3/100 Iteration 222/234: loss=0.280790 lr=0.000020 grad_norm=0.519495
Epoch 3/100 Iteration 223/234: loss=0.272837 lr=0.000020 grad_norm=0.503708
Epoch 3/100 Iteration 224/234: loss=0.260884 lr=0.000020 grad_norm=0.416735
Epoch 3/100 Iteration 225/234: loss=0.260615 lr=0.000020 grad_norm=0.635633
Epoch 3/100 Iteration 226/234: loss=0.261791 lr=0.000020 grad_norm=0.809953
Epoch 3/100 Iteration 227/234: loss=0.245850 lr=0.000020 grad_norm=0.657187
Epoch 3/100 Iteration 228/234: loss=0.266108 lr=0.000020 grad_norm=0.414265
Epoch 3/100 Iteration 229/234: loss=0.265131 lr=0.000020 grad_norm=0.627463
Epoch 3/100 Iteration 230/234: loss=0.277315 lr=0.000020 grad_norm=0.695015
Epoch 3/100 Iteration 231/234: loss=0.258492 lr=0.000020 grad_norm=0.623025
Epoch 3/100 Iteration 232/234: loss=0.275832 lr=0.000020 grad_norm=0.536027
Epoch 3/100 Iteration 233/234: loss=0.243529 lr=0.000020 grad_norm=0.601155
Epoch 3/100 Iteration 234/234: loss=0.275505 lr=0.000020 grad_norm=0.808736
Epoch 3/100 finished. Avg Loss: 0.274682
Epoch 4/100 Iteration 1/234: loss=0.262940 lr=0.000020 grad_norm=0.627366
Epoch 4/100 Iteration 2/234: loss=0.247471 lr=0.000020 grad_norm=0.714991
Epoch 4/100 Iteration 3/234: loss=0.247740 lr=0.000020 grad_norm=0.926271
Epoch 4/100 Iteration 4/234: loss=0.264844 lr=0.000020 grad_norm=0.643861
Epoch 4/100 Iteration 5/234: loss=0.288750 lr=0.000020 grad_norm=0.846733
Epoch 4/100 Iteration 6/234: loss=0.256071 lr=0.000020 grad_norm=0.728540
Epoch 4/100 Iteration 7/234: loss=0.282530 lr=0.000020 grad_norm=0.479406
Epoch 4/100 Iteration 8/234: loss=0.252389 lr=0.000020 grad_norm=0.733108
Epoch 4/100 Iteration 9/234: loss=0.235844 lr=0.000020 grad_norm=0.820731
Epoch 4/100 Iteration 10/234: loss=0.274959 lr=0.000020 grad_norm=0.541061
Epoch 4/100 Iteration 11/234: loss=0.259974 lr=0.000020 grad_norm=0.710099
Epoch 4/100 Iteration 12/234: loss=0.269560 lr=0.000020 grad_norm=0.591882
Epoch 4/100 Iteration 13/234: loss=0.264439 lr=0.000020 grad_norm=0.544800
Epoch 4/100 Iteration 14/234: loss=0.268517 lr=0.000020 grad_norm=0.678215
Epoch 4/100 Iteration 15/234: loss=0.252043 lr=0.000020 grad_norm=0.461004
Epoch 4/100 Iteration 16/234: loss=0.269497 lr=0.000020 grad_norm=0.668692
Epoch 4/100 Iteration 17/234: loss=0.266815 lr=0.000020 grad_norm=0.467758
Epoch 4/100 Iteration 18/234: loss=0.237291 lr=0.000020 grad_norm=0.660549
Epoch 4/100 Iteration 19/234: loss=0.259124 lr=0.000020 grad_norm=0.597267
Epoch 4/100 Iteration 20/234: loss=0.241617 lr=0.000020 grad_norm=0.432039
Epoch 4/100 Iteration 21/234: loss=0.258752 lr=0.000020 grad_norm=0.729903
Epoch 4/100 Iteration 22/234: loss=0.260582 lr=0.000020 grad_norm=0.568051
Epoch 4/100 Iteration 23/234: loss=0.241278 lr=0.000020 grad_norm=0.594418
Epoch 4/100 Iteration 24/234: loss=0.278169 lr=0.000020 grad_norm=0.633431
Epoch 4/100 Iteration 25/234: loss=0.271935 lr=0.000020 grad_norm=0.489836
Epoch 4/100 Iteration 26/234: loss=0.271012 lr=0.000020 grad_norm=0.535660
Epoch 4/100 Iteration 27/234: loss=0.271232 lr=0.000020 grad_norm=0.586016
Epoch 4/100 Iteration 28/234: loss=0.237773 lr=0.000020 grad_norm=0.605052
Epoch 4/100 Iteration 29/234: loss=0.261076 lr=0.000020 grad_norm=0.420116
Epoch 4/100 Iteration 30/234: loss=0.261039 lr=0.000020 grad_norm=0.640221
Epoch 4/100 Iteration 31/234: loss=0.228192 lr=0.000020 grad_norm=0.456201
Epoch 4/100 Iteration 32/234: loss=0.259419 lr=0.000020 grad_norm=0.613000
Epoch 4/100 Iteration 33/234: loss=0.274114 lr=0.000020 grad_norm=0.802704
Epoch 4/100 Iteration 34/234: loss=0.253073 lr=0.000020 grad_norm=0.698602
Epoch 4/100 Iteration 35/234: loss=0.226801 lr=0.000020 grad_norm=0.433051
Epoch 4/100 Iteration 36/234: loss=0.268433 lr=0.000020 grad_norm=0.568555
Epoch 4/100 Iteration 37/234: loss=0.258497 lr=0.000020 grad_norm=0.423611
Epoch 4/100 Iteration 38/234: loss=0.260409 lr=0.000020 grad_norm=0.567401
Epoch 4/100 Iteration 39/234: loss=0.237716 lr=0.000020 grad_norm=0.672523
Epoch 4/100 Iteration 40/234: loss=0.254444 lr=0.000020 grad_norm=0.524960
Epoch 4/100 Iteration 41/234: loss=0.264655 lr=0.000020 grad_norm=0.489703
Epoch 4/100 Iteration 42/234: loss=0.260383 lr=0.000020 grad_norm=0.523685
Epoch 4/100 Iteration 43/234: loss=0.244271 lr=0.000020 grad_norm=0.485077
Epoch 4/100 Iteration 44/234: loss=0.273031 lr=0.000020 grad_norm=0.483444
Epoch 4/100 Iteration 45/234: loss=0.261468 lr=0.000020 grad_norm=0.431532
Epoch 4/100 Iteration 46/234: loss=0.267710 lr=0.000020 grad_norm=0.388762
Epoch 4/100 Iteration 47/234: loss=0.266756 lr=0.000020 grad_norm=0.430499
Epoch 4/100 Iteration 48/234: loss=0.264108 lr=0.000020 grad_norm=0.507240
Epoch 4/100 Iteration 49/234: loss=0.262326 lr=0.000020 grad_norm=0.439551
Epoch 4/100 Iteration 50/234: loss=0.257052 lr=0.000020 grad_norm=0.572671
Epoch 4/100 Iteration 51/234: loss=0.238874 lr=0.000020 grad_norm=0.586209
Epoch 4/100 Iteration 52/234: loss=0.251070 lr=0.000020 grad_norm=0.478537
Epoch 4/100 Iteration 53/234: loss=0.248896 lr=0.000020 grad_norm=0.537650
Epoch 4/100 Iteration 54/234: loss=0.256774 lr=0.000020 grad_norm=0.792962
Epoch 4/100 Iteration 55/234: loss=0.245507 lr=0.000020 grad_norm=0.523219
Epoch 4/100 Iteration 56/234: loss=0.262469 lr=0.000020 grad_norm=0.563497
Epoch 4/100 Iteration 57/234: loss=0.262161 lr=0.000020 grad_norm=0.778535
Epoch 4/100 Iteration 58/234: loss=0.249780 lr=0.000020 grad_norm=0.459249
Epoch 4/100 Iteration 59/234: loss=0.263743 lr=0.000020 grad_norm=0.665932
Epoch 4/100 Iteration 60/234: loss=0.256733 lr=0.000020 grad_norm=0.433244
Epoch 4/100 Iteration 61/234: loss=0.248897 lr=0.000020 grad_norm=0.584322
Epoch 4/100 Iteration 62/234: loss=0.260502 lr=0.000020 grad_norm=0.576893
Epoch 4/100 Iteration 63/234: loss=0.253021 lr=0.000020 grad_norm=0.458838
Epoch 4/100 Iteration 64/234: loss=0.232784 lr=0.000020 grad_norm=0.831097
Epoch 4/100 Iteration 65/234: loss=0.242145 lr=0.000020 grad_norm=0.742328
Epoch 4/100 Iteration 66/234: loss=0.239312 lr=0.000020 grad_norm=0.446684
Epoch 4/100 Iteration 67/234: loss=0.253576 lr=0.000020 grad_norm=0.694648
Epoch 4/100 Iteration 68/234: loss=0.237766 lr=0.000020 grad_norm=0.476355
Epoch 4/100 Iteration 69/234: loss=0.246796 lr=0.000020 grad_norm=0.664150
Epoch 4/100 Iteration 70/234: loss=0.248207 lr=0.000020 grad_norm=0.866981
Epoch 4/100 Iteration 71/234: loss=0.249250 lr=0.000020 grad_norm=0.638277
Epoch 4/100 Iteration 72/234: loss=0.261587 lr=0.000020 grad_norm=0.533995
Epoch 4/100 Iteration 73/234: loss=0.255054 lr=0.000020 grad_norm=0.546475
Epoch 4/100 Iteration 74/234: loss=0.229151 lr=0.000020 grad_norm=0.420541
Epoch 4/100 Iteration 75/234: loss=0.264449 lr=0.000020 grad_norm=0.534741
Epoch 4/100 Iteration 76/234: loss=0.266862 lr=0.000020 grad_norm=0.705729
Epoch 4/100 Iteration 77/234: loss=0.256517 lr=0.000020 grad_norm=0.438905
Epoch 4/100 Iteration 78/234: loss=0.260312 lr=0.000020 grad_norm=0.591599
Epoch 4/100 Iteration 79/234: loss=0.252669 lr=0.000020 grad_norm=0.550441
Epoch 4/100 Iteration 80/234: loss=0.247209 lr=0.000020 grad_norm=0.380632
Epoch 4/100 Iteration 81/234: loss=0.242411 lr=0.000020 grad_norm=0.567045
Epoch 4/100 Iteration 82/234: loss=0.256364 lr=0.000020 grad_norm=0.440106
Epoch 4/100 Iteration 83/234: loss=0.250145 lr=0.000020 grad_norm=0.451624
Epoch 4/100 Iteration 84/234: loss=0.273690 lr=0.000020 grad_norm=0.541611
Epoch 4/100 Iteration 85/234: loss=0.268308 lr=0.000020 grad_norm=0.389777
Epoch 4/100 Iteration 86/234: loss=0.274703 lr=0.000020 grad_norm=0.701551
Epoch 4/100 Iteration 87/234: loss=0.240785 lr=0.000020 grad_norm=0.610467
Epoch 4/100 Iteration 88/234: loss=0.256551 lr=0.000020 grad_norm=0.517995
Epoch 4/100 Iteration 89/234: loss=0.267400 lr=0.000020 grad_norm=0.542279
Epoch 4/100 Iteration 90/234: loss=0.262606 lr=0.000020 grad_norm=0.637473
Epoch 4/100 Iteration 91/234: loss=0.259266 lr=0.000020 grad_norm=0.449916
Epoch 4/100 Iteration 92/234: loss=0.260389 lr=0.000020 grad_norm=0.497830
Epoch 4/100 Iteration 93/234: loss=0.244767 lr=0.000020 grad_norm=0.499236
Epoch 4/100 Iteration 94/234: loss=0.243271 lr=0.000020 grad_norm=0.487433
Epoch 4/100 Iteration 95/234: loss=0.246490 lr=0.000020 grad_norm=0.638515
Epoch 4/100 Iteration 96/234: loss=0.245347 lr=0.000020 grad_norm=0.564591
Epoch 4/100 Iteration 97/234: loss=0.261100 lr=0.000020 grad_norm=0.641388
Epoch 4/100 Iteration 98/234: loss=0.271806 lr=0.000020 grad_norm=0.726340
Epoch 4/100 Iteration 99/234: loss=0.260941 lr=0.000020 grad_norm=0.508558
Epoch 4/100 Iteration 100/234: loss=0.249385 lr=0.000020 grad_norm=0.757207
Epoch 4/100 Iteration 101/234: loss=0.268244 lr=0.000020 grad_norm=1.073790
Epoch 4/100 Iteration 102/234: loss=0.234640 lr=0.000020 grad_norm=0.645408
Epoch 4/100 Iteration 103/234: loss=0.240096 lr=0.000020 grad_norm=0.601631
Epoch 4/100 Iteration 104/234: loss=0.265634 lr=0.000020 grad_norm=0.898237
Epoch 4/100 Iteration 105/234: loss=0.246453 lr=0.000020 grad_norm=0.605294
Epoch 4/100 Iteration 106/234: loss=0.258424 lr=0.000020 grad_norm=0.602997
Epoch 4/100 Iteration 107/234: loss=0.227997 lr=0.000020 grad_norm=0.712646
Epoch 4/100 Iteration 108/234: loss=0.250557 lr=0.000020 grad_norm=0.454140
Epoch 4/100 Iteration 109/234: loss=0.244520 lr=0.000020 grad_norm=0.629252
Epoch 4/100 Iteration 110/234: loss=0.237105 lr=0.000020 grad_norm=0.534288
Epoch 4/100 Iteration 111/234: loss=0.248906 lr=0.000020 grad_norm=0.537568
Epoch 4/100 Iteration 112/234: loss=0.242154 lr=0.000020 grad_norm=0.581225
Epoch 4/100 Iteration 113/234: loss=0.251154 lr=0.000020 grad_norm=0.487988
Epoch 4/100 Iteration 114/234: loss=0.245159 lr=0.000020 grad_norm=0.795168
Epoch 4/100 Iteration 115/234: loss=0.270541 lr=0.000020 grad_norm=0.631516
Epoch 4/100 Iteration 116/234: loss=0.257507 lr=0.000020 grad_norm=0.495685
Epoch 4/100 Iteration 117/234: loss=0.230988 lr=0.000020 grad_norm=0.625753
Epoch 4/100 Iteration 118/234: loss=0.253286 lr=0.000020 grad_norm=0.473301
Epoch 4/100 Iteration 119/234: loss=0.260164 lr=0.000020 grad_norm=0.563312
Epoch 4/100 Iteration 120/234: loss=0.239583 lr=0.000020 grad_norm=0.504554
Epoch 4/100 Iteration 121/234: loss=0.263672 lr=0.000020 grad_norm=0.570662
Epoch 4/100 Iteration 122/234: loss=0.229309 lr=0.000020 grad_norm=0.669594
Epoch 4/100 Iteration 123/234: loss=0.250439 lr=0.000020 grad_norm=0.422251
Epoch 4/100 Iteration 124/234: loss=0.257833 lr=0.000020 grad_norm=0.412730
Epoch 4/100 Iteration 125/234: loss=0.252806 lr=0.000020 grad_norm=0.468314
Epoch 4/100 Iteration 126/234: loss=0.251753 lr=0.000020 grad_norm=0.417884
Epoch 4/100 Iteration 127/234: loss=0.244338 lr=0.000020 grad_norm=0.481367
Epoch 4/100 Iteration 128/234: loss=0.252880 lr=0.000020 grad_norm=0.461690
Epoch 4/100 Iteration 129/234: loss=0.231079 lr=0.000020 grad_norm=0.542424
Epoch 4/100 Iteration 130/234: loss=0.256247 lr=0.000020 grad_norm=0.539893
Epoch 4/100 Iteration 131/234: loss=0.259917 lr=0.000020 grad_norm=0.621338
Epoch 4/100 Iteration 132/234: loss=0.248814 lr=0.000020 grad_norm=0.573110
Epoch 4/100 Iteration 133/234: loss=0.226604 lr=0.000020 grad_norm=0.371851
Epoch 4/100 Iteration 134/234: loss=0.245982 lr=0.000020 grad_norm=0.468672
Epoch 4/100 Iteration 135/234: loss=0.255857 lr=0.000020 grad_norm=0.434986
Epoch 4/100 Iteration 136/234: loss=0.251724 lr=0.000020 grad_norm=0.402623
Epoch 4/100 Iteration 137/234: loss=0.234312 lr=0.000020 grad_norm=0.500842
Epoch 4/100 Iteration 138/234: loss=0.240376 lr=0.000020 grad_norm=0.596546
Epoch 4/100 Iteration 139/234: loss=0.261263 lr=0.000020 grad_norm=0.563027
Epoch 4/100 Iteration 140/234: loss=0.265378 lr=0.000020 grad_norm=0.390238
Epoch 4/100 Iteration 141/234: loss=0.246444 lr=0.000020 grad_norm=0.437154
Epoch 4/100 Iteration 142/234: loss=0.240018 lr=0.000020 grad_norm=0.488277
Epoch 4/100 Iteration 143/234: loss=0.246271 lr=0.000020 grad_norm=0.622082
Epoch 4/100 Iteration 144/234: loss=0.267735 lr=0.000020 grad_norm=0.621937
Epoch 4/100 Iteration 145/234: loss=0.246347 lr=0.000020 grad_norm=0.439133
Epoch 4/100 Iteration 146/234: loss=0.240828 lr=0.000020 grad_norm=0.397508
Epoch 4/100 Iteration 147/234: loss=0.259136 lr=0.000020 grad_norm=0.446070
Epoch 4/100 Iteration 148/234: loss=0.231846 lr=0.000020 grad_norm=0.431433
Epoch 4/100 Iteration 149/234: loss=0.250244 lr=0.000020 grad_norm=0.372503
Epoch 4/100 Iteration 150/234: loss=0.251391 lr=0.000020 grad_norm=0.419851
Epoch 4/100 Iteration 151/234: loss=0.253264 lr=0.000020 grad_norm=0.421587
Epoch 4/100 Iteration 152/234: loss=0.263436 lr=0.000020 grad_norm=0.435985
Epoch 4/100 Iteration 153/234: loss=0.243135 lr=0.000020 grad_norm=0.366442
Epoch 4/100 Iteration 154/234: loss=0.249094 lr=0.000020 grad_norm=0.423476
Epoch 4/100 Iteration 155/234: loss=0.243265 lr=0.000020 grad_norm=0.519786
Epoch 4/100 Iteration 156/234: loss=0.243663 lr=0.000020 grad_norm=0.646685
Epoch 4/100 Iteration 157/234: loss=0.263183 lr=0.000020 grad_norm=0.578306
Epoch 4/100 Iteration 158/234: loss=0.246492 lr=0.000020 grad_norm=0.363148
Epoch 4/100 Iteration 159/234: loss=0.251686 lr=0.000020 grad_norm=0.561640
Epoch 4/100 Iteration 160/234: loss=0.261394 lr=0.000020 grad_norm=0.491143
Epoch 4/100 Iteration 161/234: loss=0.267560 lr=0.000020 grad_norm=0.487388
Epoch 4/100 Iteration 162/234: loss=0.265075 lr=0.000020 grad_norm=0.320735
Epoch 4/100 Iteration 163/234: loss=0.242484 lr=0.000020 grad_norm=0.482046
Epoch 4/100 Iteration 164/234: loss=0.257273 lr=0.000020 grad_norm=0.525843
Epoch 4/100 Iteration 165/234: loss=0.229855 lr=0.000020 grad_norm=0.444780
Epoch 4/100 Iteration 166/234: loss=0.247400 lr=0.000020 grad_norm=0.518146
Epoch 4/100 Iteration 167/234: loss=0.235664 lr=0.000020 grad_norm=0.454584
Epoch 4/100 Iteration 168/234: loss=0.247460 lr=0.000020 grad_norm=0.373952
Epoch 4/100 Iteration 169/234: loss=0.251341 lr=0.000020 grad_norm=0.487247
Epoch 4/100 Iteration 170/234: loss=0.227028 lr=0.000020 grad_norm=0.398190
Epoch 4/100 Iteration 171/234: loss=0.261457 lr=0.000020 grad_norm=0.407142
Epoch 4/100 Iteration 172/234: loss=0.251599 lr=0.000020 grad_norm=0.565011
Epoch 4/100 Iteration 173/234: loss=0.250836 lr=0.000020 grad_norm=0.716387
Epoch 4/100 Iteration 174/234: loss=0.258632 lr=0.000020 grad_norm=0.650455
Epoch 4/100 Iteration 175/234: loss=0.243273 lr=0.000020 grad_norm=0.508467
Epoch 4/100 Iteration 176/234: loss=0.237080 lr=0.000020 grad_norm=0.499019
Epoch 4/100 Iteration 177/234: loss=0.253773 lr=0.000020 grad_norm=0.507700
Epoch 4/100 Iteration 178/234: loss=0.221594 lr=0.000020 grad_norm=0.474450
Epoch 4/100 Iteration 179/234: loss=0.254375 lr=0.000020 grad_norm=0.721791
Epoch 4/100 Iteration 180/234: loss=0.251949 lr=0.000020 grad_norm=0.755368
Epoch 4/100 Iteration 181/234: loss=0.236910 lr=0.000020 grad_norm=0.436033
Epoch 4/100 Iteration 182/234: loss=0.246226 lr=0.000020 grad_norm=0.562899
Epoch 4/100 Iteration 183/234: loss=0.248992 lr=0.000020 grad_norm=0.803809
Epoch 4/100 Iteration 184/234: loss=0.226614 lr=0.000020 grad_norm=0.456199
Epoch 4/100 Iteration 185/234: loss=0.262613 lr=0.000020 grad_norm=0.582789
Epoch 4/100 Iteration 186/234: loss=0.247300 lr=0.000020 grad_norm=0.617903
Epoch 4/100 Iteration 187/234: loss=0.237699 lr=0.000020 grad_norm=0.556382
Epoch 4/100 Iteration 188/234: loss=0.231810 lr=0.000020 grad_norm=0.659195
Epoch 4/100 Iteration 189/234: loss=0.249915 lr=0.000020 grad_norm=0.403467
Epoch 4/100 Iteration 190/234: loss=0.244306 lr=0.000020 grad_norm=0.464018
Epoch 4/100 Iteration 191/234: loss=0.271332 lr=0.000020 grad_norm=0.644705
Epoch 4/100 Iteration 192/234: loss=0.224228 lr=0.000020 grad_norm=0.649329
Epoch 4/100 Iteration 193/234: loss=0.266477 lr=0.000020 grad_norm=0.493293
Epoch 4/100 Iteration 194/234: loss=0.249593 lr=0.000020 grad_norm=1.191235
Epoch 4/100 Iteration 195/234: loss=0.246832 lr=0.000020 grad_norm=0.859035
Epoch 4/100 Iteration 196/234: loss=0.245383 lr=0.000020 grad_norm=0.580457
Epoch 4/100 Iteration 197/234: loss=0.271573 lr=0.000020 grad_norm=0.566369
Epoch 4/100 Iteration 198/234: loss=0.248127 lr=0.000020 grad_norm=0.441708
Epoch 4/100 Iteration 199/234: loss=0.247962 lr=0.000020 grad_norm=0.689871
Epoch 4/100 Iteration 200/234: loss=0.240062 lr=0.000020 grad_norm=0.497162
Epoch 4/100 Iteration 201/234: loss=0.264878 lr=0.000020 grad_norm=0.624499
Epoch 4/100 Iteration 202/234: loss=0.227960 lr=0.000020 grad_norm=0.702455
Epoch 4/100 Iteration 203/234: loss=0.263684 lr=0.000020 grad_norm=0.664675
Epoch 4/100 Iteration 204/234: loss=0.270131 lr=0.000020 grad_norm=0.426063
Epoch 4/100 Iteration 205/234: loss=0.256843 lr=0.000020 grad_norm=0.529824
Epoch 4/100 Iteration 206/234: loss=0.250425 lr=0.000020 grad_norm=0.554031
Epoch 4/100 Iteration 207/234: loss=0.232395 lr=0.000020 grad_norm=0.532608
Epoch 4/100 Iteration 208/234: loss=0.260408 lr=0.000020 grad_norm=0.545617
Epoch 4/100 Iteration 209/234: loss=0.255836 lr=0.000020 grad_norm=0.442648
Epoch 4/100 Iteration 210/234: loss=0.247960 lr=0.000020 grad_norm=0.445251
Epoch 4/100 Iteration 211/234: loss=0.266825 lr=0.000020 grad_norm=0.394002
Epoch 4/100 Iteration 212/234: loss=0.249658 lr=0.000020 grad_norm=0.584697
Epoch 4/100 Iteration 213/234: loss=0.267723 lr=0.000020 grad_norm=0.525384
Epoch 4/100 Iteration 214/234: loss=0.244762 lr=0.000020 grad_norm=0.401970
Epoch 4/100 Iteration 215/234: loss=0.237561 lr=0.000020 grad_norm=0.462959
Epoch 4/100 Iteration 216/234: loss=0.249014 lr=0.000020 grad_norm=0.379744
Epoch 4/100 Iteration 217/234: loss=0.248820 lr=0.000020 grad_norm=0.455970
Epoch 4/100 Iteration 218/234: loss=0.230388 lr=0.000020 grad_norm=0.370486
Epoch 4/100 Iteration 219/234: loss=0.255117 lr=0.000020 grad_norm=0.492026
Epoch 4/100 Iteration 220/234: loss=0.244266 lr=0.000020 grad_norm=0.500121
Epoch 4/100 Iteration 221/234: loss=0.233465 lr=0.000020 grad_norm=0.649069
Epoch 4/100 Iteration 222/234: loss=0.238006 lr=0.000020 grad_norm=0.570762
Epoch 4/100 Iteration 223/234: loss=0.242738 lr=0.000020 grad_norm=0.429614
Epoch 4/100 Iteration 224/234: loss=0.250615 lr=0.000020 grad_norm=0.877930
Epoch 4/100 Iteration 225/234: loss=0.226814 lr=0.000020 grad_norm=0.850195
Epoch 4/100 Iteration 226/234: loss=0.223722 lr=0.000020 grad_norm=0.389263
Epoch 4/100 Iteration 227/234: loss=0.244235 lr=0.000020 grad_norm=0.543917
Epoch 4/100 Iteration 228/234: loss=0.250738 lr=0.000020 grad_norm=0.677501
Epoch 4/100 Iteration 229/234: loss=0.265528 lr=0.000020 grad_norm=0.410110
Epoch 4/100 Iteration 230/234: loss=0.254128 lr=0.000020 grad_norm=0.433589
Epoch 4/100 Iteration 231/234: loss=0.255271 lr=0.000020 grad_norm=0.617266
Epoch 4/100 Iteration 232/234: loss=0.223965 lr=0.000020 grad_norm=0.651791
Epoch 4/100 Iteration 233/234: loss=0.247536 lr=0.000020 grad_norm=0.594097
Epoch 4/100 Iteration 234/234: loss=0.228284 lr=0.000020 grad_norm=0.482224
Epoch 4/100 finished. Avg Loss: 0.251700
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 5/100 Iteration 1/234: loss=0.233643 lr=0.000020 grad_norm=0.495288
Epoch 5/100 Iteration 2/234: loss=0.246151 lr=0.000020 grad_norm=0.511281
Epoch 5/100 Iteration 3/234: loss=0.253496 lr=0.000020 grad_norm=0.462814
Epoch 5/100 Iteration 4/234: loss=0.245274 lr=0.000020 grad_norm=0.499788
Epoch 5/100 Iteration 5/234: loss=0.249211 lr=0.000020 grad_norm=0.397876
Epoch 5/100 Iteration 6/234: loss=0.255708 lr=0.000020 grad_norm=0.474574
Epoch 5/100 Iteration 7/234: loss=0.252226 lr=0.000020 grad_norm=0.458123
Epoch 5/100 Iteration 8/234: loss=0.239822 lr=0.000020 grad_norm=0.741137
Epoch 5/100 Iteration 9/234: loss=0.231401 lr=0.000020 grad_norm=0.838090
Epoch 5/100 Iteration 10/234: loss=0.240579 lr=0.000020 grad_norm=0.504223
Epoch 5/100 Iteration 11/234: loss=0.237377 lr=0.000020 grad_norm=0.520613
Epoch 5/100 Iteration 12/234: loss=0.253618 lr=0.000020 grad_norm=0.708366
Epoch 5/100 Iteration 13/234: loss=0.246758 lr=0.000020 grad_norm=0.371318
Epoch 5/100 Iteration 14/234: loss=0.227420 lr=0.000020 grad_norm=0.460264
Epoch 5/100 Iteration 15/234: loss=0.237777 lr=0.000020 grad_norm=0.570471
Epoch 5/100 Iteration 16/234: loss=0.238077 lr=0.000020 grad_norm=0.424912
Epoch 5/100 Iteration 17/234: loss=0.239750 lr=0.000020 grad_norm=0.539415
Epoch 5/100 Iteration 18/234: loss=0.243888 lr=0.000020 grad_norm=0.743177
Epoch 5/100 Iteration 19/234: loss=0.240776 lr=0.000020 grad_norm=0.412918
Epoch 5/100 Iteration 20/234: loss=0.218823 lr=0.000020 grad_norm=0.840260
Epoch 5/100 Iteration 21/234: loss=0.227506 lr=0.000020 grad_norm=0.922154
Epoch 5/100 Iteration 22/234: loss=0.222581 lr=0.000020 grad_norm=0.472135
Epoch 5/100 Iteration 23/234: loss=0.242751 lr=0.000020 grad_norm=0.625524
Epoch 5/100 Iteration 24/234: loss=0.235385 lr=0.000020 grad_norm=0.556413
Epoch 5/100 Iteration 25/234: loss=0.248005 lr=0.000020 grad_norm=0.531228
Epoch 5/100 Iteration 26/234: loss=0.237223 lr=0.000020 grad_norm=0.534225
Epoch 5/100 Iteration 27/234: loss=0.252714 lr=0.000020 grad_norm=0.426763
Epoch 5/100 Iteration 28/234: loss=0.228811 lr=0.000020 grad_norm=0.423285
Epoch 5/100 Iteration 29/234: loss=0.242807 lr=0.000020 grad_norm=0.434441
Epoch 5/100 Iteration 30/234: loss=0.260790 lr=0.000020 grad_norm=0.369661
Epoch 5/100 Iteration 31/234: loss=0.232121 lr=0.000020 grad_norm=0.414440
Epoch 5/100 Iteration 32/234: loss=0.232357 lr=0.000020 grad_norm=0.515143
Epoch 5/100 Iteration 33/234: loss=0.222013 lr=0.000020 grad_norm=0.662943
Epoch 5/100 Iteration 34/234: loss=0.246245 lr=0.000020 grad_norm=0.660249
Epoch 5/100 Iteration 35/234: loss=0.232437 lr=0.000020 grad_norm=0.719564
Epoch 5/100 Iteration 36/234: loss=0.253668 lr=0.000020 grad_norm=0.581126
Epoch 5/100 Iteration 37/234: loss=0.244664 lr=0.000020 grad_norm=0.496802
Epoch 5/100 Iteration 38/234: loss=0.253499 lr=0.000020 grad_norm=0.632625
Epoch 5/100 Iteration 39/234: loss=0.236843 lr=0.000020 grad_norm=0.787060
Epoch 5/100 Iteration 40/234: loss=0.263937 lr=0.000020 grad_norm=0.590739
Epoch 5/100 Iteration 41/234: loss=0.254845 lr=0.000020 grad_norm=0.502573
Epoch 5/100 Iteration 42/234: loss=0.229749 lr=0.000020 grad_norm=0.548957
Epoch 5/100 Iteration 43/234: loss=0.242316 lr=0.000020 grad_norm=0.621885
Epoch 5/100 Iteration 44/234: loss=0.234358 lr=0.000020 grad_norm=0.796520
Epoch 5/100 Iteration 45/234: loss=0.235347 lr=0.000020 grad_norm=0.524776
Epoch 5/100 Iteration 46/234: loss=0.248907 lr=0.000020 grad_norm=0.409525
Epoch 5/100 Iteration 47/234: loss=0.235169 lr=0.000020 grad_norm=0.538699
Epoch 5/100 Iteration 48/234: loss=0.204518 lr=0.000020 grad_norm=0.500415
Epoch 5/100 Iteration 49/234: loss=0.242731 lr=0.000020 grad_norm=0.494276
Epoch 5/100 Iteration 50/234: loss=0.236955 lr=0.000020 grad_norm=0.483594
Epoch 5/100 Iteration 51/234: loss=0.262775 lr=0.000020 grad_norm=0.788720
Epoch 5/100 Iteration 52/234: loss=0.228663 lr=0.000020 grad_norm=0.908233
Epoch 5/100 Iteration 53/234: loss=0.244029 lr=0.000020 grad_norm=0.559441
Epoch 5/100 Iteration 54/234: loss=0.249998 lr=0.000020 grad_norm=0.508690
Epoch 5/100 Iteration 55/234: loss=0.238241 lr=0.000020 grad_norm=0.867993
Epoch 5/100 Iteration 56/234: loss=0.231457 lr=0.000020 grad_norm=0.835560
Epoch 5/100 Iteration 57/234: loss=0.235248 lr=0.000020 grad_norm=0.392719
Epoch 5/100 Iteration 58/234: loss=0.245613 lr=0.000020 grad_norm=0.711236
Epoch 5/100 Iteration 59/234: loss=0.246118 lr=0.000020 grad_norm=0.987861
Epoch 5/100 Iteration 60/234: loss=0.250467 lr=0.000020 grad_norm=0.903920
Epoch 5/100 Iteration 61/234: loss=0.249372 lr=0.000020 grad_norm=0.516122
Epoch 5/100 Iteration 62/234: loss=0.233921 lr=0.000020 grad_norm=0.640465
Epoch 5/100 Iteration 63/234: loss=0.239398 lr=0.000020 grad_norm=0.521660
Epoch 5/100 Iteration 64/234: loss=0.244234 lr=0.000020 grad_norm=0.488282
Epoch 5/100 Iteration 65/234: loss=0.245413 lr=0.000020 grad_norm=0.630646
Epoch 5/100 Iteration 66/234: loss=0.249712 lr=0.000020 grad_norm=0.404752
Epoch 5/100 Iteration 67/234: loss=0.247094 lr=0.000020 grad_norm=0.463654
Epoch 5/100 Iteration 68/234: loss=0.243548 lr=0.000020 grad_norm=0.448820
Epoch 5/100 Iteration 69/234: loss=0.240918 lr=0.000020 grad_norm=0.463032
Epoch 5/100 Iteration 70/234: loss=0.226395 lr=0.000020 grad_norm=0.383942
Epoch 5/100 Iteration 71/234: loss=0.250829 lr=0.000020 grad_norm=0.449263
Epoch 5/100 Iteration 72/234: loss=0.222132 lr=0.000020 grad_norm=0.478932
Epoch 5/100 Iteration 73/234: loss=0.247594 lr=0.000020 grad_norm=0.794460
Epoch 5/100 Iteration 74/234: loss=0.235911 lr=0.000020 grad_norm=1.010178
Epoch 5/100 Iteration 75/234: loss=0.243330 lr=0.000020 grad_norm=0.575875
Epoch 5/100 Iteration 76/234: loss=0.223949 lr=0.000020 grad_norm=0.718217
Epoch 5/100 Iteration 77/234: loss=0.252457 lr=0.000020 grad_norm=0.858472
Epoch 5/100 Iteration 78/234: loss=0.237118 lr=0.000020 grad_norm=0.374741
Epoch 5/100 Iteration 79/234: loss=0.242853 lr=0.000020 grad_norm=0.601003
Epoch 5/100 Iteration 80/234: loss=0.241132 lr=0.000020 grad_norm=0.546620
Epoch 5/100 Iteration 81/234: loss=0.239430 lr=0.000020 grad_norm=0.353568
Epoch 5/100 Iteration 82/234: loss=0.220458 lr=0.000020 grad_norm=0.484116
Epoch 5/100 Iteration 83/234: loss=0.242257 lr=0.000020 grad_norm=0.548421
Epoch 5/100 Iteration 84/234: loss=0.229966 lr=0.000020 grad_norm=0.391992
Epoch 5/100 Iteration 85/234: loss=0.236661 lr=0.000020 grad_norm=0.515598
Epoch 5/100 Iteration 86/234: loss=0.225287 lr=0.000020 grad_norm=0.572362
Epoch 5/100 Iteration 87/234: loss=0.229702 lr=0.000020 grad_norm=0.578095
Epoch 5/100 Iteration 88/234: loss=0.247895 lr=0.000020 grad_norm=0.619545
Epoch 5/100 Iteration 89/234: loss=0.243717 lr=0.000020 grad_norm=0.475782
Epoch 5/100 Iteration 90/234: loss=0.228404 lr=0.000020 grad_norm=0.574694
Epoch 5/100 Iteration 91/234: loss=0.231337 lr=0.000020 grad_norm=0.656544
Epoch 5/100 Iteration 92/234: loss=0.241521 lr=0.000020 grad_norm=0.561621
Epoch 5/100 Iteration 93/234: loss=0.230335 lr=0.000020 grad_norm=0.478253
Epoch 5/100 Iteration 94/234: loss=0.267185 lr=0.000020 grad_norm=0.602212
Epoch 5/100 Iteration 95/234: loss=0.228710 lr=0.000020 grad_norm=0.582159
Epoch 5/100 Iteration 96/234: loss=0.250786 lr=0.000020 grad_norm=0.539266
Epoch 5/100 Iteration 97/234: loss=0.240779 lr=0.000020 grad_norm=0.480542
Epoch 5/100 Iteration 98/234: loss=0.236073 lr=0.000020 grad_norm=0.550028
Epoch 5/100 Iteration 99/234: loss=0.247869 lr=0.000020 grad_norm=0.604431
Epoch 5/100 Iteration 100/234: loss=0.235236 lr=0.000020 grad_norm=0.378802
Epoch 5/100 Iteration 101/234: loss=0.215058 lr=0.000020 grad_norm=0.513604
Epoch 5/100 Iteration 102/234: loss=0.228474 lr=0.000020 grad_norm=0.707114
Epoch 5/100 Iteration 103/234: loss=0.244045 lr=0.000020 grad_norm=0.552582
Epoch 5/100 Iteration 104/234: loss=0.243197 lr=0.000020 grad_norm=0.375664
Epoch 5/100 Iteration 105/234: loss=0.238954 lr=0.000020 grad_norm=0.455397
Epoch 5/100 Iteration 106/234: loss=0.236808 lr=0.000020 grad_norm=0.482770
Epoch 5/100 Iteration 107/234: loss=0.240582 lr=0.000020 grad_norm=0.528912
Epoch 5/100 Iteration 108/234: loss=0.253774 lr=0.000020 grad_norm=0.837334
Epoch 5/100 Iteration 109/234: loss=0.230434 lr=0.000020 grad_norm=1.235851
Epoch 5/100 Iteration 110/234: loss=0.215176 lr=0.000020 grad_norm=1.028381
Epoch 5/100 Iteration 111/234: loss=0.233693 lr=0.000020 grad_norm=0.427999
Epoch 5/100 Iteration 112/234: loss=0.254300 lr=0.000020 grad_norm=1.085057
Epoch 5/100 Iteration 113/234: loss=0.234488 lr=0.000020 grad_norm=0.976939
Epoch 5/100 Iteration 114/234: loss=0.240331 lr=0.000020 grad_norm=0.735973
Epoch 5/100 Iteration 115/234: loss=0.235275 lr=0.000020 grad_norm=0.792606
Epoch 5/100 Iteration 116/234: loss=0.248056 lr=0.000020 grad_norm=0.555899
Epoch 5/100 Iteration 117/234: loss=0.215817 lr=0.000020 grad_norm=0.532230
Epoch 5/100 Iteration 118/234: loss=0.261938 lr=0.000020 grad_norm=0.496752
Epoch 5/100 Iteration 119/234: loss=0.240567 lr=0.000020 grad_norm=0.477001
Epoch 5/100 Iteration 120/234: loss=0.226814 lr=0.000020 grad_norm=0.647519
Epoch 5/100 Iteration 121/234: loss=0.236401 lr=0.000020 grad_norm=0.805309
Epoch 5/100 Iteration 122/234: loss=0.243812 lr=0.000020 grad_norm=0.631787
Epoch 5/100 Iteration 123/234: loss=0.220669 lr=0.000020 grad_norm=0.629284
Epoch 5/100 Iteration 124/234: loss=0.242943 lr=0.000020 grad_norm=0.692488
Epoch 5/100 Iteration 125/234: loss=0.220326 lr=0.000020 grad_norm=0.999280
Epoch 5/100 Iteration 126/234: loss=0.208874 lr=0.000020 grad_norm=1.074126
Epoch 5/100 Iteration 127/234: loss=0.231928 lr=0.000020 grad_norm=1.724520
Epoch 5/100 Iteration 128/234: loss=0.244107 lr=0.000020 grad_norm=2.119673
Epoch 5/100 Iteration 129/234: loss=0.234601 lr=0.000020 grad_norm=1.424653
Epoch 5/100 Iteration 130/234: loss=0.244548 lr=0.000020 grad_norm=1.442968
Epoch 5/100 Iteration 131/234: loss=0.260587 lr=0.000020 grad_norm=3.960014
Epoch 5/100 Iteration 132/234: loss=0.284479 lr=0.000020 grad_norm=5.884869
Epoch 5/100 Iteration 133/234: loss=0.311502 lr=0.000020 grad_norm=6.508167
Epoch 5/100 Iteration 134/234: loss=0.254731 lr=0.000020 grad_norm=3.729501
Epoch 5/100 Iteration 135/234: loss=0.284978 lr=0.000020 grad_norm=4.421817
Epoch 5/100 Iteration 136/234: loss=0.264466 lr=0.000020 grad_norm=4.153875
Epoch 5/100 Iteration 137/234: loss=0.241413 lr=0.000020 grad_norm=3.914463
Epoch 5/100 Iteration 138/234: loss=0.261020 lr=0.000020 grad_norm=2.692065
Epoch 5/100 Iteration 139/234: loss=0.279179 lr=0.000020 grad_norm=1.698159
Epoch 5/100 Iteration 140/234: loss=0.247441 lr=0.000020 grad_norm=1.726733
Epoch 5/100 Iteration 141/234: loss=0.265744 lr=0.000020 grad_norm=1.707972
Epoch 5/100 Iteration 142/234: loss=0.233055 lr=0.000020 grad_norm=1.401985
Epoch 5/100 Iteration 143/234: loss=0.237067 lr=0.000020 grad_norm=1.043401
Epoch 5/100 Iteration 144/234: loss=0.244113 lr=0.000020 grad_norm=1.131634
Epoch 5/100 Iteration 145/234: loss=0.224572 lr=0.000020 grad_norm=1.085049
Epoch 5/100 Iteration 146/234: loss=0.230811 lr=0.000020 grad_norm=0.768055
Epoch 5/100 Iteration 147/234: loss=0.243150 lr=0.000020 grad_norm=0.751951
Epoch 5/100 Iteration 148/234: loss=0.254200 lr=0.000020 grad_norm=0.720488
Epoch 5/100 Iteration 149/234: loss=0.255856 lr=0.000020 grad_norm=0.560896
Epoch 5/100 Iteration 150/234: loss=0.246264 lr=0.000020 grad_norm=0.648462
Epoch 5/100 Iteration 151/234: loss=0.255035 lr=0.000020 grad_norm=0.518393
Epoch 5/100 Iteration 152/234: loss=0.241781 lr=0.000020 grad_norm=0.575398
Epoch 5/100 Iteration 153/234: loss=0.232769 lr=0.000020 grad_norm=0.536306
Epoch 5/100 Iteration 154/234: loss=0.230612 lr=0.000020 grad_norm=0.504020
Epoch 5/100 Iteration 155/234: loss=0.222120 lr=0.000020 grad_norm=0.464892
Epoch 5/100 Iteration 156/234: loss=0.241287 lr=0.000020 grad_norm=0.505630
Epoch 5/100 Iteration 157/234: loss=0.246635 lr=0.000020 grad_norm=0.544132
Epoch 5/100 Iteration 158/234: loss=0.246336 lr=0.000020 grad_norm=0.501194
Epoch 5/100 Iteration 159/234: loss=0.221186 lr=0.000020 grad_norm=0.351968
Epoch 5/100 Iteration 160/234: loss=0.245264 lr=0.000020 grad_norm=0.516631
Epoch 5/100 Iteration 161/234: loss=0.242249 lr=0.000020 grad_norm=0.521668
Epoch 5/100 Iteration 162/234: loss=0.245832 lr=0.000020 grad_norm=0.375150
Epoch 5/100 Iteration 163/234: loss=0.244461 lr=0.000020 grad_norm=0.443992
Epoch 5/100 Iteration 164/234: loss=0.238277 lr=0.000020 grad_norm=0.459084
Epoch 5/100 Iteration 165/234: loss=0.240726 lr=0.000020 grad_norm=0.376285
Epoch 5/100 Iteration 166/234: loss=0.203100 lr=0.000020 grad_norm=0.389004
Epoch 5/100 Iteration 167/234: loss=0.230233 lr=0.000020 grad_norm=0.424922
Epoch 5/100 Iteration 168/234: loss=0.233998 lr=0.000020 grad_norm=0.301525
Epoch 5/100 Iteration 169/234: loss=0.230822 lr=0.000020 grad_norm=0.304152
Epoch 5/100 Iteration 170/234: loss=0.228192 lr=0.000020 grad_norm=0.407137
Epoch 5/100 Iteration 171/234: loss=0.249924 lr=0.000020 grad_norm=0.342336
Epoch 5/100 Iteration 172/234: loss=0.232109 lr=0.000020 grad_norm=0.326110
Epoch 5/100 Iteration 173/234: loss=0.250129 lr=0.000020 grad_norm=0.323901
Epoch 5/100 Iteration 174/234: loss=0.233787 lr=0.000020 grad_norm=0.325292
Epoch 5/100 Iteration 175/234: loss=0.237128 lr=0.000020 grad_norm=0.316055
Epoch 5/100 Iteration 176/234: loss=0.243936 lr=0.000020 grad_norm=0.357777
Epoch 5/100 Iteration 177/234: loss=0.248444 lr=0.000020 grad_norm=0.333686
Epoch 5/100 Iteration 178/234: loss=0.228841 lr=0.000020 grad_norm=0.301443
Epoch 5/100 Iteration 179/234: loss=0.253963 lr=0.000020 grad_norm=0.368175
Epoch 5/100 Iteration 180/234: loss=0.242733 lr=0.000020 grad_norm=0.245379
Epoch 5/100 Iteration 181/234: loss=0.236410 lr=0.000020 grad_norm=0.267351
Epoch 5/100 Iteration 182/234: loss=0.232098 lr=0.000020 grad_norm=0.373457
Epoch 5/100 Iteration 183/234: loss=0.230604 lr=0.000020 grad_norm=0.388630
Epoch 5/100 Iteration 184/234: loss=0.226430 lr=0.000020 grad_norm=0.240048
Epoch 5/100 Iteration 185/234: loss=0.250896 lr=0.000020 grad_norm=0.286487
Epoch 5/100 Iteration 186/234: loss=0.245744 lr=0.000020 grad_norm=0.288183
Epoch 5/100 Iteration 187/234: loss=0.249966 lr=0.000020 grad_norm=0.275085
Epoch 5/100 Iteration 188/234: loss=0.226246 lr=0.000020 grad_norm=0.395611
Epoch 5/100 Iteration 189/234: loss=0.239399 lr=0.000020 grad_norm=0.375699
Epoch 5/100 Iteration 190/234: loss=0.249415 lr=0.000020 grad_norm=0.328507
Epoch 5/100 Iteration 191/234: loss=0.222868 lr=0.000020 grad_norm=0.391359
Epoch 5/100 Iteration 192/234: loss=0.220331 lr=0.000020 grad_norm=0.432219
Epoch 5/100 Iteration 193/234: loss=0.235593 lr=0.000020 grad_norm=0.266155
Epoch 5/100 Iteration 194/234: loss=0.239957 lr=0.000020 grad_norm=0.432475
Epoch 5/100 Iteration 195/234: loss=0.226059 lr=0.000020 grad_norm=0.444985
Epoch 5/100 Iteration 196/234: loss=0.241822 lr=0.000020 grad_norm=0.368237
Epoch 5/100 Iteration 197/234: loss=0.237778 lr=0.000020 grad_norm=0.362603
Epoch 5/100 Iteration 198/234: loss=0.207968 lr=0.000020 grad_norm=0.494331
Epoch 5/100 Iteration 199/234: loss=0.231787 lr=0.000020 grad_norm=0.354510
Epoch 5/100 Iteration 200/234: loss=0.216572 lr=0.000020 grad_norm=0.513209
Epoch 5/100 Iteration 201/234: loss=0.256887 lr=0.000020 grad_norm=0.336729
Epoch 5/100 Iteration 202/234: loss=0.240259 lr=0.000020 grad_norm=0.491882
Epoch 5/100 Iteration 203/234: loss=0.239280 lr=0.000020 grad_norm=0.283261
Epoch 5/100 Iteration 204/234: loss=0.247673 lr=0.000020 grad_norm=0.451093
Epoch 5/100 Iteration 205/234: loss=0.224601 lr=0.000020 grad_norm=0.261786
Epoch 5/100 Iteration 206/234: loss=0.236870 lr=0.000020 grad_norm=0.415895
Epoch 5/100 Iteration 207/234: loss=0.252160 lr=0.000020 grad_norm=0.340279
Epoch 5/100 Iteration 208/234: loss=0.218703 lr=0.000020 grad_norm=0.333210
Epoch 5/100 Iteration 209/234: loss=0.245679 lr=0.000020 grad_norm=0.296506
Epoch 5/100 Iteration 210/234: loss=0.238020 lr=0.000020 grad_norm=0.445278
Epoch 5/100 Iteration 211/234: loss=0.227453 lr=0.000020 grad_norm=0.295934
Epoch 5/100 Iteration 212/234: loss=0.244791 lr=0.000020 grad_norm=0.435383
Epoch 5/100 Iteration 213/234: loss=0.208909 lr=0.000020 grad_norm=0.439357
Epoch 5/100 Iteration 214/234: loss=0.241580 lr=0.000020 grad_norm=0.434083
Epoch 5/100 Iteration 215/234: loss=0.245568 lr=0.000020 grad_norm=0.329097
Epoch 5/100 Iteration 216/234: loss=0.207473 lr=0.000020 grad_norm=0.642197
Epoch 5/100 Iteration 217/234: loss=0.224890 lr=0.000020 grad_norm=0.583458
Epoch 5/100 Iteration 218/234: loss=0.216484 lr=0.000020 grad_norm=0.551704
Epoch 5/100 Iteration 219/234: loss=0.207952 lr=0.000020 grad_norm=0.719865
Epoch 5/100 Iteration 220/234: loss=0.234274 lr=0.000020 grad_norm=0.752893
Epoch 5/100 Iteration 221/234: loss=0.236916 lr=0.000020 grad_norm=0.317557
Epoch 5/100 Iteration 222/234: loss=0.231784 lr=0.000020 grad_norm=0.654864
Epoch 5/100 Iteration 223/234: loss=0.229817 lr=0.000020 grad_norm=0.565648
Epoch 5/100 Iteration 224/234: loss=0.256619 lr=0.000020 grad_norm=0.333486
Epoch 5/100 Iteration 225/234: loss=0.253248 lr=0.000020 grad_norm=0.641742
Epoch 5/100 Iteration 226/234: loss=0.219034 lr=0.000020 grad_norm=0.407676
Epoch 5/100 Iteration 227/234: loss=0.228431 lr=0.000020 grad_norm=0.423298
Epoch 5/100 Iteration 228/234: loss=0.234273 lr=0.000020 grad_norm=0.541663
Epoch 5/100 Iteration 229/234: loss=0.233217 lr=0.000020 grad_norm=0.381918
Epoch 5/100 Iteration 230/234: loss=0.228883 lr=0.000020 grad_norm=0.396905
Epoch 5/100 Iteration 231/234: loss=0.218489 lr=0.000020 grad_norm=0.465158
Epoch 5/100 Iteration 232/234: loss=0.228353 lr=0.000020 grad_norm=0.302547
Epoch 5/100 Iteration 233/234: loss=0.221923 lr=0.000020 grad_norm=0.455336
Epoch 5/100 Iteration 234/234: loss=0.225023 lr=0.000020 grad_norm=0.306913
Epoch 5/100 finished. Avg Loss: 0.238985
Epoch 6/100 Iteration 1/234: loss=0.228989 lr=0.000020 grad_norm=0.382943
Epoch 6/100 Iteration 2/234: loss=0.227877 lr=0.000020 grad_norm=0.280325
Epoch 6/100 Iteration 3/234: loss=0.236072 lr=0.000020 grad_norm=0.340832
Epoch 6/100 Iteration 4/234: loss=0.211717 lr=0.000020 grad_norm=0.378656
Epoch 6/100 Iteration 5/234: loss=0.198732 lr=0.000020 grad_norm=0.308648
Epoch 6/100 Iteration 6/234: loss=0.226623 lr=0.000020 grad_norm=0.409119
Epoch 6/100 Iteration 7/234: loss=0.236477 lr=0.000020 grad_norm=0.369692
Epoch 6/100 Iteration 8/234: loss=0.242011 lr=0.000020 grad_norm=0.436713
Epoch 6/100 Iteration 9/234: loss=0.226568 lr=0.000020 grad_norm=0.270873
Epoch 6/100 Iteration 10/234: loss=0.218622 lr=0.000020 grad_norm=0.429294
Epoch 6/100 Iteration 11/234: loss=0.233459 lr=0.000020 grad_norm=0.418372
Epoch 6/100 Iteration 12/234: loss=0.213831 lr=0.000020 grad_norm=0.492383
Epoch 6/100 Iteration 13/234: loss=0.213106 lr=0.000020 grad_norm=0.445901
Epoch 6/100 Iteration 14/234: loss=0.240125 lr=0.000020 grad_norm=0.334765
Epoch 6/100 Iteration 15/234: loss=0.208318 lr=0.000020 grad_norm=0.417274
Epoch 6/100 Iteration 16/234: loss=0.234449 lr=0.000020 grad_norm=0.478856
Epoch 6/100 Iteration 17/234: loss=0.243348 lr=0.000020 grad_norm=0.615997
Epoch 6/100 Iteration 18/234: loss=0.245471 lr=0.000020 grad_norm=0.393335
Epoch 6/100 Iteration 19/234: loss=0.231373 lr=0.000020 grad_norm=0.500847
Epoch 6/100 Iteration 20/234: loss=0.233752 lr=0.000020 grad_norm=0.688126
Epoch 6/100 Iteration 21/234: loss=0.227394 lr=0.000020 grad_norm=0.504643
Epoch 6/100 Iteration 22/234: loss=0.243563 lr=0.000020 grad_norm=0.567381
Epoch 6/100 Iteration 23/234: loss=0.220413 lr=0.000020 grad_norm=0.725676
Epoch 6/100 Iteration 24/234: loss=0.229544 lr=0.000020 grad_norm=0.522154
Epoch 6/100 Iteration 25/234: loss=0.230233 lr=0.000020 grad_norm=0.617820
Epoch 6/100 Iteration 26/234: loss=0.240975 lr=0.000020 grad_norm=0.477144
Epoch 6/100 Iteration 27/234: loss=0.236299 lr=0.000020 grad_norm=0.449735
Epoch 6/100 Iteration 28/234: loss=0.218847 lr=0.000020 grad_norm=0.631971
Epoch 6/100 Iteration 29/234: loss=0.213844 lr=0.000020 grad_norm=0.486962
Epoch 6/100 Iteration 30/234: loss=0.216969 lr=0.000020 grad_norm=0.403366
Epoch 6/100 Iteration 31/234: loss=0.201153 lr=0.000020 grad_norm=0.511460
Epoch 6/100 Iteration 32/234: loss=0.218067 lr=0.000020 grad_norm=0.650133
Epoch 6/100 Iteration 33/234: loss=0.227656 lr=0.000020 grad_norm=0.504847
Epoch 6/100 Iteration 34/234: loss=0.236662 lr=0.000020 grad_norm=0.469782
Epoch 6/100 Iteration 35/234: loss=0.227679 lr=0.000020 grad_norm=0.437876
Epoch 6/100 Iteration 36/234: loss=0.216378 lr=0.000020 grad_norm=0.370094
Epoch 6/100 Iteration 37/234: loss=0.242663 lr=0.000020 grad_norm=0.382844
Epoch 6/100 Iteration 38/234: loss=0.216615 lr=0.000020 grad_norm=0.295757
Epoch 6/100 Iteration 39/234: loss=0.230099 lr=0.000020 grad_norm=0.568525
Epoch 6/100 Iteration 40/234: loss=0.241886 lr=0.000020 grad_norm=0.622716
Epoch 6/100 Iteration 41/234: loss=0.222419 lr=0.000020 grad_norm=0.344205
Epoch 6/100 Iteration 42/234: loss=0.214723 lr=0.000020 grad_norm=0.569490
Epoch 6/100 Iteration 43/234: loss=0.232054 lr=0.000020 grad_norm=0.526141
Epoch 6/100 Iteration 44/234: loss=0.234271 lr=0.000020 grad_norm=0.486092
Epoch 6/100 Iteration 45/234: loss=0.226580 lr=0.000020 grad_norm=0.682975
Epoch 6/100 Iteration 46/234: loss=0.223453 lr=0.000020 grad_norm=0.367585
Epoch 6/100 Iteration 47/234: loss=0.212767 lr=0.000020 grad_norm=0.515732
Epoch 6/100 Iteration 48/234: loss=0.220700 lr=0.000020 grad_norm=0.373742
Epoch 6/100 Iteration 49/234: loss=0.233427 lr=0.000020 grad_norm=0.380557
Epoch 6/100 Iteration 50/234: loss=0.236974 lr=0.000020 grad_norm=0.324405
Epoch 6/100 Iteration 51/234: loss=0.232756 lr=0.000020 grad_norm=0.413396
Epoch 6/100 Iteration 52/234: loss=0.229720 lr=0.000020 grad_norm=0.336225
Epoch 6/100 Iteration 53/234: loss=0.226781 lr=0.000020 grad_norm=0.314315
Epoch 6/100 Iteration 54/234: loss=0.233683 lr=0.000020 grad_norm=0.324436
Epoch 6/100 Iteration 55/234: loss=0.245678 lr=0.000020 grad_norm=0.303519
Epoch 6/100 Iteration 56/234: loss=0.234488 lr=0.000020 grad_norm=0.458466
Epoch 6/100 Iteration 57/234: loss=0.244526 lr=0.000020 grad_norm=0.358692
Epoch 6/100 Iteration 58/234: loss=0.237567 lr=0.000020 grad_norm=0.388237
Epoch 6/100 Iteration 59/234: loss=0.229295 lr=0.000020 grad_norm=0.577507
Epoch 6/100 Iteration 60/234: loss=0.229511 lr=0.000020 grad_norm=0.424222
Epoch 6/100 Iteration 61/234: loss=0.225284 lr=0.000020 grad_norm=0.466966
Epoch 6/100 Iteration 62/234: loss=0.222376 lr=0.000020 grad_norm=0.748982
Epoch 6/100 Iteration 63/234: loss=0.238941 lr=0.000020 grad_norm=0.363148
Epoch 6/100 Iteration 64/234: loss=0.213301 lr=0.000020 grad_norm=0.502444
Epoch 6/100 Iteration 65/234: loss=0.211510 lr=0.000020 grad_norm=0.599631
Epoch 6/100 Iteration 66/234: loss=0.218969 lr=0.000020 grad_norm=0.443468
Epoch 6/100 Iteration 67/234: loss=0.218570 lr=0.000020 grad_norm=0.877328
Epoch 6/100 Iteration 68/234: loss=0.201214 lr=0.000020 grad_norm=0.590416
Epoch 6/100 Iteration 69/234: loss=0.230448 lr=0.000020 grad_norm=0.561056
Epoch 6/100 Iteration 70/234: loss=0.217530 lr=0.000020 grad_norm=0.822809
Epoch 6/100 Iteration 71/234: loss=0.207952 lr=0.000020 grad_norm=0.437730
Epoch 6/100 Iteration 72/234: loss=0.232401 lr=0.000020 grad_norm=0.702040
Epoch 6/100 Iteration 73/234: loss=0.225549 lr=0.000020 grad_norm=0.310772
Epoch 6/100 Iteration 74/234: loss=0.228837 lr=0.000020 grad_norm=0.687381
Epoch 6/100 Iteration 75/234: loss=0.226620 lr=0.000020 grad_norm=0.684958
Epoch 6/100 Iteration 76/234: loss=0.228988 lr=0.000020 grad_norm=0.302410
Epoch 6/100 Iteration 77/234: loss=0.217597 lr=0.000020 grad_norm=0.455004
Epoch 6/100 Iteration 78/234: loss=0.195562 lr=0.000020 grad_norm=0.395136
Epoch 6/100 Iteration 79/234: loss=0.220235 lr=0.000020 grad_norm=0.548772
Epoch 6/100 Iteration 80/234: loss=0.226599 lr=0.000020 grad_norm=0.688328
Epoch 6/100 Iteration 81/234: loss=0.222747 lr=0.000020 grad_norm=0.313415
Epoch 6/100 Iteration 82/234: loss=0.222703 lr=0.000020 grad_norm=0.610546
Epoch 6/100 Iteration 83/234: loss=0.226331 lr=0.000020 grad_norm=0.389646
Epoch 6/100 Iteration 84/234: loss=0.218864 lr=0.000020 grad_norm=0.538238
Epoch 6/100 Iteration 85/234: loss=0.219957 lr=0.000020 grad_norm=0.495018
Epoch 6/100 Iteration 86/234: loss=0.229302 lr=0.000020 grad_norm=0.408071
Epoch 6/100 Iteration 87/234: loss=0.208849 lr=0.000020 grad_norm=0.380047
Epoch 6/100 Iteration 88/234: loss=0.225705 lr=0.000020 grad_norm=0.404899
Epoch 6/100 Iteration 89/234: loss=0.229078 lr=0.000020 grad_norm=0.435718
Epoch 6/100 Iteration 90/234: loss=0.251907 lr=0.000020 grad_norm=0.394445
Epoch 6/100 Iteration 91/234: loss=0.231701 lr=0.000020 grad_norm=0.561768
Epoch 6/100 Iteration 92/234: loss=0.218921 lr=0.000020 grad_norm=0.348431
Epoch 6/100 Iteration 93/234: loss=0.216648 lr=0.000020 grad_norm=0.501723
Epoch 6/100 Iteration 94/234: loss=0.214971 lr=0.000020 grad_norm=0.407076
Epoch 6/100 Iteration 95/234: loss=0.206249 lr=0.000020 grad_norm=0.338168
Epoch 6/100 Iteration 96/234: loss=0.221888 lr=0.000020 grad_norm=0.538797
Epoch 6/100 Iteration 97/234: loss=0.209044 lr=0.000020 grad_norm=0.302042
Epoch 6/100 Iteration 98/234: loss=0.225807 lr=0.000020 grad_norm=0.479817
Epoch 6/100 Iteration 99/234: loss=0.230262 lr=0.000020 grad_norm=0.587228
Epoch 6/100 Iteration 100/234: loss=0.223455 lr=0.000020 grad_norm=0.345377
Epoch 6/100 Iteration 101/234: loss=0.210477 lr=0.000020 grad_norm=0.337150
Epoch 6/100 Iteration 102/234: loss=0.236187 lr=0.000020 grad_norm=0.525993
Epoch 6/100 Iteration 103/234: loss=0.220134 lr=0.000020 grad_norm=0.415416
Epoch 6/100 Iteration 104/234: loss=0.218607 lr=0.000020 grad_norm=0.397851
Epoch 6/100 Iteration 105/234: loss=0.234339 lr=0.000020 grad_norm=0.441830
Epoch 6/100 Iteration 106/234: loss=0.241074 lr=0.000020 grad_norm=0.419232
Epoch 6/100 Iteration 107/234: loss=0.210913 lr=0.000020 grad_norm=0.420760
Epoch 6/100 Iteration 108/234: loss=0.219110 lr=0.000020 grad_norm=0.450933
Epoch 6/100 Iteration 109/234: loss=0.216850 lr=0.000020 grad_norm=0.324630
Epoch 6/100 Iteration 110/234: loss=0.232294 lr=0.000020 grad_norm=0.371786
Epoch 6/100 Iteration 111/234: loss=0.247683 lr=0.000020 grad_norm=0.399728
Epoch 6/100 Iteration 112/234: loss=0.226983 lr=0.000020 grad_norm=0.528546
Epoch 6/100 Iteration 113/234: loss=0.222279 lr=0.000020 grad_norm=0.328235
Epoch 6/100 Iteration 114/234: loss=0.215137 lr=0.000020 grad_norm=0.368982
Epoch 6/100 Iteration 115/234: loss=0.218792 lr=0.000020 grad_norm=0.361937
Epoch 6/100 Iteration 116/234: loss=0.235276 lr=0.000020 grad_norm=0.514839
Epoch 6/100 Iteration 117/234: loss=0.237861 lr=0.000020 grad_norm=0.567194
Epoch 6/100 Iteration 118/234: loss=0.194113 lr=0.000020 grad_norm=0.400902
Epoch 6/100 Iteration 119/234: loss=0.225558 lr=0.000020 grad_norm=0.557180
Epoch 6/100 Iteration 120/234: loss=0.224363 lr=0.000020 grad_norm=0.509774
Epoch 6/100 Iteration 121/234: loss=0.234646 lr=0.000020 grad_norm=0.379508
Epoch 6/100 Iteration 122/234: loss=0.213610 lr=0.000020 grad_norm=0.559634
Epoch 6/100 Iteration 123/234: loss=0.226872 lr=0.000020 grad_norm=0.590023
Epoch 6/100 Iteration 124/234: loss=0.197778 lr=0.000020 grad_norm=0.448970
Epoch 6/100 Iteration 125/234: loss=0.217720 lr=0.000020 grad_norm=0.561024
Epoch 6/100 Iteration 126/234: loss=0.222362 lr=0.000020 grad_norm=0.372285
Epoch 6/100 Iteration 127/234: loss=0.227211 lr=0.000020 grad_norm=0.368859
Epoch 6/100 Iteration 128/234: loss=0.212333 lr=0.000020 grad_norm=0.481921
Epoch 6/100 Iteration 129/234: loss=0.218350 lr=0.000020 grad_norm=0.326897
Epoch 6/100 Iteration 130/234: loss=0.205674 lr=0.000020 grad_norm=0.559025
Epoch 6/100 Iteration 131/234: loss=0.217822 lr=0.000020 grad_norm=0.491191
Epoch 6/100 Iteration 132/234: loss=0.211776 lr=0.000020 grad_norm=0.439544
Epoch 6/100 Iteration 133/234: loss=0.230086 lr=0.000020 grad_norm=0.443653
Epoch 6/100 Iteration 134/234: loss=0.224434 lr=0.000020 grad_norm=0.517061
Epoch 6/100 Iteration 135/234: loss=0.205071 lr=0.000020 grad_norm=0.495180
Epoch 6/100 Iteration 136/234: loss=0.226085 lr=0.000020 grad_norm=0.447758
Epoch 6/100 Iteration 137/234: loss=0.230523 lr=0.000020 grad_norm=0.507327
Epoch 6/100 Iteration 138/234: loss=0.220623 lr=0.000020 grad_norm=0.496754
Epoch 6/100 Iteration 139/234: loss=0.211542 lr=0.000020 grad_norm=0.393162
Epoch 6/100 Iteration 140/234: loss=0.235176 lr=0.000020 grad_norm=0.452031
Epoch 6/100 Iteration 141/234: loss=0.238864 lr=0.000020 grad_norm=0.420151
Epoch 6/100 Iteration 142/234: loss=0.232330 lr=0.000020 grad_norm=0.585865
Epoch 6/100 Iteration 143/234: loss=0.231024 lr=0.000020 grad_norm=0.474278
Epoch 6/100 Iteration 144/234: loss=0.227474 lr=0.000020 grad_norm=0.394712
Epoch 6/100 Iteration 145/234: loss=0.240086 lr=0.000020 grad_norm=0.406526
Epoch 6/100 Iteration 146/234: loss=0.223854 lr=0.000020 grad_norm=0.517246
Epoch 6/100 Iteration 147/234: loss=0.210589 lr=0.000020 grad_norm=0.517771
Epoch 6/100 Iteration 148/234: loss=0.224123 lr=0.000020 grad_norm=0.420523
Epoch 6/100 Iteration 149/234: loss=0.223442 lr=0.000020 grad_norm=0.517675
Epoch 6/100 Iteration 150/234: loss=0.207686 lr=0.000020 grad_norm=0.488552
Epoch 6/100 Iteration 151/234: loss=0.222206 lr=0.000020 grad_norm=0.462672
Epoch 6/100 Iteration 152/234: loss=0.219338 lr=0.000020 grad_norm=0.527626
Epoch 6/100 Iteration 153/234: loss=0.202399 lr=0.000020 grad_norm=0.288318
Epoch 6/100 Iteration 154/234: loss=0.226993 lr=0.000020 grad_norm=0.492873
Epoch 6/100 Iteration 155/234: loss=0.233009 lr=0.000020 grad_norm=0.495308
Epoch 6/100 Iteration 156/234: loss=0.210123 lr=0.000020 grad_norm=0.422098
Epoch 6/100 Iteration 157/234: loss=0.211772 lr=0.000020 grad_norm=0.411698
Epoch 6/100 Iteration 158/234: loss=0.212409 lr=0.000020 grad_norm=0.399180
Epoch 6/100 Iteration 159/234: loss=0.197241 lr=0.000020 grad_norm=0.369278
Epoch 6/100 Iteration 160/234: loss=0.214339 lr=0.000020 grad_norm=0.327673
Epoch 6/100 Iteration 161/234: loss=0.221484 lr=0.000020 grad_norm=0.353136
Epoch 6/100 Iteration 162/234: loss=0.224430 lr=0.000020 grad_norm=0.350549
Epoch 6/100 Iteration 163/234: loss=0.214885 lr=0.000020 grad_norm=0.275492
Epoch 6/100 Iteration 164/234: loss=0.227199 lr=0.000020 grad_norm=0.315500
Epoch 6/100 Iteration 165/234: loss=0.222234 lr=0.000020 grad_norm=0.383057
Epoch 6/100 Iteration 166/234: loss=0.222169 lr=0.000020 grad_norm=0.266631
Epoch 6/100 Iteration 167/234: loss=0.226460 lr=0.000020 grad_norm=0.370125
Epoch 6/100 Iteration 168/234: loss=0.236193 lr=0.000020 grad_norm=0.569940
Epoch 6/100 Iteration 169/234: loss=0.227221 lr=0.000020 grad_norm=0.545480
Epoch 6/100 Iteration 170/234: loss=0.217644 lr=0.000020 grad_norm=0.390509
Epoch 6/100 Iteration 171/234: loss=0.197837 lr=0.000020 grad_norm=0.577967
Epoch 6/100 Iteration 172/234: loss=0.214776 lr=0.000020 grad_norm=0.408959
Epoch 6/100 Iteration 173/234: loss=0.198488 lr=0.000020 grad_norm=0.425336
Epoch 6/100 Iteration 174/234: loss=0.223875 lr=0.000020 grad_norm=0.700999
Epoch 6/100 Iteration 175/234: loss=0.223838 lr=0.000020 grad_norm=0.703554
Epoch 6/100 Iteration 176/234: loss=0.221201 lr=0.000020 grad_norm=0.431459
Epoch 6/100 Iteration 177/234: loss=0.219834 lr=0.000020 grad_norm=0.547406
Epoch 6/100 Iteration 178/234: loss=0.217323 lr=0.000020 grad_norm=0.703745
Epoch 6/100 Iteration 179/234: loss=0.230013 lr=0.000020 grad_norm=0.381034
Epoch 6/100 Iteration 180/234: loss=0.219626 lr=0.000020 grad_norm=0.492299
Epoch 6/100 Iteration 181/234: loss=0.236538 lr=0.000020 grad_norm=0.394254
Epoch 6/100 Iteration 182/234: loss=0.220737 lr=0.000020 grad_norm=0.524335
Epoch 6/100 Iteration 183/234: loss=0.210325 lr=0.000020 grad_norm=0.656003
Epoch 6/100 Iteration 184/234: loss=0.207043 lr=0.000020 grad_norm=0.459606
Epoch 6/100 Iteration 185/234: loss=0.226777 lr=0.000020 grad_norm=0.479366
Epoch 6/100 Iteration 186/234: loss=0.217903 lr=0.000020 grad_norm=0.530418
Epoch 6/100 Iteration 187/234: loss=0.211603 lr=0.000020 grad_norm=0.368290
Epoch 6/100 Iteration 188/234: loss=0.201199 lr=0.000020 grad_norm=0.422688
Epoch 6/100 Iteration 189/234: loss=0.215674 lr=0.000020 grad_norm=0.508337
Epoch 6/100 Iteration 190/234: loss=0.216789 lr=0.000020 grad_norm=0.528088
Epoch 6/100 Iteration 191/234: loss=0.236412 lr=0.000020 grad_norm=0.494667
Epoch 6/100 Iteration 192/234: loss=0.226260 lr=0.000020 grad_norm=0.577555
Epoch 6/100 Iteration 193/234: loss=0.249901 lr=0.000020 grad_norm=0.727497
Epoch 6/100 Iteration 194/234: loss=0.232471 lr=0.000020 grad_norm=0.418226
Epoch 6/100 Iteration 195/234: loss=0.230220 lr=0.000020 grad_norm=0.740871
Epoch 6/100 Iteration 196/234: loss=0.218989 lr=0.000020 grad_norm=0.875666
Epoch 6/100 Iteration 197/234: loss=0.213522 lr=0.000020 grad_norm=0.598462
Epoch 6/100 Iteration 198/234: loss=0.215172 lr=0.000020 grad_norm=0.398190
Epoch 6/100 Iteration 199/234: loss=0.221164 lr=0.000020 grad_norm=0.649936
Epoch 6/100 Iteration 200/234: loss=0.222871 lr=0.000020 grad_norm=0.421611
Epoch 6/100 Iteration 201/234: loss=0.230035 lr=0.000020 grad_norm=0.305732
Epoch 6/100 Iteration 202/234: loss=0.215338 lr=0.000020 grad_norm=0.364276
Epoch 6/100 Iteration 203/234: loss=0.223315 lr=0.000020 grad_norm=0.325497
Epoch 6/100 Iteration 204/234: loss=0.217394 lr=0.000020 grad_norm=0.420168
Epoch 6/100 Iteration 205/234: loss=0.214979 lr=0.000020 grad_norm=0.409319
Epoch 6/100 Iteration 206/234: loss=0.225550 lr=0.000020 grad_norm=0.345363
Epoch 6/100 Iteration 207/234: loss=0.202032 lr=0.000020 grad_norm=0.369157
Epoch 6/100 Iteration 208/234: loss=0.191509 lr=0.000020 grad_norm=0.427708
Epoch 6/100 Iteration 209/234: loss=0.234191 lr=0.000020 grad_norm=0.416930
Epoch 6/100 Iteration 210/234: loss=0.205081 lr=0.000020 grad_norm=0.468781
Epoch 6/100 Iteration 211/234: loss=0.209691 lr=0.000020 grad_norm=0.874159
Epoch 6/100 Iteration 212/234: loss=0.210710 lr=0.000020 grad_norm=0.931402
Epoch 6/100 Iteration 213/234: loss=0.230957 lr=0.000020 grad_norm=0.517976
Epoch 6/100 Iteration 214/234: loss=0.235999 lr=0.000020 grad_norm=0.755365
Epoch 6/100 Iteration 215/234: loss=0.214445 lr=0.000020 grad_norm=0.920992
Epoch 6/100 Iteration 216/234: loss=0.242192 lr=0.000020 grad_norm=0.462671
Epoch 6/100 Iteration 217/234: loss=0.219440 lr=0.000020 grad_norm=0.768006
Epoch 6/100 Iteration 218/234: loss=0.208400 lr=0.000020 grad_norm=1.014179
Epoch 6/100 Iteration 219/234: loss=0.216161 lr=0.000020 grad_norm=0.555771
Epoch 6/100 Iteration 220/234: loss=0.202887 lr=0.000020 grad_norm=0.709205
Epoch 6/100 Iteration 221/234: loss=0.220196 lr=0.000020 grad_norm=0.813523
Epoch 6/100 Iteration 222/234: loss=0.219690 lr=0.000020 grad_norm=0.719491
Epoch 6/100 Iteration 223/234: loss=0.206462 lr=0.000020 grad_norm=0.776157
Epoch 6/100 Iteration 224/234: loss=0.209978 lr=0.000020 grad_norm=0.586874
Epoch 6/100 Iteration 225/234: loss=0.196848 lr=0.000020 grad_norm=0.671262
Epoch 6/100 Iteration 226/234: loss=0.214723 lr=0.000020 grad_norm=0.755129
Epoch 6/100 Iteration 227/234: loss=0.201066 lr=0.000020 grad_norm=0.628593
Epoch 6/100 Iteration 228/234: loss=0.208590 lr=0.000020 grad_norm=0.840693
Epoch 6/100 Iteration 229/234: loss=0.216954 lr=0.000020 grad_norm=0.472629
Epoch 6/100 Iteration 230/234: loss=0.233889 lr=0.000020 grad_norm=0.672120
Epoch 6/100 Iteration 231/234: loss=0.214690 lr=0.000020 grad_norm=0.679674
Epoch 6/100 Iteration 232/234: loss=0.191748 lr=0.000020 grad_norm=0.526513
Epoch 6/100 Iteration 233/234: loss=0.201544 lr=0.000020 grad_norm=0.665375
Epoch 6/100 Iteration 234/234: loss=0.209650 lr=0.000020 grad_norm=0.682872
Epoch 6/100 finished. Avg Loss: 0.222110
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 7/100 Iteration 1/234: loss=0.239133 lr=0.000020 grad_norm=0.538840
Epoch 7/100 Iteration 2/234: loss=0.225305 lr=0.000020 grad_norm=0.417740
Epoch 7/100 Iteration 3/234: loss=0.222275 lr=0.000020 grad_norm=0.433967
Epoch 7/100 Iteration 4/234: loss=0.205860 lr=0.000020 grad_norm=0.502137
Epoch 7/100 Iteration 5/234: loss=0.200965 lr=0.000020 grad_norm=0.318556
Epoch 7/100 Iteration 6/234: loss=0.233557 lr=0.000020 grad_norm=0.374152
Epoch 7/100 Iteration 7/234: loss=0.224100 lr=0.000020 grad_norm=0.347487
Epoch 7/100 Iteration 8/234: loss=0.224261 lr=0.000020 grad_norm=0.481085
Epoch 7/100 Iteration 9/234: loss=0.228618 lr=0.000020 grad_norm=0.297320
Epoch 7/100 Iteration 10/234: loss=0.207285 lr=0.000020 grad_norm=0.529117
Epoch 7/100 Iteration 11/234: loss=0.208828 lr=0.000020 grad_norm=0.423978
Epoch 7/100 Iteration 12/234: loss=0.222069 lr=0.000020 grad_norm=0.682140
Epoch 7/100 Iteration 13/234: loss=0.225251 lr=0.000020 grad_norm=0.377094
Epoch 7/100 Iteration 14/234: loss=0.211264 lr=0.000020 grad_norm=0.511164
Epoch 7/100 Iteration 15/234: loss=0.196779 lr=0.000020 grad_norm=0.360006
Epoch 7/100 Iteration 16/234: loss=0.207752 lr=0.000020 grad_norm=0.398642
Epoch 7/100 Iteration 17/234: loss=0.214171 lr=0.000020 grad_norm=0.464961
Epoch 7/100 Iteration 18/234: loss=0.212266 lr=0.000020 grad_norm=0.627498
Epoch 7/100 Iteration 19/234: loss=0.214144 lr=0.000020 grad_norm=0.581341
Epoch 7/100 Iteration 20/234: loss=0.194382 lr=0.000020 grad_norm=0.439981
Epoch 7/100 Iteration 21/234: loss=0.237032 lr=0.000020 grad_norm=0.391380
Epoch 7/100 Iteration 22/234: loss=0.219558 lr=0.000020 grad_norm=0.858615
Epoch 7/100 Iteration 23/234: loss=0.208905 lr=0.000020 grad_norm=0.679997
Epoch 7/100 Iteration 24/234: loss=0.232573 lr=0.000020 grad_norm=0.364697
Epoch 7/100 Iteration 25/234: loss=0.209089 lr=0.000020 grad_norm=0.577220
Epoch 7/100 Iteration 26/234: loss=0.205560 lr=0.000020 grad_norm=0.624599
Epoch 7/100 Iteration 27/234: loss=0.226653 lr=0.000020 grad_norm=0.408516
Epoch 7/100 Iteration 28/234: loss=0.198881 lr=0.000020 grad_norm=0.377231
Epoch 7/100 Iteration 29/234: loss=0.214970 lr=0.000020 grad_norm=0.495462
Epoch 7/100 Iteration 30/234: loss=0.208377 lr=0.000020 grad_norm=0.384787
Epoch 7/100 Iteration 31/234: loss=0.224872 lr=0.000020 grad_norm=0.656681
Epoch 7/100 Iteration 32/234: loss=0.205528 lr=0.000020 grad_norm=0.464268
Epoch 7/100 Iteration 33/234: loss=0.201618 lr=0.000020 grad_norm=0.409189
Epoch 7/100 Iteration 34/234: loss=0.202698 lr=0.000020 grad_norm=0.412689
Epoch 7/100 Iteration 35/234: loss=0.200828 lr=0.000020 grad_norm=0.393472
Epoch 7/100 Iteration 36/234: loss=0.218956 lr=0.000020 grad_norm=0.340167
Epoch 7/100 Iteration 37/234: loss=0.203120 lr=0.000020 grad_norm=0.374140
Epoch 7/100 Iteration 38/234: loss=0.219287 lr=0.000020 grad_norm=0.442727
Epoch 7/100 Iteration 39/234: loss=0.208918 lr=0.000020 grad_norm=0.414307
Epoch 7/100 Iteration 40/234: loss=0.229286 lr=0.000020 grad_norm=0.505652
Epoch 7/100 Iteration 41/234: loss=0.221536 lr=0.000020 grad_norm=0.344559
Epoch 7/100 Iteration 42/234: loss=0.207228 lr=0.000020 grad_norm=0.407104
Epoch 7/100 Iteration 43/234: loss=0.225292 lr=0.000020 grad_norm=0.453920
Epoch 7/100 Iteration 44/234: loss=0.209290 lr=0.000020 grad_norm=0.860136
Epoch 7/100 Iteration 45/234: loss=0.219094 lr=0.000020 grad_norm=0.503193
Epoch 7/100 Iteration 46/234: loss=0.205409 lr=0.000020 grad_norm=0.566745
Epoch 7/100 Iteration 47/234: loss=0.197952 lr=0.000020 grad_norm=0.581928
Epoch 7/100 Iteration 48/234: loss=0.211770 lr=0.000020 grad_norm=0.379574
Epoch 7/100 Iteration 49/234: loss=0.190159 lr=0.000020 grad_norm=0.347479
Epoch 7/100 Iteration 50/234: loss=0.223573 lr=0.000020 grad_norm=0.313206
Epoch 7/100 Iteration 51/234: loss=0.214192 lr=0.000020 grad_norm=0.335960
Epoch 7/100 Iteration 52/234: loss=0.200029 lr=0.000020 grad_norm=0.550274
Epoch 7/100 Iteration 53/234: loss=0.197553 lr=0.000020 grad_norm=0.782505
Epoch 7/100 Iteration 54/234: loss=0.207577 lr=0.000020 grad_norm=0.620764
Epoch 7/100 Iteration 55/234: loss=0.213057 lr=0.000020 grad_norm=0.346953
Epoch 7/100 Iteration 56/234: loss=0.226289 lr=0.000020 grad_norm=0.450876
Epoch 7/100 Iteration 57/234: loss=0.220799 lr=0.000020 grad_norm=0.637669
Epoch 7/100 Iteration 58/234: loss=0.208859 lr=0.000020 grad_norm=0.435156
Epoch 7/100 Iteration 59/234: loss=0.211385 lr=0.000020 grad_norm=0.367935
Epoch 7/100 Iteration 60/234: loss=0.208642 lr=0.000020 grad_norm=0.333347
Epoch 7/100 Iteration 61/234: loss=0.212817 lr=0.000020 grad_norm=0.483512
Epoch 7/100 Iteration 62/234: loss=0.211169 lr=0.000020 grad_norm=0.596148
Epoch 7/100 Iteration 63/234: loss=0.207242 lr=0.000020 grad_norm=0.427261
Epoch 7/100 Iteration 64/234: loss=0.195285 lr=0.000020 grad_norm=0.354957
Epoch 7/100 Iteration 65/234: loss=0.207990 lr=0.000020 grad_norm=0.403765
Epoch 7/100 Iteration 66/234: loss=0.225724 lr=0.000020 grad_norm=0.452973
Epoch 7/100 Iteration 67/234: loss=0.208300 lr=0.000020 grad_norm=0.477089
Epoch 7/100 Iteration 68/234: loss=0.215349 lr=0.000020 grad_norm=0.477661
Epoch 7/100 Iteration 69/234: loss=0.228502 lr=0.000020 grad_norm=0.462693
Epoch 7/100 Iteration 70/234: loss=0.208604 lr=0.000020 grad_norm=0.368907
Epoch 7/100 Iteration 71/234: loss=0.220500 lr=0.000020 grad_norm=0.412705
Epoch 7/100 Iteration 72/234: loss=0.216125 lr=0.000020 grad_norm=0.419726
Epoch 7/100 Iteration 73/234: loss=0.227601 lr=0.000020 grad_norm=0.303680
Epoch 7/100 Iteration 74/234: loss=0.203842 lr=0.000020 grad_norm=0.401439
Epoch 7/100 Iteration 75/234: loss=0.203208 lr=0.000020 grad_norm=0.301780
Epoch 7/100 Iteration 76/234: loss=0.208735 lr=0.000020 grad_norm=0.369790
Epoch 7/100 Iteration 77/234: loss=0.230077 lr=0.000020 grad_norm=0.370215
Epoch 7/100 Iteration 78/234: loss=0.196188 lr=0.000020 grad_norm=0.478491
Epoch 7/100 Iteration 79/234: loss=0.201106 lr=0.000020 grad_norm=0.471103
Epoch 7/100 Iteration 80/234: loss=0.217250 lr=0.000020 grad_norm=0.740523
Epoch 7/100 Iteration 81/234: loss=0.211727 lr=0.000020 grad_norm=0.833186
Epoch 7/100 Iteration 82/234: loss=0.209454 lr=0.000020 grad_norm=0.469852
Epoch 7/100 Iteration 83/234: loss=0.218438 lr=0.000020 grad_norm=0.905800
Epoch 7/100 Iteration 84/234: loss=0.231148 lr=0.000020 grad_norm=0.849536
Epoch 7/100 Iteration 85/234: loss=0.221408 lr=0.000020 grad_norm=0.427781
Epoch 7/100 Iteration 86/234: loss=0.210003 lr=0.000020 grad_norm=1.200062
Epoch 7/100 Iteration 87/234: loss=0.218226 lr=0.000020 grad_norm=0.973485
Epoch 7/100 Iteration 88/234: loss=0.208442 lr=0.000020 grad_norm=0.331276
Epoch 7/100 Iteration 89/234: loss=0.207004 lr=0.000020 grad_norm=1.049278
Epoch 7/100 Iteration 90/234: loss=0.237731 lr=0.000020 grad_norm=0.959741
Epoch 7/100 Iteration 91/234: loss=0.216444 lr=0.000020 grad_norm=0.480155
Epoch 7/100 Iteration 92/234: loss=0.219475 lr=0.000020 grad_norm=0.717378
Epoch 7/100 Iteration 93/234: loss=0.213701 lr=0.000020 grad_norm=0.435316
Epoch 7/100 Iteration 94/234: loss=0.216919 lr=0.000020 grad_norm=0.728946
Epoch 7/100 Iteration 95/234: loss=0.225567 lr=0.000020 grad_norm=0.853221
Epoch 7/100 Iteration 96/234: loss=0.225676 lr=0.000020 grad_norm=0.493689
Epoch 7/100 Iteration 97/234: loss=0.181445 lr=0.000020 grad_norm=1.086967
Epoch 7/100 Iteration 98/234: loss=0.200693 lr=0.000020 grad_norm=0.527675
Epoch 7/100 Iteration 99/234: loss=0.219834 lr=0.000020 grad_norm=0.761989
Epoch 7/100 Iteration 100/234: loss=0.216747 lr=0.000020 grad_norm=0.818743
Epoch 7/100 Iteration 101/234: loss=0.194749 lr=0.000020 grad_norm=0.463513
Epoch 7/100 Iteration 102/234: loss=0.214591 lr=0.000020 grad_norm=1.331819
Epoch 7/100 Iteration 103/234: loss=0.215725 lr=0.000020 grad_norm=0.748782
Epoch 7/100 Iteration 104/234: loss=0.201415 lr=0.000020 grad_norm=0.741340
Epoch 7/100 Iteration 105/234: loss=0.213216 lr=0.000020 grad_norm=0.583575
Epoch 7/100 Iteration 106/234: loss=0.217965 lr=0.000020 grad_norm=0.758822
Epoch 7/100 Iteration 107/234: loss=0.192427 lr=0.000020 grad_norm=0.734404
Epoch 7/100 Iteration 108/234: loss=0.186719 lr=0.000020 grad_norm=0.457929
Epoch 7/100 Iteration 109/234: loss=0.224190 lr=0.000020 grad_norm=0.735697
Epoch 7/100 Iteration 110/234: loss=0.221980 lr=0.000020 grad_norm=0.342208
Epoch 7/100 Iteration 111/234: loss=0.193312 lr=0.000020 grad_norm=0.709656
Epoch 7/100 Iteration 112/234: loss=0.199480 lr=0.000020 grad_norm=0.419432
Epoch 7/100 Iteration 113/234: loss=0.188645 lr=0.000020 grad_norm=0.850627
Epoch 7/100 Iteration 114/234: loss=0.222393 lr=0.000020 grad_norm=0.798773
Epoch 7/100 Iteration 115/234: loss=0.219929 lr=0.000020 grad_norm=0.434079
Epoch 7/100 Iteration 116/234: loss=0.219568 lr=0.000020 grad_norm=0.871083
Epoch 7/100 Iteration 117/234: loss=0.204032 lr=0.000020 grad_norm=0.326987
Epoch 7/100 Iteration 118/234: loss=0.220470 lr=0.000020 grad_norm=0.667659
Epoch 7/100 Iteration 119/234: loss=0.204578 lr=0.000020 grad_norm=0.387000
Epoch 7/100 Iteration 120/234: loss=0.206517 lr=0.000020 grad_norm=0.592459
Epoch 7/100 Iteration 121/234: loss=0.214568 lr=0.000020 grad_norm=0.393110
Epoch 7/100 Iteration 122/234: loss=0.212131 lr=0.000020 grad_norm=0.443959
Epoch 7/100 Iteration 123/234: loss=0.203986 lr=0.000020 grad_norm=0.472033
Epoch 7/100 Iteration 124/234: loss=0.195358 lr=0.000020 grad_norm=0.349200
Epoch 7/100 Iteration 125/234: loss=0.217933 lr=0.000020 grad_norm=0.573925
Epoch 7/100 Iteration 126/234: loss=0.212508 lr=0.000020 grad_norm=0.321521
Epoch 7/100 Iteration 127/234: loss=0.208770 lr=0.000020 grad_norm=0.497635
Epoch 7/100 Iteration 128/234: loss=0.213787 lr=0.000020 grad_norm=0.615500
Epoch 7/100 Iteration 129/234: loss=0.201584 lr=0.000020 grad_norm=0.482890
Epoch 7/100 Iteration 130/234: loss=0.222615 lr=0.000020 grad_norm=0.452684
Epoch 7/100 Iteration 131/234: loss=0.219289 lr=0.000020 grad_norm=0.642220
Epoch 7/100 Iteration 132/234: loss=0.206218 lr=0.000020 grad_norm=0.402624
Epoch 7/100 Iteration 133/234: loss=0.202562 lr=0.000020 grad_norm=0.436505
Epoch 7/100 Iteration 134/234: loss=0.223481 lr=0.000020 grad_norm=0.555455
Epoch 7/100 Iteration 135/234: loss=0.228643 lr=0.000020 grad_norm=0.521813
Epoch 7/100 Iteration 136/234: loss=0.211611 lr=0.000020 grad_norm=0.380606
Epoch 7/100 Iteration 137/234: loss=0.228292 lr=0.000020 grad_norm=0.370871
Epoch 7/100 Iteration 138/234: loss=0.222741 lr=0.000020 grad_norm=0.419841
Epoch 7/100 Iteration 139/234: loss=0.206952 lr=0.000020 grad_norm=0.337174
Epoch 7/100 Iteration 140/234: loss=0.211128 lr=0.000020 grad_norm=0.366622
Epoch 7/100 Iteration 141/234: loss=0.207612 lr=0.000020 grad_norm=0.449369
Epoch 7/100 Iteration 142/234: loss=0.195787 lr=0.000020 grad_norm=0.612830
Epoch 7/100 Iteration 143/234: loss=0.212358 lr=0.000020 grad_norm=0.341956
Epoch 7/100 Iteration 144/234: loss=0.214588 lr=0.000020 grad_norm=0.542654
Epoch 7/100 Iteration 145/234: loss=0.211369 lr=0.000020 grad_norm=0.778630
Epoch 7/100 Iteration 146/234: loss=0.214239 lr=0.000020 grad_norm=0.776979
Epoch 7/100 Iteration 147/234: loss=0.195174 lr=0.000020 grad_norm=0.341002
Epoch 7/100 Iteration 148/234: loss=0.189069 lr=0.000020 grad_norm=0.477379
Epoch 7/100 Iteration 149/234: loss=0.213546 lr=0.000020 grad_norm=0.367617
Epoch 7/100 Iteration 150/234: loss=0.201250 lr=0.000020 grad_norm=0.477977
Epoch 7/100 Iteration 151/234: loss=0.216902 lr=0.000020 grad_norm=0.467651
Epoch 7/100 Iteration 152/234: loss=0.229623 lr=0.000020 grad_norm=0.410556
Epoch 7/100 Iteration 153/234: loss=0.224985 lr=0.000020 grad_norm=0.582008
Epoch 7/100 Iteration 154/234: loss=0.207476 lr=0.000020 grad_norm=0.418962
Epoch 7/100 Iteration 155/234: loss=0.212840 lr=0.000020 grad_norm=0.613013
Epoch 7/100 Iteration 156/234: loss=0.201107 lr=0.000020 grad_norm=0.623248
Epoch 7/100 Iteration 157/234: loss=0.201403 lr=0.000020 grad_norm=0.325388
Epoch 7/100 Iteration 158/234: loss=0.226718 lr=0.000020 grad_norm=0.707609
Epoch 7/100 Iteration 159/234: loss=0.208097 lr=0.000020 grad_norm=0.796800
Epoch 7/100 Iteration 160/234: loss=0.216890 lr=0.000020 grad_norm=0.445502
Epoch 7/100 Iteration 161/234: loss=0.176152 lr=0.000020 grad_norm=0.304808
Epoch 7/100 Iteration 162/234: loss=0.185493 lr=0.000020 grad_norm=0.366225
Epoch 7/100 Iteration 163/234: loss=0.226466 lr=0.000020 grad_norm=0.322061
Epoch 7/100 Iteration 164/234: loss=0.226650 lr=0.000020 grad_norm=0.292268
Epoch 7/100 Iteration 165/234: loss=0.197172 lr=0.000020 grad_norm=0.308624
Epoch 7/100 Iteration 166/234: loss=0.214083 lr=0.000020 grad_norm=0.382404
Epoch 7/100 Iteration 167/234: loss=0.214052 lr=0.000020 grad_norm=0.541356
Epoch 7/100 Iteration 168/234: loss=0.201575 lr=0.000020 grad_norm=0.345052
Epoch 7/100 Iteration 169/234: loss=0.202452 lr=0.000020 grad_norm=0.555010
Epoch 7/100 Iteration 170/234: loss=0.214742 lr=0.000020 grad_norm=0.669867
Epoch 7/100 Iteration 171/234: loss=0.206997 lr=0.000020 grad_norm=0.367089
Epoch 7/100 Iteration 172/234: loss=0.223833 lr=0.000020 grad_norm=0.498519
Epoch 7/100 Iteration 173/234: loss=0.209637 lr=0.000020 grad_norm=0.546610
Epoch 7/100 Iteration 174/234: loss=0.212532 lr=0.000020 grad_norm=0.526367
Epoch 7/100 Iteration 175/234: loss=0.203596 lr=0.000020 grad_norm=0.436790
Epoch 7/100 Iteration 176/234: loss=0.213090 lr=0.000020 grad_norm=0.445777
Epoch 7/100 Iteration 177/234: loss=0.212490 lr=0.000020 grad_norm=0.321888
Epoch 7/100 Iteration 178/234: loss=0.194130 lr=0.000020 grad_norm=0.418820
Epoch 7/100 Iteration 179/234: loss=0.185410 lr=0.000020 grad_norm=0.393996
Epoch 7/100 Iteration 180/234: loss=0.213468 lr=0.000020 grad_norm=0.482312
Epoch 7/100 Iteration 181/234: loss=0.217825 lr=0.000020 grad_norm=0.398176
Epoch 7/100 Iteration 182/234: loss=0.193040 lr=0.000020 grad_norm=0.669571
Epoch 7/100 Iteration 183/234: loss=0.196574 lr=0.000020 grad_norm=0.749700
Epoch 7/100 Iteration 184/234: loss=0.210019 lr=0.000020 grad_norm=0.698250
Epoch 7/100 Iteration 185/234: loss=0.201612 lr=0.000020 grad_norm=0.578845
Epoch 7/100 Iteration 186/234: loss=0.220630 lr=0.000020 grad_norm=0.491490
Epoch 7/100 Iteration 187/234: loss=0.243759 lr=0.000020 grad_norm=0.504336
Epoch 7/100 Iteration 188/234: loss=0.217420 lr=0.000020 grad_norm=0.342388
Epoch 7/100 Iteration 189/234: loss=0.202481 lr=0.000020 grad_norm=0.551325
Epoch 7/100 Iteration 190/234: loss=0.215969 lr=0.000020 grad_norm=0.416736
Epoch 7/100 Iteration 191/234: loss=0.201059 lr=0.000020 grad_norm=0.533456
Epoch 7/100 Iteration 192/234: loss=0.225874 lr=0.000020 grad_norm=0.563206
Epoch 7/100 Iteration 193/234: loss=0.188578 lr=0.000020 grad_norm=0.454376
Epoch 7/100 Iteration 194/234: loss=0.217914 lr=0.000020 grad_norm=0.550475
Epoch 7/100 Iteration 195/234: loss=0.194410 lr=0.000020 grad_norm=0.553204
Epoch 7/100 Iteration 196/234: loss=0.206075 lr=0.000020 grad_norm=0.647965
Epoch 7/100 Iteration 197/234: loss=0.207724 lr=0.000020 grad_norm=0.565512
Epoch 7/100 Iteration 198/234: loss=0.205370 lr=0.000020 grad_norm=1.015500
Epoch 7/100 Iteration 199/234: loss=0.204337 lr=0.000020 grad_norm=0.737433
Epoch 7/100 Iteration 200/234: loss=0.212938 lr=0.000020 grad_norm=0.487980
Epoch 7/100 Iteration 201/234: loss=0.215538 lr=0.000020 grad_norm=0.798505
Epoch 7/100 Iteration 202/234: loss=0.210108 lr=0.000020 grad_norm=0.665429
Epoch 7/100 Iteration 203/234: loss=0.197250 lr=0.000020 grad_norm=0.576346
Epoch 7/100 Iteration 204/234: loss=0.203046 lr=0.000020 grad_norm=0.551743
Epoch 7/100 Iteration 205/234: loss=0.198815 lr=0.000020 grad_norm=0.533911
Epoch 7/100 Iteration 206/234: loss=0.202182 lr=0.000020 grad_norm=0.631357
Epoch 7/100 Iteration 207/234: loss=0.212353 lr=0.000020 grad_norm=0.629440
Epoch 7/100 Iteration 208/234: loss=0.205672 lr=0.000020 grad_norm=0.669590
Epoch 7/100 Iteration 209/234: loss=0.198486 lr=0.000020 grad_norm=0.566026
Epoch 7/100 Iteration 210/234: loss=0.195250 lr=0.000020 grad_norm=0.758149
Epoch 7/100 Iteration 211/234: loss=0.175045 lr=0.000020 grad_norm=0.554270
Epoch 7/100 Iteration 212/234: loss=0.192222 lr=0.000020 grad_norm=0.722134
Epoch 7/100 Iteration 213/234: loss=0.192044 lr=0.000020 grad_norm=0.878261
Epoch 7/100 Iteration 214/234: loss=0.231039 lr=0.000020 grad_norm=0.354652
Epoch 7/100 Iteration 215/234: loss=0.204637 lr=0.000020 grad_norm=0.882296
Epoch 7/100 Iteration 216/234: loss=0.189844 lr=0.000020 grad_norm=0.897310
Epoch 7/100 Iteration 217/234: loss=0.204102 lr=0.000020 grad_norm=0.460902
Epoch 7/100 Iteration 218/234: loss=0.218875 lr=0.000020 grad_norm=0.952276
Epoch 7/100 Iteration 219/234: loss=0.202173 lr=0.000020 grad_norm=1.145061
Epoch 7/100 Iteration 220/234: loss=0.202641 lr=0.000020 grad_norm=0.369687
Epoch 7/100 Iteration 221/234: loss=0.193218 lr=0.000020 grad_norm=0.777118
Epoch 7/100 Iteration 222/234: loss=0.215832 lr=0.000020 grad_norm=0.641460
Epoch 7/100 Iteration 223/234: loss=0.212990 lr=0.000020 grad_norm=0.552356
Epoch 7/100 Iteration 224/234: loss=0.197280 lr=0.000020 grad_norm=0.798940
Epoch 7/100 Iteration 225/234: loss=0.223999 lr=0.000020 grad_norm=0.390170
Epoch 7/100 Iteration 226/234: loss=0.154038 lr=0.000020 grad_norm=0.688131
Epoch 7/100 Iteration 227/234: loss=0.194768 lr=0.000020 grad_norm=0.416789
Epoch 7/100 Iteration 228/234: loss=0.196648 lr=0.000020 grad_norm=1.079856
Epoch 7/100 Iteration 229/234: loss=0.219684 lr=0.000020 grad_norm=0.883177
Epoch 7/100 Iteration 230/234: loss=0.212986 lr=0.000020 grad_norm=0.442251
Epoch 7/100 Iteration 231/234: loss=0.210947 lr=0.000020 grad_norm=0.749226
Epoch 7/100 Iteration 232/234: loss=0.202764 lr=0.000020 grad_norm=0.358119
Epoch 7/100 Iteration 233/234: loss=0.207564 lr=0.000020 grad_norm=0.494515
Epoch 7/100 Iteration 234/234: loss=0.197030 lr=0.000020 grad_norm=0.359479
Epoch 7/100 finished. Avg Loss: 0.210283
Epoch 8/100 Iteration 1/234: loss=0.188327 lr=0.000020 grad_norm=0.456239
Epoch 8/100 Iteration 2/234: loss=0.211782 lr=0.000020 grad_norm=0.395349
Epoch 8/100 Iteration 3/234: loss=0.221751 lr=0.000020 grad_norm=0.565484
Epoch 8/100 Iteration 4/234: loss=0.193988 lr=0.000020 grad_norm=0.469548
Epoch 8/100 Iteration 5/234: loss=0.206379 lr=0.000020 grad_norm=0.484132
Epoch 8/100 Iteration 6/234: loss=0.199850 lr=0.000020 grad_norm=0.418429
Epoch 8/100 Iteration 7/234: loss=0.221138 lr=0.000020 grad_norm=0.533708
Epoch 8/100 Iteration 8/234: loss=0.189009 lr=0.000020 grad_norm=0.594717
Epoch 8/100 Iteration 9/234: loss=0.194018 lr=0.000020 grad_norm=0.504996
Epoch 8/100 Iteration 10/234: loss=0.216705 lr=0.000020 grad_norm=0.458066
Epoch 8/100 Iteration 11/234: loss=0.206003 lr=0.000020 grad_norm=0.509678
Epoch 8/100 Iteration 12/234: loss=0.196769 lr=0.000020 grad_norm=0.413139
Epoch 8/100 Iteration 13/234: loss=0.203795 lr=0.000020 grad_norm=0.414207
Epoch 8/100 Iteration 14/234: loss=0.210554 lr=0.000020 grad_norm=0.859673
Epoch 8/100 Iteration 15/234: loss=0.204325 lr=0.000020 grad_norm=1.111746
Epoch 8/100 Iteration 16/234: loss=0.210527 lr=0.000020 grad_norm=0.503692
Epoch 8/100 Iteration 17/234: loss=0.195134 lr=0.000020 grad_norm=0.977026
Epoch 8/100 Iteration 18/234: loss=0.202800 lr=0.000020 grad_norm=0.853258
Epoch 8/100 Iteration 19/234: loss=0.193596 lr=0.000020 grad_norm=0.575850
Epoch 8/100 Iteration 20/234: loss=0.193592 lr=0.000020 grad_norm=0.523345
Epoch 8/100 Iteration 21/234: loss=0.192101 lr=0.000020 grad_norm=0.545543
Epoch 8/100 Iteration 22/234: loss=0.205530 lr=0.000020 grad_norm=0.512066
Epoch 8/100 Iteration 23/234: loss=0.204386 lr=0.000020 grad_norm=0.405449
Epoch 8/100 Iteration 24/234: loss=0.201791 lr=0.000020 grad_norm=0.498700
Epoch 8/100 Iteration 25/234: loss=0.202514 lr=0.000020 grad_norm=0.486297
Epoch 8/100 Iteration 26/234: loss=0.203655 lr=0.000020 grad_norm=0.340508
Epoch 8/100 Iteration 27/234: loss=0.209008 lr=0.000020 grad_norm=0.420977
Epoch 8/100 Iteration 28/234: loss=0.207939 lr=0.000020 grad_norm=0.454646
Epoch 8/100 Iteration 29/234: loss=0.199577 lr=0.000020 grad_norm=0.510183
Epoch 8/100 Iteration 30/234: loss=0.217845 lr=0.000020 grad_norm=0.364704
Epoch 8/100 Iteration 31/234: loss=0.207237 lr=0.000020 grad_norm=0.474350
Epoch 8/100 Iteration 32/234: loss=0.210873 lr=0.000020 grad_norm=0.340668
Epoch 8/100 Iteration 33/234: loss=0.193352 lr=0.000020 grad_norm=0.526111
Epoch 8/100 Iteration 34/234: loss=0.194624 lr=0.000020 grad_norm=0.401296
Epoch 8/100 Iteration 35/234: loss=0.216689 lr=0.000020 grad_norm=0.511746
Epoch 8/100 Iteration 36/234: loss=0.205753 lr=0.000020 grad_norm=0.325835
Epoch 8/100 Iteration 37/234: loss=0.190163 lr=0.000020 grad_norm=0.427819
Epoch 8/100 Iteration 38/234: loss=0.197282 lr=0.000020 grad_norm=0.350404
Epoch 8/100 Iteration 39/234: loss=0.214603 lr=0.000020 grad_norm=0.362213
Epoch 8/100 Iteration 40/234: loss=0.180785 lr=0.000020 grad_norm=0.359779
Epoch 8/100 Iteration 41/234: loss=0.214807 lr=0.000020 grad_norm=0.471647
Epoch 8/100 Iteration 42/234: loss=0.196258 lr=0.000020 grad_norm=0.732011
Epoch 8/100 Iteration 43/234: loss=0.223189 lr=0.000020 grad_norm=0.580874
Epoch 8/100 Iteration 44/234: loss=0.199619 lr=0.000020 grad_norm=0.402460
Epoch 8/100 Iteration 45/234: loss=0.198109 lr=0.000020 grad_norm=0.334735
Epoch 8/100 Iteration 46/234: loss=0.190936 lr=0.000020 grad_norm=0.422978
Epoch 8/100 Iteration 47/234: loss=0.213624 lr=0.000020 grad_norm=0.444777
Epoch 8/100 Iteration 48/234: loss=0.211827 lr=0.000020 grad_norm=0.362557
Epoch 8/100 Iteration 49/234: loss=0.180378 lr=0.000020 grad_norm=0.498421
Epoch 8/100 Iteration 50/234: loss=0.224694 lr=0.000020 grad_norm=0.538730
Epoch 8/100 Iteration 51/234: loss=0.197299 lr=0.000020 grad_norm=0.614756
Epoch 8/100 Iteration 52/234: loss=0.200058 lr=0.000020 grad_norm=0.481204
Epoch 8/100 Iteration 53/234: loss=0.207806 lr=0.000020 grad_norm=0.781345
Epoch 8/100 Iteration 54/234: loss=0.203465 lr=0.000020 grad_norm=0.579780
Epoch 8/100 Iteration 55/234: loss=0.198181 lr=0.000020 grad_norm=0.452665
Epoch 8/100 Iteration 56/234: loss=0.187167 lr=0.000020 grad_norm=0.453090
Epoch 8/100 Iteration 57/234: loss=0.191458 lr=0.000020 grad_norm=0.468553
Epoch 8/100 Iteration 58/234: loss=0.196799 lr=0.000020 grad_norm=0.473650
Epoch 8/100 Iteration 59/234: loss=0.217722 lr=0.000020 grad_norm=0.522441
Epoch 8/100 Iteration 60/234: loss=0.193303 lr=0.000020 grad_norm=0.423970
Epoch 8/100 Iteration 61/234: loss=0.205503 lr=0.000020 grad_norm=0.682760
Epoch 8/100 Iteration 62/234: loss=0.210432 lr=0.000020 grad_norm=0.447102
Epoch 8/100 Iteration 63/234: loss=0.205518 lr=0.000020 grad_norm=0.522006
Epoch 8/100 Iteration 64/234: loss=0.209991 lr=0.000020 grad_norm=0.598610
Epoch 8/100 Iteration 65/234: loss=0.206844 lr=0.000020 grad_norm=0.343541
Epoch 8/100 Iteration 66/234: loss=0.192513 lr=0.000020 grad_norm=0.534379
Epoch 8/100 Iteration 67/234: loss=0.199035 lr=0.000020 grad_norm=0.446560
Epoch 8/100 Iteration 68/234: loss=0.203699 lr=0.000020 grad_norm=0.466498
Epoch 8/100 Iteration 69/234: loss=0.196576 lr=0.000020 grad_norm=0.546517
Epoch 8/100 Iteration 70/234: loss=0.224658 lr=0.000020 grad_norm=0.729430
Epoch 8/100 Iteration 71/234: loss=0.181214 lr=0.000020 grad_norm=0.648824
Epoch 8/100 Iteration 72/234: loss=0.213479 lr=0.000020 grad_norm=0.315870
Epoch 8/100 Iteration 73/234: loss=0.189124 lr=0.000020 grad_norm=0.932822
Epoch 8/100 Iteration 74/234: loss=0.213054 lr=0.000020 grad_norm=1.039269
Epoch 8/100 Iteration 75/234: loss=0.189849 lr=0.000020 grad_norm=0.714826
Epoch 8/100 Iteration 76/234: loss=0.204430 lr=0.000020 grad_norm=0.493514
Epoch 8/100 Iteration 77/234: loss=0.203342 lr=0.000020 grad_norm=0.721385
Epoch 8/100 Iteration 78/234: loss=0.206731 lr=0.000020 grad_norm=0.677870
Epoch 8/100 Iteration 79/234: loss=0.203174 lr=0.000020 grad_norm=0.520595
Epoch 8/100 Iteration 80/234: loss=0.195181 lr=0.000020 grad_norm=0.390826
Epoch 8/100 Iteration 81/234: loss=0.197412 lr=0.000020 grad_norm=0.532162
Epoch 8/100 Iteration 82/234: loss=0.220416 lr=0.000020 grad_norm=0.524057
Epoch 8/100 Iteration 83/234: loss=0.205382 lr=0.000020 grad_norm=0.730546
Epoch 8/100 Iteration 84/234: loss=0.191873 lr=0.000020 grad_norm=0.589808
Epoch 8/100 Iteration 85/234: loss=0.211455 lr=0.000020 grad_norm=0.560795
Epoch 8/100 Iteration 86/234: loss=0.216186 lr=0.000020 grad_norm=0.472049
Epoch 8/100 Iteration 87/234: loss=0.182550 lr=0.000020 grad_norm=0.772182
Epoch 8/100 Iteration 88/234: loss=0.205948 lr=0.000020 grad_norm=0.644126
Epoch 8/100 Iteration 89/234: loss=0.198410 lr=0.000020 grad_norm=0.495340
Epoch 8/100 Iteration 90/234: loss=0.188349 lr=0.000020 grad_norm=0.699276
Epoch 8/100 Iteration 91/234: loss=0.188658 lr=0.000020 grad_norm=0.415436
Epoch 8/100 Iteration 92/234: loss=0.190996 lr=0.000020 grad_norm=0.923359
Epoch 8/100 Iteration 93/234: loss=0.204049 lr=0.000020 grad_norm=0.863267
Epoch 8/100 Iteration 94/234: loss=0.205583 lr=0.000020 grad_norm=0.533995
Epoch 8/100 Iteration 95/234: loss=0.197146 lr=0.000020 grad_norm=1.316352
Epoch 8/100 Iteration 96/234: loss=0.193869 lr=0.000020 grad_norm=0.897020
Epoch 8/100 Iteration 97/234: loss=0.194172 lr=0.000020 grad_norm=0.472167
Epoch 8/100 Iteration 98/234: loss=0.196981 lr=0.000020 grad_norm=1.064353
Epoch 8/100 Iteration 99/234: loss=0.213555 lr=0.000020 grad_norm=0.546040
Epoch 8/100 Iteration 100/234: loss=0.193796 lr=0.000020 grad_norm=0.600034
Epoch 8/100 Iteration 101/234: loss=0.185857 lr=0.000020 grad_norm=0.603676
Epoch 8/100 Iteration 102/234: loss=0.203388 lr=0.000020 grad_norm=0.495651
Epoch 8/100 Iteration 103/234: loss=0.208939 lr=0.000020 grad_norm=0.703351
Epoch 8/100 Iteration 104/234: loss=0.209198 lr=0.000020 grad_norm=0.437648
Epoch 8/100 Iteration 105/234: loss=0.207803 lr=0.000020 grad_norm=0.692764
Epoch 8/100 Iteration 106/234: loss=0.211261 lr=0.000020 grad_norm=0.673937
Epoch 8/100 Iteration 107/234: loss=0.197294 lr=0.000020 grad_norm=0.400451
Epoch 8/100 Iteration 108/234: loss=0.214883 lr=0.000020 grad_norm=0.840503
Epoch 8/100 Iteration 109/234: loss=0.206392 lr=0.000020 grad_norm=0.538788
Epoch 8/100 Iteration 110/234: loss=0.171437 lr=0.000020 grad_norm=0.560801
Epoch 8/100 Iteration 111/234: loss=0.205891 lr=0.000020 grad_norm=0.676730
Epoch 8/100 Iteration 112/234: loss=0.205742 lr=0.000020 grad_norm=0.578439
Epoch 8/100 Iteration 113/234: loss=0.192974 lr=0.000020 grad_norm=0.805612
Epoch 8/100 Iteration 114/234: loss=0.213740 lr=0.000020 grad_norm=0.643939
Epoch 8/100 Iteration 115/234: loss=0.204187 lr=0.000020 grad_norm=0.800042
Epoch 8/100 Iteration 116/234: loss=0.214773 lr=0.000020 grad_norm=1.136802
Epoch 8/100 Iteration 117/234: loss=0.192180 lr=0.000020 grad_norm=0.421469
Epoch 8/100 Iteration 118/234: loss=0.212773 lr=0.000020 grad_norm=0.902194
Epoch 8/100 Iteration 119/234: loss=0.204964 lr=0.000020 grad_norm=0.876554
Epoch 8/100 Iteration 120/234: loss=0.197310 lr=0.000020 grad_norm=0.507419
Epoch 8/100 Iteration 121/234: loss=0.193637 lr=0.000020 grad_norm=0.633919
Epoch 8/100 Iteration 122/234: loss=0.200391 lr=0.000020 grad_norm=0.528454
Epoch 8/100 Iteration 123/234: loss=0.204020 lr=0.000020 grad_norm=0.509075
Epoch 8/100 Iteration 124/234: loss=0.190719 lr=0.000020 grad_norm=0.463181
Epoch 8/100 Iteration 125/234: loss=0.186857 lr=0.000020 grad_norm=0.462520
Epoch 8/100 Iteration 126/234: loss=0.175500 lr=0.000020 grad_norm=0.749020
Epoch 8/100 Iteration 127/234: loss=0.208483 lr=0.000020 grad_norm=0.738235
Epoch 8/100 Iteration 128/234: loss=0.198645 lr=0.000020 grad_norm=0.374318
Epoch 8/100 Iteration 129/234: loss=0.199120 lr=0.000020 grad_norm=0.775067
Epoch 8/100 Iteration 130/234: loss=0.196356 lr=0.000020 grad_norm=0.394190
Epoch 8/100 Iteration 131/234: loss=0.214820 lr=0.000020 grad_norm=0.613575
Epoch 8/100 Iteration 132/234: loss=0.208611 lr=0.000020 grad_norm=0.459884
Epoch 8/100 Iteration 133/234: loss=0.193725 lr=0.000020 grad_norm=0.603076
Epoch 8/100 Iteration 134/234: loss=0.203171 lr=0.000020 grad_norm=0.881951
Epoch 8/100 Iteration 135/234: loss=0.188117 lr=0.000020 grad_norm=0.426444
Epoch 8/100 Iteration 136/234: loss=0.208923 lr=0.000020 grad_norm=0.612031
Epoch 8/100 Iteration 137/234: loss=0.202600 lr=0.000020 grad_norm=0.475609
Epoch 8/100 Iteration 138/234: loss=0.210621 lr=0.000020 grad_norm=0.510396
Epoch 8/100 Iteration 139/234: loss=0.197310 lr=0.000020 grad_norm=0.541354
Epoch 8/100 Iteration 140/234: loss=0.197674 lr=0.000020 grad_norm=0.363302
Epoch 8/100 Iteration 141/234: loss=0.209151 lr=0.000020 grad_norm=0.551355
Epoch 8/100 Iteration 142/234: loss=0.198347 lr=0.000020 grad_norm=0.595846
Epoch 8/100 Iteration 143/234: loss=0.213632 lr=0.000020 grad_norm=0.348920
Epoch 8/100 Iteration 144/234: loss=0.187318 lr=0.000020 grad_norm=0.392448
Epoch 8/100 Iteration 145/234: loss=0.190612 lr=0.000020 grad_norm=0.520407
Epoch 8/100 Iteration 146/234: loss=0.201475 lr=0.000020 grad_norm=0.342858
Epoch 8/100 Iteration 147/234: loss=0.210989 lr=0.000020 grad_norm=0.542112
Epoch 8/100 Iteration 148/234: loss=0.213041 lr=0.000020 grad_norm=0.672039
Epoch 8/100 Iteration 149/234: loss=0.207673 lr=0.000020 grad_norm=0.304758
Epoch 8/100 Iteration 150/234: loss=0.184482 lr=0.000020 grad_norm=0.526548
Epoch 8/100 Iteration 151/234: loss=0.205831 lr=0.000020 grad_norm=0.330920
Epoch 8/100 Iteration 152/234: loss=0.191891 lr=0.000020 grad_norm=0.309006
Epoch 8/100 Iteration 153/234: loss=0.191845 lr=0.000020 grad_norm=0.278709
Epoch 8/100 Iteration 154/234: loss=0.206103 lr=0.000020 grad_norm=0.290574
Epoch 8/100 Iteration 155/234: loss=0.199120 lr=0.000020 grad_norm=0.433434
Epoch 8/100 Iteration 156/234: loss=0.205937 lr=0.000020 grad_norm=0.474774
Epoch 8/100 Iteration 157/234: loss=0.202032 lr=0.000020 grad_norm=0.332037
Epoch 8/100 Iteration 158/234: loss=0.197835 lr=0.000020 grad_norm=0.386483
Epoch 8/100 Iteration 159/234: loss=0.200401 lr=0.000020 grad_norm=0.420123
Epoch 8/100 Iteration 160/234: loss=0.175499 lr=0.000020 grad_norm=0.444918
Epoch 8/100 Iteration 161/234: loss=0.212733 lr=0.000020 grad_norm=0.577557
Epoch 8/100 Iteration 162/234: loss=0.189296 lr=0.000020 grad_norm=0.358425
Epoch 8/100 Iteration 163/234: loss=0.202890 lr=0.000020 grad_norm=0.346137
Epoch 8/100 Iteration 164/234: loss=0.188735 lr=0.000020 grad_norm=0.494225
Epoch 8/100 Iteration 165/234: loss=0.178586 lr=0.000020 grad_norm=0.754505
Epoch 8/100 Iteration 166/234: loss=0.211923 lr=0.000020 grad_norm=0.661578
Epoch 8/100 Iteration 167/234: loss=0.195509 lr=0.000020 grad_norm=0.444314
Epoch 8/100 Iteration 168/234: loss=0.197788 lr=0.000020 grad_norm=1.077487
Epoch 8/100 Iteration 169/234: loss=0.195666 lr=0.000020 grad_norm=0.755022
Epoch 8/100 Iteration 170/234: loss=0.201031 lr=0.000020 grad_norm=0.422099
Epoch 8/100 Iteration 171/234: loss=0.195157 lr=0.000020 grad_norm=0.503422
Epoch 8/100 Iteration 172/234: loss=0.210008 lr=0.000020 grad_norm=0.409129
Epoch 8/100 Iteration 173/234: loss=0.191671 lr=0.000020 grad_norm=0.491113
Epoch 8/100 Iteration 174/234: loss=0.196449 lr=0.000020 grad_norm=0.352081
Epoch 8/100 Iteration 175/234: loss=0.196823 lr=0.000020 grad_norm=0.325195
Epoch 8/100 Iteration 176/234: loss=0.207370 lr=0.000020 grad_norm=0.382993
Epoch 8/100 Iteration 177/234: loss=0.187970 lr=0.000020 grad_norm=0.443122
Epoch 8/100 Iteration 178/234: loss=0.198678 lr=0.000020 grad_norm=0.408337
Epoch 8/100 Iteration 179/234: loss=0.198411 lr=0.000020 grad_norm=0.414740
Epoch 8/100 Iteration 180/234: loss=0.194886 lr=0.000020 grad_norm=0.332261
Epoch 8/100 Iteration 181/234: loss=0.197384 lr=0.000020 grad_norm=0.493312
Epoch 8/100 Iteration 182/234: loss=0.178476 lr=0.000020 grad_norm=0.419115
Epoch 8/100 Iteration 183/234: loss=0.208677 lr=0.000020 grad_norm=0.282424
Epoch 8/100 Iteration 184/234: loss=0.193030 lr=0.000020 grad_norm=0.425106
Epoch 8/100 Iteration 185/234: loss=0.210219 lr=0.000020 grad_norm=0.302584
Epoch 8/100 Iteration 186/234: loss=0.192885 lr=0.000020 grad_norm=0.264600
Epoch 8/100 Iteration 187/234: loss=0.188345 lr=0.000020 grad_norm=0.416274
Epoch 8/100 Iteration 188/234: loss=0.203408 lr=0.000020 grad_norm=0.378068
Epoch 8/100 Iteration 189/234: loss=0.194802 lr=0.000020 grad_norm=0.366575
Epoch 8/100 Iteration 190/234: loss=0.195165 lr=0.000020 grad_norm=0.399833
Epoch 8/100 Iteration 191/234: loss=0.192702 lr=0.000020 grad_norm=0.633134
Epoch 8/100 Iteration 192/234: loss=0.196099 lr=0.000020 grad_norm=0.729001
Epoch 8/100 Iteration 193/234: loss=0.209093 lr=0.000020 grad_norm=0.551902
Epoch 8/100 Iteration 194/234: loss=0.173274 lr=0.000020 grad_norm=0.545971
Epoch 8/100 Iteration 195/234: loss=0.183045 lr=0.000020 grad_norm=0.527061
Epoch 8/100 Iteration 196/234: loss=0.183992 lr=0.000020 grad_norm=0.355238
Epoch 8/100 Iteration 197/234: loss=0.201134 lr=0.000020 grad_norm=0.466317
Epoch 8/100 Iteration 198/234: loss=0.183260 lr=0.000020 grad_norm=0.473635
Epoch 8/100 Iteration 199/234: loss=0.224878 lr=0.000020 grad_norm=0.444413
Epoch 8/100 Iteration 200/234: loss=0.197170 lr=0.000020 grad_norm=0.571969
Epoch 8/100 Iteration 201/234: loss=0.205943 lr=0.000020 grad_norm=0.885296
Epoch 8/100 Iteration 202/234: loss=0.194840 lr=0.000020 grad_norm=0.633478
Epoch 8/100 Iteration 203/234: loss=0.172365 lr=0.000020 grad_norm=0.459563
Epoch 8/100 Iteration 204/234: loss=0.196945 lr=0.000020 grad_norm=0.471969
Epoch 8/100 Iteration 205/234: loss=0.200615 lr=0.000020 grad_norm=0.722949
Epoch 8/100 Iteration 206/234: loss=0.189399 lr=0.000020 grad_norm=0.710304
Epoch 8/100 Iteration 207/234: loss=0.199360 lr=0.000020 grad_norm=0.398454
Epoch 8/100 Iteration 208/234: loss=0.217158 lr=0.000020 grad_norm=0.368645
Epoch 8/100 Iteration 209/234: loss=0.212367 lr=0.000020 grad_norm=0.575679
Epoch 8/100 Iteration 210/234: loss=0.188924 lr=0.000020 grad_norm=0.498402
Epoch 8/100 Iteration 211/234: loss=0.203100 lr=0.000020 grad_norm=0.342586
Epoch 8/100 Iteration 212/234: loss=0.186811 lr=0.000020 grad_norm=0.681373
Epoch 8/100 Iteration 213/234: loss=0.179190 lr=0.000020 grad_norm=0.523301
Epoch 8/100 Iteration 214/234: loss=0.215606 lr=0.000020 grad_norm=0.507129
Epoch 8/100 Iteration 215/234: loss=0.188986 lr=0.000020 grad_norm=1.024109
Epoch 8/100 Iteration 216/234: loss=0.209391 lr=0.000020 grad_norm=0.739592
Epoch 8/100 Iteration 217/234: loss=0.204136 lr=0.000020 grad_norm=0.705407
Epoch 8/100 Iteration 218/234: loss=0.200454 lr=0.000020 grad_norm=1.520937
Epoch 8/100 Iteration 219/234: loss=0.196248 lr=0.000020 grad_norm=1.086370
Epoch 8/100 Iteration 220/234: loss=0.198285 lr=0.000020 grad_norm=0.577503
Epoch 8/100 Iteration 221/234: loss=0.193561 lr=0.000020 grad_norm=0.703303
Epoch 8/100 Iteration 222/234: loss=0.193729 lr=0.000020 grad_norm=0.605227
Epoch 8/100 Iteration 223/234: loss=0.196623 lr=0.000020 grad_norm=0.951959
Epoch 8/100 Iteration 224/234: loss=0.192047 lr=0.000020 grad_norm=0.495595
Epoch 8/100 Iteration 225/234: loss=0.194245 lr=0.000020 grad_norm=0.882339
Epoch 8/100 Iteration 226/234: loss=0.183804 lr=0.000020 grad_norm=0.616313
Epoch 8/100 Iteration 227/234: loss=0.189169 lr=0.000020 grad_norm=0.666037
Epoch 8/100 Iteration 228/234: loss=0.211706 lr=0.000020 grad_norm=1.302676
Epoch 8/100 Iteration 229/234: loss=0.190975 lr=0.000020 grad_norm=0.849666
Epoch 8/100 Iteration 230/234: loss=0.186995 lr=0.000020 grad_norm=0.696303
Epoch 8/100 Iteration 231/234: loss=0.196640 lr=0.000020 grad_norm=0.632763
Epoch 8/100 Iteration 232/234: loss=0.202557 lr=0.000020 grad_norm=0.473672
Epoch 8/100 Iteration 233/234: loss=0.194862 lr=0.000020 grad_norm=0.689782
Epoch 8/100 Iteration 234/234: loss=0.198969 lr=0.000020 grad_norm=0.465324
Epoch 8/100 finished. Avg Loss: 0.199965
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 9/100 Iteration 1/234: loss=0.199879 lr=0.000020 grad_norm=0.515917
Epoch 9/100 Iteration 2/234: loss=0.204964 lr=0.000020 grad_norm=0.669252
Epoch 9/100 Iteration 3/234: loss=0.197605 lr=0.000020 grad_norm=0.385477
Epoch 9/100 Iteration 4/234: loss=0.180598 lr=0.000020 grad_norm=0.651608
Epoch 9/100 Iteration 5/234: loss=0.196477 lr=0.000020 grad_norm=0.751569
Epoch 9/100 Iteration 6/234: loss=0.185097 lr=0.000020 grad_norm=0.346239
Epoch 9/100 Iteration 7/234: loss=0.191598 lr=0.000020 grad_norm=0.442210
Epoch 9/100 Iteration 8/234: loss=0.197695 lr=0.000020 grad_norm=0.556251
Epoch 9/100 Iteration 9/234: loss=0.209719 lr=0.000020 grad_norm=0.439253
Epoch 9/100 Iteration 10/234: loss=0.204525 lr=0.000020 grad_norm=0.362542
Epoch 9/100 Iteration 11/234: loss=0.193479 lr=0.000020 grad_norm=0.294226
Epoch 9/100 Iteration 12/234: loss=0.203891 lr=0.000020 grad_norm=0.388069
Epoch 9/100 Iteration 13/234: loss=0.176887 lr=0.000020 grad_norm=0.281447
Epoch 9/100 Iteration 14/234: loss=0.195121 lr=0.000020 grad_norm=0.431552
Epoch 9/100 Iteration 15/234: loss=0.199484 lr=0.000020 grad_norm=0.379770
Epoch 9/100 Iteration 16/234: loss=0.198723 lr=0.000020 grad_norm=0.513798
Epoch 9/100 Iteration 17/234: loss=0.190190 lr=0.000020 grad_norm=0.413243
Epoch 9/100 Iteration 18/234: loss=0.197016 lr=0.000020 grad_norm=0.631708
Epoch 9/100 Iteration 19/234: loss=0.181065 lr=0.000020 grad_norm=0.484061
Epoch 9/100 Iteration 20/234: loss=0.168701 lr=0.000020 grad_norm=0.444786
Epoch 9/100 Iteration 21/234: loss=0.187752 lr=0.000020 grad_norm=0.400755
Epoch 9/100 Iteration 22/234: loss=0.201513 lr=0.000020 grad_norm=0.496360
Epoch 9/100 Iteration 23/234: loss=0.197124 lr=0.000020 grad_norm=0.648412
Epoch 9/100 Iteration 24/234: loss=0.181356 lr=0.000020 grad_norm=0.559817
Epoch 9/100 Iteration 25/234: loss=0.179427 lr=0.000020 grad_norm=0.328387
Epoch 9/100 Iteration 26/234: loss=0.201172 lr=0.000020 grad_norm=0.610241
Epoch 9/100 Iteration 27/234: loss=0.194261 lr=0.000020 grad_norm=0.360653
Epoch 9/100 Iteration 28/234: loss=0.204280 lr=0.000020 grad_norm=0.522089
Epoch 9/100 Iteration 29/234: loss=0.182780 lr=0.000020 grad_norm=0.618947
Epoch 9/100 Iteration 30/234: loss=0.204263 lr=0.000020 grad_norm=0.505087
Epoch 9/100 Iteration 31/234: loss=0.186389 lr=0.000020 grad_norm=0.697041
Epoch 9/100 Iteration 32/234: loss=0.195433 lr=0.000020 grad_norm=0.762931
Epoch 9/100 Iteration 33/234: loss=0.178607 lr=0.000020 grad_norm=0.437212
Epoch 9/100 Iteration 34/234: loss=0.203277 lr=0.000020 grad_norm=0.650159
Epoch 9/100 Iteration 35/234: loss=0.179965 lr=0.000020 grad_norm=0.774957
Epoch 9/100 Iteration 36/234: loss=0.193852 lr=0.000020 grad_norm=0.430623
Epoch 9/100 Iteration 37/234: loss=0.196862 lr=0.000020 grad_norm=0.745846
Epoch 9/100 Iteration 38/234: loss=0.171469 lr=0.000020 grad_norm=0.960437
Epoch 9/100 Iteration 39/234: loss=0.198586 lr=0.000020 grad_norm=0.928020
Epoch 9/100 Iteration 40/234: loss=0.198194 lr=0.000020 grad_norm=0.694483
Epoch 9/100 Iteration 41/234: loss=0.206810 lr=0.000020 grad_norm=0.435480
Epoch 9/100 Iteration 42/234: loss=0.187149 lr=0.000020 grad_norm=0.742769
Epoch 9/100 Iteration 43/234: loss=0.201667 lr=0.000020 grad_norm=1.178746
Epoch 9/100 Iteration 44/234: loss=0.191906 lr=0.000020 grad_norm=0.753762
Epoch 9/100 Iteration 45/234: loss=0.181539 lr=0.000020 grad_norm=0.732096
Epoch 9/100 Iteration 46/234: loss=0.188543 lr=0.000020 grad_norm=0.645906
Epoch 9/100 Iteration 47/234: loss=0.208517 lr=0.000020 grad_norm=0.854991
Epoch 9/100 Iteration 48/234: loss=0.178584 lr=0.000020 grad_norm=0.672295
Epoch 9/100 Iteration 49/234: loss=0.190277 lr=0.000020 grad_norm=0.438667
Epoch 9/100 Iteration 50/234: loss=0.187719 lr=0.000020 grad_norm=0.586920
Epoch 9/100 Iteration 51/234: loss=0.205877 lr=0.000020 grad_norm=0.739984
Epoch 9/100 Iteration 52/234: loss=0.188567 lr=0.000020 grad_norm=0.362186
Epoch 9/100 Iteration 53/234: loss=0.178353 lr=0.000020 grad_norm=0.573207
Epoch 9/100 Iteration 54/234: loss=0.190136 lr=0.000020 grad_norm=0.521967
Epoch 9/100 Iteration 55/234: loss=0.191999 lr=0.000020 grad_norm=0.410163
Epoch 9/100 Iteration 56/234: loss=0.189883 lr=0.000020 grad_norm=0.434064
Epoch 9/100 Iteration 57/234: loss=0.192307 lr=0.000020 grad_norm=0.376968
Epoch 9/100 Iteration 58/234: loss=0.185042 lr=0.000020 grad_norm=0.527508
Epoch 9/100 Iteration 59/234: loss=0.164617 lr=0.000020 grad_norm=0.551437
Epoch 9/100 Iteration 60/234: loss=0.200943 lr=0.000020 grad_norm=0.383280
Epoch 9/100 Iteration 61/234: loss=0.184643 lr=0.000020 grad_norm=0.516249
Epoch 9/100 Iteration 62/234: loss=0.187030 lr=0.000020 grad_norm=0.432885
Epoch 9/100 Iteration 63/234: loss=0.186305 lr=0.000020 grad_norm=0.380499
Epoch 9/100 Iteration 64/234: loss=0.194413 lr=0.000020 grad_norm=0.451816
Epoch 9/100 Iteration 65/234: loss=0.190037 lr=0.000020 grad_norm=0.559537
Epoch 9/100 Iteration 66/234: loss=0.189237 lr=0.000020 grad_norm=0.463653
Epoch 9/100 Iteration 67/234: loss=0.210307 lr=0.000020 grad_norm=0.309859
Epoch 9/100 Iteration 68/234: loss=0.175309 lr=0.000020 grad_norm=0.415297
Epoch 9/100 Iteration 69/234: loss=0.177246 lr=0.000020 grad_norm=0.602873
Epoch 9/100 Iteration 70/234: loss=0.191351 lr=0.000020 grad_norm=0.596360
Epoch 9/100 Iteration 71/234: loss=0.187505 lr=0.000020 grad_norm=0.440173
Epoch 9/100 Iteration 72/234: loss=0.200029 lr=0.000020 grad_norm=0.657825
Epoch 9/100 Iteration 73/234: loss=0.169661 lr=0.000020 grad_norm=0.711666
Epoch 9/100 Iteration 74/234: loss=0.215974 lr=0.000020 grad_norm=0.683552
Epoch 9/100 Iteration 75/234: loss=0.208090 lr=0.000020 grad_norm=0.861451
Epoch 9/100 Iteration 76/234: loss=0.166897 lr=0.000020 grad_norm=0.610661
Epoch 9/100 Iteration 77/234: loss=0.192415 lr=0.000020 grad_norm=0.861526
Epoch 9/100 Iteration 78/234: loss=0.194637 lr=0.000020 grad_norm=1.220869
Epoch 9/100 Iteration 79/234: loss=0.197403 lr=0.000020 grad_norm=0.487512
Epoch 9/100 Iteration 80/234: loss=0.202292 lr=0.000020 grad_norm=1.140087
Epoch 9/100 Iteration 81/234: loss=0.210346 lr=0.000020 grad_norm=1.236986
Epoch 9/100 Iteration 82/234: loss=0.197582 lr=0.000020 grad_norm=0.463495
Epoch 9/100 Iteration 83/234: loss=0.190811 lr=0.000020 grad_norm=0.649171
Epoch 9/100 Iteration 84/234: loss=0.201849 lr=0.000020 grad_norm=0.380764
Epoch 9/100 Iteration 85/234: loss=0.190172 lr=0.000020 grad_norm=0.551732
Epoch 9/100 Iteration 86/234: loss=0.192032 lr=0.000020 grad_norm=0.482013
Epoch 9/100 Iteration 87/234: loss=0.168507 lr=0.000020 grad_norm=0.465317
Epoch 9/100 Iteration 88/234: loss=0.209705 lr=0.000020 grad_norm=0.554986
Epoch 9/100 Iteration 89/234: loss=0.178306 lr=0.000020 grad_norm=0.348797
Epoch 9/100 Iteration 90/234: loss=0.189435 lr=0.000020 grad_norm=0.434398
Epoch 9/100 Iteration 91/234: loss=0.194880 lr=0.000020 grad_norm=0.382564
Epoch 9/100 Iteration 92/234: loss=0.169813 lr=0.000020 grad_norm=0.361405
Epoch 9/100 Iteration 93/234: loss=0.215856 lr=0.000020 grad_norm=0.549460
Epoch 9/100 Iteration 94/234: loss=0.200731 lr=0.000020 grad_norm=0.909472
Epoch 9/100 Iteration 95/234: loss=0.193607 lr=0.000020 grad_norm=0.694588
Epoch 9/100 Iteration 96/234: loss=0.201636 lr=0.000020 grad_norm=0.492595
Epoch 9/100 Iteration 97/234: loss=0.185457 lr=0.000020 grad_norm=0.701886
Epoch 9/100 Iteration 98/234: loss=0.182518 lr=0.000020 grad_norm=0.458533
Epoch 9/100 Iteration 99/234: loss=0.196929 lr=0.000020 grad_norm=0.504183
Epoch 9/100 Iteration 100/234: loss=0.205152 lr=0.000020 grad_norm=0.363123
Epoch 9/100 Iteration 101/234: loss=0.206110 lr=0.000020 grad_norm=0.473998
Epoch 9/100 Iteration 102/234: loss=0.178531 lr=0.000020 grad_norm=0.490241
Epoch 9/100 Iteration 103/234: loss=0.198496 lr=0.000020 grad_norm=0.448144
Epoch 9/100 Iteration 104/234: loss=0.175640 lr=0.000020 grad_norm=0.656310
Epoch 9/100 Iteration 105/234: loss=0.222421 lr=0.000020 grad_norm=0.619167
Epoch 9/100 Iteration 106/234: loss=0.192891 lr=0.000020 grad_norm=0.671751
Epoch 9/100 Iteration 107/234: loss=0.195705 lr=0.000020 grad_norm=0.485509
Epoch 9/100 Iteration 108/234: loss=0.189066 lr=0.000020 grad_norm=0.944497
Epoch 9/100 Iteration 109/234: loss=0.175194 lr=0.000020 grad_norm=0.381391
Epoch 9/100 Iteration 110/234: loss=0.207575 lr=0.000020 grad_norm=0.860710
Epoch 9/100 Iteration 111/234: loss=0.195229 lr=0.000020 grad_norm=0.728597
Epoch 9/100 Iteration 112/234: loss=0.194949 lr=0.000020 grad_norm=0.630483
Epoch 9/100 Iteration 113/234: loss=0.194877 lr=0.000020 grad_norm=0.552364
Epoch 9/100 Iteration 114/234: loss=0.180709 lr=0.000020 grad_norm=0.367025
Epoch 9/100 Iteration 115/234: loss=0.177373 lr=0.000020 grad_norm=0.511003
Epoch 9/100 Iteration 116/234: loss=0.203020 lr=0.000020 grad_norm=0.553155
Epoch 9/100 Iteration 117/234: loss=0.190528 lr=0.000020 grad_norm=0.500721
Epoch 9/100 Iteration 118/234: loss=0.193064 lr=0.000020 grad_norm=0.606132
Epoch 9/100 Iteration 119/234: loss=0.199501 lr=0.000020 grad_norm=0.637182
Epoch 9/100 Iteration 120/234: loss=0.176104 lr=0.000020 grad_norm=0.607391
Epoch 9/100 Iteration 121/234: loss=0.196467 lr=0.000020 grad_norm=0.589964
Epoch 9/100 Iteration 122/234: loss=0.182722 lr=0.000020 grad_norm=0.737326
Epoch 9/100 Iteration 123/234: loss=0.195243 lr=0.000020 grad_norm=0.409670
Epoch 9/100 Iteration 124/234: loss=0.210222 lr=0.000020 grad_norm=0.759941
Epoch 9/100 Iteration 125/234: loss=0.193208 lr=0.000020 grad_norm=0.807042
Epoch 9/100 Iteration 126/234: loss=0.185849 lr=0.000020 grad_norm=0.341190
Epoch 9/100 Iteration 127/234: loss=0.184572 lr=0.000020 grad_norm=0.861972
Epoch 9/100 Iteration 128/234: loss=0.194427 lr=0.000020 grad_norm=0.545849
Epoch 9/100 Iteration 129/234: loss=0.191639 lr=0.000020 grad_norm=0.510759
Epoch 9/100 Iteration 130/234: loss=0.175106 lr=0.000020 grad_norm=0.809981
Epoch 9/100 Iteration 131/234: loss=0.192927 lr=0.000020 grad_norm=0.514014
Epoch 9/100 Iteration 132/234: loss=0.205267 lr=0.000020 grad_norm=0.684817
Epoch 9/100 Iteration 133/234: loss=0.192420 lr=0.000020 grad_norm=0.524021
Epoch 9/100 Iteration 134/234: loss=0.207250 lr=0.000020 grad_norm=0.512326
Epoch 9/100 Iteration 135/234: loss=0.213638 lr=0.000020 grad_norm=0.625691
Epoch 9/100 Iteration 136/234: loss=0.174680 lr=0.000020 grad_norm=0.730690
Epoch 9/100 Iteration 137/234: loss=0.185764 lr=0.000020 grad_norm=0.832020
Epoch 9/100 Iteration 138/234: loss=0.182271 lr=0.000020 grad_norm=0.545088
Epoch 9/100 Iteration 139/234: loss=0.191281 lr=0.000020 grad_norm=0.738884
Epoch 9/100 Iteration 140/234: loss=0.200966 lr=0.000020 grad_norm=0.740983
Epoch 9/100 Iteration 141/234: loss=0.212286 lr=0.000020 grad_norm=0.481977
Epoch 9/100 Iteration 142/234: loss=0.195977 lr=0.000020 grad_norm=0.896407
Epoch 9/100 Iteration 143/234: loss=0.197995 lr=0.000020 grad_norm=0.612187
Epoch 9/100 Iteration 144/234: loss=0.196252 lr=0.000020 grad_norm=0.533909
Epoch 9/100 Iteration 145/234: loss=0.210219 lr=0.000020 grad_norm=0.933006
Epoch 9/100 Iteration 146/234: loss=0.181301 lr=0.000020 grad_norm=0.454800
Epoch 9/100 Iteration 147/234: loss=0.193485 lr=0.000020 grad_norm=0.817545
Epoch 9/100 Iteration 148/234: loss=0.171445 lr=0.000020 grad_norm=0.968368
Epoch 9/100 Iteration 149/234: loss=0.191517 lr=0.000020 grad_norm=0.384203
Epoch 9/100 Iteration 150/234: loss=0.211135 lr=0.000020 grad_norm=0.791338
Epoch 9/100 Iteration 151/234: loss=0.199279 lr=0.000020 grad_norm=0.393587
Epoch 9/100 Iteration 152/234: loss=0.168256 lr=0.000020 grad_norm=1.004044
Epoch 9/100 Iteration 153/234: loss=0.200675 lr=0.000020 grad_norm=0.657816
Epoch 9/100 Iteration 154/234: loss=0.174826 lr=0.000020 grad_norm=0.741864
Epoch 9/100 Iteration 155/234: loss=0.180975 lr=0.000020 grad_norm=0.841120
Epoch 9/100 Iteration 156/234: loss=0.167591 lr=0.000020 grad_norm=0.263294
Epoch 9/100 Iteration 157/234: loss=0.202685 lr=0.000020 grad_norm=0.777624
Epoch 9/100 Iteration 158/234: loss=0.186940 lr=0.000020 grad_norm=0.607535
Epoch 9/100 Iteration 159/234: loss=0.199935 lr=0.000020 grad_norm=0.549821
Epoch 9/100 Iteration 160/234: loss=0.187280 lr=0.000020 grad_norm=0.734984
Epoch 9/100 Iteration 161/234: loss=0.212005 lr=0.000020 grad_norm=0.464274
Epoch 9/100 Iteration 162/234: loss=0.191651 lr=0.000020 grad_norm=0.775860
Epoch 9/100 Iteration 163/234: loss=0.184199 lr=0.000020 grad_norm=0.610556
Epoch 9/100 Iteration 164/234: loss=0.173727 lr=0.000020 grad_norm=0.554924
Epoch 9/100 Iteration 165/234: loss=0.206147 lr=0.000020 grad_norm=0.744906
Epoch 9/100 Iteration 166/234: loss=0.200557 lr=0.000020 grad_norm=0.461399
Epoch 9/100 Iteration 167/234: loss=0.198702 lr=0.000020 grad_norm=0.887828
Epoch 9/100 Iteration 168/234: loss=0.184430 lr=0.000020 grad_norm=0.586465
Epoch 9/100 Iteration 169/234: loss=0.179231 lr=0.000020 grad_norm=0.446421
Epoch 9/100 Iteration 170/234: loss=0.186389 lr=0.000020 grad_norm=0.579098
Epoch 9/100 Iteration 171/234: loss=0.206156 lr=0.000020 grad_norm=0.405303
Epoch 9/100 Iteration 172/234: loss=0.196534 lr=0.000020 grad_norm=0.777110
Epoch 9/100 Iteration 173/234: loss=0.192095 lr=0.000020 grad_norm=0.777801
Epoch 9/100 Iteration 174/234: loss=0.191595 lr=0.000020 grad_norm=0.378625
Epoch 9/100 Iteration 175/234: loss=0.181008 lr=0.000020 grad_norm=0.590930
Epoch 9/100 Iteration 176/234: loss=0.202966 lr=0.000020 grad_norm=0.688203
Epoch 9/100 Iteration 177/234: loss=0.183676 lr=0.000020 grad_norm=0.434639
Epoch 9/100 Iteration 178/234: loss=0.159734 lr=0.000020 grad_norm=0.618410
Epoch 9/100 Iteration 179/234: loss=0.205315 lr=0.000020 grad_norm=0.449738
Epoch 9/100 Iteration 180/234: loss=0.199211 lr=0.000020 grad_norm=0.566795
Epoch 9/100 Iteration 181/234: loss=0.176901 lr=0.000020 grad_norm=0.840734
Epoch 9/100 Iteration 182/234: loss=0.198690 lr=0.000020 grad_norm=0.316628
Epoch 9/100 Iteration 183/234: loss=0.193923 lr=0.000020 grad_norm=0.762233
Epoch 9/100 Iteration 184/234: loss=0.198261 lr=0.000020 grad_norm=0.352533
Epoch 9/100 Iteration 185/234: loss=0.166948 lr=0.000020 grad_norm=0.837189
Epoch 9/100 Iteration 186/234: loss=0.181155 lr=0.000020 grad_norm=0.628801
Epoch 9/100 Iteration 187/234: loss=0.199167 lr=0.000020 grad_norm=0.451812
Epoch 9/100 Iteration 188/234: loss=0.183617 lr=0.000020 grad_norm=0.468208
Epoch 9/100 Iteration 189/234: loss=0.173430 lr=0.000020 grad_norm=0.458189
Epoch 9/100 Iteration 190/234: loss=0.195887 lr=0.000020 grad_norm=0.692839
Epoch 9/100 Iteration 191/234: loss=0.188973 lr=0.000020 grad_norm=0.463727
Epoch 9/100 Iteration 192/234: loss=0.180840 lr=0.000020 grad_norm=0.333078
Epoch 9/100 Iteration 193/234: loss=0.178641 lr=0.000020 grad_norm=0.403545
Epoch 9/100 Iteration 194/234: loss=0.214146 lr=0.000020 grad_norm=0.337613
Epoch 9/100 Iteration 195/234: loss=0.190392 lr=0.000020 grad_norm=0.477659
Epoch 9/100 Iteration 196/234: loss=0.212737 lr=0.000020 grad_norm=0.572583
Epoch 9/100 Iteration 197/234: loss=0.182297 lr=0.000020 grad_norm=0.479613
Epoch 9/100 Iteration 198/234: loss=0.186900 lr=0.000020 grad_norm=0.501828
Epoch 9/100 Iteration 199/234: loss=0.196355 lr=0.000020 grad_norm=0.405080
Epoch 9/100 Iteration 200/234: loss=0.187557 lr=0.000020 grad_norm=0.676410
Epoch 9/100 Iteration 201/234: loss=0.201219 lr=0.000020 grad_norm=0.670592
Epoch 9/100 Iteration 202/234: loss=0.181788 lr=0.000020 grad_norm=0.707191
Epoch 9/100 Iteration 203/234: loss=0.194323 lr=0.000020 grad_norm=1.129265
Epoch 9/100 Iteration 204/234: loss=0.176155 lr=0.000020 grad_norm=0.955684
Epoch 9/100 Iteration 205/234: loss=0.182973 lr=0.000020 grad_norm=0.884179
Epoch 9/100 Iteration 206/234: loss=0.177271 lr=0.000020 grad_norm=0.808373
Epoch 9/100 Iteration 207/234: loss=0.191905 lr=0.000020 grad_norm=0.568605
Epoch 9/100 Iteration 208/234: loss=0.177595 lr=0.000020 grad_norm=0.526001
Epoch 9/100 Iteration 209/234: loss=0.189389 lr=0.000020 grad_norm=0.684404
Epoch 9/100 Iteration 210/234: loss=0.199198 lr=0.000020 grad_norm=0.470621
Epoch 9/100 Iteration 211/234: loss=0.187373 lr=0.000020 grad_norm=0.361628
Epoch 9/100 Iteration 212/234: loss=0.169123 lr=0.000020 grad_norm=0.483881
Epoch 9/100 Iteration 213/234: loss=0.207651 lr=0.000020 grad_norm=0.601434
Epoch 9/100 Iteration 214/234: loss=0.196043 lr=0.000020 grad_norm=0.604992
Epoch 9/100 Iteration 215/234: loss=0.184680 lr=0.000020 grad_norm=0.511208
Epoch 9/100 Iteration 216/234: loss=0.178050 lr=0.000020 grad_norm=0.545204
Epoch 9/100 Iteration 217/234: loss=0.194936 lr=0.000020 grad_norm=0.377586
Epoch 9/100 Iteration 218/234: loss=0.175358 lr=0.000020 grad_norm=0.374383
Epoch 9/100 Iteration 219/234: loss=0.203321 lr=0.000020 grad_norm=0.501940
Epoch 9/100 Iteration 220/234: loss=0.186053 lr=0.000020 grad_norm=0.557026
Epoch 9/100 Iteration 221/234: loss=0.184256 lr=0.000020 grad_norm=0.773106
Epoch 9/100 Iteration 222/234: loss=0.192768 lr=0.000020 grad_norm=0.910546
Epoch 9/100 Iteration 223/234: loss=0.191198 lr=0.000020 grad_norm=0.760713
Epoch 9/100 Iteration 224/234: loss=0.188696 lr=0.000020 grad_norm=0.589860
Epoch 9/100 Iteration 225/234: loss=0.178242 lr=0.000020 grad_norm=1.143478
Epoch 9/100 Iteration 226/234: loss=0.179285 lr=0.000020 grad_norm=1.201204
Epoch 9/100 Iteration 227/234: loss=0.202121 lr=0.000020 grad_norm=0.484628
Epoch 9/100 Iteration 228/234: loss=0.165210 lr=0.000020 grad_norm=0.724094
Epoch 9/100 Iteration 229/234: loss=0.187821 lr=0.000020 grad_norm=0.925817
Epoch 9/100 Iteration 230/234: loss=0.176418 lr=0.000020 grad_norm=0.420010
Epoch 9/100 Iteration 231/234: loss=0.187530 lr=0.000020 grad_norm=0.660967
Epoch 9/100 Iteration 232/234: loss=0.205734 lr=0.000020 grad_norm=0.597912
Epoch 9/100 Iteration 233/234: loss=0.186124 lr=0.000020 grad_norm=0.670893
Epoch 9/100 Iteration 234/234: loss=0.198926 lr=0.000020 grad_norm=0.361704
Epoch 9/100 finished. Avg Loss: 0.191238
Epoch 10/100 Iteration 1/234: loss=0.181548 lr=0.000020 grad_norm=0.470074
Epoch 10/100 Iteration 2/234: loss=0.183898 lr=0.000020 grad_norm=0.502416
Epoch 10/100 Iteration 3/234: loss=0.204155 lr=0.000020 grad_norm=0.508903
Epoch 10/100 Iteration 4/234: loss=0.183562 lr=0.000020 grad_norm=0.674979
Epoch 10/100 Iteration 5/234: loss=0.193986 lr=0.000020 grad_norm=0.399872
Epoch 10/100 Iteration 6/234: loss=0.178507 lr=0.000020 grad_norm=0.483888
Epoch 10/100 Iteration 7/234: loss=0.188280 lr=0.000020 grad_norm=0.736226
Epoch 10/100 Iteration 8/234: loss=0.179981 lr=0.000020 grad_norm=0.591406
Epoch 10/100 Iteration 9/234: loss=0.169862 lr=0.000020 grad_norm=0.308384
Epoch 10/100 Iteration 10/234: loss=0.192974 lr=0.000020 grad_norm=0.446188
Epoch 10/100 Iteration 11/234: loss=0.186758 lr=0.000020 grad_norm=0.565668
Epoch 10/100 Iteration 12/234: loss=0.187795 lr=0.000020 grad_norm=0.461856
Epoch 10/100 Iteration 13/234: loss=0.184152 lr=0.000020 grad_norm=0.416966
Epoch 10/100 Iteration 14/234: loss=0.197772 lr=0.000020 grad_norm=0.448258
Epoch 10/100 Iteration 15/234: loss=0.189033 lr=0.000020 grad_norm=0.382056
Epoch 10/100 Iteration 16/234: loss=0.200480 lr=0.000020 grad_norm=0.412972
Epoch 10/100 Iteration 17/234: loss=0.186237 lr=0.000020 grad_norm=0.544284
Epoch 10/100 Iteration 18/234: loss=0.175012 lr=0.000020 grad_norm=0.525088
Epoch 10/100 Iteration 19/234: loss=0.178337 lr=0.000020 grad_norm=0.382329
Epoch 10/100 Iteration 20/234: loss=0.197997 lr=0.000020 grad_norm=0.497851
Epoch 10/100 Iteration 21/234: loss=0.201744 lr=0.000020 grad_norm=0.692876
Epoch 10/100 Iteration 22/234: loss=0.184732 lr=0.000020 grad_norm=0.696499
Epoch 10/100 Iteration 23/234: loss=0.173495 lr=0.000020 grad_norm=0.438419
Epoch 10/100 Iteration 24/234: loss=0.196228 lr=0.000020 grad_norm=0.954581
Epoch 10/100 Iteration 25/234: loss=0.190843 lr=0.000020 grad_norm=1.045088
Epoch 10/100 Iteration 26/234: loss=0.201678 lr=0.000020 grad_norm=0.424266
Epoch 10/100 Iteration 27/234: loss=0.186015 lr=0.000020 grad_norm=0.996192
Epoch 10/100 Iteration 28/234: loss=0.195122 lr=0.000020 grad_norm=0.945754
Epoch 10/100 Iteration 29/234: loss=0.174249 lr=0.000020 grad_norm=0.413115
Epoch 10/100 Iteration 30/234: loss=0.197900 lr=0.000020 grad_norm=0.827361
Epoch 10/100 Iteration 31/234: loss=0.179594 lr=0.000020 grad_norm=0.547001
Epoch 10/100 Iteration 32/234: loss=0.180580 lr=0.000020 grad_norm=0.541286
Epoch 10/100 Iteration 33/234: loss=0.196280 lr=0.000020 grad_norm=0.573951
Epoch 10/100 Iteration 34/234: loss=0.174531 lr=0.000020 grad_norm=0.472670
Epoch 10/100 Iteration 35/234: loss=0.173640 lr=0.000020 grad_norm=0.915208
Epoch 10/100 Iteration 36/234: loss=0.195186 lr=0.000020 grad_norm=0.845833
Epoch 10/100 Iteration 37/234: loss=0.185499 lr=0.000020 grad_norm=0.609691
Epoch 10/100 Iteration 38/234: loss=0.172281 lr=0.000020 grad_norm=0.806261
Epoch 10/100 Iteration 39/234: loss=0.185423 lr=0.000020 grad_norm=0.863246
Epoch 10/100 Iteration 40/234: loss=0.193708 lr=0.000020 grad_norm=0.644294
Epoch 10/100 Iteration 41/234: loss=0.194312 lr=0.000020 grad_norm=0.728048
Epoch 10/100 Iteration 42/234: loss=0.207074 lr=0.000020 grad_norm=1.163341
Epoch 10/100 Iteration 43/234: loss=0.189138 lr=0.000020 grad_norm=1.279733
Epoch 10/100 Iteration 44/234: loss=0.191793 lr=0.000020 grad_norm=0.723449
Epoch 10/100 Iteration 45/234: loss=0.180983 lr=0.000020 grad_norm=0.859788
Epoch 10/100 Iteration 46/234: loss=0.190360 lr=0.000020 grad_norm=1.470120
Epoch 10/100 Iteration 47/234: loss=0.198818 lr=0.000020 grad_norm=1.129314
Epoch 10/100 Iteration 48/234: loss=0.170068 lr=0.000020 grad_norm=0.401086
Epoch 10/100 Iteration 49/234: loss=0.191150 lr=0.000020 grad_norm=0.748384
Epoch 10/100 Iteration 50/234: loss=0.185339 lr=0.000020 grad_norm=0.825967
Epoch 10/100 Iteration 51/234: loss=0.180547 lr=0.000020 grad_norm=0.637951
Epoch 10/100 Iteration 52/234: loss=0.180419 lr=0.000020 grad_norm=0.642344
Epoch 10/100 Iteration 53/234: loss=0.182647 lr=0.000020 grad_norm=0.614197
Epoch 10/100 Iteration 54/234: loss=0.201004 lr=0.000020 grad_norm=0.625713
Epoch 10/100 Iteration 55/234: loss=0.184248 lr=0.000020 grad_norm=0.540721
Epoch 10/100 Iteration 56/234: loss=0.194632 lr=0.000020 grad_norm=0.589344
Epoch 10/100 Iteration 57/234: loss=0.179238 lr=0.000020 grad_norm=0.708073
Epoch 10/100 Iteration 58/234: loss=0.183100 lr=0.000020 grad_norm=0.505882
Epoch 10/100 Iteration 59/234: loss=0.185576 lr=0.000020 grad_norm=0.461849
Epoch 10/100 Iteration 60/234: loss=0.196628 lr=0.000020 grad_norm=0.531450
Epoch 10/100 Iteration 61/234: loss=0.200524 lr=0.000020 grad_norm=0.496826
Epoch 10/100 Iteration 62/234: loss=0.180860 lr=0.000020 grad_norm=0.349842
Epoch 10/100 Iteration 63/234: loss=0.180985 lr=0.000020 grad_norm=0.416724
Epoch 10/100 Iteration 64/234: loss=0.189957 lr=0.000020 grad_norm=0.473596
Epoch 10/100 Iteration 65/234: loss=0.184161 lr=0.000020 grad_norm=0.466058
Epoch 10/100 Iteration 66/234: loss=0.153284 lr=0.000020 grad_norm=0.308271
Epoch 10/100 Iteration 67/234: loss=0.177593 lr=0.000020 grad_norm=0.435025
Epoch 10/100 Iteration 68/234: loss=0.192628 lr=0.000020 grad_norm=0.377011
Epoch 10/100 Iteration 69/234: loss=0.179867 lr=0.000020 grad_norm=0.463273
Epoch 10/100 Iteration 70/234: loss=0.182278 lr=0.000020 grad_norm=0.348876
Epoch 10/100 Iteration 71/234: loss=0.193169 lr=0.000020 grad_norm=0.396553
Epoch 10/100 Iteration 72/234: loss=0.178894 lr=0.000020 grad_norm=0.353239
Epoch 10/100 Iteration 73/234: loss=0.167188 lr=0.000020 grad_norm=0.427196
Epoch 10/100 Iteration 74/234: loss=0.178195 lr=0.000020 grad_norm=0.692146
Epoch 10/100 Iteration 75/234: loss=0.205397 lr=0.000020 grad_norm=0.623109
Epoch 10/100 Iteration 76/234: loss=0.184440 lr=0.000020 grad_norm=0.382393
Epoch 10/100 Iteration 77/234: loss=0.200842 lr=0.000020 grad_norm=0.660392
Epoch 10/100 Iteration 78/234: loss=0.190763 lr=0.000020 grad_norm=0.426316
Epoch 10/100 Iteration 79/234: loss=0.199099 lr=0.000020 grad_norm=0.376034
Epoch 10/100 Iteration 80/234: loss=0.179251 lr=0.000020 grad_norm=0.508376
Epoch 10/100 Iteration 81/234: loss=0.179697 lr=0.000020 grad_norm=0.433616
Epoch 10/100 Iteration 82/234: loss=0.175098 lr=0.000020 grad_norm=0.343526
Epoch 10/100 Iteration 83/234: loss=0.173609 lr=0.000020 grad_norm=0.664953
Epoch 10/100 Iteration 84/234: loss=0.188768 lr=0.000020 grad_norm=0.665578
Epoch 10/100 Iteration 85/234: loss=0.186007 lr=0.000020 grad_norm=0.385951
Epoch 10/100 Iteration 86/234: loss=0.182199 lr=0.000020 grad_norm=0.594135
Epoch 10/100 Iteration 87/234: loss=0.167485 lr=0.000020 grad_norm=0.712488
Epoch 10/100 Iteration 88/234: loss=0.196638 lr=0.000020 grad_norm=0.561183
Epoch 10/100 Iteration 89/234: loss=0.178162 lr=0.000020 grad_norm=0.343874
Epoch 10/100 Iteration 90/234: loss=0.178196 lr=0.000020 grad_norm=0.527897
Epoch 10/100 Iteration 91/234: loss=0.194757 lr=0.000020 grad_norm=0.448782
Epoch 10/100 Iteration 92/234: loss=0.182735 lr=0.000020 grad_norm=0.358299
Epoch 10/100 Iteration 93/234: loss=0.195743 lr=0.000020 grad_norm=0.370974
Epoch 10/100 Iteration 94/234: loss=0.208112 lr=0.000020 grad_norm=0.340088
Epoch 10/100 Iteration 95/234: loss=0.189650 lr=0.000020 grad_norm=0.440843
Epoch 10/100 Iteration 96/234: loss=0.196030 lr=0.000020 grad_norm=0.489989
Epoch 10/100 Iteration 97/234: loss=0.186143 lr=0.000020 grad_norm=0.534713
Epoch 10/100 Iteration 98/234: loss=0.188083 lr=0.000020 grad_norm=0.461352
Epoch 10/100 Iteration 99/234: loss=0.191647 lr=0.000020 grad_norm=0.584132
Epoch 10/100 Iteration 100/234: loss=0.195496 lr=0.000020 grad_norm=0.456976
Epoch 10/100 Iteration 101/234: loss=0.199123 lr=0.000020 grad_norm=0.385054
Epoch 10/100 Iteration 102/234: loss=0.192649 lr=0.000020 grad_norm=0.475467
Epoch 10/100 Iteration 103/234: loss=0.195971 lr=0.000020 grad_norm=0.471728
Epoch 10/100 Iteration 104/234: loss=0.178729 lr=0.000020 grad_norm=0.384993
Epoch 10/100 Iteration 105/234: loss=0.173850 lr=0.000020 grad_norm=0.509217
Epoch 10/100 Iteration 106/234: loss=0.178101 lr=0.000020 grad_norm=0.673027
Epoch 10/100 Iteration 107/234: loss=0.195279 lr=0.000020 grad_norm=0.579434
Epoch 10/100 Iteration 108/234: loss=0.196937 lr=0.000020 grad_norm=0.466356
Epoch 10/100 Iteration 109/234: loss=0.192341 lr=0.000020 grad_norm=0.546800
Epoch 10/100 Iteration 110/234: loss=0.158741 lr=0.000020 grad_norm=1.026359
Epoch 10/100 Iteration 111/234: loss=0.173110 lr=0.000020 grad_norm=0.923563
Epoch 10/100 Iteration 112/234: loss=0.195864 lr=0.000020 grad_norm=0.728610
Epoch 10/100 Iteration 113/234: loss=0.203316 lr=0.000020 grad_norm=0.781846
Epoch 10/100 Iteration 114/234: loss=0.185041 lr=0.000020 grad_norm=0.517757
Epoch 10/100 Iteration 115/234: loss=0.188240 lr=0.000020 grad_norm=0.933856
Epoch 10/100 Iteration 116/234: loss=0.170378 lr=0.000020 grad_norm=0.478622
Epoch 10/100 Iteration 117/234: loss=0.184306 lr=0.000020 grad_norm=0.818913
Epoch 10/100 Iteration 118/234: loss=0.166127 lr=0.000020 grad_norm=0.814340
Epoch 10/100 Iteration 119/234: loss=0.182704 lr=0.000020 grad_norm=0.395438
Epoch 10/100 Iteration 120/234: loss=0.165794 lr=0.000020 grad_norm=0.763443
Epoch 10/100 Iteration 121/234: loss=0.201643 lr=0.000020 grad_norm=0.777005
Epoch 10/100 Iteration 122/234: loss=0.186173 lr=0.000020 grad_norm=0.624457
Epoch 10/100 Iteration 123/234: loss=0.182039 lr=0.000020 grad_norm=0.484155
Epoch 10/100 Iteration 124/234: loss=0.179457 lr=0.000020 grad_norm=0.429470
Epoch 10/100 Iteration 125/234: loss=0.173924 lr=0.000020 grad_norm=0.604294
Epoch 10/100 Iteration 126/234: loss=0.166251 lr=0.000020 grad_norm=0.514682
Epoch 10/100 Iteration 127/234: loss=0.176450 lr=0.000020 grad_norm=0.529696
Epoch 10/100 Iteration 128/234: loss=0.177151 lr=0.000020 grad_norm=0.460685
Epoch 10/100 Iteration 129/234: loss=0.180467 lr=0.000020 grad_norm=0.524983
Epoch 10/100 Iteration 130/234: loss=0.190419 lr=0.000020 grad_norm=0.459439
Epoch 10/100 Iteration 131/234: loss=0.171410 lr=0.000020 grad_norm=0.426507
Epoch 10/100 Iteration 132/234: loss=0.169383 lr=0.000020 grad_norm=0.496183
Epoch 10/100 Iteration 133/234: loss=0.178262 lr=0.000020 grad_norm=0.563355
Epoch 10/100 Iteration 134/234: loss=0.185382 lr=0.000020 grad_norm=0.590124
Epoch 10/100 Iteration 135/234: loss=0.203138 lr=0.000020 grad_norm=0.756961
Epoch 10/100 Iteration 136/234: loss=0.179663 lr=0.000020 grad_norm=0.674961
Epoch 10/100 Iteration 137/234: loss=0.184743 lr=0.000020 grad_norm=0.501184
Epoch 10/100 Iteration 138/234: loss=0.160593 lr=0.000020 grad_norm=0.454734
Epoch 10/100 Iteration 139/234: loss=0.194882 lr=0.000020 grad_norm=0.968016
Epoch 10/100 Iteration 140/234: loss=0.168310 lr=0.000020 grad_norm=1.051112
Epoch 10/100 Iteration 141/234: loss=0.189117 lr=0.000020 grad_norm=0.672995
Epoch 10/100 Iteration 142/234: loss=0.189051 lr=0.000020 grad_norm=0.701318
Epoch 10/100 Iteration 143/234: loss=0.196589 lr=0.000020 grad_norm=0.612055
Epoch 10/100 Iteration 144/234: loss=0.182504 lr=0.000020 grad_norm=0.456208
Epoch 10/100 Iteration 145/234: loss=0.168320 lr=0.000020 grad_norm=0.727388
Epoch 10/100 Iteration 146/234: loss=0.178512 lr=0.000020 grad_norm=0.592508
Epoch 10/100 Iteration 147/234: loss=0.175050 lr=0.000020 grad_norm=0.354611
Epoch 10/100 Iteration 148/234: loss=0.196789 lr=0.000020 grad_norm=0.610779
Epoch 10/100 Iteration 149/234: loss=0.186676 lr=0.000020 grad_norm=0.496379
Epoch 10/100 Iteration 150/234: loss=0.178061 lr=0.000020 grad_norm=0.401571
Epoch 10/100 Iteration 151/234: loss=0.193807 lr=0.000020 grad_norm=0.588446
Epoch 10/100 Iteration 152/234: loss=0.178977 lr=0.000020 grad_norm=0.309891
Epoch 10/100 Iteration 153/234: loss=0.171342 lr=0.000020 grad_norm=0.352469
Epoch 10/100 Iteration 154/234: loss=0.176172 lr=0.000020 grad_norm=0.401044
Epoch 10/100 Iteration 155/234: loss=0.176782 lr=0.000020 grad_norm=0.575740
Epoch 10/100 Iteration 156/234: loss=0.183123 lr=0.000020 grad_norm=0.586265
Epoch 10/100 Iteration 157/234: loss=0.196719 lr=0.000020 grad_norm=0.311493
Epoch 10/100 Iteration 158/234: loss=0.182195 lr=0.000020 grad_norm=0.427391
Epoch 10/100 Iteration 159/234: loss=0.189006 lr=0.000020 grad_norm=0.496812
Epoch 10/100 Iteration 160/234: loss=0.184844 lr=0.000020 grad_norm=0.387977
Epoch 10/100 Iteration 161/234: loss=0.182772 lr=0.000020 grad_norm=0.341973
Epoch 10/100 Iteration 162/234: loss=0.182564 lr=0.000020 grad_norm=0.446794
Epoch 10/100 Iteration 163/234: loss=0.175869 lr=0.000020 grad_norm=0.426494
Epoch 10/100 Iteration 164/234: loss=0.189219 lr=0.000020 grad_norm=0.386967
Epoch 10/100 Iteration 165/234: loss=0.177178 lr=0.000020 grad_norm=0.283043
Epoch 10/100 Iteration 166/234: loss=0.165385 lr=0.000020 grad_norm=0.349565
Epoch 10/100 Iteration 167/234: loss=0.197096 lr=0.000020 grad_norm=0.514064
Epoch 10/100 Iteration 168/234: loss=0.171863 lr=0.000020 grad_norm=0.551153
Epoch 10/100 Iteration 169/234: loss=0.159291 lr=0.000020 grad_norm=0.430573
Epoch 10/100 Iteration 170/234: loss=0.197165 lr=0.000020 grad_norm=0.538252
Epoch 10/100 Iteration 171/234: loss=0.206665 lr=0.000020 grad_norm=0.437957
Epoch 10/100 Iteration 172/234: loss=0.190803 lr=0.000020 grad_norm=0.474874
Epoch 10/100 Iteration 173/234: loss=0.168015 lr=0.000020 grad_norm=0.368684
Epoch 10/100 Iteration 174/234: loss=0.176069 lr=0.000020 grad_norm=0.601692
Epoch 10/100 Iteration 175/234: loss=0.172421 lr=0.000020 grad_norm=0.509533
Epoch 10/100 Iteration 176/234: loss=0.165949 lr=0.000020 grad_norm=0.574369
Epoch 10/100 Iteration 177/234: loss=0.156229 lr=0.000020 grad_norm=0.633947
Epoch 10/100 Iteration 178/234: loss=0.194820 lr=0.000020 grad_norm=0.835590
Epoch 10/100 Iteration 179/234: loss=0.183217 lr=0.000020 grad_norm=0.541334
Epoch 10/100 Iteration 180/234: loss=0.201312 lr=0.000020 grad_norm=0.531383
Epoch 10/100 Iteration 181/234: loss=0.174734 lr=0.000020 grad_norm=1.020613
Epoch 10/100 Iteration 182/234: loss=0.191159 lr=0.000020 grad_norm=1.037142
Epoch 10/100 Iteration 183/234: loss=0.181464 lr=0.000020 grad_norm=0.879242
Epoch 10/100 Iteration 184/234: loss=0.186153 lr=0.000020 grad_norm=0.459949
Epoch 10/100 Iteration 185/234: loss=0.163562 lr=0.000020 grad_norm=0.721090
Epoch 10/100 Iteration 186/234: loss=0.174553 lr=0.000020 grad_norm=0.776808
Epoch 10/100 Iteration 187/234: loss=0.176608 lr=0.000020 grad_norm=0.500663
Epoch 10/100 Iteration 188/234: loss=0.185360 lr=0.000020 grad_norm=0.489563
Epoch 10/100 Iteration 189/234: loss=0.186288 lr=0.000020 grad_norm=0.431020
Epoch 10/100 Iteration 190/234: loss=0.169070 lr=0.000020 grad_norm=0.455553
Epoch 10/100 Iteration 191/234: loss=0.180631 lr=0.000020 grad_norm=0.605378
Epoch 10/100 Iteration 192/234: loss=0.193671 lr=0.000020 grad_norm=0.545403
Epoch 10/100 Iteration 193/234: loss=0.183269 lr=0.000020 grad_norm=0.421809
Epoch 10/100 Iteration 194/234: loss=0.196561 lr=0.000020 grad_norm=0.468371
Epoch 10/100 Iteration 195/234: loss=0.185315 lr=0.000020 grad_norm=0.717747
Epoch 10/100 Iteration 196/234: loss=0.182309 lr=0.000020 grad_norm=0.568792
Epoch 10/100 Iteration 197/234: loss=0.181730 lr=0.000020 grad_norm=0.368371
Epoch 10/100 Iteration 198/234: loss=0.184427 lr=0.000020 grad_norm=0.319164
Epoch 10/100 Iteration 199/234: loss=0.185144 lr=0.000020 grad_norm=0.353883
Epoch 10/100 Iteration 200/234: loss=0.165266 lr=0.000020 grad_norm=0.416883
Epoch 10/100 Iteration 201/234: loss=0.174466 lr=0.000020 grad_norm=0.453556
Epoch 10/100 Iteration 202/234: loss=0.196669 lr=0.000020 grad_norm=0.612886
Epoch 10/100 Iteration 203/234: loss=0.190267 lr=0.000020 grad_norm=0.671730
Epoch 10/100 Iteration 204/234: loss=0.173891 lr=0.000020 grad_norm=0.651664
Epoch 10/100 Iteration 205/234: loss=0.173914 lr=0.000020 grad_norm=0.764448
Epoch 10/100 Iteration 206/234: loss=0.178537 lr=0.000020 grad_norm=0.608313
Epoch 10/100 Iteration 207/234: loss=0.179335 lr=0.000020 grad_norm=0.441032
Epoch 10/100 Iteration 208/234: loss=0.181926 lr=0.000020 grad_norm=0.964571
Epoch 10/100 Iteration 209/234: loss=0.185680 lr=0.000020 grad_norm=0.807068
Epoch 10/100 Iteration 210/234: loss=0.180168 lr=0.000020 grad_norm=0.496970
Epoch 10/100 Iteration 211/234: loss=0.155371 lr=0.000020 grad_norm=0.538289
Epoch 10/100 Iteration 212/234: loss=0.178548 lr=0.000020 grad_norm=0.395647
Epoch 10/100 Iteration 213/234: loss=0.169544 lr=0.000020 grad_norm=0.338636
Epoch 10/100 Iteration 214/234: loss=0.194702 lr=0.000020 grad_norm=0.451096
Epoch 10/100 Iteration 215/234: loss=0.191075 lr=0.000020 grad_norm=0.432420
Epoch 10/100 Iteration 216/234: loss=0.173354 lr=0.000020 grad_norm=0.329523
Epoch 10/100 Iteration 217/234: loss=0.180492 lr=0.000020 grad_norm=0.386142
Epoch 10/100 Iteration 218/234: loss=0.176426 lr=0.000020 grad_norm=0.500356
Epoch 10/100 Iteration 219/234: loss=0.178591 lr=0.000020 grad_norm=0.485875
Epoch 10/100 Iteration 220/234: loss=0.173908 lr=0.000020 grad_norm=0.461866
Epoch 10/100 Iteration 221/234: loss=0.192666 lr=0.000020 grad_norm=0.515968
Epoch 10/100 Iteration 222/234: loss=0.168517 lr=0.000020 grad_norm=0.614337
Epoch 10/100 Iteration 223/234: loss=0.170366 lr=0.000020 grad_norm=0.772544
Epoch 10/100 Iteration 224/234: loss=0.168785 lr=0.000020 grad_norm=1.001253
Epoch 10/100 Iteration 225/234: loss=0.185965 lr=0.000020 grad_norm=0.777905
Epoch 10/100 Iteration 226/234: loss=0.177437 lr=0.000020 grad_norm=0.433462
Epoch 10/100 Iteration 227/234: loss=0.198592 lr=0.000020 grad_norm=0.751740
Epoch 10/100 Iteration 228/234: loss=0.185749 lr=0.000020 grad_norm=0.762442
Epoch 10/100 Iteration 229/234: loss=0.170394 lr=0.000020 grad_norm=0.548121
Epoch 10/100 Iteration 230/234: loss=0.172614 lr=0.000020 grad_norm=0.472164
Epoch 10/100 Iteration 231/234: loss=0.176277 lr=0.000020 grad_norm=0.704397
Epoch 10/100 Iteration 232/234: loss=0.171079 lr=0.000020 grad_norm=0.681004
Epoch 10/100 Iteration 233/234: loss=0.170229 lr=0.000020 grad_norm=0.512096
Epoch 10/100 Iteration 234/234: loss=0.163072 lr=0.000020 grad_norm=0.822816
Epoch 10/100 finished. Avg Loss: 0.183502
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 11/100 Iteration 1/234: loss=0.198790 lr=0.000020 grad_norm=0.796501
Epoch 11/100 Iteration 2/234: loss=0.182383 lr=0.000020 grad_norm=0.510539
Epoch 11/100 Iteration 3/234: loss=0.172247 lr=0.000020 grad_norm=0.629059
Epoch 11/100 Iteration 4/234: loss=0.179866 lr=0.000020 grad_norm=0.827662
Epoch 11/100 Iteration 5/234: loss=0.179567 lr=0.000020 grad_norm=0.834953
Epoch 11/100 Iteration 6/234: loss=0.177131 lr=0.000020 grad_norm=1.027522
Epoch 11/100 Iteration 7/234: loss=0.184908 lr=0.000020 grad_norm=1.096033
Epoch 11/100 Iteration 8/234: loss=0.171579 lr=0.000020 grad_norm=0.966220
Epoch 11/100 Iteration 9/234: loss=0.171080 lr=0.000020 grad_norm=1.341992
Epoch 11/100 Iteration 10/234: loss=0.181872 lr=0.000020 grad_norm=0.886157
Epoch 11/100 Iteration 11/234: loss=0.202252 lr=0.000020 grad_norm=0.925442
Epoch 11/100 Iteration 12/234: loss=0.188291 lr=0.000020 grad_norm=1.115752
Epoch 11/100 Iteration 13/234: loss=0.193003 lr=0.000020 grad_norm=0.643383
Epoch 11/100 Iteration 14/234: loss=0.171921 lr=0.000020 grad_norm=0.675721
Epoch 11/100 Iteration 15/234: loss=0.174018 lr=0.000020 grad_norm=0.904936
Epoch 11/100 Iteration 16/234: loss=0.198312 lr=0.000020 grad_norm=0.682946
Epoch 11/100 Iteration 17/234: loss=0.178520 lr=0.000020 grad_norm=0.583478
Epoch 11/100 Iteration 18/234: loss=0.174554 lr=0.000020 grad_norm=0.641096
Epoch 11/100 Iteration 19/234: loss=0.190066 lr=0.000020 grad_norm=0.502035
Epoch 11/100 Iteration 20/234: loss=0.179441 lr=0.000020 grad_norm=0.488039
Epoch 11/100 Iteration 21/234: loss=0.171937 lr=0.000020 grad_norm=0.558154
Epoch 11/100 Iteration 22/234: loss=0.188742 lr=0.000020 grad_norm=0.805884
Epoch 11/100 Iteration 23/234: loss=0.174427 lr=0.000020 grad_norm=0.638807
Epoch 11/100 Iteration 24/234: loss=0.169301 lr=0.000020 grad_norm=0.351063
Epoch 11/100 Iteration 25/234: loss=0.181517 lr=0.000020 grad_norm=0.711221
Epoch 11/100 Iteration 26/234: loss=0.184914 lr=0.000020 grad_norm=0.850708
Epoch 11/100 Iteration 27/234: loss=0.167383 lr=0.000020 grad_norm=0.374036
Epoch 11/100 Iteration 28/234: loss=0.183047 lr=0.000020 grad_norm=0.452531
Epoch 11/100 Iteration 29/234: loss=0.170885 lr=0.000020 grad_norm=0.477708
Epoch 11/100 Iteration 30/234: loss=0.162056 lr=0.000020 grad_norm=0.535116
Epoch 11/100 Iteration 31/234: loss=0.183810 lr=0.000020 grad_norm=0.534476
Epoch 11/100 Iteration 32/234: loss=0.179851 lr=0.000020 grad_norm=0.364679
Epoch 11/100 Iteration 33/234: loss=0.173437 lr=0.000020 grad_norm=0.591011
Epoch 11/100 Iteration 34/234: loss=0.183545 lr=0.000020 grad_norm=0.441669
Epoch 11/100 Iteration 35/234: loss=0.196609 lr=0.000020 grad_norm=0.622282
Epoch 11/100 Iteration 36/234: loss=0.188133 lr=0.000020 grad_norm=0.513839
Epoch 11/100 Iteration 37/234: loss=0.179700 lr=0.000020 grad_norm=0.412768
Epoch 11/100 Iteration 38/234: loss=0.179748 lr=0.000020 grad_norm=0.526280
Epoch 11/100 Iteration 39/234: loss=0.177428 lr=0.000020 grad_norm=0.689683
Epoch 11/100 Iteration 40/234: loss=0.170314 lr=0.000020 grad_norm=0.548631
Epoch 11/100 Iteration 41/234: loss=0.174810 lr=0.000020 grad_norm=0.434729
Epoch 11/100 Iteration 42/234: loss=0.178385 lr=0.000020 grad_norm=0.724749
Epoch 11/100 Iteration 43/234: loss=0.166939 lr=0.000020 grad_norm=0.600075
Epoch 11/100 Iteration 44/234: loss=0.159657 lr=0.000020 grad_norm=0.804630
Epoch 11/100 Iteration 45/234: loss=0.178006 lr=0.000020 grad_norm=0.894163
Epoch 11/100 Iteration 46/234: loss=0.164153 lr=0.000020 grad_norm=0.336132
Epoch 11/100 Iteration 47/234: loss=0.196690 lr=0.000020 grad_norm=0.659750
Epoch 11/100 Iteration 48/234: loss=0.173382 lr=0.000020 grad_norm=0.738312
Epoch 11/100 Iteration 49/234: loss=0.185269 lr=0.000020 grad_norm=0.464850
Epoch 11/100 Iteration 50/234: loss=0.168097 lr=0.000020 grad_norm=0.700509
Epoch 11/100 Iteration 51/234: loss=0.165824 lr=0.000020 grad_norm=0.733971
Epoch 11/100 Iteration 52/234: loss=0.192731 lr=0.000020 grad_norm=0.460472
Epoch 11/100 Iteration 53/234: loss=0.193702 lr=0.000020 grad_norm=0.451904
Epoch 11/100 Iteration 54/234: loss=0.164872 lr=0.000020 grad_norm=0.876179
Epoch 11/100 Iteration 55/234: loss=0.160563 lr=0.000020 grad_norm=0.920068
Epoch 11/100 Iteration 56/234: loss=0.170268 lr=0.000020 grad_norm=0.591154
Epoch 11/100 Iteration 57/234: loss=0.176139 lr=0.000020 grad_norm=0.880936
Epoch 11/100 Iteration 58/234: loss=0.176240 lr=0.000020 grad_norm=0.506188
Epoch 11/100 Iteration 59/234: loss=0.191897 lr=0.000020 grad_norm=0.690573
Epoch 11/100 Iteration 60/234: loss=0.176766 lr=0.000020 grad_norm=0.808064
Epoch 11/100 Iteration 61/234: loss=0.176237 lr=0.000020 grad_norm=0.507791
Epoch 11/100 Iteration 62/234: loss=0.192979 lr=0.000020 grad_norm=0.890287
Epoch 11/100 Iteration 63/234: loss=0.191374 lr=0.000020 grad_norm=0.706971
Epoch 11/100 Iteration 64/234: loss=0.181173 lr=0.000020 grad_norm=0.426160
Epoch 11/100 Iteration 65/234: loss=0.161525 lr=0.000020 grad_norm=0.874337
Epoch 11/100 Iteration 66/234: loss=0.166388 lr=0.000020 grad_norm=0.959291
Epoch 11/100 Iteration 67/234: loss=0.172875 lr=0.000020 grad_norm=0.805446
Epoch 11/100 Iteration 68/234: loss=0.157970 lr=0.000020 grad_norm=0.517091
Epoch 11/100 Iteration 69/234: loss=0.168324 lr=0.000020 grad_norm=0.410749
Epoch 11/100 Iteration 70/234: loss=0.180331 lr=0.000020 grad_norm=0.498136
Epoch 11/100 Iteration 71/234: loss=0.191647 lr=0.000020 grad_norm=0.658666
Epoch 11/100 Iteration 72/234: loss=0.177499 lr=0.000020 grad_norm=0.940535
Epoch 11/100 Iteration 73/234: loss=0.180240 lr=0.000020 grad_norm=0.805277
Epoch 11/100 Iteration 74/234: loss=0.164557 lr=0.000020 grad_norm=0.469705
Epoch 11/100 Iteration 75/234: loss=0.166871 lr=0.000020 grad_norm=0.678677
Epoch 11/100 Iteration 76/234: loss=0.184709 lr=0.000020 grad_norm=0.841298
Epoch 11/100 Iteration 77/234: loss=0.179609 lr=0.000020 grad_norm=0.439598
Epoch 11/100 Iteration 78/234: loss=0.162333 lr=0.000020 grad_norm=0.496702
Epoch 11/100 Iteration 79/234: loss=0.168794 lr=0.000020 grad_norm=0.429393
Epoch 11/100 Iteration 80/234: loss=0.191288 lr=0.000020 grad_norm=0.572834
Epoch 11/100 Iteration 81/234: loss=0.191734 lr=0.000020 grad_norm=0.641423
Epoch 11/100 Iteration 82/234: loss=0.178891 lr=0.000020 grad_norm=0.486131
Epoch 11/100 Iteration 83/234: loss=0.198890 lr=0.000020 grad_norm=0.500006
Epoch 11/100 Iteration 84/234: loss=0.165454 lr=0.000020 grad_norm=0.530213
Epoch 11/100 Iteration 85/234: loss=0.187316 lr=0.000020 grad_norm=0.391017
Epoch 11/100 Iteration 86/234: loss=0.181511 lr=0.000020 grad_norm=0.811421
Epoch 11/100 Iteration 87/234: loss=0.178816 lr=0.000020 grad_norm=0.903214
Epoch 11/100 Iteration 88/234: loss=0.167164 lr=0.000020 grad_norm=0.447743
Epoch 11/100 Iteration 89/234: loss=0.180560 lr=0.000020 grad_norm=0.674769
Epoch 11/100 Iteration 90/234: loss=0.170815 lr=0.000020 grad_norm=0.336229
Epoch 11/100 Iteration 91/234: loss=0.183223 lr=0.000020 grad_norm=0.560476
Epoch 11/100 Iteration 92/234: loss=0.168702 lr=0.000020 grad_norm=0.699464
Epoch 11/100 Iteration 93/234: loss=0.168090 lr=0.000020 grad_norm=0.643070
Epoch 11/100 Iteration 94/234: loss=0.163661 lr=0.000020 grad_norm=0.514031
Epoch 11/100 Iteration 95/234: loss=0.169694 lr=0.000020 grad_norm=0.560047
Epoch 11/100 Iteration 96/234: loss=0.175655 lr=0.000020 grad_norm=0.613008
Epoch 11/100 Iteration 97/234: loss=0.185487 lr=0.000020 grad_norm=0.503188
Epoch 11/100 Iteration 98/234: loss=0.164287 lr=0.000020 grad_norm=0.618654
Epoch 11/100 Iteration 99/234: loss=0.171348 lr=0.000020 grad_norm=0.441366
Epoch 11/100 Iteration 100/234: loss=0.186122 lr=0.000020 grad_norm=0.533595
Epoch 11/100 Iteration 101/234: loss=0.170764 lr=0.000020 grad_norm=0.492637
Epoch 11/100 Iteration 102/234: loss=0.177615 lr=0.000020 grad_norm=0.419575
Epoch 11/100 Iteration 103/234: loss=0.178703 lr=0.000020 grad_norm=0.405051
Epoch 11/100 Iteration 104/234: loss=0.159614 lr=0.000020 grad_norm=0.488600
Epoch 11/100 Iteration 105/234: loss=0.167836 lr=0.000020 grad_norm=0.598131
Epoch 11/100 Iteration 106/234: loss=0.174104 lr=0.000020 grad_norm=0.501950
Epoch 11/100 Iteration 107/234: loss=0.161027 lr=0.000020 grad_norm=0.455318
Epoch 11/100 Iteration 108/234: loss=0.185302 lr=0.000020 grad_norm=0.819194
Epoch 11/100 Iteration 109/234: loss=0.172391 lr=0.000020 grad_norm=0.618863
Epoch 11/100 Iteration 110/234: loss=0.189296 lr=0.000020 grad_norm=0.664449
Epoch 11/100 Iteration 111/234: loss=0.163152 lr=0.000020 grad_norm=0.716029
Epoch 11/100 Iteration 112/234: loss=0.184965 lr=0.000020 grad_norm=0.522015
Epoch 11/100 Iteration 113/234: loss=0.178002 lr=0.000020 grad_norm=0.443085
Epoch 11/100 Iteration 114/234: loss=0.169538 lr=0.000020 grad_norm=0.461702
Epoch 11/100 Iteration 115/234: loss=0.153494 lr=0.000020 grad_norm=0.642004
Epoch 11/100 Iteration 116/234: loss=0.172743 lr=0.000020 grad_norm=0.529574
Epoch 11/100 Iteration 117/234: loss=0.180434 lr=0.000020 grad_norm=0.456823
Epoch 11/100 Iteration 118/234: loss=0.165140 lr=0.000020 grad_norm=0.430069
Epoch 11/100 Iteration 119/234: loss=0.181237 lr=0.000020 grad_norm=0.420810
Epoch 11/100 Iteration 120/234: loss=0.189106 lr=0.000020 grad_norm=0.337559
Epoch 11/100 Iteration 121/234: loss=0.178257 lr=0.000020 grad_norm=0.318400
Epoch 11/100 Iteration 122/234: loss=0.180397 lr=0.000020 grad_norm=0.365043
Epoch 11/100 Iteration 123/234: loss=0.186992 lr=0.000020 grad_norm=0.465249
Epoch 11/100 Iteration 124/234: loss=0.155217 lr=0.000020 grad_norm=0.907431
Epoch 11/100 Iteration 125/234: loss=0.176601 lr=0.000020 grad_norm=0.948437
Epoch 11/100 Iteration 126/234: loss=0.173509 lr=0.000020 grad_norm=0.320518
Epoch 11/100 Iteration 127/234: loss=0.179195 lr=0.000020 grad_norm=0.671644
Epoch 11/100 Iteration 128/234: loss=0.179659 lr=0.000020 grad_norm=0.642989
Epoch 11/100 Iteration 129/234: loss=0.181591 lr=0.000020 grad_norm=0.371491
Epoch 11/100 Iteration 130/234: loss=0.177753 lr=0.000020 grad_norm=0.562093
Epoch 11/100 Iteration 131/234: loss=0.196267 lr=0.000020 grad_norm=0.357963
Epoch 11/100 Iteration 132/234: loss=0.181603 lr=0.000020 grad_norm=0.594631
Epoch 11/100 Iteration 133/234: loss=0.171804 lr=0.000020 grad_norm=0.935358
Epoch 11/100 Iteration 134/234: loss=0.191762 lr=0.000020 grad_norm=0.998019
Epoch 11/100 Iteration 135/234: loss=0.182675 lr=0.000020 grad_norm=0.417870
Epoch 11/100 Iteration 136/234: loss=0.179896 lr=0.000020 grad_norm=0.610485
Epoch 11/100 Iteration 137/234: loss=0.170653 lr=0.000020 grad_norm=0.818926
Epoch 11/100 Iteration 138/234: loss=0.188731 lr=0.000020 grad_norm=0.978678
Epoch 11/100 Iteration 139/234: loss=0.162962 lr=0.000020 grad_norm=1.350435
Epoch 11/100 Iteration 140/234: loss=0.166278 lr=0.000020 grad_norm=1.323279
Epoch 11/100 Iteration 141/234: loss=0.195268 lr=0.000020 grad_norm=0.834839
Epoch 11/100 Iteration 142/234: loss=0.173220 lr=0.000020 grad_norm=0.855490
Epoch 11/100 Iteration 143/234: loss=0.176266 lr=0.000020 grad_norm=1.112577
Epoch 11/100 Iteration 144/234: loss=0.169932 lr=0.000020 grad_norm=1.389307
Epoch 11/100 Iteration 145/234: loss=0.176416 lr=0.000020 grad_norm=1.098498
Epoch 11/100 Iteration 146/234: loss=0.190258 lr=0.000020 grad_norm=0.866635
Epoch 11/100 Iteration 147/234: loss=0.191396 lr=0.000020 grad_norm=1.469331
Epoch 11/100 Iteration 148/234: loss=0.170030 lr=0.000020 grad_norm=1.807833
Epoch 11/100 Iteration 149/234: loss=0.170877 lr=0.000020 grad_norm=1.403957
Epoch 11/100 Iteration 150/234: loss=0.173595 lr=0.000020 grad_norm=1.036092
Epoch 11/100 Iteration 151/234: loss=0.176991 lr=0.000020 grad_norm=1.151926
Epoch 11/100 Iteration 152/234: loss=0.170412 lr=0.000020 grad_norm=0.906569
Epoch 11/100 Iteration 153/234: loss=0.187038 lr=0.000020 grad_norm=0.636771
Epoch 11/100 Iteration 154/234: loss=0.176377 lr=0.000020 grad_norm=0.740609
Epoch 11/100 Iteration 155/234: loss=0.172103 lr=0.000020 grad_norm=0.551860
Epoch 11/100 Iteration 156/234: loss=0.172001 lr=0.000020 grad_norm=0.578794
Epoch 11/100 Iteration 157/234: loss=0.184963 lr=0.000020 grad_norm=0.568488
Epoch 11/100 Iteration 158/234: loss=0.161734 lr=0.000020 grad_norm=0.500055
Epoch 11/100 Iteration 159/234: loss=0.160918 lr=0.000020 grad_norm=0.495673
Epoch 11/100 Iteration 160/234: loss=0.186408 lr=0.000020 grad_norm=0.507070
Epoch 11/100 Iteration 161/234: loss=0.167277 lr=0.000020 grad_norm=0.545893
Epoch 11/100 Iteration 162/234: loss=0.176275 lr=0.000020 grad_norm=0.495734
Epoch 11/100 Iteration 163/234: loss=0.165661 lr=0.000020 grad_norm=0.424680
Epoch 11/100 Iteration 164/234: loss=0.189006 lr=0.000020 grad_norm=0.457473
Epoch 11/100 Iteration 165/234: loss=0.178868 lr=0.000020 grad_norm=0.414414
Epoch 11/100 Iteration 166/234: loss=0.179733 lr=0.000020 grad_norm=0.432435
Epoch 11/100 Iteration 167/234: loss=0.177970 lr=0.000020 grad_norm=0.375677
Epoch 11/100 Iteration 168/234: loss=0.198465 lr=0.000020 grad_norm=0.374289
Epoch 11/100 Iteration 169/234: loss=0.153952 lr=0.000020 grad_norm=0.335605
Epoch 11/100 Iteration 170/234: loss=0.167634 lr=0.000020 grad_norm=0.595608
Epoch 11/100 Iteration 171/234: loss=0.174076 lr=0.000020 grad_norm=0.568607
Epoch 11/100 Iteration 172/234: loss=0.176169 lr=0.000020 grad_norm=0.361280
Epoch 11/100 Iteration 173/234: loss=0.176249 lr=0.000020 grad_norm=0.443153
Epoch 11/100 Iteration 174/234: loss=0.155337 lr=0.000020 grad_norm=0.434196
Epoch 11/100 Iteration 175/234: loss=0.183260 lr=0.000020 grad_norm=0.443678
Epoch 11/100 Iteration 176/234: loss=0.156955 lr=0.000020 grad_norm=0.422167
Epoch 11/100 Iteration 177/234: loss=0.175078 lr=0.000020 grad_norm=0.560601
Epoch 11/100 Iteration 178/234: loss=0.177282 lr=0.000020 grad_norm=0.579004
Epoch 11/100 Iteration 179/234: loss=0.170633 lr=0.000020 grad_norm=0.874063
Epoch 11/100 Iteration 180/234: loss=0.177810 lr=0.000020 grad_norm=0.520896
Epoch 11/100 Iteration 181/234: loss=0.157036 lr=0.000020 grad_norm=0.549612
Epoch 11/100 Iteration 182/234: loss=0.152597 lr=0.000020 grad_norm=0.699997
Epoch 11/100 Iteration 183/234: loss=0.152326 lr=0.000020 grad_norm=0.408003
Epoch 11/100 Iteration 184/234: loss=0.169261 lr=0.000020 grad_norm=0.423237
Epoch 11/100 Iteration 185/234: loss=0.166736 lr=0.000020 grad_norm=0.469364
Epoch 11/100 Iteration 186/234: loss=0.164431 lr=0.000020 grad_norm=0.648944
Epoch 11/100 Iteration 187/234: loss=0.154246 lr=0.000020 grad_norm=0.754111
Epoch 11/100 Iteration 188/234: loss=0.180427 lr=0.000020 grad_norm=0.585608
Epoch 11/100 Iteration 189/234: loss=0.203638 lr=0.000020 grad_norm=0.514059
Epoch 11/100 Iteration 190/234: loss=0.167315 lr=0.000020 grad_norm=0.846144
Epoch 11/100 Iteration 191/234: loss=0.170300 lr=0.000020 grad_norm=0.664304
Epoch 11/100 Iteration 192/234: loss=0.179467 lr=0.000020 grad_norm=0.554515
Epoch 11/100 Iteration 193/234: loss=0.179586 lr=0.000020 grad_norm=0.401249
Epoch 11/100 Iteration 194/234: loss=0.174302 lr=0.000020 grad_norm=0.548104
Epoch 11/100 Iteration 195/234: loss=0.176078 lr=0.000020 grad_norm=0.489049
Epoch 11/100 Iteration 196/234: loss=0.160434 lr=0.000020 grad_norm=0.286072
Epoch 11/100 Iteration 197/234: loss=0.179936 lr=0.000020 grad_norm=0.398598
Epoch 11/100 Iteration 198/234: loss=0.158743 lr=0.000020 grad_norm=0.515737
Epoch 11/100 Iteration 199/234: loss=0.164161 lr=0.000020 grad_norm=0.451203
Epoch 11/100 Iteration 200/234: loss=0.165526 lr=0.000020 grad_norm=0.372680
Epoch 11/100 Iteration 201/234: loss=0.160142 lr=0.000020 grad_norm=0.439591
Epoch 11/100 Iteration 202/234: loss=0.165201 lr=0.000020 grad_norm=0.383112
Epoch 11/100 Iteration 203/234: loss=0.166826 lr=0.000020 grad_norm=0.473232
Epoch 11/100 Iteration 204/234: loss=0.173551 lr=0.000020 grad_norm=0.629519
Epoch 11/100 Iteration 205/234: loss=0.172925 lr=0.000020 grad_norm=0.703272
Epoch 11/100 Iteration 206/234: loss=0.181790 lr=0.000020 grad_norm=0.750858
Epoch 11/100 Iteration 207/234: loss=0.181297 lr=0.000020 grad_norm=0.511418
Epoch 11/100 Iteration 208/234: loss=0.164524 lr=0.000020 grad_norm=0.465564
Epoch 11/100 Iteration 209/234: loss=0.169022 lr=0.000020 grad_norm=0.725184
Epoch 11/100 Iteration 210/234: loss=0.178041 lr=0.000020 grad_norm=0.600143
Epoch 11/100 Iteration 211/234: loss=0.176628 lr=0.000020 grad_norm=0.529753
Epoch 11/100 Iteration 212/234: loss=0.172285 lr=0.000020 grad_norm=1.067458
Epoch 11/100 Iteration 213/234: loss=0.169222 lr=0.000020 grad_norm=1.083406
Epoch 11/100 Iteration 214/234: loss=0.178519 lr=0.000020 grad_norm=0.435313
Epoch 11/100 Iteration 215/234: loss=0.171878 lr=0.000020 grad_norm=0.708284
Epoch 11/100 Iteration 216/234: loss=0.165018 lr=0.000020 grad_norm=1.198903
Epoch 11/100 Iteration 217/234: loss=0.167116 lr=0.000020 grad_norm=1.102537
Epoch 11/100 Iteration 218/234: loss=0.175936 lr=0.000020 grad_norm=0.400106
Epoch 11/100 Iteration 219/234: loss=0.151811 lr=0.000020 grad_norm=0.644465
Epoch 11/100 Iteration 220/234: loss=0.156939 lr=0.000020 grad_norm=0.527574
Epoch 11/100 Iteration 221/234: loss=0.186348 lr=0.000020 grad_norm=0.532686
Epoch 11/100 Iteration 222/234: loss=0.173046 lr=0.000020 grad_norm=0.571266
Epoch 11/100 Iteration 223/234: loss=0.158266 lr=0.000020 grad_norm=0.586285
Epoch 11/100 Iteration 224/234: loss=0.167152 lr=0.000020 grad_norm=0.475477
Epoch 11/100 Iteration 225/234: loss=0.160992 lr=0.000020 grad_norm=0.562160
Epoch 11/100 Iteration 226/234: loss=0.179420 lr=0.000020 grad_norm=0.310691
Epoch 11/100 Iteration 227/234: loss=0.189617 lr=0.000020 grad_norm=0.800399
Epoch 11/100 Iteration 228/234: loss=0.183270 lr=0.000020 grad_norm=0.773039
Epoch 11/100 Iteration 229/234: loss=0.181686 lr=0.000020 grad_norm=0.504253
Epoch 11/100 Iteration 230/234: loss=0.182047 lr=0.000020 grad_norm=0.290878
Epoch 11/100 Iteration 231/234: loss=0.169863 lr=0.000020 grad_norm=0.434557
Epoch 11/100 Iteration 232/234: loss=0.174215 lr=0.000020 grad_norm=0.268287
Epoch 11/100 Iteration 233/234: loss=0.177115 lr=0.000020 grad_norm=0.384602
Epoch 11/100 Iteration 234/234: loss=0.165364 lr=0.000020 grad_norm=0.392329
Epoch 11/100 finished. Avg Loss: 0.175465
Epoch 12/100 Iteration 1/234: loss=0.173665 lr=0.000020 grad_norm=0.360221
Epoch 12/100 Iteration 2/234: loss=0.173101 lr=0.000020 grad_norm=0.374305
Epoch 12/100 Iteration 3/234: loss=0.178566 lr=0.000020 grad_norm=0.573656
Epoch 12/100 Iteration 4/234: loss=0.184658 lr=0.000020 grad_norm=0.844057
Epoch 12/100 Iteration 5/234: loss=0.176909 lr=0.000020 grad_norm=0.705786
Epoch 12/100 Iteration 6/234: loss=0.166841 lr=0.000020 grad_norm=0.351627
Epoch 12/100 Iteration 7/234: loss=0.161744 lr=0.000020 grad_norm=0.574440
Epoch 12/100 Iteration 8/234: loss=0.183648 lr=0.000020 grad_norm=0.525648
Epoch 12/100 Iteration 9/234: loss=0.151386 lr=0.000020 grad_norm=0.626807
Epoch 12/100 Iteration 10/234: loss=0.169654 lr=0.000020 grad_norm=0.579279
Epoch 12/100 Iteration 11/234: loss=0.173110 lr=0.000020 grad_norm=0.513333
Epoch 12/100 Iteration 12/234: loss=0.159139 lr=0.000020 grad_norm=0.385202
Epoch 12/100 Iteration 13/234: loss=0.188744 lr=0.000020 grad_norm=0.492817
Epoch 12/100 Iteration 14/234: loss=0.169628 lr=0.000020 grad_norm=0.910603
Epoch 12/100 Iteration 15/234: loss=0.152440 lr=0.000020 grad_norm=1.091588
Epoch 12/100 Iteration 16/234: loss=0.169775 lr=0.000020 grad_norm=0.919656
Epoch 12/100 Iteration 17/234: loss=0.162504 lr=0.000020 grad_norm=0.440892
Epoch 12/100 Iteration 18/234: loss=0.172868 lr=0.000020 grad_norm=0.731297
Epoch 12/100 Iteration 19/234: loss=0.174955 lr=0.000020 grad_norm=0.976759
Epoch 12/100 Iteration 20/234: loss=0.181340 lr=0.000020 grad_norm=0.494901
Epoch 12/100 Iteration 21/234: loss=0.164456 lr=0.000020 grad_norm=0.513462
Epoch 12/100 Iteration 22/234: loss=0.178495 lr=0.000020 grad_norm=0.900070
Epoch 12/100 Iteration 23/234: loss=0.182992 lr=0.000020 grad_norm=0.858850
Epoch 12/100 Iteration 24/234: loss=0.175073 lr=0.000020 grad_norm=0.558484
Epoch 12/100 Iteration 25/234: loss=0.168926 lr=0.000020 grad_norm=0.411460
Epoch 12/100 Iteration 26/234: loss=0.169894 lr=0.000020 grad_norm=0.562691
Epoch 12/100 Iteration 27/234: loss=0.167343 lr=0.000020 grad_norm=0.713875
Epoch 12/100 Iteration 28/234: loss=0.174140 lr=0.000020 grad_norm=0.465682
Epoch 12/100 Iteration 29/234: loss=0.166917 lr=0.000020 grad_norm=0.354411
Epoch 12/100 Iteration 30/234: loss=0.177406 lr=0.000020 grad_norm=0.648790
Epoch 12/100 Iteration 31/234: loss=0.155663 lr=0.000020 grad_norm=0.602360
Epoch 12/100 Iteration 32/234: loss=0.173434 lr=0.000020 grad_norm=0.466194
Epoch 12/100 Iteration 33/234: loss=0.158881 lr=0.000020 grad_norm=0.431403
Epoch 12/100 Iteration 34/234: loss=0.170538 lr=0.000020 grad_norm=0.505870
Epoch 12/100 Iteration 35/234: loss=0.162191 lr=0.000020 grad_norm=0.607818
Epoch 12/100 Iteration 36/234: loss=0.172299 lr=0.000020 grad_norm=0.838720
Epoch 12/100 Iteration 37/234: loss=0.168645 lr=0.000020 grad_norm=0.813455
Epoch 12/100 Iteration 38/234: loss=0.159077 lr=0.000020 grad_norm=0.406337
Epoch 12/100 Iteration 39/234: loss=0.173633 lr=0.000020 grad_norm=0.892405
Epoch 12/100 Iteration 40/234: loss=0.184501 lr=0.000020 grad_norm=0.617762
Epoch 12/100 Iteration 41/234: loss=0.178062 lr=0.000020 grad_norm=0.685721
Epoch 12/100 Iteration 42/234: loss=0.179002 lr=0.000020 grad_norm=1.194831
Epoch 12/100 Iteration 43/234: loss=0.178833 lr=0.000020 grad_norm=0.935020
Epoch 12/100 Iteration 44/234: loss=0.161403 lr=0.000020 grad_norm=0.369956
Epoch 12/100 Iteration 45/234: loss=0.151363 lr=0.000020 grad_norm=0.912767
Epoch 12/100 Iteration 46/234: loss=0.158722 lr=0.000020 grad_norm=0.524802
Epoch 12/100 Iteration 47/234: loss=0.182796 lr=0.000020 grad_norm=0.688332
Epoch 12/100 Iteration 48/234: loss=0.170333 lr=0.000020 grad_norm=0.924885
Epoch 12/100 Iteration 49/234: loss=0.180124 lr=0.000020 grad_norm=0.423505
Epoch 12/100 Iteration 50/234: loss=0.164611 lr=0.000020 grad_norm=0.519736
Epoch 12/100 Iteration 51/234: loss=0.185638 lr=0.000020 grad_norm=0.583327
Epoch 12/100 Iteration 52/234: loss=0.168026 lr=0.000020 grad_norm=0.422257
Epoch 12/100 Iteration 53/234: loss=0.167605 lr=0.000020 grad_norm=0.587379
Epoch 12/100 Iteration 54/234: loss=0.174011 lr=0.000020 grad_norm=0.693197
Epoch 12/100 Iteration 55/234: loss=0.167262 lr=0.000020 grad_norm=0.324906
Epoch 12/100 Iteration 56/234: loss=0.160207 lr=0.000020 grad_norm=0.855865
Epoch 12/100 Iteration 57/234: loss=0.179343 lr=0.000020 grad_norm=1.212361
Epoch 12/100 Iteration 58/234: loss=0.156075 lr=0.000020 grad_norm=0.978115
Epoch 12/100 Iteration 59/234: loss=0.166741 lr=0.000020 grad_norm=0.434460
Epoch 12/100 Iteration 60/234: loss=0.177177 lr=0.000020 grad_norm=0.712216
Epoch 12/100 Iteration 61/234: loss=0.167534 lr=0.000020 grad_norm=0.407453
Epoch 12/100 Iteration 62/234: loss=0.142534 lr=0.000020 grad_norm=0.474769
Epoch 12/100 Iteration 63/234: loss=0.181372 lr=0.000020 grad_norm=0.532116
Epoch 12/100 Iteration 64/234: loss=0.169290 lr=0.000020 grad_norm=0.417189
Epoch 12/100 Iteration 65/234: loss=0.168538 lr=0.000020 grad_norm=0.537736
Epoch 12/100 Iteration 66/234: loss=0.177940 lr=0.000020 grad_norm=0.894231
Epoch 12/100 Iteration 67/234: loss=0.171134 lr=0.000020 grad_norm=0.450060
Epoch 12/100 Iteration 68/234: loss=0.169458 lr=0.000020 grad_norm=1.035264
Epoch 12/100 Iteration 69/234: loss=0.160557 lr=0.000020 grad_norm=1.329114
Epoch 12/100 Iteration 70/234: loss=0.169332 lr=0.000020 grad_norm=0.506443
Epoch 12/100 Iteration 71/234: loss=0.182212 lr=0.000020 grad_norm=1.836562
Epoch 12/100 Iteration 72/234: loss=0.162475 lr=0.000020 grad_norm=1.317214
Epoch 12/100 Iteration 73/234: loss=0.173304 lr=0.000020 grad_norm=0.792365
Epoch 12/100 Iteration 74/234: loss=0.171824 lr=0.000020 grad_norm=0.997435
Epoch 12/100 Iteration 75/234: loss=0.170760 lr=0.000020 grad_norm=0.605975
Epoch 12/100 Iteration 76/234: loss=0.157631 lr=0.000020 grad_norm=0.830852
Epoch 12/100 Iteration 77/234: loss=0.183823 lr=0.000020 grad_norm=0.573722
Epoch 12/100 Iteration 78/234: loss=0.184613 lr=0.000020 grad_norm=1.088177
Epoch 12/100 Iteration 79/234: loss=0.170784 lr=0.000020 grad_norm=0.463941
Epoch 12/100 Iteration 80/234: loss=0.172402 lr=0.000020 grad_norm=0.742057
Epoch 12/100 Iteration 81/234: loss=0.149234 lr=0.000020 grad_norm=0.823945
Epoch 12/100 Iteration 82/234: loss=0.161969 lr=0.000020 grad_norm=0.392270
Epoch 12/100 Iteration 83/234: loss=0.172119 lr=0.000020 grad_norm=0.682535
Epoch 12/100 Iteration 84/234: loss=0.168080 lr=0.000020 grad_norm=0.603158
Epoch 12/100 Iteration 85/234: loss=0.153022 lr=0.000020 grad_norm=0.337753
Epoch 12/100 Iteration 86/234: loss=0.167585 lr=0.000020 grad_norm=0.552071
Epoch 12/100 Iteration 87/234: loss=0.173842 lr=0.000020 grad_norm=0.450871
Epoch 12/100 Iteration 88/234: loss=0.152141 lr=0.000020 grad_norm=0.351566
Epoch 12/100 Iteration 89/234: loss=0.165360 lr=0.000020 grad_norm=0.456358
Epoch 12/100 Iteration 90/234: loss=0.144958 lr=0.000020 grad_norm=0.430865
Epoch 12/100 Iteration 91/234: loss=0.161194 lr=0.000020 grad_norm=0.480218
Epoch 12/100 Iteration 92/234: loss=0.164645 lr=0.000020 grad_norm=0.336484
Epoch 12/100 Iteration 93/234: loss=0.169619 lr=0.000020 grad_norm=0.444951
Epoch 12/100 Iteration 94/234: loss=0.174586 lr=0.000020 grad_norm=0.539466
Epoch 12/100 Iteration 95/234: loss=0.169005 lr=0.000020 grad_norm=0.374603
Epoch 12/100 Iteration 96/234: loss=0.171953 lr=0.000020 grad_norm=0.361240
Epoch 12/100 Iteration 97/234: loss=0.170691 lr=0.000020 grad_norm=0.442410
Epoch 12/100 Iteration 98/234: loss=0.165602 lr=0.000020 grad_norm=0.423751
Epoch 12/100 Iteration 99/234: loss=0.159093 lr=0.000020 grad_norm=0.464955
Epoch 12/100 Iteration 100/234: loss=0.161633 lr=0.000020 grad_norm=0.512571
Epoch 12/100 Iteration 101/234: loss=0.159034 lr=0.000020 grad_norm=0.503313
Epoch 12/100 Iteration 102/234: loss=0.180419 lr=0.000020 grad_norm=0.716819
Epoch 12/100 Iteration 103/234: loss=0.166873 lr=0.000020 grad_norm=0.570181
Epoch 12/100 Iteration 104/234: loss=0.165295 lr=0.000020 grad_norm=0.628435
Epoch 12/100 Iteration 105/234: loss=0.152785 lr=0.000020 grad_norm=0.806158
Epoch 12/100 Iteration 106/234: loss=0.175155 lr=0.000020 grad_norm=0.585430
Epoch 12/100 Iteration 107/234: loss=0.177527 lr=0.000020 grad_norm=0.671663
Epoch 12/100 Iteration 108/234: loss=0.166573 lr=0.000020 grad_norm=0.765006
Epoch 12/100 Iteration 109/234: loss=0.180683 lr=0.000020 grad_norm=0.423706
Epoch 12/100 Iteration 110/234: loss=0.155758 lr=0.000020 grad_norm=0.517877
Epoch 12/100 Iteration 111/234: loss=0.179619 lr=0.000020 grad_norm=0.426283
Epoch 12/100 Iteration 112/234: loss=0.190991 lr=0.000020 grad_norm=0.549289
Epoch 12/100 Iteration 113/234: loss=0.174193 lr=0.000020 grad_norm=0.719029
Epoch 12/100 Iteration 114/234: loss=0.140890 lr=0.000020 grad_norm=0.503080
Epoch 12/100 Iteration 115/234: loss=0.163381 lr=0.000020 grad_norm=0.311474
Epoch 12/100 Iteration 116/234: loss=0.187586 lr=0.000020 grad_norm=0.653824
Epoch 12/100 Iteration 117/234: loss=0.153600 lr=0.000020 grad_norm=0.771843
Epoch 12/100 Iteration 118/234: loss=0.178887 lr=0.000020 grad_norm=0.372261
Epoch 12/100 Iteration 119/234: loss=0.179451 lr=0.000020 grad_norm=0.474919
Epoch 12/100 Iteration 120/234: loss=0.168900 lr=0.000020 grad_norm=0.483965
Epoch 12/100 Iteration 121/234: loss=0.181061 lr=0.000020 grad_norm=0.445874
Epoch 12/100 Iteration 122/234: loss=0.184029 lr=0.000020 grad_norm=0.425902
Epoch 12/100 Iteration 123/234: loss=0.154252 lr=0.000020 grad_norm=0.552426
Epoch 12/100 Iteration 124/234: loss=0.177732 lr=0.000020 grad_norm=0.316839
Epoch 12/100 Iteration 125/234: loss=0.190581 lr=0.000020 grad_norm=0.580121
Epoch 12/100 Iteration 126/234: loss=0.151728 lr=0.000020 grad_norm=0.840468
Epoch 12/100 Iteration 127/234: loss=0.166704 lr=0.000020 grad_norm=0.387644
Epoch 12/100 Iteration 128/234: loss=0.160810 lr=0.000020 grad_norm=0.756573
Epoch 12/100 Iteration 129/234: loss=0.173512 lr=0.000020 grad_norm=0.523629
Epoch 12/100 Iteration 130/234: loss=0.167271 lr=0.000020 grad_norm=0.449595
Epoch 12/100 Iteration 131/234: loss=0.186307 lr=0.000020 grad_norm=0.444158
Epoch 12/100 Iteration 132/234: loss=0.162595 lr=0.000020 grad_norm=0.490144
Epoch 12/100 Iteration 133/234: loss=0.160909 lr=0.000020 grad_norm=0.589382
Epoch 12/100 Iteration 134/234: loss=0.164359 lr=0.000020 grad_norm=0.792370
Epoch 12/100 Iteration 135/234: loss=0.160497 lr=0.000020 grad_norm=0.721715
Epoch 12/100 Iteration 136/234: loss=0.169790 lr=0.000020 grad_norm=0.726965
Epoch 12/100 Iteration 137/234: loss=0.180284 lr=0.000020 grad_norm=0.838803
Epoch 12/100 Iteration 138/234: loss=0.176554 lr=0.000020 grad_norm=0.576308
Epoch 12/100 Iteration 139/234: loss=0.173170 lr=0.000020 grad_norm=0.647714
Epoch 12/100 Iteration 140/234: loss=0.151696 lr=0.000020 grad_norm=0.877432
Epoch 12/100 Iteration 141/234: loss=0.171862 lr=0.000020 grad_norm=0.555935
Epoch 12/100 Iteration 142/234: loss=0.182255 lr=0.000020 grad_norm=0.551119
Epoch 12/100 Iteration 143/234: loss=0.167712 lr=0.000020 grad_norm=0.924284
Epoch 12/100 Iteration 144/234: loss=0.175042 lr=0.000020 grad_norm=0.735304
Epoch 12/100 Iteration 145/234: loss=0.155664 lr=0.000020 grad_norm=0.643308
Epoch 12/100 Iteration 146/234: loss=0.176441 lr=0.000020 grad_norm=0.731523
Epoch 12/100 Iteration 147/234: loss=0.173429 lr=0.000020 grad_norm=0.668614
Epoch 12/100 Iteration 148/234: loss=0.158933 lr=0.000020 grad_norm=0.361261
Epoch 12/100 Iteration 149/234: loss=0.142720 lr=0.000020 grad_norm=0.575040
Epoch 12/100 Iteration 150/234: loss=0.154773 lr=0.000020 grad_norm=0.764361
Epoch 12/100 Iteration 151/234: loss=0.180331 lr=0.000020 grad_norm=0.820137
Epoch 12/100 Iteration 152/234: loss=0.157211 lr=0.000020 grad_norm=0.673819
Epoch 12/100 Iteration 153/234: loss=0.175729 lr=0.000020 grad_norm=0.363453
Epoch 12/100 Iteration 154/234: loss=0.160873 lr=0.000020 grad_norm=0.837866
Epoch 12/100 Iteration 155/234: loss=0.170101 lr=0.000020 grad_norm=0.939506
Epoch 12/100 Iteration 156/234: loss=0.147652 lr=0.000020 grad_norm=0.481002
Epoch 12/100 Iteration 157/234: loss=0.169127 lr=0.000020 grad_norm=0.558973
Epoch 12/100 Iteration 158/234: loss=0.165792 lr=0.000020 grad_norm=0.713341
Epoch 12/100 Iteration 159/234: loss=0.162970 lr=0.000020 grad_norm=0.413145
Epoch 12/100 Iteration 160/234: loss=0.162958 lr=0.000020 grad_norm=0.634613
Epoch 12/100 Iteration 161/234: loss=0.165333 lr=0.000020 grad_norm=0.950274
Epoch 12/100 Iteration 162/234: loss=0.157349 lr=0.000020 grad_norm=0.725403
Epoch 12/100 Iteration 163/234: loss=0.169010 lr=0.000020 grad_norm=0.372031
Epoch 12/100 Iteration 164/234: loss=0.175761 lr=0.000020 grad_norm=0.351333
Epoch 12/100 Iteration 165/234: loss=0.164428 lr=0.000020 grad_norm=0.245843
Epoch 12/100 Iteration 166/234: loss=0.174834 lr=0.000020 grad_norm=0.292779
Epoch 12/100 Iteration 167/234: loss=0.181344 lr=0.000020 grad_norm=0.347221
Epoch 12/100 Iteration 168/234: loss=0.163580 lr=0.000020 grad_norm=0.377361
Epoch 12/100 Iteration 169/234: loss=0.162296 lr=0.000020 grad_norm=0.352638
Epoch 12/100 Iteration 170/234: loss=0.185559 lr=0.000020 grad_norm=0.253144
Epoch 12/100 Iteration 171/234: loss=0.160850 lr=0.000020 grad_norm=0.404891
Epoch 12/100 Iteration 172/234: loss=0.149691 lr=0.000020 grad_norm=0.409912
Epoch 12/100 Iteration 173/234: loss=0.157586 lr=0.000020 grad_norm=0.339433
Epoch 12/100 Iteration 174/234: loss=0.183708 lr=0.000020 grad_norm=0.448608
Epoch 12/100 Iteration 175/234: loss=0.170625 lr=0.000020 grad_norm=0.495778
Epoch 12/100 Iteration 176/234: loss=0.185050 lr=0.000020 grad_norm=0.635527
Epoch 12/100 Iteration 177/234: loss=0.153296 lr=0.000020 grad_norm=0.931769
Epoch 12/100 Iteration 178/234: loss=0.157320 lr=0.000020 grad_norm=0.766251
Epoch 12/100 Iteration 179/234: loss=0.192959 lr=0.000020 grad_norm=0.257322
Epoch 12/100 Iteration 180/234: loss=0.167017 lr=0.000020 grad_norm=0.797588
Epoch 12/100 Iteration 181/234: loss=0.185882 lr=0.000020 grad_norm=0.986574
Epoch 12/100 Iteration 182/234: loss=0.174363 lr=0.000020 grad_norm=0.547026
Epoch 12/100 Iteration 183/234: loss=0.173535 lr=0.000020 grad_norm=0.661814
Epoch 12/100 Iteration 184/234: loss=0.174140 lr=0.000020 grad_norm=1.015514
Epoch 12/100 Iteration 185/234: loss=0.164135 lr=0.000020 grad_norm=0.787215
Epoch 12/100 Iteration 186/234: loss=0.174567 lr=0.000020 grad_norm=0.404681
Epoch 12/100 Iteration 187/234: loss=0.171636 lr=0.000020 grad_norm=0.660379
Epoch 12/100 Iteration 188/234: loss=0.171363 lr=0.000020 grad_norm=0.658716
Epoch 12/100 Iteration 189/234: loss=0.163891 lr=0.000020 grad_norm=0.515866
Epoch 12/100 Iteration 190/234: loss=0.161272 lr=0.000020 grad_norm=0.457749
Epoch 12/100 Iteration 191/234: loss=0.156008 lr=0.000020 grad_norm=0.429745
Epoch 12/100 Iteration 192/234: loss=0.147674 lr=0.000020 grad_norm=0.512796
Epoch 12/100 Iteration 193/234: loss=0.162425 lr=0.000020 grad_norm=0.553210
Epoch 12/100 Iteration 194/234: loss=0.176272 lr=0.000020 grad_norm=0.572480
Epoch 12/100 Iteration 195/234: loss=0.172007 lr=0.000020 grad_norm=0.531661
Epoch 12/100 Iteration 196/234: loss=0.170299 lr=0.000020 grad_norm=0.886774
Epoch 12/100 Iteration 197/234: loss=0.157079 lr=0.000020 grad_norm=0.498984
Epoch 12/100 Iteration 198/234: loss=0.180406 lr=0.000020 grad_norm=0.631309
Epoch 12/100 Iteration 199/234: loss=0.152014 lr=0.000020 grad_norm=0.907284
Epoch 12/100 Iteration 200/234: loss=0.159427 lr=0.000020 grad_norm=0.455863
Epoch 12/100 Iteration 201/234: loss=0.155306 lr=0.000020 grad_norm=0.611464
Epoch 12/100 Iteration 202/234: loss=0.167312 lr=0.000020 grad_norm=0.684118
Epoch 12/100 Iteration 203/234: loss=0.160789 lr=0.000020 grad_norm=0.445253
Epoch 12/100 Iteration 204/234: loss=0.155654 lr=0.000020 grad_norm=0.464573
Epoch 12/100 Iteration 205/234: loss=0.150128 lr=0.000020 grad_norm=0.454660
Epoch 12/100 Iteration 206/234: loss=0.171977 lr=0.000020 grad_norm=0.510848
Epoch 12/100 Iteration 207/234: loss=0.158992 lr=0.000020 grad_norm=0.481377
Epoch 12/100 Iteration 208/234: loss=0.159295 lr=0.000020 grad_norm=0.405298
Epoch 12/100 Iteration 209/234: loss=0.168065 lr=0.000020 grad_norm=0.345786
Epoch 12/100 Iteration 210/234: loss=0.184684 lr=0.000020 grad_norm=0.565485
Epoch 12/100 Iteration 211/234: loss=0.170387 lr=0.000020 grad_norm=0.764190
Epoch 12/100 Iteration 212/234: loss=0.163874 lr=0.000020 grad_norm=0.853091
Epoch 12/100 Iteration 213/234: loss=0.175688 lr=0.000020 grad_norm=0.996701
Epoch 12/100 Iteration 214/234: loss=0.159296 lr=0.000020 grad_norm=0.841434
Epoch 12/100 Iteration 215/234: loss=0.163307 lr=0.000020 grad_norm=0.478652
Epoch 12/100 Iteration 216/234: loss=0.173769 lr=0.000020 grad_norm=0.749698
Epoch 12/100 Iteration 217/234: loss=0.147919 lr=0.000020 grad_norm=1.038306
Epoch 12/100 Iteration 218/234: loss=0.190571 lr=0.000020 grad_norm=1.089969
Epoch 12/100 Iteration 219/234: loss=0.166808 lr=0.000020 grad_norm=0.894497
Epoch 12/100 Iteration 220/234: loss=0.170827 lr=0.000020 grad_norm=0.464010
Epoch 12/100 Iteration 221/234: loss=0.164363 lr=0.000020 grad_norm=0.863165
Epoch 12/100 Iteration 222/234: loss=0.155180 lr=0.000020 grad_norm=0.948050
Epoch 12/100 Iteration 223/234: loss=0.182830 lr=0.000020 grad_norm=0.538257
Epoch 12/100 Iteration 224/234: loss=0.166574 lr=0.000020 grad_norm=1.031574
Epoch 12/100 Iteration 225/234: loss=0.158075 lr=0.000020 grad_norm=1.430609
Epoch 12/100 Iteration 226/234: loss=0.167141 lr=0.000020 grad_norm=1.226485
Epoch 12/100 Iteration 227/234: loss=0.178767 lr=0.000020 grad_norm=0.449720
Epoch 12/100 Iteration 228/234: loss=0.169192 lr=0.000020 grad_norm=0.839672
Epoch 12/100 Iteration 229/234: loss=0.151050 lr=0.000020 grad_norm=0.885191
Epoch 12/100 Iteration 230/234: loss=0.173087 lr=0.000020 grad_norm=0.566497
Epoch 12/100 Iteration 231/234: loss=0.171868 lr=0.000020 grad_norm=0.952970
Epoch 12/100 Iteration 232/234: loss=0.177741 lr=0.000020 grad_norm=0.988368
Epoch 12/100 Iteration 233/234: loss=0.169516 lr=0.000020 grad_norm=0.414169
Epoch 12/100 Iteration 234/234: loss=0.169557 lr=0.000020 grad_norm=0.826988
Epoch 12/100 finished. Avg Loss: 0.168457
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 13/100 Iteration 1/234: loss=0.158429 lr=0.000020 grad_norm=0.772896
Epoch 13/100 Iteration 2/234: loss=0.169445 lr=0.000020 grad_norm=0.412235
Epoch 13/100 Iteration 3/234: loss=0.178664 lr=0.000020 grad_norm=0.626133
Epoch 13/100 Iteration 4/234: loss=0.161445 lr=0.000020 grad_norm=0.857375
Epoch 13/100 Iteration 5/234: loss=0.168227 lr=0.000020 grad_norm=0.550651
Epoch 13/100 Iteration 6/234: loss=0.156699 lr=0.000020 grad_norm=0.595181
Epoch 13/100 Iteration 7/234: loss=0.155503 lr=0.000020 grad_norm=0.998443
Epoch 13/100 Iteration 8/234: loss=0.155382 lr=0.000020 grad_norm=0.526414
Epoch 13/100 Iteration 9/234: loss=0.185068 lr=0.000020 grad_norm=0.702189
Epoch 13/100 Iteration 10/234: loss=0.170291 lr=0.000020 grad_norm=0.836287
Epoch 13/100 Iteration 11/234: loss=0.152445 lr=0.000020 grad_norm=0.378460
Epoch 13/100 Iteration 12/234: loss=0.169607 lr=0.000020 grad_norm=0.632008
Epoch 13/100 Iteration 13/234: loss=0.161710 lr=0.000020 grad_norm=0.769902
Epoch 13/100 Iteration 14/234: loss=0.164268 lr=0.000020 grad_norm=0.401396
Epoch 13/100 Iteration 15/234: loss=0.164642 lr=0.000020 grad_norm=0.496246
Epoch 13/100 Iteration 16/234: loss=0.167547 lr=0.000020 grad_norm=0.816950
Epoch 13/100 Iteration 17/234: loss=0.154595 lr=0.000020 grad_norm=0.642404
Epoch 13/100 Iteration 18/234: loss=0.166880 lr=0.000020 grad_norm=0.385111
Epoch 13/100 Iteration 19/234: loss=0.162000 lr=0.000020 grad_norm=0.744285
Epoch 13/100 Iteration 20/234: loss=0.166050 lr=0.000020 grad_norm=0.820583
Epoch 13/100 Iteration 21/234: loss=0.184033 lr=0.000020 grad_norm=0.595159
Epoch 13/100 Iteration 22/234: loss=0.166854 lr=0.000020 grad_norm=0.316742
Epoch 13/100 Iteration 23/234: loss=0.184670 lr=0.000020 grad_norm=0.964961
Epoch 13/100 Iteration 24/234: loss=0.174409 lr=0.000020 grad_norm=0.944651
Epoch 13/100 Iteration 25/234: loss=0.157814 lr=0.000020 grad_norm=0.334453
Epoch 13/100 Iteration 26/234: loss=0.174386 lr=0.000020 grad_norm=0.792969
Epoch 13/100 Iteration 27/234: loss=0.186756 lr=0.000020 grad_norm=0.723494
Epoch 13/100 Iteration 28/234: loss=0.139838 lr=0.000020 grad_norm=0.544763
Epoch 13/100 Iteration 29/234: loss=0.161280 lr=0.000020 grad_norm=0.385184
Epoch 13/100 Iteration 30/234: loss=0.175498 lr=0.000020 grad_norm=0.443870
Epoch 13/100 Iteration 31/234: loss=0.153852 lr=0.000020 grad_norm=0.487550
Epoch 13/100 Iteration 32/234: loss=0.174505 lr=0.000020 grad_norm=0.479789
Epoch 13/100 Iteration 33/234: loss=0.161222 lr=0.000020 grad_norm=0.373340
Epoch 13/100 Iteration 34/234: loss=0.162143 lr=0.000020 grad_norm=0.394695
Epoch 13/100 Iteration 35/234: loss=0.140497 lr=0.000020 grad_norm=0.654694
Epoch 13/100 Iteration 36/234: loss=0.165233 lr=0.000020 grad_norm=0.626895
Epoch 13/100 Iteration 37/234: loss=0.161395 lr=0.000020 grad_norm=0.473853
Epoch 13/100 Iteration 38/234: loss=0.158950 lr=0.000020 grad_norm=0.384185
Epoch 13/100 Iteration 39/234: loss=0.157295 lr=0.000020 grad_norm=0.389949
Epoch 13/100 Iteration 40/234: loss=0.155385 lr=0.000020 grad_norm=0.435051
Epoch 13/100 Iteration 41/234: loss=0.158882 lr=0.000020 grad_norm=0.393584
Epoch 13/100 Iteration 42/234: loss=0.168873 lr=0.000020 grad_norm=0.366518
Epoch 13/100 Iteration 43/234: loss=0.163092 lr=0.000020 grad_norm=0.373213
Epoch 13/100 Iteration 44/234: loss=0.186438 lr=0.000020 grad_norm=0.555426
Epoch 13/100 Iteration 45/234: loss=0.155106 lr=0.000020 grad_norm=0.519675
Epoch 13/100 Iteration 46/234: loss=0.168572 lr=0.000020 grad_norm=0.535519
Epoch 13/100 Iteration 47/234: loss=0.166497 lr=0.000020 grad_norm=0.595598
Epoch 13/100 Iteration 48/234: loss=0.153535 lr=0.000020 grad_norm=0.395965
Epoch 13/100 Iteration 49/234: loss=0.157123 lr=0.000020 grad_norm=0.312787
Epoch 13/100 Iteration 50/234: loss=0.169808 lr=0.000020 grad_norm=0.667477
Epoch 13/100 Iteration 51/234: loss=0.164198 lr=0.000020 grad_norm=0.964416
Epoch 13/100 Iteration 52/234: loss=0.165899 lr=0.000020 grad_norm=0.775810
Epoch 13/100 Iteration 53/234: loss=0.159951 lr=0.000020 grad_norm=0.362068
Epoch 13/100 Iteration 54/234: loss=0.163405 lr=0.000020 grad_norm=0.748547
Epoch 13/100 Iteration 55/234: loss=0.162178 lr=0.000020 grad_norm=0.677505
Epoch 13/100 Iteration 56/234: loss=0.151268 lr=0.000020 grad_norm=0.645801
Epoch 13/100 Iteration 57/234: loss=0.159287 lr=0.000020 grad_norm=1.136275
Epoch 13/100 Iteration 58/234: loss=0.149475 lr=0.000020 grad_norm=0.893360
Epoch 13/100 Iteration 59/234: loss=0.159678 lr=0.000020 grad_norm=0.525569
Epoch 13/100 Iteration 60/234: loss=0.163781 lr=0.000020 grad_norm=0.891426
Epoch 13/100 Iteration 61/234: loss=0.158445 lr=0.000020 grad_norm=0.614446
Epoch 13/100 Iteration 62/234: loss=0.161691 lr=0.000020 grad_norm=0.368702
Epoch 13/100 Iteration 63/234: loss=0.169807 lr=0.000020 grad_norm=0.534511
Epoch 13/100 Iteration 64/234: loss=0.164390 lr=0.000020 grad_norm=0.528323
Epoch 13/100 Iteration 65/234: loss=0.182297 lr=0.000020 grad_norm=0.412502
Epoch 13/100 Iteration 66/234: loss=0.166035 lr=0.000020 grad_norm=0.773426
Epoch 13/100 Iteration 67/234: loss=0.179221 lr=0.000020 grad_norm=0.706759
Epoch 13/100 Iteration 68/234: loss=0.156869 lr=0.000020 grad_norm=0.625015
Epoch 13/100 Iteration 69/234: loss=0.182090 lr=0.000020 grad_norm=0.593412
Epoch 13/100 Iteration 70/234: loss=0.153804 lr=0.000020 grad_norm=0.534308
Epoch 13/100 Iteration 71/234: loss=0.169335 lr=0.000020 grad_norm=0.453787
Epoch 13/100 Iteration 72/234: loss=0.162761 lr=0.000020 grad_norm=0.550023
Epoch 13/100 Iteration 73/234: loss=0.186903 lr=0.000020 grad_norm=0.494611
Epoch 13/100 Iteration 74/234: loss=0.162091 lr=0.000020 grad_norm=0.658813
Epoch 13/100 Iteration 75/234: loss=0.167900 lr=0.000020 grad_norm=0.760346
Epoch 13/100 Iteration 76/234: loss=0.162941 lr=0.000020 grad_norm=0.610041
Epoch 13/100 Iteration 77/234: loss=0.156543 lr=0.000020 grad_norm=0.463727
Epoch 13/100 Iteration 78/234: loss=0.180782 lr=0.000020 grad_norm=0.696980
Epoch 13/100 Iteration 79/234: loss=0.175906 lr=0.000020 grad_norm=0.874218
Epoch 13/100 Iteration 80/234: loss=0.157286 lr=0.000020 grad_norm=0.570121
Epoch 13/100 Iteration 81/234: loss=0.161995 lr=0.000020 grad_norm=0.501267
Epoch 13/100 Iteration 82/234: loss=0.159542 lr=0.000020 grad_norm=0.653020
Epoch 13/100 Iteration 83/234: loss=0.159272 lr=0.000020 grad_norm=0.467026
Epoch 13/100 Iteration 84/234: loss=0.164811 lr=0.000020 grad_norm=0.662406
Epoch 13/100 Iteration 85/234: loss=0.147804 lr=0.000020 grad_norm=0.712420
Epoch 13/100 Iteration 86/234: loss=0.169947 lr=0.000020 grad_norm=0.531986
Epoch 13/100 Iteration 87/234: loss=0.164447 lr=0.000020 grad_norm=1.018003
Epoch 13/100 Iteration 88/234: loss=0.162458 lr=0.000020 grad_norm=1.196202
Epoch 13/100 Iteration 89/234: loss=0.184215 lr=0.000020 grad_norm=0.801854
Epoch 13/100 Iteration 90/234: loss=0.158328 lr=0.000020 grad_norm=0.621980
Epoch 13/100 Iteration 91/234: loss=0.187550 lr=0.000020 grad_norm=0.479875
Epoch 13/100 Iteration 92/234: loss=0.167116 lr=0.000020 grad_norm=0.592962
Epoch 13/100 Iteration 93/234: loss=0.146022 lr=0.000020 grad_norm=0.612873
Epoch 13/100 Iteration 94/234: loss=0.149466 lr=0.000020 grad_norm=0.384483
Epoch 13/100 Iteration 95/234: loss=0.160991 lr=0.000020 grad_norm=0.771830
Epoch 13/100 Iteration 96/234: loss=0.145456 lr=0.000020 grad_norm=0.790776
Epoch 13/100 Iteration 97/234: loss=0.160737 lr=0.000020 grad_norm=0.665050
Epoch 13/100 Iteration 98/234: loss=0.161243 lr=0.000020 grad_norm=0.778396
Epoch 13/100 Iteration 99/234: loss=0.163824 lr=0.000020 grad_norm=0.346542
Epoch 13/100 Iteration 100/234: loss=0.144211 lr=0.000020 grad_norm=0.791095
Epoch 13/100 Iteration 101/234: loss=0.152784 lr=0.000020 grad_norm=0.632124
Epoch 13/100 Iteration 102/234: loss=0.144833 lr=0.000020 grad_norm=0.381757
Epoch 13/100 Iteration 103/234: loss=0.143346 lr=0.000020 grad_norm=0.372283
Epoch 13/100 Iteration 104/234: loss=0.158745 lr=0.000020 grad_norm=0.355402
Epoch 13/100 Iteration 105/234: loss=0.156871 lr=0.000020 grad_norm=0.263625
Epoch 13/100 Iteration 106/234: loss=0.156127 lr=0.000020 grad_norm=0.459950
Epoch 13/100 Iteration 107/234: loss=0.155829 lr=0.000020 grad_norm=0.445149
Epoch 13/100 Iteration 108/234: loss=0.148635 lr=0.000020 grad_norm=0.531614
Epoch 13/100 Iteration 109/234: loss=0.162244 lr=0.000020 grad_norm=0.707723
Epoch 13/100 Iteration 110/234: loss=0.141439 lr=0.000020 grad_norm=0.622269
Epoch 13/100 Iteration 111/234: loss=0.172137 lr=0.000020 grad_norm=0.547000
Epoch 13/100 Iteration 112/234: loss=0.166137 lr=0.000020 grad_norm=0.558488
Epoch 13/100 Iteration 113/234: loss=0.147051 lr=0.000020 grad_norm=0.664448
Epoch 13/100 Iteration 114/234: loss=0.142628 lr=0.000020 grad_norm=0.619004
Epoch 13/100 Iteration 115/234: loss=0.161031 lr=0.000020 grad_norm=0.453280
Epoch 13/100 Iteration 116/234: loss=0.150160 lr=0.000020 grad_norm=0.637995
Epoch 13/100 Iteration 117/234: loss=0.147938 lr=0.000020 grad_norm=0.624303
Epoch 13/100 Iteration 118/234: loss=0.153483 lr=0.000020 grad_norm=0.714594
Epoch 13/100 Iteration 119/234: loss=0.144993 lr=0.000020 grad_norm=1.152309
Epoch 13/100 Iteration 120/234: loss=0.166269 lr=0.000020 grad_norm=1.175016
Epoch 13/100 Iteration 121/234: loss=0.163576 lr=0.000020 grad_norm=0.622462
Epoch 13/100 Iteration 122/234: loss=0.178046 lr=0.000020 grad_norm=0.408669
Epoch 13/100 Iteration 123/234: loss=0.159522 lr=0.000020 grad_norm=0.864245
Epoch 13/100 Iteration 124/234: loss=0.143948 lr=0.000020 grad_norm=1.024726
Epoch 13/100 Iteration 125/234: loss=0.182679 lr=0.000020 grad_norm=0.640989
Epoch 13/100 Iteration 126/234: loss=0.166476 lr=0.000020 grad_norm=0.448541
Epoch 13/100 Iteration 127/234: loss=0.148659 lr=0.000020 grad_norm=1.010079
Epoch 13/100 Iteration 128/234: loss=0.165978 lr=0.000020 grad_norm=0.906200
Epoch 13/100 Iteration 129/234: loss=0.163328 lr=0.000020 grad_norm=0.592742
Epoch 13/100 Iteration 130/234: loss=0.157239 lr=0.000020 grad_norm=0.548278
Epoch 13/100 Iteration 131/234: loss=0.157885 lr=0.000020 grad_norm=0.573801
Epoch 13/100 Iteration 132/234: loss=0.168450 lr=0.000020 grad_norm=0.996272
Epoch 13/100 Iteration 133/234: loss=0.173868 lr=0.000020 grad_norm=1.032669
Epoch 13/100 Iteration 134/234: loss=0.169690 lr=0.000020 grad_norm=0.425261
Epoch 13/100 Iteration 135/234: loss=0.164459 lr=0.000020 grad_norm=0.698932
Epoch 13/100 Iteration 136/234: loss=0.163493 lr=0.000020 grad_norm=0.588227
Epoch 13/100 Iteration 137/234: loss=0.168052 lr=0.000020 grad_norm=0.602265
Epoch 13/100 Iteration 138/234: loss=0.160158 lr=0.000020 grad_norm=0.532491
Epoch 13/100 Iteration 139/234: loss=0.164760 lr=0.000020 grad_norm=0.375797
Epoch 13/100 Iteration 140/234: loss=0.169576 lr=0.000020 grad_norm=0.411364
Epoch 13/100 Iteration 141/234: loss=0.146421 lr=0.000020 grad_norm=0.449756
Epoch 13/100 Iteration 142/234: loss=0.166952 lr=0.000020 grad_norm=0.411150
Epoch 13/100 Iteration 143/234: loss=0.160619 lr=0.000020 grad_norm=0.367407
Epoch 13/100 Iteration 144/234: loss=0.154095 lr=0.000020 grad_norm=0.549635
Epoch 13/100 Iteration 145/234: loss=0.156475 lr=0.000020 grad_norm=0.530416
Epoch 13/100 Iteration 146/234: loss=0.162467 lr=0.000020 grad_norm=0.546626
Epoch 13/100 Iteration 147/234: loss=0.165989 lr=0.000020 grad_norm=0.456699
Epoch 13/100 Iteration 148/234: loss=0.149988 lr=0.000020 grad_norm=0.372227
Epoch 13/100 Iteration 149/234: loss=0.170682 lr=0.000020 grad_norm=0.548918
Epoch 13/100 Iteration 150/234: loss=0.158821 lr=0.000020 grad_norm=0.806618
Epoch 13/100 Iteration 151/234: loss=0.169861 lr=0.000020 grad_norm=0.422489
Epoch 13/100 Iteration 152/234: loss=0.151267 lr=0.000020 grad_norm=0.490451
Epoch 13/100 Iteration 153/234: loss=0.161247 lr=0.000020 grad_norm=0.729939
Epoch 13/100 Iteration 154/234: loss=0.167944 lr=0.000020 grad_norm=0.556935
Epoch 13/100 Iteration 155/234: loss=0.159622 lr=0.000020 grad_norm=0.444670
Epoch 13/100 Iteration 156/234: loss=0.149741 lr=0.000020 grad_norm=0.754952
Epoch 13/100 Iteration 157/234: loss=0.166731 lr=0.000020 grad_norm=0.534659
Epoch 13/100 Iteration 158/234: loss=0.165995 lr=0.000020 grad_norm=0.348970
Epoch 13/100 Iteration 159/234: loss=0.150511 lr=0.000020 grad_norm=0.726723
Epoch 13/100 Iteration 160/234: loss=0.136918 lr=0.000020 grad_norm=0.560644
Epoch 13/100 Iteration 161/234: loss=0.172206 lr=0.000020 grad_norm=0.482727
Epoch 13/100 Iteration 162/234: loss=0.153888 lr=0.000020 grad_norm=0.547888
Epoch 13/100 Iteration 163/234: loss=0.166019 lr=0.000020 grad_norm=0.441495
Epoch 13/100 Iteration 164/234: loss=0.151139 lr=0.000020 grad_norm=0.375235
Epoch 13/100 Iteration 165/234: loss=0.163522 lr=0.000020 grad_norm=0.567087
Epoch 13/100 Iteration 166/234: loss=0.155033 lr=0.000020 grad_norm=0.354850
Epoch 13/100 Iteration 167/234: loss=0.169406 lr=0.000020 grad_norm=0.663496
Epoch 13/100 Iteration 168/234: loss=0.147030 lr=0.000020 grad_norm=0.679181
Epoch 13/100 Iteration 169/234: loss=0.163926 lr=0.000020 grad_norm=0.900518
Epoch 13/100 Iteration 170/234: loss=0.139037 lr=0.000020 grad_norm=1.283126
Epoch 13/100 Iteration 171/234: loss=0.159251 lr=0.000020 grad_norm=0.988380
Epoch 13/100 Iteration 172/234: loss=0.172316 lr=0.000020 grad_norm=0.628310
Epoch 13/100 Iteration 173/234: loss=0.167439 lr=0.000020 grad_norm=0.622432
Epoch 13/100 Iteration 174/234: loss=0.163697 lr=0.000020 grad_norm=0.842095
Epoch 13/100 Iteration 175/234: loss=0.165093 lr=0.000020 grad_norm=0.559461
Epoch 13/100 Iteration 176/234: loss=0.155798 lr=0.000020 grad_norm=0.401293
Epoch 13/100 Iteration 177/234: loss=0.167684 lr=0.000020 grad_norm=0.722253
Epoch 13/100 Iteration 178/234: loss=0.149248 lr=0.000020 grad_norm=0.765285
Epoch 13/100 Iteration 179/234: loss=0.151767 lr=0.000020 grad_norm=0.464585
Epoch 13/100 Iteration 180/234: loss=0.159413 lr=0.000020 grad_norm=0.782667
Epoch 13/100 Iteration 181/234: loss=0.178623 lr=0.000020 grad_norm=0.994957
Epoch 13/100 Iteration 182/234: loss=0.170877 lr=0.000020 grad_norm=0.644814
Epoch 13/100 Iteration 183/234: loss=0.179113 lr=0.000020 grad_norm=0.515802
Epoch 13/100 Iteration 184/234: loss=0.150480 lr=0.000020 grad_norm=0.525012
Epoch 13/100 Iteration 185/234: loss=0.167082 lr=0.000020 grad_norm=0.532600
Epoch 13/100 Iteration 186/234: loss=0.167358 lr=0.000020 grad_norm=0.494743
Epoch 13/100 Iteration 187/234: loss=0.163823 lr=0.000020 grad_norm=0.446005
Epoch 13/100 Iteration 188/234: loss=0.182666 lr=0.000020 grad_norm=0.513087
Epoch 13/100 Iteration 189/234: loss=0.174065 lr=0.000020 grad_norm=0.414206
Epoch 13/100 Iteration 190/234: loss=0.163055 lr=0.000020 grad_norm=0.298772
Epoch 13/100 Iteration 191/234: loss=0.153398 lr=0.000020 grad_norm=0.458508
Epoch 13/100 Iteration 192/234: loss=0.185929 lr=0.000020 grad_norm=0.292862
Epoch 13/100 Iteration 193/234: loss=0.153220 lr=0.000020 grad_norm=0.501911
Epoch 13/100 Iteration 194/234: loss=0.142343 lr=0.000020 grad_norm=0.509562
Epoch 13/100 Iteration 195/234: loss=0.177041 lr=0.000020 grad_norm=0.358545
Epoch 13/100 Iteration 196/234: loss=0.163051 lr=0.000020 grad_norm=0.432265
Epoch 13/100 Iteration 197/234: loss=0.168561 lr=0.000020 grad_norm=0.325805
Epoch 13/100 Iteration 198/234: loss=0.158753 lr=0.000020 grad_norm=0.332724
Epoch 13/100 Iteration 199/234: loss=0.163779 lr=0.000020 grad_norm=0.421039
Epoch 13/100 Iteration 200/234: loss=0.175363 lr=0.000020 grad_norm=0.553391
Epoch 13/100 Iteration 201/234: loss=0.178503 lr=0.000020 grad_norm=0.915110
Epoch 13/100 Iteration 202/234: loss=0.164588 lr=0.000020 grad_norm=1.070388
Epoch 13/100 Iteration 203/234: loss=0.168805 lr=0.000020 grad_norm=0.915667
Epoch 13/100 Iteration 204/234: loss=0.144093 lr=0.000020 grad_norm=0.410524
Epoch 13/100 Iteration 205/234: loss=0.156757 lr=0.000020 grad_norm=0.627610
Epoch 13/100 Iteration 206/234: loss=0.161263 lr=0.000020 grad_norm=0.650565
Epoch 13/100 Iteration 207/234: loss=0.173393 lr=0.000020 grad_norm=0.288632
Epoch 13/100 Iteration 208/234: loss=0.146393 lr=0.000020 grad_norm=0.624038
Epoch 13/100 Iteration 209/234: loss=0.133751 lr=0.000020 grad_norm=0.896617
Epoch 13/100 Iteration 210/234: loss=0.168013 lr=0.000020 grad_norm=0.865617
Epoch 13/100 Iteration 211/234: loss=0.171964 lr=0.000020 grad_norm=0.574018
Epoch 13/100 Iteration 212/234: loss=0.163784 lr=0.000020 grad_norm=0.937992
Epoch 13/100 Iteration 213/234: loss=0.166262 lr=0.000020 grad_norm=1.248539
Epoch 13/100 Iteration 214/234: loss=0.153323 lr=0.000020 grad_norm=0.995111
Epoch 13/100 Iteration 215/234: loss=0.146980 lr=0.000020 grad_norm=0.395845
Epoch 13/100 Iteration 216/234: loss=0.162261 lr=0.000020 grad_norm=0.608060
Epoch 13/100 Iteration 217/234: loss=0.152959 lr=0.000020 grad_norm=0.902780
Epoch 13/100 Iteration 218/234: loss=0.157170 lr=0.000020 grad_norm=0.819389
Epoch 13/100 Iteration 219/234: loss=0.168623 lr=0.000020 grad_norm=0.459460
Epoch 13/100 Iteration 220/234: loss=0.168654 lr=0.000020 grad_norm=1.232791
Epoch 13/100 Iteration 221/234: loss=0.152871 lr=0.000020 grad_norm=0.988709
Epoch 13/100 Iteration 222/234: loss=0.151588 lr=0.000020 grad_norm=0.366496
Epoch 13/100 Iteration 223/234: loss=0.147871 lr=0.000020 grad_norm=0.709722
Epoch 13/100 Iteration 224/234: loss=0.161926 lr=0.000020 grad_norm=0.810784
Epoch 13/100 Iteration 225/234: loss=0.174082 lr=0.000020 grad_norm=0.414233
Epoch 13/100 Iteration 226/234: loss=0.159713 lr=0.000020 grad_norm=0.481218
Epoch 13/100 Iteration 227/234: loss=0.143545 lr=0.000020 grad_norm=0.606024
Epoch 13/100 Iteration 228/234: loss=0.158458 lr=0.000020 grad_norm=0.604684
Epoch 13/100 Iteration 229/234: loss=0.137292 lr=0.000020 grad_norm=0.394078
Epoch 13/100 Iteration 230/234: loss=0.171572 lr=0.000020 grad_norm=0.360916
Epoch 13/100 Iteration 231/234: loss=0.151561 lr=0.000020 grad_norm=0.325997
Epoch 13/100 Iteration 232/234: loss=0.159747 lr=0.000020 grad_norm=0.404924
Epoch 13/100 Iteration 233/234: loss=0.170392 lr=0.000020 grad_norm=0.522833
Epoch 13/100 Iteration 234/234: loss=0.166255 lr=0.000020 grad_norm=0.294389
Epoch 13/100 finished. Avg Loss: 0.161920
Epoch 14/100 Iteration 1/234: loss=0.160695 lr=0.000020 grad_norm=0.587992
Epoch 14/100 Iteration 2/234: loss=0.159659 lr=0.000020 grad_norm=0.766884
Epoch 14/100 Iteration 3/234: loss=0.169227 lr=0.000020 grad_norm=0.546868
Epoch 14/100 Iteration 4/234: loss=0.143495 lr=0.000020 grad_norm=0.445069
Epoch 14/100 Iteration 5/234: loss=0.169206 lr=0.000020 grad_norm=0.473858
Epoch 14/100 Iteration 6/234: loss=0.162692 lr=0.000020 grad_norm=0.519912
Epoch 14/100 Iteration 7/234: loss=0.155020 lr=0.000020 grad_norm=0.655047
Epoch 14/100 Iteration 8/234: loss=0.168268 lr=0.000020 grad_norm=0.612544
Epoch 14/100 Iteration 9/234: loss=0.153461 lr=0.000020 grad_norm=0.810173
Epoch 14/100 Iteration 10/234: loss=0.162306 lr=0.000020 grad_norm=0.695393
Epoch 14/100 Iteration 11/234: loss=0.160622 lr=0.000020 grad_norm=0.657448
Epoch 14/100 Iteration 12/234: loss=0.152696 lr=0.000020 grad_norm=0.476919
Epoch 14/100 Iteration 13/234: loss=0.171123 lr=0.000020 grad_norm=0.559051
Epoch 14/100 Iteration 14/234: loss=0.162518 lr=0.000020 grad_norm=0.518335
Epoch 14/100 Iteration 15/234: loss=0.165682 lr=0.000020 grad_norm=0.644323
Epoch 14/100 Iteration 16/234: loss=0.169012 lr=0.000020 grad_norm=0.528898
Epoch 14/100 Iteration 17/234: loss=0.161048 lr=0.000020 grad_norm=0.513183
Epoch 14/100 Iteration 18/234: loss=0.156897 lr=0.000020 grad_norm=0.475425
Epoch 14/100 Iteration 19/234: loss=0.154121 lr=0.000020 grad_norm=0.377064
Epoch 14/100 Iteration 20/234: loss=0.161810 lr=0.000020 grad_norm=0.447474
Epoch 14/100 Iteration 21/234: loss=0.142465 lr=0.000020 grad_norm=0.350001
Epoch 14/100 Iteration 22/234: loss=0.133151 lr=0.000020 grad_norm=0.628673
Epoch 14/100 Iteration 23/234: loss=0.156786 lr=0.000020 grad_norm=0.789695
Epoch 14/100 Iteration 24/234: loss=0.169185 lr=0.000020 grad_norm=0.786526
Epoch 14/100 Iteration 25/234: loss=0.132708 lr=0.000020 grad_norm=0.848220
Epoch 14/100 Iteration 26/234: loss=0.138797 lr=0.000020 grad_norm=0.842241
Epoch 14/100 Iteration 27/234: loss=0.174110 lr=0.000020 grad_norm=0.722833
Epoch 14/100 Iteration 28/234: loss=0.151418 lr=0.000020 grad_norm=1.006393
Epoch 14/100 Iteration 29/234: loss=0.156780 lr=0.000020 grad_norm=0.965263
Epoch 14/100 Iteration 30/234: loss=0.151999 lr=0.000020 grad_norm=0.508362
Epoch 14/100 Iteration 31/234: loss=0.141511 lr=0.000020 grad_norm=1.045391
Epoch 14/100 Iteration 32/234: loss=0.144260 lr=0.000020 grad_norm=0.709548
Epoch 14/100 Iteration 33/234: loss=0.153095 lr=0.000020 grad_norm=0.792465
Epoch 14/100 Iteration 34/234: loss=0.163069 lr=0.000020 grad_norm=0.793987
Epoch 14/100 Iteration 35/234: loss=0.171722 lr=0.000020 grad_norm=1.055870
Epoch 14/100 Iteration 36/234: loss=0.171068 lr=0.000020 grad_norm=1.217025
Epoch 14/100 Iteration 37/234: loss=0.170120 lr=0.000020 grad_norm=0.531148
Epoch 14/100 Iteration 38/234: loss=0.178096 lr=0.000020 grad_norm=1.274776
Epoch 14/100 Iteration 39/234: loss=0.154557 lr=0.000020 grad_norm=1.411729
Epoch 14/100 Iteration 40/234: loss=0.167026 lr=0.000020 grad_norm=0.444364
Epoch 14/100 Iteration 41/234: loss=0.167611 lr=0.000020 grad_norm=1.582659
Epoch 14/100 Iteration 42/234: loss=0.139338 lr=0.000020 grad_norm=1.147369
Epoch 14/100 Iteration 43/234: loss=0.162613 lr=0.000020 grad_norm=0.771269
Epoch 14/100 Iteration 44/234: loss=0.153697 lr=0.000020 grad_norm=1.216019
Epoch 14/100 Iteration 45/234: loss=0.152027 lr=0.000020 grad_norm=0.602645
Epoch 14/100 Iteration 46/234: loss=0.175774 lr=0.000020 grad_norm=0.943414
Epoch 14/100 Iteration 47/234: loss=0.162934 lr=0.000020 grad_norm=1.212108
Epoch 14/100 Iteration 48/234: loss=0.139816 lr=0.000020 grad_norm=0.510128
Epoch 14/100 Iteration 49/234: loss=0.149710 lr=0.000020 grad_norm=0.867979
Epoch 14/100 Iteration 50/234: loss=0.148616 lr=0.000020 grad_norm=0.858081
Epoch 14/100 Iteration 51/234: loss=0.151507 lr=0.000020 grad_norm=0.528475
Epoch 14/100 Iteration 52/234: loss=0.143748 lr=0.000020 grad_norm=0.747616
Epoch 14/100 Iteration 53/234: loss=0.157275 lr=0.000020 grad_norm=0.582686
Epoch 14/100 Iteration 54/234: loss=0.143746 lr=0.000020 grad_norm=0.396581
Epoch 14/100 Iteration 55/234: loss=0.150211 lr=0.000020 grad_norm=0.387172
Epoch 14/100 Iteration 56/234: loss=0.143315 lr=0.000020 grad_norm=0.418577
Epoch 14/100 Iteration 57/234: loss=0.160738 lr=0.000020 grad_norm=0.323980
Epoch 14/100 Iteration 58/234: loss=0.151527 lr=0.000020 grad_norm=0.601638
Epoch 14/100 Iteration 59/234: loss=0.155563 lr=0.000020 grad_norm=0.837431
Epoch 14/100 Iteration 60/234: loss=0.150515 lr=0.000020 grad_norm=0.594485
Epoch 14/100 Iteration 61/234: loss=0.147068 lr=0.000020 grad_norm=0.554798
Epoch 14/100 Iteration 62/234: loss=0.148610 lr=0.000020 grad_norm=1.001696
Epoch 14/100 Iteration 63/234: loss=0.159884 lr=0.000020 grad_norm=0.731291
Epoch 14/100 Iteration 64/234: loss=0.177509 lr=0.000020 grad_norm=0.459364
Epoch 14/100 Iteration 65/234: loss=0.176710 lr=0.000020 grad_norm=0.611804
Epoch 14/100 Iteration 66/234: loss=0.152289 lr=0.000020 grad_norm=0.654568
Epoch 14/100 Iteration 67/234: loss=0.171134 lr=0.000020 grad_norm=0.394248
Epoch 14/100 Iteration 68/234: loss=0.153596 lr=0.000020 grad_norm=0.526473
Epoch 14/100 Iteration 69/234: loss=0.155570 lr=0.000020 grad_norm=0.535824
Epoch 14/100 Iteration 70/234: loss=0.155484 lr=0.000020 grad_norm=0.532697
Epoch 14/100 Iteration 71/234: loss=0.154669 lr=0.000020 grad_norm=0.733250
Epoch 14/100 Iteration 72/234: loss=0.159299 lr=0.000020 grad_norm=0.549758
Epoch 14/100 Iteration 73/234: loss=0.151512 lr=0.000020 grad_norm=0.340852
Epoch 14/100 Iteration 74/234: loss=0.154559 lr=0.000020 grad_norm=0.421042
Epoch 14/100 Iteration 75/234: loss=0.170167 lr=0.000020 grad_norm=0.746739
Epoch 14/100 Iteration 76/234: loss=0.155105 lr=0.000020 grad_norm=0.933073
Epoch 14/100 Iteration 77/234: loss=0.163167 lr=0.000020 grad_norm=0.855587
Epoch 14/100 Iteration 78/234: loss=0.179938 lr=0.000020 grad_norm=0.484652
Epoch 14/100 Iteration 79/234: loss=0.158662 lr=0.000020 grad_norm=0.438368
Epoch 14/100 Iteration 80/234: loss=0.150655 lr=0.000020 grad_norm=0.810034
Epoch 14/100 Iteration 81/234: loss=0.156552 lr=0.000020 grad_norm=1.082106
Epoch 14/100 Iteration 82/234: loss=0.170216 lr=0.000020 grad_norm=0.929666
Epoch 14/100 Iteration 83/234: loss=0.167943 lr=0.000020 grad_norm=0.410652
Epoch 14/100 Iteration 84/234: loss=0.153898 lr=0.000020 grad_norm=0.436620
Epoch 14/100 Iteration 85/234: loss=0.155361 lr=0.000020 grad_norm=0.442382
Epoch 14/100 Iteration 86/234: loss=0.141061 lr=0.000020 grad_norm=0.449634
Epoch 14/100 Iteration 87/234: loss=0.175524 lr=0.000020 grad_norm=0.497730
Epoch 14/100 Iteration 88/234: loss=0.150494 lr=0.000020 grad_norm=0.403043
Epoch 14/100 Iteration 89/234: loss=0.166186 lr=0.000020 grad_norm=0.749379
Epoch 14/100 Iteration 90/234: loss=0.158512 lr=0.000020 grad_norm=0.926969
Epoch 14/100 Iteration 91/234: loss=0.157985 lr=0.000020 grad_norm=0.648774
Epoch 14/100 Iteration 92/234: loss=0.154861 lr=0.000020 grad_norm=0.554343
Epoch 14/100 Iteration 93/234: loss=0.161876 lr=0.000020 grad_norm=0.672916
Epoch 14/100 Iteration 94/234: loss=0.147616 lr=0.000020 grad_norm=0.565694
Epoch 14/100 Iteration 95/234: loss=0.146943 lr=0.000020 grad_norm=0.880613
Epoch 14/100 Iteration 96/234: loss=0.151890 lr=0.000020 grad_norm=0.875416
Epoch 14/100 Iteration 97/234: loss=0.136601 lr=0.000020 grad_norm=0.274878
Epoch 14/100 Iteration 98/234: loss=0.168386 lr=0.000020 grad_norm=0.765383
Epoch 14/100 Iteration 99/234: loss=0.141342 lr=0.000020 grad_norm=0.668334
Epoch 14/100 Iteration 100/234: loss=0.142111 lr=0.000020 grad_norm=0.306992
Epoch 14/100 Iteration 101/234: loss=0.146759 lr=0.000020 grad_norm=0.678511
Epoch 14/100 Iteration 102/234: loss=0.141165 lr=0.000020 grad_norm=0.859960
Epoch 14/100 Iteration 103/234: loss=0.142925 lr=0.000020 grad_norm=0.511227
Epoch 14/100 Iteration 104/234: loss=0.166859 lr=0.000020 grad_norm=0.408653
Epoch 14/100 Iteration 105/234: loss=0.132691 lr=0.000020 grad_norm=0.714973
Epoch 14/100 Iteration 106/234: loss=0.161135 lr=0.000020 grad_norm=0.785925
Epoch 14/100 Iteration 107/234: loss=0.164515 lr=0.000020 grad_norm=0.511037
Epoch 14/100 Iteration 108/234: loss=0.152567 lr=0.000020 grad_norm=0.329340
Epoch 14/100 Iteration 109/234: loss=0.153915 lr=0.000020 grad_norm=0.468981
Epoch 14/100 Iteration 110/234: loss=0.144605 lr=0.000020 grad_norm=0.607526
Epoch 14/100 Iteration 111/234: loss=0.165704 lr=0.000020 grad_norm=0.640231
Epoch 14/100 Iteration 112/234: loss=0.156567 lr=0.000020 grad_norm=0.433367
Epoch 14/100 Iteration 113/234: loss=0.153248 lr=0.000020 grad_norm=0.506787
Epoch 14/100 Iteration 114/234: loss=0.164867 lr=0.000020 grad_norm=0.820237
Epoch 14/100 Iteration 115/234: loss=0.174958 lr=0.000020 grad_norm=0.733213
Epoch 14/100 Iteration 116/234: loss=0.162207 lr=0.000020 grad_norm=0.428701
Epoch 14/100 Iteration 117/234: loss=0.151331 lr=0.000020 grad_norm=0.719277
Epoch 14/100 Iteration 118/234: loss=0.162447 lr=0.000020 grad_norm=0.521410
Epoch 14/100 Iteration 119/234: loss=0.149296 lr=0.000020 grad_norm=0.600306
Epoch 14/100 Iteration 120/234: loss=0.156702 lr=0.000020 grad_norm=0.504498
Epoch 14/100 Iteration 121/234: loss=0.174775 lr=0.000020 grad_norm=0.543805
Epoch 14/100 Iteration 122/234: loss=0.157990 lr=0.000020 grad_norm=0.409534
Epoch 14/100 Iteration 123/234: loss=0.154887 lr=0.000020 grad_norm=0.634461
Epoch 14/100 Iteration 124/234: loss=0.172827 lr=0.000020 grad_norm=0.628637
Epoch 14/100 Iteration 125/234: loss=0.166466 lr=0.000020 grad_norm=0.372937
Epoch 14/100 Iteration 126/234: loss=0.158452 lr=0.000020 grad_norm=0.511969
Epoch 14/100 Iteration 127/234: loss=0.149111 lr=0.000020 grad_norm=1.112143
Epoch 14/100 Iteration 128/234: loss=0.152482 lr=0.000020 grad_norm=0.979329
Epoch 14/100 Iteration 129/234: loss=0.145190 lr=0.000020 grad_norm=0.479640
Epoch 14/100 Iteration 130/234: loss=0.159126 lr=0.000020 grad_norm=0.596415
Epoch 14/100 Iteration 131/234: loss=0.164130 lr=0.000020 grad_norm=1.171314
Epoch 14/100 Iteration 132/234: loss=0.168072 lr=0.000020 grad_norm=1.468770
Epoch 14/100 Iteration 133/234: loss=0.158515 lr=0.000020 grad_norm=1.116794
Epoch 14/100 Iteration 134/234: loss=0.158124 lr=0.000020 grad_norm=0.502150
Epoch 14/100 Iteration 135/234: loss=0.152363 lr=0.000020 grad_norm=0.878423
Epoch 14/100 Iteration 136/234: loss=0.163327 lr=0.000020 grad_norm=0.708232
Epoch 14/100 Iteration 137/234: loss=0.151275 lr=0.000020 grad_norm=0.559248
Epoch 14/100 Iteration 138/234: loss=0.151148 lr=0.000020 grad_norm=0.450996
Epoch 14/100 Iteration 139/234: loss=0.159023 lr=0.000020 grad_norm=0.471001
Epoch 14/100 Iteration 140/234: loss=0.164614 lr=0.000020 grad_norm=0.498020
Epoch 14/100 Iteration 141/234: loss=0.157305 lr=0.000020 grad_norm=0.760299
Epoch 14/100 Iteration 142/234: loss=0.141418 lr=0.000020 grad_norm=0.831475
Epoch 14/100 Iteration 143/234: loss=0.167768 lr=0.000020 grad_norm=0.655098
Epoch 14/100 Iteration 144/234: loss=0.152045 lr=0.000020 grad_norm=0.315094
Epoch 14/100 Iteration 145/234: loss=0.161075 lr=0.000020 grad_norm=0.524149
Epoch 14/100 Iteration 146/234: loss=0.160147 lr=0.000020 grad_norm=0.718911
Epoch 14/100 Iteration 147/234: loss=0.151004 lr=0.000020 grad_norm=0.695007
Epoch 14/100 Iteration 148/234: loss=0.155815 lr=0.000020 grad_norm=0.780555
Epoch 14/100 Iteration 149/234: loss=0.173803 lr=0.000020 grad_norm=0.683279
Epoch 14/100 Iteration 150/234: loss=0.166307 lr=0.000020 grad_norm=0.653163
Epoch 14/100 Iteration 151/234: loss=0.146297 lr=0.000020 grad_norm=0.904264
Epoch 14/100 Iteration 152/234: loss=0.132296 lr=0.000020 grad_norm=0.794347
Epoch 14/100 Iteration 153/234: loss=0.166052 lr=0.000020 grad_norm=0.371237
Epoch 14/100 Iteration 154/234: loss=0.166734 lr=0.000020 grad_norm=0.646263
Epoch 14/100 Iteration 155/234: loss=0.150579 lr=0.000020 grad_norm=0.742892
Epoch 14/100 Iteration 156/234: loss=0.147456 lr=0.000020 grad_norm=0.594832
Epoch 14/100 Iteration 157/234: loss=0.169515 lr=0.000020 grad_norm=0.925536
Epoch 14/100 Iteration 158/234: loss=0.150602 lr=0.000020 grad_norm=0.821722
Epoch 14/100 Iteration 159/234: loss=0.149737 lr=0.000020 grad_norm=0.514169
Epoch 14/100 Iteration 160/234: loss=0.148757 lr=0.000020 grad_norm=0.560079
Epoch 14/100 Iteration 161/234: loss=0.160882 lr=0.000020 grad_norm=0.519550
Epoch 14/100 Iteration 162/234: loss=0.149588 lr=0.000020 grad_norm=0.776697
Epoch 14/100 Iteration 163/234: loss=0.149051 lr=0.000020 grad_norm=0.776641
Epoch 14/100 Iteration 164/234: loss=0.162259 lr=0.000020 grad_norm=0.608823
Epoch 14/100 Iteration 165/234: loss=0.155827 lr=0.000020 grad_norm=0.702791
Epoch 14/100 Iteration 166/234: loss=0.148468 lr=0.000020 grad_norm=0.662547
Epoch 14/100 Iteration 167/234: loss=0.143032 lr=0.000020 grad_norm=0.735712
Epoch 14/100 Iteration 168/234: loss=0.146868 lr=0.000020 grad_norm=0.809882
Epoch 14/100 Iteration 169/234: loss=0.157900 lr=0.000020 grad_norm=0.540905
Epoch 14/100 Iteration 170/234: loss=0.169684 lr=0.000020 grad_norm=0.495020
Epoch 14/100 Iteration 171/234: loss=0.160744 lr=0.000020 grad_norm=0.857959
Epoch 14/100 Iteration 172/234: loss=0.160629 lr=0.000020 grad_norm=0.489644
Epoch 14/100 Iteration 173/234: loss=0.174854 lr=0.000020 grad_norm=0.774279
Epoch 14/100 Iteration 174/234: loss=0.158743 lr=0.000020 grad_norm=0.885743
Epoch 14/100 Iteration 175/234: loss=0.160027 lr=0.000020 grad_norm=0.657466
Epoch 14/100 Iteration 176/234: loss=0.156251 lr=0.000020 grad_norm=0.562287
Epoch 14/100 Iteration 177/234: loss=0.155450 lr=0.000020 grad_norm=0.415553
Epoch 14/100 Iteration 178/234: loss=0.137540 lr=0.000020 grad_norm=0.531714
Epoch 14/100 Iteration 179/234: loss=0.137607 lr=0.000020 grad_norm=0.487039
Epoch 14/100 Iteration 180/234: loss=0.158643 lr=0.000020 grad_norm=0.324790
Epoch 14/100 Iteration 181/234: loss=0.149730 lr=0.000020 grad_norm=0.371061
Epoch 14/100 Iteration 182/234: loss=0.158395 lr=0.000020 grad_norm=0.682518
Epoch 14/100 Iteration 183/234: loss=0.159717 lr=0.000020 grad_norm=0.697686
Epoch 14/100 Iteration 184/234: loss=0.162851 lr=0.000020 grad_norm=0.572532
Epoch 14/100 Iteration 185/234: loss=0.155955 lr=0.000020 grad_norm=0.754966
Epoch 14/100 Iteration 186/234: loss=0.169953 lr=0.000020 grad_norm=0.779405
Epoch 14/100 Iteration 187/234: loss=0.155562 lr=0.000020 grad_norm=0.622732
Epoch 14/100 Iteration 188/234: loss=0.147421 lr=0.000020 grad_norm=0.440615
Epoch 14/100 Iteration 189/234: loss=0.157190 lr=0.000020 grad_norm=0.465244
Epoch 14/100 Iteration 190/234: loss=0.140776 lr=0.000020 grad_norm=0.414081
Epoch 14/100 Iteration 191/234: loss=0.140200 lr=0.000020 grad_norm=0.295962
Epoch 14/100 Iteration 192/234: loss=0.162303 lr=0.000020 grad_norm=0.362472
Epoch 14/100 Iteration 193/234: loss=0.158213 lr=0.000020 grad_norm=0.302964
Epoch 14/100 Iteration 194/234: loss=0.158626 lr=0.000020 grad_norm=0.362996
Epoch 14/100 Iteration 195/234: loss=0.138042 lr=0.000020 grad_norm=0.408759
Epoch 14/100 Iteration 196/234: loss=0.183506 lr=0.000020 grad_norm=0.536216
Epoch 14/100 Iteration 197/234: loss=0.143871 lr=0.000020 grad_norm=1.178505
Epoch 14/100 Iteration 198/234: loss=0.149421 lr=0.000020 grad_norm=1.161077
Epoch 14/100 Iteration 199/234: loss=0.152100 lr=0.000020 grad_norm=0.396725
Epoch 14/100 Iteration 200/234: loss=0.157347 lr=0.000020 grad_norm=0.866592
Epoch 14/100 Iteration 201/234: loss=0.158328 lr=0.000020 grad_norm=0.793711
Epoch 14/100 Iteration 202/234: loss=0.164904 lr=0.000020 grad_norm=0.303676
Epoch 14/100 Iteration 203/234: loss=0.162525 lr=0.000020 grad_norm=0.904213
Epoch 14/100 Iteration 204/234: loss=0.164192 lr=0.000020 grad_norm=1.034154
Epoch 14/100 Iteration 205/234: loss=0.170644 lr=0.000020 grad_norm=0.636299
Epoch 14/100 Iteration 206/234: loss=0.149023 lr=0.000020 grad_norm=0.568927
Epoch 14/100 Iteration 207/234: loss=0.155194 lr=0.000020 grad_norm=0.832890
Epoch 14/100 Iteration 208/234: loss=0.158744 lr=0.000020 grad_norm=0.656451
Epoch 14/100 Iteration 209/234: loss=0.165397 lr=0.000020 grad_norm=0.427108
Epoch 14/100 Iteration 210/234: loss=0.142994 lr=0.000020 grad_norm=0.682341
Epoch 14/100 Iteration 211/234: loss=0.166800 lr=0.000020 grad_norm=0.639326
Epoch 14/100 Iteration 212/234: loss=0.146299 lr=0.000020 grad_norm=0.547042
Epoch 14/100 Iteration 213/234: loss=0.146739 lr=0.000020 grad_norm=0.400156
Epoch 14/100 Iteration 214/234: loss=0.151818 lr=0.000020 grad_norm=0.451795
Epoch 14/100 Iteration 215/234: loss=0.171253 lr=0.000020 grad_norm=0.542665
Epoch 14/100 Iteration 216/234: loss=0.144874 lr=0.000020 grad_norm=0.567451
Epoch 14/100 Iteration 217/234: loss=0.156387 lr=0.000020 grad_norm=0.476369
Epoch 14/100 Iteration 218/234: loss=0.151802 lr=0.000020 grad_norm=0.404166
Epoch 14/100 Iteration 219/234: loss=0.141669 lr=0.000020 grad_norm=0.767219
Epoch 14/100 Iteration 220/234: loss=0.160785 lr=0.000020 grad_norm=0.570218
Epoch 14/100 Iteration 221/234: loss=0.143803 lr=0.000020 grad_norm=0.602283
Epoch 14/100 Iteration 222/234: loss=0.153350 lr=0.000020 grad_norm=1.012735
Epoch 14/100 Iteration 223/234: loss=0.160757 lr=0.000020 grad_norm=0.991550
Epoch 14/100 Iteration 224/234: loss=0.157425 lr=0.000020 grad_norm=0.638208
Epoch 14/100 Iteration 225/234: loss=0.173767 lr=0.000020 grad_norm=0.491835
Epoch 14/100 Iteration 226/234: loss=0.164701 lr=0.000020 grad_norm=0.916352
Epoch 14/100 Iteration 227/234: loss=0.142654 lr=0.000020 grad_norm=0.662321
Epoch 14/100 Iteration 228/234: loss=0.133258 lr=0.000020 grad_norm=0.493242
Epoch 14/100 Iteration 229/234: loss=0.149079 lr=0.000020 grad_norm=0.553894
Epoch 14/100 Iteration 230/234: loss=0.160859 lr=0.000020 grad_norm=0.523354
Epoch 14/100 Iteration 231/234: loss=0.144383 lr=0.000020 grad_norm=0.458929
Epoch 14/100 Iteration 232/234: loss=0.156493 lr=0.000020 grad_norm=0.615786
Epoch 14/100 Iteration 233/234: loss=0.133739 lr=0.000020 grad_norm=0.368691
Epoch 14/100 Iteration 234/234: loss=0.159170 lr=0.000020 grad_norm=0.677517
Epoch 14/100 finished. Avg Loss: 0.156294
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 15/100 Iteration 1/234: loss=0.153128 lr=0.000020 grad_norm=0.632796
Epoch 15/100 Iteration 2/234: loss=0.151754 lr=0.000020 grad_norm=0.332428
Epoch 15/100 Iteration 3/234: loss=0.135905 lr=0.000020 grad_norm=0.621617
Epoch 15/100 Iteration 4/234: loss=0.137360 lr=0.000020 grad_norm=0.358958
Epoch 15/100 Iteration 5/234: loss=0.155459 lr=0.000020 grad_norm=0.849857
Epoch 15/100 Iteration 6/234: loss=0.158775 lr=0.000020 grad_norm=1.112935
Epoch 15/100 Iteration 7/234: loss=0.152838 lr=0.000020 grad_norm=0.928648
Epoch 15/100 Iteration 8/234: loss=0.154523 lr=0.000020 grad_norm=0.554441
Epoch 15/100 Iteration 9/234: loss=0.134514 lr=0.000020 grad_norm=0.522628
Epoch 15/100 Iteration 10/234: loss=0.163543 lr=0.000020 grad_norm=0.878828
Epoch 15/100 Iteration 11/234: loss=0.156783 lr=0.000020 grad_norm=0.520019
Epoch 15/100 Iteration 12/234: loss=0.132478 lr=0.000020 grad_norm=0.658929
Epoch 15/100 Iteration 13/234: loss=0.131444 lr=0.000020 grad_norm=0.887684
Epoch 15/100 Iteration 14/234: loss=0.154897 lr=0.000020 grad_norm=0.369843
Epoch 15/100 Iteration 15/234: loss=0.138838 lr=0.000020 grad_norm=0.959176
Epoch 15/100 Iteration 16/234: loss=0.163561 lr=0.000020 grad_norm=1.141018
Epoch 15/100 Iteration 17/234: loss=0.162563 lr=0.000020 grad_norm=0.835199
Epoch 15/100 Iteration 18/234: loss=0.154719 lr=0.000020 grad_norm=0.471529
Epoch 15/100 Iteration 19/234: loss=0.160551 lr=0.000020 grad_norm=0.497480
Epoch 15/100 Iteration 20/234: loss=0.143784 lr=0.000020 grad_norm=0.549119
Epoch 15/100 Iteration 21/234: loss=0.160076 lr=0.000020 grad_norm=0.628864
Epoch 15/100 Iteration 22/234: loss=0.145821 lr=0.000020 grad_norm=0.567269
Epoch 15/100 Iteration 23/234: loss=0.156553 lr=0.000020 grad_norm=0.446886
Epoch 15/100 Iteration 24/234: loss=0.170703 lr=0.000020 grad_norm=0.469627
Epoch 15/100 Iteration 25/234: loss=0.146410 lr=0.000020 grad_norm=0.536057
Epoch 15/100 Iteration 26/234: loss=0.136103 lr=0.000020 grad_norm=0.589917
Epoch 15/100 Iteration 27/234: loss=0.147435 lr=0.000020 grad_norm=0.410771
Epoch 15/100 Iteration 28/234: loss=0.162502 lr=0.000020 grad_norm=0.520923
Epoch 15/100 Iteration 29/234: loss=0.166242 lr=0.000020 grad_norm=0.464357
Epoch 15/100 Iteration 30/234: loss=0.163679 lr=0.000020 grad_norm=0.579241
Epoch 15/100 Iteration 31/234: loss=0.143161 lr=0.000020 grad_norm=0.432961
Epoch 15/100 Iteration 32/234: loss=0.152794 lr=0.000020 grad_norm=0.543205
Epoch 15/100 Iteration 33/234: loss=0.153252 lr=0.000020 grad_norm=0.699204
Epoch 15/100 Iteration 34/234: loss=0.143148 lr=0.000020 grad_norm=0.682430
Epoch 15/100 Iteration 35/234: loss=0.163497 lr=0.000020 grad_norm=0.381125
Epoch 15/100 Iteration 36/234: loss=0.162022 lr=0.000020 grad_norm=0.708209
Epoch 15/100 Iteration 37/234: loss=0.149168 lr=0.000020 grad_norm=1.005198
Epoch 15/100 Iteration 38/234: loss=0.149842 lr=0.000020 grad_norm=0.919635
Epoch 15/100 Iteration 39/234: loss=0.158009 lr=0.000020 grad_norm=0.610037
Epoch 15/100 Iteration 40/234: loss=0.152610 lr=0.000020 grad_norm=0.608796
Epoch 15/100 Iteration 41/234: loss=0.157326 lr=0.000020 grad_norm=0.854606
Epoch 15/100 Iteration 42/234: loss=0.149337 lr=0.000020 grad_norm=0.759017
Epoch 15/100 Iteration 43/234: loss=0.140377 lr=0.000020 grad_norm=0.755111
Epoch 15/100 Iteration 44/234: loss=0.153858 lr=0.000020 grad_norm=0.677239
Epoch 15/100 Iteration 45/234: loss=0.152962 lr=0.000020 grad_norm=0.533149
Epoch 15/100 Iteration 46/234: loss=0.141180 lr=0.000020 grad_norm=0.865120
Epoch 15/100 Iteration 47/234: loss=0.138410 lr=0.000020 grad_norm=1.207533
Epoch 15/100 Iteration 48/234: loss=0.155159 lr=0.000020 grad_norm=1.321522
Epoch 15/100 Iteration 49/234: loss=0.150770 lr=0.000020 grad_norm=0.998138
Epoch 15/100 Iteration 50/234: loss=0.153778 lr=0.000020 grad_norm=0.491038
Epoch 15/100 Iteration 51/234: loss=0.169554 lr=0.000020 grad_norm=0.816364
Epoch 15/100 Iteration 52/234: loss=0.181835 lr=0.000020 grad_norm=0.941845
Epoch 15/100 Iteration 53/234: loss=0.168336 lr=0.000020 grad_norm=0.646663
Epoch 15/100 Iteration 54/234: loss=0.170238 lr=0.000020 grad_norm=0.668024
Epoch 15/100 Iteration 55/234: loss=0.158551 lr=0.000020 grad_norm=0.752055
Epoch 15/100 Iteration 56/234: loss=0.173415 lr=0.000020 grad_norm=0.621615
Epoch 15/100 Iteration 57/234: loss=0.131324 lr=0.000020 grad_norm=0.767267
Epoch 15/100 Iteration 58/234: loss=0.145739 lr=0.000020 grad_norm=0.784240
Epoch 15/100 Iteration 59/234: loss=0.158219 lr=0.000020 grad_norm=0.583155
Epoch 15/100 Iteration 60/234: loss=0.159047 lr=0.000020 grad_norm=0.566852
Epoch 15/100 Iteration 61/234: loss=0.163285 lr=0.000020 grad_norm=1.089579
Epoch 15/100 Iteration 62/234: loss=0.172075 lr=0.000020 grad_norm=1.164429
Epoch 15/100 Iteration 63/234: loss=0.153976 lr=0.000020 grad_norm=0.492451
Epoch 15/100 Iteration 64/234: loss=0.152270 lr=0.000020 grad_norm=1.051660
Epoch 15/100 Iteration 65/234: loss=0.165548 lr=0.000020 grad_norm=1.058991
Epoch 15/100 Iteration 66/234: loss=0.165533 lr=0.000020 grad_norm=0.495970
Epoch 15/100 Iteration 67/234: loss=0.153179 lr=0.000020 grad_norm=1.053821
Epoch 15/100 Iteration 68/234: loss=0.130569 lr=0.000020 grad_norm=0.511408
Epoch 15/100 Iteration 69/234: loss=0.166111 lr=0.000020 grad_norm=1.011307
Epoch 15/100 Iteration 70/234: loss=0.129091 lr=0.000020 grad_norm=1.463188
Epoch 15/100 Iteration 71/234: loss=0.151181 lr=0.000020 grad_norm=0.953553
Epoch 15/100 Iteration 72/234: loss=0.159617 lr=0.000020 grad_norm=0.623587
Epoch 15/100 Iteration 73/234: loss=0.162859 lr=0.000020 grad_norm=1.061004
Epoch 15/100 Iteration 74/234: loss=0.158432 lr=0.000020 grad_norm=0.584371
Epoch 15/100 Iteration 75/234: loss=0.135575 lr=0.000020 grad_norm=0.658154
Epoch 15/100 Iteration 76/234: loss=0.151035 lr=0.000020 grad_norm=0.545200
Epoch 15/100 Iteration 77/234: loss=0.138883 lr=0.000020 grad_norm=0.428761
Epoch 15/100 Iteration 78/234: loss=0.154834 lr=0.000020 grad_norm=0.546359
Epoch 15/100 Iteration 79/234: loss=0.148623 lr=0.000020 grad_norm=0.473281
Epoch 15/100 Iteration 80/234: loss=0.158921 lr=0.000020 grad_norm=0.428618
Epoch 15/100 Iteration 81/234: loss=0.156028 lr=0.000020 grad_norm=0.511732
Epoch 15/100 Iteration 82/234: loss=0.154150 lr=0.000020 grad_norm=0.659953
Epoch 15/100 Iteration 83/234: loss=0.153056 lr=0.000020 grad_norm=1.012323
Epoch 15/100 Iteration 84/234: loss=0.162327 lr=0.000020 grad_norm=0.691290
Epoch 15/100 Iteration 85/234: loss=0.137956 lr=0.000020 grad_norm=0.387631
Epoch 15/100 Iteration 86/234: loss=0.166629 lr=0.000020 grad_norm=0.786774
Epoch 15/100 Iteration 87/234: loss=0.155967 lr=0.000020 grad_norm=0.713971
Epoch 15/100 Iteration 88/234: loss=0.169471 lr=0.000020 grad_norm=0.536253
Epoch 15/100 Iteration 89/234: loss=0.169792 lr=0.000020 grad_norm=0.634168
Epoch 15/100 Iteration 90/234: loss=0.154021 lr=0.000020 grad_norm=0.697964
Epoch 15/100 Iteration 91/234: loss=0.145483 lr=0.000020 grad_norm=0.533472
Epoch 15/100 Iteration 92/234: loss=0.161892 lr=0.000020 grad_norm=0.718272
Epoch 15/100 Iteration 93/234: loss=0.140118 lr=0.000020 grad_norm=0.812964
Epoch 15/100 Iteration 94/234: loss=0.163863 lr=0.000020 grad_norm=0.744228
Epoch 15/100 Iteration 95/234: loss=0.157144 lr=0.000020 grad_norm=0.322007
Epoch 15/100 Iteration 96/234: loss=0.144236 lr=0.000020 grad_norm=0.810129
Epoch 15/100 Iteration 97/234: loss=0.152867 lr=0.000020 grad_norm=0.778881
Epoch 15/100 Iteration 98/234: loss=0.176667 lr=0.000020 grad_norm=0.529395
Epoch 15/100 Iteration 99/234: loss=0.150323 lr=0.000020 grad_norm=0.362385
Epoch 15/100 Iteration 100/234: loss=0.143024 lr=0.000020 grad_norm=0.380435
Epoch 15/100 Iteration 101/234: loss=0.152399 lr=0.000020 grad_norm=0.382814
Epoch 15/100 Iteration 102/234: loss=0.157715 lr=0.000020 grad_norm=0.344740
Epoch 15/100 Iteration 103/234: loss=0.145660 lr=0.000020 grad_norm=0.405082
Epoch 15/100 Iteration 104/234: loss=0.163955 lr=0.000020 grad_norm=0.672350
Epoch 15/100 Iteration 105/234: loss=0.142185 lr=0.000020 grad_norm=0.951872
Epoch 15/100 Iteration 106/234: loss=0.145807 lr=0.000020 grad_norm=0.905399
Epoch 15/100 Iteration 107/234: loss=0.142569 lr=0.000020 grad_norm=0.505647
Epoch 15/100 Iteration 108/234: loss=0.181619 lr=0.000020 grad_norm=0.937754
Epoch 15/100 Iteration 109/234: loss=0.145020 lr=0.000020 grad_norm=1.189197
Epoch 15/100 Iteration 110/234: loss=0.164218 lr=0.000020 grad_norm=0.409985
Epoch 15/100 Iteration 111/234: loss=0.161252 lr=0.000020 grad_norm=1.206629
Epoch 15/100 Iteration 112/234: loss=0.146413 lr=0.000020 grad_norm=0.817761
Epoch 15/100 Iteration 113/234: loss=0.163294 lr=0.000020 grad_norm=0.746031
Epoch 15/100 Iteration 114/234: loss=0.157055 lr=0.000020 grad_norm=1.092115
Epoch 15/100 Iteration 115/234: loss=0.147489 lr=0.000020 grad_norm=0.450527
Epoch 15/100 Iteration 116/234: loss=0.139328 lr=0.000020 grad_norm=0.818214
Epoch 15/100 Iteration 117/234: loss=0.155966 lr=0.000020 grad_norm=0.600804
Epoch 15/100 Iteration 118/234: loss=0.143128 lr=0.000020 grad_norm=0.320809
Epoch 15/100 Iteration 119/234: loss=0.145963 lr=0.000020 grad_norm=0.563388
Epoch 15/100 Iteration 120/234: loss=0.152220 lr=0.000020 grad_norm=0.642849
Epoch 15/100 Iteration 121/234: loss=0.160886 lr=0.000020 grad_norm=0.472861
Epoch 15/100 Iteration 122/234: loss=0.129313 lr=0.000020 grad_norm=0.380255
Epoch 15/100 Iteration 123/234: loss=0.146124 lr=0.000020 grad_norm=0.500224
Epoch 15/100 Iteration 124/234: loss=0.143830 lr=0.000020 grad_norm=0.614185
Epoch 15/100 Iteration 125/234: loss=0.157747 lr=0.000020 grad_norm=0.570540
Epoch 15/100 Iteration 126/234: loss=0.167359 lr=0.000020 grad_norm=0.442430
Epoch 15/100 Iteration 127/234: loss=0.163044 lr=0.000020 grad_norm=0.827933
Epoch 15/100 Iteration 128/234: loss=0.145274 lr=0.000020 grad_norm=0.857422
Epoch 15/100 Iteration 129/234: loss=0.157556 lr=0.000020 grad_norm=0.387378
Epoch 15/100 Iteration 130/234: loss=0.155939 lr=0.000020 grad_norm=0.524451
Epoch 15/100 Iteration 131/234: loss=0.153619 lr=0.000020 grad_norm=0.736454
Epoch 15/100 Iteration 132/234: loss=0.129519 lr=0.000020 grad_norm=0.578026
Epoch 15/100 Iteration 133/234: loss=0.152833 lr=0.000020 grad_norm=0.374053
Epoch 15/100 Iteration 134/234: loss=0.151747 lr=0.000020 grad_norm=0.862883
Epoch 15/100 Iteration 135/234: loss=0.139386 lr=0.000020 grad_norm=0.816057
Epoch 15/100 Iteration 136/234: loss=0.129401 lr=0.000020 grad_norm=0.405244
Epoch 15/100 Iteration 137/234: loss=0.153006 lr=0.000020 grad_norm=0.955449
Epoch 15/100 Iteration 138/234: loss=0.145090 lr=0.000020 grad_norm=1.145219
Epoch 15/100 Iteration 139/234: loss=0.154327 lr=0.000020 grad_norm=0.699639
Epoch 15/100 Iteration 140/234: loss=0.153202 lr=0.000020 grad_norm=0.706347
Epoch 15/100 Iteration 141/234: loss=0.162834 lr=0.000020 grad_norm=1.352599
Epoch 15/100 Iteration 142/234: loss=0.160993 lr=0.000020 grad_norm=1.194586
Epoch 15/100 Iteration 143/234: loss=0.158204 lr=0.000020 grad_norm=0.531360
Epoch 15/100 Iteration 144/234: loss=0.151143 lr=0.000020 grad_norm=1.051865
Epoch 15/100 Iteration 145/234: loss=0.144216 lr=0.000020 grad_norm=0.510389
Epoch 15/100 Iteration 146/234: loss=0.141905 lr=0.000020 grad_norm=1.037744
Epoch 15/100 Iteration 147/234: loss=0.138413 lr=0.000020 grad_norm=1.133185
Epoch 15/100 Iteration 148/234: loss=0.156761 lr=0.000020 grad_norm=0.388856
Epoch 15/100 Iteration 149/234: loss=0.151779 lr=0.000020 grad_norm=1.049750
Epoch 15/100 Iteration 150/234: loss=0.135445 lr=0.000020 grad_norm=0.533524
Epoch 15/100 Iteration 151/234: loss=0.143785 lr=0.000020 grad_norm=0.627732
Epoch 15/100 Iteration 152/234: loss=0.146231 lr=0.000020 grad_norm=0.546346
Epoch 15/100 Iteration 153/234: loss=0.162672 lr=0.000020 grad_norm=0.493496
Epoch 15/100 Iteration 154/234: loss=0.140616 lr=0.000020 grad_norm=0.657790
Epoch 15/100 Iteration 155/234: loss=0.157237 lr=0.000020 grad_norm=0.906404
Epoch 15/100 Iteration 156/234: loss=0.154561 lr=0.000020 grad_norm=0.960215
Epoch 15/100 Iteration 157/234: loss=0.150704 lr=0.000020 grad_norm=0.754929
Epoch 15/100 Iteration 158/234: loss=0.154960 lr=0.000020 grad_norm=0.506005
Epoch 15/100 Iteration 159/234: loss=0.146119 lr=0.000020 grad_norm=0.573554
Epoch 15/100 Iteration 160/234: loss=0.158221 lr=0.000020 grad_norm=0.843713
Epoch 15/100 Iteration 161/234: loss=0.165112 lr=0.000020 grad_norm=0.675373
Epoch 15/100 Iteration 162/234: loss=0.154041 lr=0.000020 grad_norm=0.368510
Epoch 15/100 Iteration 163/234: loss=0.161585 lr=0.000020 grad_norm=0.586739
Epoch 15/100 Iteration 164/234: loss=0.157441 lr=0.000020 grad_norm=0.690509
Epoch 15/100 Iteration 165/234: loss=0.142942 lr=0.000020 grad_norm=0.659792
Epoch 15/100 Iteration 166/234: loss=0.171873 lr=0.000020 grad_norm=0.607845
Epoch 15/100 Iteration 167/234: loss=0.151730 lr=0.000020 grad_norm=0.424007
Epoch 15/100 Iteration 168/234: loss=0.137529 lr=0.000020 grad_norm=0.569217
Epoch 15/100 Iteration 169/234: loss=0.137305 lr=0.000020 grad_norm=0.604478
Epoch 15/100 Iteration 170/234: loss=0.159988 lr=0.000020 grad_norm=0.633678
Epoch 15/100 Iteration 171/234: loss=0.161318 lr=0.000020 grad_norm=0.829494
Epoch 15/100 Iteration 172/234: loss=0.148006 lr=0.000020 grad_norm=0.650704
Epoch 15/100 Iteration 173/234: loss=0.146483 lr=0.000020 grad_norm=0.966318
Epoch 15/100 Iteration 174/234: loss=0.139006 lr=0.000020 grad_norm=1.244500
Epoch 15/100 Iteration 175/234: loss=0.157367 lr=0.000020 grad_norm=0.708602
Epoch 15/100 Iteration 176/234: loss=0.155214 lr=0.000020 grad_norm=0.606544
Epoch 15/100 Iteration 177/234: loss=0.133750 lr=0.000020 grad_norm=0.863359
Epoch 15/100 Iteration 178/234: loss=0.148014 lr=0.000020 grad_norm=0.816697
Epoch 15/100 Iteration 179/234: loss=0.172467 lr=0.000020 grad_norm=1.500314
Epoch 15/100 Iteration 180/234: loss=0.157204 lr=0.000020 grad_norm=1.566521
Epoch 15/100 Iteration 181/234: loss=0.152499 lr=0.000020 grad_norm=0.889602
Epoch 15/100 Iteration 182/234: loss=0.141991 lr=0.000020 grad_norm=0.625811
Epoch 15/100 Iteration 183/234: loss=0.139388 lr=0.000020 grad_norm=1.487141
Epoch 15/100 Iteration 184/234: loss=0.147209 lr=0.000020 grad_norm=1.046094
Epoch 15/100 Iteration 185/234: loss=0.152912 lr=0.000020 grad_norm=0.674740
Epoch 15/100 Iteration 186/234: loss=0.145175 lr=0.000020 grad_norm=1.063612
Epoch 15/100 Iteration 187/234: loss=0.151694 lr=0.000020 grad_norm=0.719684
Epoch 15/100 Iteration 188/234: loss=0.165411 lr=0.000020 grad_norm=0.985327
Epoch 15/100 Iteration 189/234: loss=0.150868 lr=0.000020 grad_norm=0.584924
Epoch 15/100 Iteration 190/234: loss=0.136733 lr=0.000020 grad_norm=0.684944
Epoch 15/100 Iteration 191/234: loss=0.164039 lr=0.000020 grad_norm=0.556081
Epoch 15/100 Iteration 192/234: loss=0.155833 lr=0.000020 grad_norm=0.380046
Epoch 15/100 Iteration 193/234: loss=0.159814 lr=0.000020 grad_norm=0.451632
Epoch 15/100 Iteration 194/234: loss=0.171145 lr=0.000020 grad_norm=0.686306
Epoch 15/100 Iteration 195/234: loss=0.160193 lr=0.000020 grad_norm=0.552699
Epoch 15/100 Iteration 196/234: loss=0.143319 lr=0.000020 grad_norm=0.481156
Epoch 15/100 Iteration 197/234: loss=0.154835 lr=0.000020 grad_norm=0.841448
Epoch 15/100 Iteration 198/234: loss=0.156686 lr=0.000020 grad_norm=0.548959
Epoch 15/100 Iteration 199/234: loss=0.154965 lr=0.000020 grad_norm=0.686676
Epoch 15/100 Iteration 200/234: loss=0.145248 lr=0.000020 grad_norm=0.841586
Epoch 15/100 Iteration 201/234: loss=0.140331 lr=0.000020 grad_norm=0.325311
Epoch 15/100 Iteration 202/234: loss=0.150497 lr=0.000020 grad_norm=0.762309
Epoch 15/100 Iteration 203/234: loss=0.146469 lr=0.000020 grad_norm=0.717170
Epoch 15/100 Iteration 204/234: loss=0.161739 lr=0.000020 grad_norm=0.430049
Epoch 15/100 Iteration 205/234: loss=0.120853 lr=0.000020 grad_norm=0.874599
Epoch 15/100 Iteration 206/234: loss=0.144837 lr=0.000020 grad_norm=0.648958
Epoch 15/100 Iteration 207/234: loss=0.161846 lr=0.000020 grad_norm=0.409330
Epoch 15/100 Iteration 208/234: loss=0.141272 lr=0.000020 grad_norm=0.507202
Epoch 15/100 Iteration 209/234: loss=0.157526 lr=0.000020 grad_norm=0.391409
Epoch 15/100 Iteration 210/234: loss=0.158076 lr=0.000020 grad_norm=0.798754
Epoch 15/100 Iteration 211/234: loss=0.151682 lr=0.000020 grad_norm=1.359220
Epoch 15/100 Iteration 212/234: loss=0.133192 lr=0.000020 grad_norm=1.276540
Epoch 15/100 Iteration 213/234: loss=0.119061 lr=0.000020 grad_norm=0.488336
Epoch 15/100 Iteration 214/234: loss=0.169758 lr=0.000020 grad_norm=1.152733
Epoch 15/100 Iteration 215/234: loss=0.158456 lr=0.000020 grad_norm=0.624393
Epoch 15/100 Iteration 216/234: loss=0.150817 lr=0.000020 grad_norm=0.678950
Epoch 15/100 Iteration 217/234: loss=0.134858 lr=0.000020 grad_norm=0.811527
Epoch 15/100 Iteration 218/234: loss=0.147852 lr=0.000020 grad_norm=0.579600
Epoch 15/100 Iteration 219/234: loss=0.157937 lr=0.000020 grad_norm=0.529314
Epoch 15/100 Iteration 220/234: loss=0.152541 lr=0.000020 grad_norm=0.709979
Epoch 15/100 Iteration 221/234: loss=0.144770 lr=0.000020 grad_norm=0.583479
Epoch 15/100 Iteration 222/234: loss=0.158518 lr=0.000020 grad_norm=0.631613
Epoch 15/100 Iteration 223/234: loss=0.148708 lr=0.000020 grad_norm=1.063457
Epoch 15/100 Iteration 224/234: loss=0.145754 lr=0.000020 grad_norm=1.035057
Epoch 15/100 Iteration 225/234: loss=0.137826 lr=0.000020 grad_norm=0.439387
Epoch 15/100 Iteration 226/234: loss=0.155181 lr=0.000020 grad_norm=0.705705
Epoch 15/100 Iteration 227/234: loss=0.146818 lr=0.000020 grad_norm=0.846010
Epoch 15/100 Iteration 228/234: loss=0.157304 lr=0.000020 grad_norm=0.552623
Epoch 15/100 Iteration 229/234: loss=0.152535 lr=0.000020 grad_norm=0.624935
Epoch 15/100 Iteration 230/234: loss=0.114356 lr=0.000020 grad_norm=0.831023
Epoch 15/100 Iteration 231/234: loss=0.164325 lr=0.000020 grad_norm=0.660881
Epoch 15/100 Iteration 232/234: loss=0.155064 lr=0.000020 grad_norm=0.677364
Epoch 15/100 Iteration 233/234: loss=0.157953 lr=0.000020 grad_norm=1.039460
Epoch 15/100 Iteration 234/234: loss=0.133525 lr=0.000020 grad_norm=1.033502
Epoch 15/100 finished. Avg Loss: 0.152035
Epoch 16/100 Iteration 1/234: loss=0.142638 lr=0.000020 grad_norm=0.593435
Epoch 16/100 Iteration 2/234: loss=0.143760 lr=0.000020 grad_norm=0.655917
Epoch 16/100 Iteration 3/234: loss=0.149317 lr=0.000020 grad_norm=0.639491
Epoch 16/100 Iteration 4/234: loss=0.150036 lr=0.000020 grad_norm=0.428939
Epoch 16/100 Iteration 5/234: loss=0.145376 lr=0.000020 grad_norm=0.555495
Epoch 16/100 Iteration 6/234: loss=0.160773 lr=0.000020 grad_norm=0.485444
Epoch 16/100 Iteration 7/234: loss=0.157595 lr=0.000020 grad_norm=0.451216
Epoch 16/100 Iteration 8/234: loss=0.149465 lr=0.000020 grad_norm=0.639564
Epoch 16/100 Iteration 9/234: loss=0.154604 lr=0.000020 grad_norm=0.953284
Epoch 16/100 Iteration 10/234: loss=0.148444 lr=0.000020 grad_norm=0.934334
Epoch 16/100 Iteration 11/234: loss=0.137139 lr=0.000020 grad_norm=0.534144
Epoch 16/100 Iteration 12/234: loss=0.131251 lr=0.000020 grad_norm=0.487774
Epoch 16/100 Iteration 13/234: loss=0.159173 lr=0.000020 grad_norm=0.754075
Epoch 16/100 Iteration 14/234: loss=0.144742 lr=0.000020 grad_norm=1.126664
Epoch 16/100 Iteration 15/234: loss=0.162019 lr=0.000020 grad_norm=0.838953
Epoch 16/100 Iteration 16/234: loss=0.160243 lr=0.000020 grad_norm=0.453679
Epoch 16/100 Iteration 17/234: loss=0.156314 lr=0.000020 grad_norm=0.621980
Epoch 16/100 Iteration 18/234: loss=0.150640 lr=0.000020 grad_norm=0.484232
Epoch 16/100 Iteration 19/234: loss=0.142361 lr=0.000020 grad_norm=0.475776
Epoch 16/100 Iteration 20/234: loss=0.173550 lr=0.000020 grad_norm=0.398686
Epoch 16/100 Iteration 21/234: loss=0.129017 lr=0.000020 grad_norm=0.337508
Epoch 16/100 Iteration 22/234: loss=0.152090 lr=0.000020 grad_norm=0.412841
Epoch 16/100 Iteration 23/234: loss=0.139789 lr=0.000020 grad_norm=0.484925
Epoch 16/100 Iteration 24/234: loss=0.160361 lr=0.000020 grad_norm=0.788294
Epoch 16/100 Iteration 25/234: loss=0.146550 lr=0.000020 grad_norm=0.687145
Epoch 16/100 Iteration 26/234: loss=0.145341 lr=0.000020 grad_norm=0.448711
Epoch 16/100 Iteration 27/234: loss=0.153301 lr=0.000020 grad_norm=0.519905
Epoch 16/100 Iteration 28/234: loss=0.154851 lr=0.000020 grad_norm=0.426907
Epoch 16/100 Iteration 29/234: loss=0.153176 lr=0.000020 grad_norm=0.567690
Epoch 16/100 Iteration 30/234: loss=0.138347 lr=0.000020 grad_norm=0.704296
Epoch 16/100 Iteration 31/234: loss=0.162053 lr=0.000020 grad_norm=0.444756
Epoch 16/100 Iteration 32/234: loss=0.156633 lr=0.000020 grad_norm=0.483407
Epoch 16/100 Iteration 33/234: loss=0.137658 lr=0.000020 grad_norm=0.655298
Epoch 16/100 Iteration 34/234: loss=0.149953 lr=0.000020 grad_norm=0.837647
Epoch 16/100 Iteration 35/234: loss=0.165859 lr=0.000020 grad_norm=0.989929
Epoch 16/100 Iteration 36/234: loss=0.157847 lr=0.000020 grad_norm=1.017466
Epoch 16/100 Iteration 37/234: loss=0.158015 lr=0.000020 grad_norm=0.566725
Epoch 16/100 Iteration 38/234: loss=0.153140 lr=0.000020 grad_norm=0.816378
Epoch 16/100 Iteration 39/234: loss=0.136925 lr=0.000020 grad_norm=1.003043
Epoch 16/100 Iteration 40/234: loss=0.156527 lr=0.000020 grad_norm=0.430560
Epoch 16/100 Iteration 41/234: loss=0.149215 lr=0.000020 grad_norm=0.638867
Epoch 16/100 Iteration 42/234: loss=0.149718 lr=0.000020 grad_norm=0.856541
Epoch 16/100 Iteration 43/234: loss=0.154538 lr=0.000020 grad_norm=0.807117
Epoch 16/100 Iteration 44/234: loss=0.152113 lr=0.000020 grad_norm=0.691021
Epoch 16/100 Iteration 45/234: loss=0.141736 lr=0.000020 grad_norm=0.523638
Epoch 16/100 Iteration 46/234: loss=0.152022 lr=0.000020 grad_norm=0.481462
Epoch 16/100 Iteration 47/234: loss=0.129017 lr=0.000020 grad_norm=0.419018
Epoch 16/100 Iteration 48/234: loss=0.162227 lr=0.000020 grad_norm=0.739481
Epoch 16/100 Iteration 49/234: loss=0.153505 lr=0.000020 grad_norm=0.714757
Epoch 16/100 Iteration 50/234: loss=0.165751 lr=0.000020 grad_norm=0.408653
Epoch 16/100 Iteration 51/234: loss=0.152698 lr=0.000020 grad_norm=0.915909
Epoch 16/100 Iteration 52/234: loss=0.149612 lr=0.000020 grad_norm=0.962937
Epoch 16/100 Iteration 53/234: loss=0.177178 lr=0.000020 grad_norm=0.482198
Epoch 16/100 Iteration 54/234: loss=0.149359 lr=0.000020 grad_norm=0.779150
Epoch 16/100 Iteration 55/234: loss=0.149024 lr=0.000020 grad_norm=1.161934
Epoch 16/100 Iteration 56/234: loss=0.155026 lr=0.000020 grad_norm=0.923279
Epoch 16/100 Iteration 57/234: loss=0.149696 lr=0.000020 grad_norm=0.693816
Epoch 16/100 Iteration 58/234: loss=0.167544 lr=0.000020 grad_norm=0.614696
Epoch 16/100 Iteration 59/234: loss=0.134803 lr=0.000020 grad_norm=0.383925
Epoch 16/100 Iteration 60/234: loss=0.128864 lr=0.000020 grad_norm=0.482785
Epoch 16/100 Iteration 61/234: loss=0.148074 lr=0.000020 grad_norm=0.692912
Epoch 16/100 Iteration 62/234: loss=0.153546 lr=0.000020 grad_norm=0.764892
Epoch 16/100 Iteration 63/234: loss=0.154671 lr=0.000020 grad_norm=0.516851
Epoch 16/100 Iteration 64/234: loss=0.159799 lr=0.000020 grad_norm=0.646476
Epoch 16/100 Iteration 65/234: loss=0.141283 lr=0.000020 grad_norm=0.517319
Epoch 16/100 Iteration 66/234: loss=0.145608 lr=0.000020 grad_norm=0.588827
Epoch 16/100 Iteration 67/234: loss=0.154649 lr=0.000020 grad_norm=0.749403
Epoch 16/100 Iteration 68/234: loss=0.145953 lr=0.000020 grad_norm=0.784969
Epoch 16/100 Iteration 69/234: loss=0.159729 lr=0.000020 grad_norm=0.504613
Epoch 16/100 Iteration 70/234: loss=0.141524 lr=0.000020 grad_norm=0.701924
Epoch 16/100 Iteration 71/234: loss=0.151894 lr=0.000020 grad_norm=0.853502
Epoch 16/100 Iteration 72/234: loss=0.117342 lr=0.000020 grad_norm=1.127669
Epoch 16/100 Iteration 73/234: loss=0.165429 lr=0.000020 grad_norm=0.798013
Epoch 16/100 Iteration 74/234: loss=0.128230 lr=0.000020 grad_norm=0.809674
Epoch 16/100 Iteration 75/234: loss=0.144873 lr=0.000020 grad_norm=1.323734
Epoch 16/100 Iteration 76/234: loss=0.144593 lr=0.000020 grad_norm=0.716404
Epoch 16/100 Iteration 77/234: loss=0.160925 lr=0.000020 grad_norm=0.729430
Epoch 16/100 Iteration 78/234: loss=0.160691 lr=0.000020 grad_norm=1.037403
Epoch 16/100 Iteration 79/234: loss=0.147960 lr=0.000020 grad_norm=0.867839
Epoch 16/100 Iteration 80/234: loss=0.144144 lr=0.000020 grad_norm=0.341519
Epoch 16/100 Iteration 81/234: loss=0.145342 lr=0.000020 grad_norm=0.699139
Epoch 16/100 Iteration 82/234: loss=0.142976 lr=0.000020 grad_norm=0.745235
Epoch 16/100 Iteration 83/234: loss=0.118201 lr=0.000020 grad_norm=0.655067
Epoch 16/100 Iteration 84/234: loss=0.134955 lr=0.000020 grad_norm=0.459384
Epoch 16/100 Iteration 85/234: loss=0.149333 lr=0.000020 grad_norm=0.820301
Epoch 16/100 Iteration 86/234: loss=0.138704 lr=0.000020 grad_norm=0.531314
Epoch 16/100 Iteration 87/234: loss=0.155305 lr=0.000020 grad_norm=1.175335
Epoch 16/100 Iteration 88/234: loss=0.146766 lr=0.000020 grad_norm=1.081194
Epoch 16/100 Iteration 89/234: loss=0.145691 lr=0.000020 grad_norm=0.478998
Epoch 16/100 Iteration 90/234: loss=0.149184 lr=0.000020 grad_norm=1.632335
Epoch 16/100 Iteration 91/234: loss=0.160720 lr=0.000020 grad_norm=1.460483
Epoch 16/100 Iteration 92/234: loss=0.154735 lr=0.000020 grad_norm=0.588108
Epoch 16/100 Iteration 93/234: loss=0.158569 lr=0.000020 grad_norm=1.128696
Epoch 16/100 Iteration 94/234: loss=0.139691 lr=0.000020 grad_norm=0.783918
Epoch 16/100 Iteration 95/234: loss=0.144736 lr=0.000020 grad_norm=0.499088
Epoch 16/100 Iteration 96/234: loss=0.146638 lr=0.000020 grad_norm=0.954899
Epoch 16/100 Iteration 97/234: loss=0.143080 lr=0.000020 grad_norm=0.532441
Epoch 16/100 Iteration 98/234: loss=0.141934 lr=0.000020 grad_norm=0.577358
Epoch 16/100 Iteration 99/234: loss=0.151837 lr=0.000020 grad_norm=0.745776
Epoch 16/100 Iteration 100/234: loss=0.156158 lr=0.000020 grad_norm=0.483249
Epoch 16/100 Iteration 101/234: loss=0.127654 lr=0.000020 grad_norm=0.635142
Epoch 16/100 Iteration 102/234: loss=0.127217 lr=0.000020 grad_norm=0.625713
Epoch 16/100 Iteration 103/234: loss=0.140426 lr=0.000020 grad_norm=0.451821
Epoch 16/100 Iteration 104/234: loss=0.138874 lr=0.000020 grad_norm=0.672431
Epoch 16/100 Iteration 105/234: loss=0.155047 lr=0.000020 grad_norm=0.353821
Epoch 16/100 Iteration 106/234: loss=0.147404 lr=0.000020 grad_norm=0.513502
Epoch 16/100 Iteration 107/234: loss=0.135289 lr=0.000020 grad_norm=0.676463
Epoch 16/100 Iteration 108/234: loss=0.136008 lr=0.000020 grad_norm=0.530349
Epoch 16/100 Iteration 109/234: loss=0.153348 lr=0.000020 grad_norm=0.380080
Epoch 16/100 Iteration 110/234: loss=0.139625 lr=0.000020 grad_norm=0.263428
Epoch 16/100 Iteration 111/234: loss=0.130007 lr=0.000020 grad_norm=0.313785
Epoch 16/100 Iteration 112/234: loss=0.136189 lr=0.000020 grad_norm=0.389971
Epoch 16/100 Iteration 113/234: loss=0.148361 lr=0.000020 grad_norm=0.342905
Epoch 16/100 Iteration 114/234: loss=0.136578 lr=0.000020 grad_norm=0.443233
Epoch 16/100 Iteration 115/234: loss=0.144033 lr=0.000020 grad_norm=0.353680
Epoch 16/100 Iteration 116/234: loss=0.145306 lr=0.000020 grad_norm=0.499230
Epoch 16/100 Iteration 117/234: loss=0.138343 lr=0.000020 grad_norm=0.413627
Epoch 16/100 Iteration 118/234: loss=0.148578 lr=0.000020 grad_norm=0.331132
Epoch 16/100 Iteration 119/234: loss=0.135064 lr=0.000020 grad_norm=0.465636
Epoch 16/100 Iteration 120/234: loss=0.133979 lr=0.000020 grad_norm=0.477340
Epoch 16/100 Iteration 121/234: loss=0.141428 lr=0.000020 grad_norm=0.563173
Epoch 16/100 Iteration 122/234: loss=0.134997 lr=0.000020 grad_norm=0.442623
Epoch 16/100 Iteration 123/234: loss=0.144950 lr=0.000020 grad_norm=0.342069
Epoch 16/100 Iteration 124/234: loss=0.150804 lr=0.000020 grad_norm=0.263351
Epoch 16/100 Iteration 125/234: loss=0.149827 lr=0.000020 grad_norm=0.418057
Epoch 16/100 Iteration 126/234: loss=0.138245 lr=0.000020 grad_norm=0.408894
Epoch 16/100 Iteration 127/234: loss=0.145214 lr=0.000020 grad_norm=0.527246
Epoch 16/100 Iteration 128/234: loss=0.140077 lr=0.000020 grad_norm=0.368454
Epoch 16/100 Iteration 129/234: loss=0.138447 lr=0.000020 grad_norm=0.427017
Epoch 16/100 Iteration 130/234: loss=0.137178 lr=0.000020 grad_norm=0.577579
Epoch 16/100 Iteration 131/234: loss=0.128633 lr=0.000020 grad_norm=0.567099
Epoch 16/100 Iteration 132/234: loss=0.154591 lr=0.000020 grad_norm=0.459359
Epoch 16/100 Iteration 133/234: loss=0.160781 lr=0.000020 grad_norm=0.822151
Epoch 16/100 Iteration 134/234: loss=0.145982 lr=0.000020 grad_norm=1.290840
Epoch 16/100 Iteration 135/234: loss=0.144115 lr=0.000020 grad_norm=0.953487
Epoch 16/100 Iteration 136/234: loss=0.162508 lr=0.000020 grad_norm=0.425678
Epoch 16/100 Iteration 137/234: loss=0.136546 lr=0.000020 grad_norm=0.793310
Epoch 16/100 Iteration 138/234: loss=0.151248 lr=0.000020 grad_norm=0.814913
Epoch 16/100 Iteration 139/234: loss=0.135812 lr=0.000020 grad_norm=0.468648
Epoch 16/100 Iteration 140/234: loss=0.141435 lr=0.000020 grad_norm=0.553717
Epoch 16/100 Iteration 141/234: loss=0.126833 lr=0.000020 grad_norm=0.609183
Epoch 16/100 Iteration 142/234: loss=0.153196 lr=0.000020 grad_norm=0.518002
Epoch 16/100 Iteration 143/234: loss=0.163493 lr=0.000020 grad_norm=0.600646
Epoch 16/100 Iteration 144/234: loss=0.152084 lr=0.000020 grad_norm=0.624086
Epoch 16/100 Iteration 145/234: loss=0.126134 lr=0.000020 grad_norm=0.452150
Epoch 16/100 Iteration 146/234: loss=0.135338 lr=0.000020 grad_norm=0.342398
Epoch 16/100 Iteration 147/234: loss=0.146967 lr=0.000020 grad_norm=0.524825
Epoch 16/100 Iteration 148/234: loss=0.147504 lr=0.000020 grad_norm=0.762808
Epoch 16/100 Iteration 149/234: loss=0.146116 lr=0.000020 grad_norm=0.676315
Epoch 16/100 Iteration 150/234: loss=0.141661 lr=0.000020 grad_norm=0.713794
Epoch 16/100 Iteration 151/234: loss=0.149386 lr=0.000020 grad_norm=0.703561
Epoch 16/100 Iteration 152/234: loss=0.153735 lr=0.000020 grad_norm=0.394310
Epoch 16/100 Iteration 153/234: loss=0.142437 lr=0.000020 grad_norm=0.470362
Epoch 16/100 Iteration 154/234: loss=0.143670 lr=0.000020 grad_norm=1.173303
Epoch 16/100 Iteration 155/234: loss=0.143074 lr=0.000020 grad_norm=1.237196
Epoch 16/100 Iteration 156/234: loss=0.120228 lr=0.000020 grad_norm=0.659229
Epoch 16/100 Iteration 157/234: loss=0.144658 lr=0.000020 grad_norm=1.268085
Epoch 16/100 Iteration 158/234: loss=0.139180 lr=0.000020 grad_norm=0.879213
Epoch 16/100 Iteration 159/234: loss=0.142179 lr=0.000020 grad_norm=0.810729
Epoch 16/100 Iteration 160/234: loss=0.143349 lr=0.000020 grad_norm=1.605627
Epoch 16/100 Iteration 161/234: loss=0.130628 lr=0.000020 grad_norm=0.861321
Epoch 16/100 Iteration 162/234: loss=0.140263 lr=0.000020 grad_norm=0.895035
Epoch 16/100 Iteration 163/234: loss=0.129724 lr=0.000020 grad_norm=1.190136
Epoch 16/100 Iteration 164/234: loss=0.150313 lr=0.000020 grad_norm=0.488190
Epoch 16/100 Iteration 165/234: loss=0.133161 lr=0.000020 grad_norm=0.789579
Epoch 16/100 Iteration 166/234: loss=0.158158 lr=0.000020 grad_norm=0.712059
Epoch 16/100 Iteration 167/234: loss=0.133458 lr=0.000020 grad_norm=0.317217
Epoch 16/100 Iteration 168/234: loss=0.147245 lr=0.000020 grad_norm=0.505002
Epoch 16/100 Iteration 169/234: loss=0.155171 lr=0.000020 grad_norm=0.428662
Epoch 16/100 Iteration 170/234: loss=0.147097 lr=0.000020 grad_norm=0.503503
Epoch 16/100 Iteration 171/234: loss=0.152415 lr=0.000020 grad_norm=0.655367
Epoch 16/100 Iteration 172/234: loss=0.154454 lr=0.000020 grad_norm=0.620758
Epoch 16/100 Iteration 173/234: loss=0.156559 lr=0.000020 grad_norm=0.457777
Epoch 16/100 Iteration 174/234: loss=0.140940 lr=0.000020 grad_norm=0.640866
Epoch 16/100 Iteration 175/234: loss=0.145119 lr=0.000020 grad_norm=0.588566
Epoch 16/100 Iteration 176/234: loss=0.132399 lr=0.000020 grad_norm=0.425121
Epoch 16/100 Iteration 177/234: loss=0.150603 lr=0.000020 grad_norm=0.429720
Epoch 16/100 Iteration 178/234: loss=0.155981 lr=0.000020 grad_norm=0.428071
Epoch 16/100 Iteration 179/234: loss=0.144018 lr=0.000020 grad_norm=0.376149
Epoch 16/100 Iteration 180/234: loss=0.133494 lr=0.000020 grad_norm=0.458504
Epoch 16/100 Iteration 181/234: loss=0.132465 lr=0.000020 grad_norm=0.616179
Epoch 16/100 Iteration 182/234: loss=0.132384 lr=0.000020 grad_norm=0.864334
Epoch 16/100 Iteration 183/234: loss=0.124208 lr=0.000020 grad_norm=0.820496
Epoch 16/100 Iteration 184/234: loss=0.137786 lr=0.000020 grad_norm=0.485780
Epoch 16/100 Iteration 185/234: loss=0.161626 lr=0.000020 grad_norm=0.628296
Epoch 16/100 Iteration 186/234: loss=0.155427 lr=0.000020 grad_norm=0.844270
Epoch 16/100 Iteration 187/234: loss=0.136515 lr=0.000020 grad_norm=0.666154
Epoch 16/100 Iteration 188/234: loss=0.136211 lr=0.000020 grad_norm=0.409287
Epoch 16/100 Iteration 189/234: loss=0.136872 lr=0.000020 grad_norm=0.454277
Epoch 16/100 Iteration 190/234: loss=0.153857 lr=0.000020 grad_norm=0.488078
Epoch 16/100 Iteration 191/234: loss=0.143574 lr=0.000020 grad_norm=0.404479
Epoch 16/100 Iteration 192/234: loss=0.139439 lr=0.000020 grad_norm=0.514081
Epoch 16/100 Iteration 193/234: loss=0.142247 lr=0.000020 grad_norm=0.585685
Epoch 16/100 Iteration 194/234: loss=0.168954 lr=0.000020 grad_norm=0.438956
Epoch 16/100 Iteration 195/234: loss=0.164365 lr=0.000020 grad_norm=0.725167
Epoch 16/100 Iteration 196/234: loss=0.139091 lr=0.000020 grad_norm=1.129469
Epoch 16/100 Iteration 197/234: loss=0.146926 lr=0.000020 grad_norm=1.365378
Epoch 16/100 Iteration 198/234: loss=0.154409 lr=0.000020 grad_norm=1.013061
Epoch 16/100 Iteration 199/234: loss=0.139559 lr=0.000020 grad_norm=0.388237
Epoch 16/100 Iteration 200/234: loss=0.137081 lr=0.000020 grad_norm=1.108245
Epoch 16/100 Iteration 201/234: loss=0.133256 lr=0.000020 grad_norm=1.359532
Epoch 16/100 Iteration 202/234: loss=0.136207 lr=0.000020 grad_norm=0.854585
Epoch 16/100 Iteration 203/234: loss=0.146252 lr=0.000020 grad_norm=1.096856
Epoch 16/100 Iteration 204/234: loss=0.139095 lr=0.000020 grad_norm=1.354453
Epoch 16/100 Iteration 205/234: loss=0.146311 lr=0.000020 grad_norm=0.735752
Epoch 16/100 Iteration 206/234: loss=0.148136 lr=0.000020 grad_norm=1.385175
Epoch 16/100 Iteration 207/234: loss=0.141815 lr=0.000020 grad_norm=1.113806
Epoch 16/100 Iteration 208/234: loss=0.148203 lr=0.000020 grad_norm=0.441751
Epoch 16/100 Iteration 209/234: loss=0.143526 lr=0.000020 grad_norm=0.706579
Epoch 16/100 Iteration 210/234: loss=0.121571 lr=0.000020 grad_norm=0.528735
Epoch 16/100 Iteration 211/234: loss=0.145049 lr=0.000020 grad_norm=0.966570
Epoch 16/100 Iteration 212/234: loss=0.132522 lr=0.000020 grad_norm=0.939918
Epoch 16/100 Iteration 213/234: loss=0.126528 lr=0.000020 grad_norm=0.457976
Epoch 16/100 Iteration 214/234: loss=0.113653 lr=0.000020 grad_norm=1.119581
Epoch 16/100 Iteration 215/234: loss=0.135731 lr=0.000020 grad_norm=0.730482
Epoch 16/100 Iteration 216/234: loss=0.150057 lr=0.000020 grad_norm=0.597449
Epoch 16/100 Iteration 217/234: loss=0.138296 lr=0.000020 grad_norm=1.173798
Epoch 16/100 Iteration 218/234: loss=0.147772 lr=0.000020 grad_norm=0.547492
Epoch 16/100 Iteration 219/234: loss=0.137847 lr=0.000020 grad_norm=0.691645
Epoch 16/100 Iteration 220/234: loss=0.125088 lr=0.000020 grad_norm=0.540376
Epoch 16/100 Iteration 221/234: loss=0.125068 lr=0.000020 grad_norm=0.700001
Epoch 16/100 Iteration 222/234: loss=0.130596 lr=0.000020 grad_norm=1.047282
Epoch 16/100 Iteration 223/234: loss=0.154147 lr=0.000020 grad_norm=0.758901
Epoch 16/100 Iteration 224/234: loss=0.148076 lr=0.000020 grad_norm=0.405186
Epoch 16/100 Iteration 225/234: loss=0.135758 lr=0.000020 grad_norm=0.871376
Epoch 16/100 Iteration 226/234: loss=0.124859 lr=0.000020 grad_norm=0.600563
Epoch 16/100 Iteration 227/234: loss=0.136045 lr=0.000020 grad_norm=0.779131
Epoch 16/100 Iteration 228/234: loss=0.144339 lr=0.000020 grad_norm=1.254131
Epoch 16/100 Iteration 229/234: loss=0.164315 lr=0.000020 grad_norm=0.875225
Epoch 16/100 Iteration 230/234: loss=0.138102 lr=0.000020 grad_norm=0.617864
Epoch 16/100 Iteration 231/234: loss=0.152311 lr=0.000020 grad_norm=0.604769
Epoch 16/100 Iteration 232/234: loss=0.141484 lr=0.000020 grad_norm=0.538359
Epoch 16/100 Iteration 233/234: loss=0.137325 lr=0.000020 grad_norm=0.546225
Epoch 16/100 Iteration 234/234: loss=0.158997 lr=0.000020 grad_norm=0.614882
Epoch 16/100 finished. Avg Loss: 0.145341
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 17/100 Iteration 1/234: loss=0.134949 lr=0.000020 grad_norm=0.412714
Epoch 17/100 Iteration 2/234: loss=0.145221 lr=0.000020 grad_norm=0.793984
Epoch 17/100 Iteration 3/234: loss=0.145394 lr=0.000020 grad_norm=0.782564
Epoch 17/100 Iteration 4/234: loss=0.127267 lr=0.000020 grad_norm=0.488335
Epoch 17/100 Iteration 5/234: loss=0.143375 lr=0.000020 grad_norm=0.785225
Epoch 17/100 Iteration 6/234: loss=0.117285 lr=0.000020 grad_norm=0.868023
Epoch 17/100 Iteration 7/234: loss=0.144090 lr=0.000020 grad_norm=0.576278
Epoch 17/100 Iteration 8/234: loss=0.145836 lr=0.000020 grad_norm=0.949113
Epoch 17/100 Iteration 9/234: loss=0.137236 lr=0.000020 grad_norm=0.614751
Epoch 17/100 Iteration 10/234: loss=0.153193 lr=0.000020 grad_norm=0.541644
Epoch 17/100 Iteration 11/234: loss=0.147310 lr=0.000020 grad_norm=0.640123
Epoch 17/100 Iteration 12/234: loss=0.152067 lr=0.000020 grad_norm=0.535925
Epoch 17/100 Iteration 13/234: loss=0.126479 lr=0.000020 grad_norm=0.579701
Epoch 17/100 Iteration 14/234: loss=0.142376 lr=0.000020 grad_norm=0.426048
Epoch 17/100 Iteration 15/234: loss=0.134068 lr=0.000020 grad_norm=0.584480
Epoch 17/100 Iteration 16/234: loss=0.146803 lr=0.000020 grad_norm=0.509847
Epoch 17/100 Iteration 17/234: loss=0.130293 lr=0.000020 grad_norm=0.562508
Epoch 17/100 Iteration 18/234: loss=0.136874 lr=0.000020 grad_norm=0.469121
Epoch 17/100 Iteration 19/234: loss=0.143388 lr=0.000020 grad_norm=0.642379
Epoch 17/100 Iteration 20/234: loss=0.145770 lr=0.000020 grad_norm=0.381428
Epoch 17/100 Iteration 21/234: loss=0.133333 lr=0.000020 grad_norm=0.671995
Epoch 17/100 Iteration 22/234: loss=0.129994 lr=0.000020 grad_norm=0.829090
Epoch 17/100 Iteration 23/234: loss=0.145876 lr=0.000020 grad_norm=0.596137
Epoch 17/100 Iteration 24/234: loss=0.123714 lr=0.000020 grad_norm=0.795335
Epoch 17/100 Iteration 25/234: loss=0.144907 lr=0.000020 grad_norm=0.649831
Epoch 17/100 Iteration 26/234: loss=0.153036 lr=0.000020 grad_norm=0.696071
Epoch 17/100 Iteration 27/234: loss=0.154004 lr=0.000020 grad_norm=0.861534
Epoch 17/100 Iteration 28/234: loss=0.147818 lr=0.000020 grad_norm=0.392937
Epoch 17/100 Iteration 29/234: loss=0.142779 lr=0.000020 grad_norm=0.784776
Epoch 17/100 Iteration 30/234: loss=0.130849 lr=0.000020 grad_norm=0.610222
Epoch 17/100 Iteration 31/234: loss=0.139533 lr=0.000020 grad_norm=0.623043
Epoch 17/100 Iteration 32/234: loss=0.152795 lr=0.000020 grad_norm=0.442545
Epoch 17/100 Iteration 33/234: loss=0.147418 lr=0.000020 grad_norm=0.503001
Epoch 17/100 Iteration 34/234: loss=0.135016 lr=0.000020 grad_norm=1.065163
Epoch 17/100 Iteration 35/234: loss=0.146648 lr=0.000020 grad_norm=1.226972
Epoch 17/100 Iteration 36/234: loss=0.146264 lr=0.000020 grad_norm=0.625445
Epoch 17/100 Iteration 37/234: loss=0.139094 lr=0.000020 grad_norm=0.745974
Epoch 17/100 Iteration 38/234: loss=0.140029 lr=0.000020 grad_norm=0.762716
Epoch 17/100 Iteration 39/234: loss=0.134918 lr=0.000020 grad_norm=0.616980
Epoch 17/100 Iteration 40/234: loss=0.138733 lr=0.000020 grad_norm=0.892142
Epoch 17/100 Iteration 41/234: loss=0.138536 lr=0.000020 grad_norm=0.491362
Epoch 17/100 Iteration 42/234: loss=0.133431 lr=0.000020 grad_norm=0.852762
Epoch 17/100 Iteration 43/234: loss=0.133338 lr=0.000020 grad_norm=0.502762
Epoch 17/100 Iteration 44/234: loss=0.137358 lr=0.000020 grad_norm=0.522658
Epoch 17/100 Iteration 45/234: loss=0.146962 lr=0.000020 grad_norm=0.585321
Epoch 17/100 Iteration 46/234: loss=0.135168 lr=0.000020 grad_norm=0.529137
Epoch 17/100 Iteration 47/234: loss=0.141823 lr=0.000020 grad_norm=0.361361
Epoch 17/100 Iteration 48/234: loss=0.163499 lr=0.000020 grad_norm=0.564026
Epoch 17/100 Iteration 49/234: loss=0.148707 lr=0.000020 grad_norm=0.515466
Epoch 17/100 Iteration 50/234: loss=0.152154 lr=0.000020 grad_norm=0.505175
Epoch 17/100 Iteration 51/234: loss=0.144022 lr=0.000020 grad_norm=0.689737
Epoch 17/100 Iteration 52/234: loss=0.149989 lr=0.000020 grad_norm=1.015042
Epoch 17/100 Iteration 53/234: loss=0.139836 lr=0.000020 grad_norm=0.777510
Epoch 17/100 Iteration 54/234: loss=0.167488 lr=0.000020 grad_norm=0.463928
Epoch 17/100 Iteration 55/234: loss=0.124335 lr=0.000020 grad_norm=0.700357
Epoch 17/100 Iteration 56/234: loss=0.133310 lr=0.000020 grad_norm=0.333522
Epoch 17/100 Iteration 57/234: loss=0.122994 lr=0.000020 grad_norm=0.649636
Epoch 17/100 Iteration 58/234: loss=0.147459 lr=0.000020 grad_norm=0.454120
Epoch 17/100 Iteration 59/234: loss=0.151623 lr=0.000020 grad_norm=0.699910
Epoch 17/100 Iteration 60/234: loss=0.146747 lr=0.000020 grad_norm=0.994703
Epoch 17/100 Iteration 61/234: loss=0.135660 lr=0.000020 grad_norm=0.538307
Epoch 17/100 Iteration 62/234: loss=0.136489 lr=0.000020 grad_norm=0.604763
Epoch 17/100 Iteration 63/234: loss=0.150209 lr=0.000020 grad_norm=0.711154
Epoch 17/100 Iteration 64/234: loss=0.127452 lr=0.000020 grad_norm=0.490630
Epoch 17/100 Iteration 65/234: loss=0.140514 lr=0.000020 grad_norm=0.431258
Epoch 17/100 Iteration 66/234: loss=0.153111 lr=0.000020 grad_norm=0.375099
Epoch 17/100 Iteration 67/234: loss=0.137406 lr=0.000020 grad_norm=0.512298
Epoch 17/100 Iteration 68/234: loss=0.163181 lr=0.000020 grad_norm=0.540540
Epoch 17/100 Iteration 69/234: loss=0.141141 lr=0.000020 grad_norm=0.642022
Epoch 17/100 Iteration 70/234: loss=0.142959 lr=0.000020 grad_norm=0.450511
Epoch 17/100 Iteration 71/234: loss=0.126284 lr=0.000020 grad_norm=0.500152
Epoch 17/100 Iteration 72/234: loss=0.155587 lr=0.000020 grad_norm=0.341413
Epoch 17/100 Iteration 73/234: loss=0.161365 lr=0.000020 grad_norm=0.579087
Epoch 17/100 Iteration 74/234: loss=0.127130 lr=0.000020 grad_norm=0.331778
Epoch 17/100 Iteration 75/234: loss=0.144059 lr=0.000020 grad_norm=0.628510
Epoch 17/100 Iteration 76/234: loss=0.130219 lr=0.000020 grad_norm=0.388279
Epoch 17/100 Iteration 77/234: loss=0.129812 lr=0.000020 grad_norm=0.509180
Epoch 17/100 Iteration 78/234: loss=0.138154 lr=0.000020 grad_norm=0.573754
Epoch 17/100 Iteration 79/234: loss=0.142646 lr=0.000020 grad_norm=0.708509
Epoch 17/100 Iteration 80/234: loss=0.123278 lr=0.000020 grad_norm=0.791389
Epoch 17/100 Iteration 81/234: loss=0.150483 lr=0.000020 grad_norm=0.409649
Epoch 17/100 Iteration 82/234: loss=0.150044 lr=0.000020 grad_norm=1.106644
Epoch 17/100 Iteration 83/234: loss=0.129734 lr=0.000020 grad_norm=1.037040
Epoch 17/100 Iteration 84/234: loss=0.133717 lr=0.000020 grad_norm=0.872282
Epoch 17/100 Iteration 85/234: loss=0.140846 lr=0.000020 grad_norm=0.854946
Epoch 17/100 Iteration 86/234: loss=0.130092 lr=0.000020 grad_norm=0.410141
Epoch 17/100 Iteration 87/234: loss=0.137603 lr=0.000020 grad_norm=0.641212
Epoch 17/100 Iteration 88/234: loss=0.151435 lr=0.000020 grad_norm=0.887273
Epoch 17/100 Iteration 89/234: loss=0.158148 lr=0.000020 grad_norm=0.689597
Epoch 17/100 Iteration 90/234: loss=0.131858 lr=0.000020 grad_norm=0.399034
Epoch 17/100 Iteration 91/234: loss=0.141343 lr=0.000020 grad_norm=0.603838
Epoch 17/100 Iteration 92/234: loss=0.142954 lr=0.000020 grad_norm=0.923885
Epoch 17/100 Iteration 93/234: loss=0.137956 lr=0.000020 grad_norm=1.038647
Epoch 17/100 Iteration 94/234: loss=0.131567 lr=0.000020 grad_norm=0.435876
Epoch 17/100 Iteration 95/234: loss=0.149356 lr=0.000020 grad_norm=0.950836
Epoch 17/100 Iteration 96/234: loss=0.135134 lr=0.000020 grad_norm=1.460463
Epoch 17/100 Iteration 97/234: loss=0.148603 lr=0.000020 grad_norm=0.857418
Epoch 17/100 Iteration 98/234: loss=0.145162 lr=0.000020 grad_norm=0.640311
Epoch 17/100 Iteration 99/234: loss=0.140220 lr=0.000020 grad_norm=1.112119
Epoch 17/100 Iteration 100/234: loss=0.142103 lr=0.000020 grad_norm=0.516362
Epoch 17/100 Iteration 101/234: loss=0.147682 lr=0.000020 grad_norm=0.842228
Epoch 17/100 Iteration 102/234: loss=0.123259 lr=0.000020 grad_norm=1.272953
Epoch 17/100 Iteration 103/234: loss=0.134889 lr=0.000020 grad_norm=0.668731
Epoch 17/100 Iteration 104/234: loss=0.143482 lr=0.000020 grad_norm=0.963651
Epoch 17/100 Iteration 105/234: loss=0.138720 lr=0.000020 grad_norm=1.366411
Epoch 17/100 Iteration 106/234: loss=0.126711 lr=0.000020 grad_norm=0.684870
Epoch 17/100 Iteration 107/234: loss=0.153235 lr=0.000020 grad_norm=0.966910
Epoch 17/100 Iteration 108/234: loss=0.134537 lr=0.000020 grad_norm=0.914115
Epoch 17/100 Iteration 109/234: loss=0.124004 lr=0.000020 grad_norm=0.514434
Epoch 17/100 Iteration 110/234: loss=0.157546 lr=0.000020 grad_norm=0.795815
Epoch 17/100 Iteration 111/234: loss=0.158863 lr=0.000020 grad_norm=0.799465
Epoch 17/100 Iteration 112/234: loss=0.147117 lr=0.000020 grad_norm=0.672352
Epoch 17/100 Iteration 113/234: loss=0.134730 lr=0.000020 grad_norm=0.589048
Epoch 17/100 Iteration 114/234: loss=0.140784 lr=0.000020 grad_norm=0.618156
Epoch 17/100 Iteration 115/234: loss=0.136261 lr=0.000020 grad_norm=0.885191
Epoch 17/100 Iteration 116/234: loss=0.148592 lr=0.000020 grad_norm=0.853011
Epoch 17/100 Iteration 117/234: loss=0.140335 lr=0.000020 grad_norm=0.338592
Epoch 17/100 Iteration 118/234: loss=0.138133 lr=0.000020 grad_norm=0.753348
Epoch 17/100 Iteration 119/234: loss=0.146336 lr=0.000020 grad_norm=0.666085
Epoch 17/100 Iteration 120/234: loss=0.145068 lr=0.000020 grad_norm=1.409558
Epoch 17/100 Iteration 121/234: loss=0.143694 lr=0.000020 grad_norm=0.975892
Epoch 17/100 Iteration 122/234: loss=0.147758 lr=0.000020 grad_norm=0.690783
Epoch 17/100 Iteration 123/234: loss=0.137949 lr=0.000020 grad_norm=0.928135
Epoch 17/100 Iteration 124/234: loss=0.149537 lr=0.000020 grad_norm=0.344222
Epoch 17/100 Iteration 125/234: loss=0.137295 lr=0.000020 grad_norm=1.281315
Epoch 17/100 Iteration 126/234: loss=0.142502 lr=0.000020 grad_norm=1.210215
Epoch 17/100 Iteration 127/234: loss=0.131419 lr=0.000020 grad_norm=0.597689
Epoch 17/100 Iteration 128/234: loss=0.131135 lr=0.000020 grad_norm=1.124430
Epoch 17/100 Iteration 129/234: loss=0.147097 lr=0.000020 grad_norm=0.464235
Epoch 17/100 Iteration 130/234: loss=0.133926 lr=0.000020 grad_norm=0.988276
Epoch 17/100 Iteration 131/234: loss=0.131299 lr=0.000020 grad_norm=0.795531
Epoch 17/100 Iteration 132/234: loss=0.131489 lr=0.000020 grad_norm=0.613853
Epoch 17/100 Iteration 133/234: loss=0.139073 lr=0.000020 grad_norm=0.514968
Epoch 17/100 Iteration 134/234: loss=0.143286 lr=0.000020 grad_norm=0.388946
Epoch 17/100 Iteration 135/234: loss=0.152106 lr=0.000020 grad_norm=0.676980
Epoch 17/100 Iteration 136/234: loss=0.136721 lr=0.000020 grad_norm=0.452549
Epoch 17/100 Iteration 137/234: loss=0.123948 lr=0.000020 grad_norm=0.680818
Epoch 17/100 Iteration 138/234: loss=0.139764 lr=0.000020 grad_norm=0.748979
Epoch 17/100 Iteration 139/234: loss=0.141395 lr=0.000020 grad_norm=1.053435
Epoch 17/100 Iteration 140/234: loss=0.147715 lr=0.000020 grad_norm=1.517651
Epoch 17/100 Iteration 141/234: loss=0.145388 lr=0.000020 grad_norm=0.837412
Epoch 17/100 Iteration 142/234: loss=0.140394 lr=0.000020 grad_norm=1.049042
Epoch 17/100 Iteration 143/234: loss=0.129376 lr=0.000020 grad_norm=1.807035
Epoch 17/100 Iteration 144/234: loss=0.131101 lr=0.000020 grad_norm=0.948159
Epoch 17/100 Iteration 145/234: loss=0.134037 lr=0.000020 grad_norm=0.692399
Epoch 17/100 Iteration 146/234: loss=0.143090 lr=0.000020 grad_norm=1.022042
Epoch 17/100 Iteration 147/234: loss=0.136049 lr=0.000020 grad_norm=0.399969
Epoch 17/100 Iteration 148/234: loss=0.156016 lr=0.000020 grad_norm=0.768458
Epoch 17/100 Iteration 149/234: loss=0.134975 lr=0.000020 grad_norm=0.566932
Epoch 17/100 Iteration 150/234: loss=0.150635 lr=0.000020 grad_norm=0.689005
Epoch 17/100 Iteration 151/234: loss=0.132540 lr=0.000020 grad_norm=0.921174
Epoch 17/100 Iteration 152/234: loss=0.122830 lr=0.000020 grad_norm=0.413148
Epoch 17/100 Iteration 153/234: loss=0.138581 lr=0.000020 grad_norm=0.854943
Epoch 17/100 Iteration 154/234: loss=0.142173 lr=0.000020 grad_norm=0.538115
Epoch 17/100 Iteration 155/234: loss=0.128806 lr=0.000020 grad_norm=0.812138
Epoch 17/100 Iteration 156/234: loss=0.137412 lr=0.000020 grad_norm=0.786590
Epoch 17/100 Iteration 157/234: loss=0.141777 lr=0.000020 grad_norm=0.505889
Epoch 17/100 Iteration 158/234: loss=0.152874 lr=0.000020 grad_norm=0.885354
Epoch 17/100 Iteration 159/234: loss=0.125277 lr=0.000020 grad_norm=0.756282
Epoch 17/100 Iteration 160/234: loss=0.130057 lr=0.000020 grad_norm=0.432827
Epoch 17/100 Iteration 161/234: loss=0.141002 lr=0.000020 grad_norm=0.720050
Epoch 17/100 Iteration 162/234: loss=0.135290 lr=0.000020 grad_norm=0.978677
Epoch 17/100 Iteration 163/234: loss=0.128297 lr=0.000020 grad_norm=0.646155
Epoch 17/100 Iteration 164/234: loss=0.131983 lr=0.000020 grad_norm=0.622849
Epoch 17/100 Iteration 165/234: loss=0.133942 lr=0.000020 grad_norm=1.034797
Epoch 17/100 Iteration 166/234: loss=0.131149 lr=0.000020 grad_norm=0.531615
Epoch 17/100 Iteration 167/234: loss=0.126808 lr=0.000020 grad_norm=0.901639
Epoch 17/100 Iteration 168/234: loss=0.148556 lr=0.000020 grad_norm=1.472742
Epoch 17/100 Iteration 169/234: loss=0.138560 lr=0.000020 grad_norm=1.040976
Epoch 17/100 Iteration 170/234: loss=0.144637 lr=0.000020 grad_norm=0.568223
Epoch 17/100 Iteration 171/234: loss=0.141358 lr=0.000020 grad_norm=0.540460
Epoch 17/100 Iteration 172/234: loss=0.117808 lr=0.000020 grad_norm=0.505581
Epoch 17/100 Iteration 173/234: loss=0.147854 lr=0.000020 grad_norm=0.532213
Epoch 17/100 Iteration 174/234: loss=0.141626 lr=0.000020 grad_norm=0.490161
Epoch 17/100 Iteration 175/234: loss=0.146117 lr=0.000020 grad_norm=0.485273
Epoch 17/100 Iteration 176/234: loss=0.123085 lr=0.000020 grad_norm=0.520134
Epoch 17/100 Iteration 177/234: loss=0.132888 lr=0.000020 grad_norm=0.441689
Epoch 17/100 Iteration 178/234: loss=0.138002 lr=0.000020 grad_norm=0.376333
Epoch 17/100 Iteration 179/234: loss=0.144123 lr=0.000020 grad_norm=0.617268
Epoch 17/100 Iteration 180/234: loss=0.135260 lr=0.000020 grad_norm=0.558804
Epoch 17/100 Iteration 181/234: loss=0.143058 lr=0.000020 grad_norm=0.325503
Epoch 17/100 Iteration 182/234: loss=0.146642 lr=0.000020 grad_norm=0.374255
Epoch 17/100 Iteration 183/234: loss=0.137059 lr=0.000020 grad_norm=0.321019
Epoch 17/100 Iteration 184/234: loss=0.161095 lr=0.000020 grad_norm=0.364705
Epoch 17/100 Iteration 185/234: loss=0.131309 lr=0.000020 grad_norm=0.349660
Epoch 17/100 Iteration 186/234: loss=0.139266 lr=0.000020 grad_norm=0.437653
Epoch 17/100 Iteration 187/234: loss=0.146923 lr=0.000020 grad_norm=0.675006
Epoch 17/100 Iteration 188/234: loss=0.154866 lr=0.000020 grad_norm=0.681696
Epoch 17/100 Iteration 189/234: loss=0.139310 lr=0.000020 grad_norm=0.495709
Epoch 17/100 Iteration 190/234: loss=0.119626 lr=0.000020 grad_norm=0.371941
Epoch 17/100 Iteration 191/234: loss=0.138792 lr=0.000020 grad_norm=0.588306
Epoch 17/100 Iteration 192/234: loss=0.143208 lr=0.000020 grad_norm=0.827434
Epoch 17/100 Iteration 193/234: loss=0.120944 lr=0.000020 grad_norm=0.632715
Epoch 17/100 Iteration 194/234: loss=0.150490 lr=0.000020 grad_norm=0.399029
Epoch 17/100 Iteration 195/234: loss=0.144764 lr=0.000020 grad_norm=0.832899
Epoch 17/100 Iteration 196/234: loss=0.118569 lr=0.000020 grad_norm=0.829282
Epoch 17/100 Iteration 197/234: loss=0.147400 lr=0.000020 grad_norm=0.452803
Epoch 17/100 Iteration 198/234: loss=0.148725 lr=0.000020 grad_norm=0.501752
Epoch 17/100 Iteration 199/234: loss=0.145780 lr=0.000020 grad_norm=0.846310
Epoch 17/100 Iteration 200/234: loss=0.142508 lr=0.000020 grad_norm=0.520005
Epoch 17/100 Iteration 201/234: loss=0.138925 lr=0.000020 grad_norm=0.650726
Epoch 17/100 Iteration 202/234: loss=0.136183 lr=0.000020 grad_norm=1.000270
Epoch 17/100 Iteration 203/234: loss=0.126966 lr=0.000020 grad_norm=0.627407
Epoch 17/100 Iteration 204/234: loss=0.124750 lr=0.000020 grad_norm=0.661785
Epoch 17/100 Iteration 205/234: loss=0.138828 lr=0.000020 grad_norm=0.764718
Epoch 17/100 Iteration 206/234: loss=0.134306 lr=0.000020 grad_norm=0.562168
Epoch 17/100 Iteration 207/234: loss=0.145084 lr=0.000020 grad_norm=0.636614
Epoch 17/100 Iteration 208/234: loss=0.138946 lr=0.000020 grad_norm=0.523857
Epoch 17/100 Iteration 209/234: loss=0.145801 lr=0.000020 grad_norm=0.592676
Epoch 17/100 Iteration 210/234: loss=0.141390 lr=0.000020 grad_norm=0.711506
Epoch 17/100 Iteration 211/234: loss=0.124718 lr=0.000020 grad_norm=0.680286
Epoch 17/100 Iteration 212/234: loss=0.134902 lr=0.000020 grad_norm=0.461197
Epoch 17/100 Iteration 213/234: loss=0.136307 lr=0.000020 grad_norm=0.535786
Epoch 17/100 Iteration 214/234: loss=0.152726 lr=0.000020 grad_norm=0.642947
Epoch 17/100 Iteration 215/234: loss=0.139181 lr=0.000020 grad_norm=0.483064
Epoch 17/100 Iteration 216/234: loss=0.138239 lr=0.000020 grad_norm=0.515130
Epoch 17/100 Iteration 217/234: loss=0.132090 lr=0.000020 grad_norm=0.623672
Epoch 17/100 Iteration 218/234: loss=0.149493 lr=0.000020 grad_norm=0.514507
Epoch 17/100 Iteration 219/234: loss=0.133955 lr=0.000020 grad_norm=0.720201
Epoch 17/100 Iteration 220/234: loss=0.114410 lr=0.000020 grad_norm=0.488235
Epoch 17/100 Iteration 221/234: loss=0.144669 lr=0.000020 grad_norm=0.827712
Epoch 17/100 Iteration 222/234: loss=0.143120 lr=0.000020 grad_norm=0.910496
Epoch 17/100 Iteration 223/234: loss=0.127056 lr=0.000020 grad_norm=0.464131
Epoch 17/100 Iteration 224/234: loss=0.134045 lr=0.000020 grad_norm=0.719211
Epoch 17/100 Iteration 225/234: loss=0.124607 lr=0.000020 grad_norm=0.687455
Epoch 17/100 Iteration 226/234: loss=0.129008 lr=0.000020 grad_norm=0.568740
Epoch 17/100 Iteration 227/234: loss=0.132230 lr=0.000020 grad_norm=0.761996
Epoch 17/100 Iteration 228/234: loss=0.147131 lr=0.000020 grad_norm=0.575093
Epoch 17/100 Iteration 229/234: loss=0.129062 lr=0.000020 grad_norm=0.352475
Epoch 17/100 Iteration 230/234: loss=0.145870 lr=0.000020 grad_norm=0.575620
Epoch 17/100 Iteration 231/234: loss=0.147766 lr=0.000020 grad_norm=0.745994
Epoch 17/100 Iteration 232/234: loss=0.141497 lr=0.000020 grad_norm=0.432121
Epoch 17/100 Iteration 233/234: loss=0.127722 lr=0.000020 grad_norm=0.596121
Epoch 17/100 Iteration 234/234: loss=0.122956 lr=0.000020 grad_norm=0.614963
Epoch 17/100 finished. Avg Loss: 0.139552
Epoch 18/100 Iteration 1/234: loss=0.129533 lr=0.000020 grad_norm=0.645728
Epoch 18/100 Iteration 2/234: loss=0.118361 lr=0.000020 grad_norm=0.565395
Epoch 18/100 Iteration 3/234: loss=0.152604 lr=0.000020 grad_norm=0.546505
Epoch 18/100 Iteration 4/234: loss=0.141883 lr=0.000020 grad_norm=0.784171
Epoch 18/100 Iteration 5/234: loss=0.144195 lr=0.000020 grad_norm=0.443267
Epoch 18/100 Iteration 6/234: loss=0.137051 lr=0.000020 grad_norm=0.703024
Epoch 18/100 Iteration 7/234: loss=0.151974 lr=0.000020 grad_norm=0.929138
Epoch 18/100 Iteration 8/234: loss=0.145634 lr=0.000020 grad_norm=0.729691
Epoch 18/100 Iteration 9/234: loss=0.133334 lr=0.000020 grad_norm=0.335911
Epoch 18/100 Iteration 10/234: loss=0.140926 lr=0.000020 grad_norm=0.746813
Epoch 18/100 Iteration 11/234: loss=0.113740 lr=0.000020 grad_norm=0.603749
Epoch 18/100 Iteration 12/234: loss=0.140150 lr=0.000020 grad_norm=0.386688
Epoch 18/100 Iteration 13/234: loss=0.156147 lr=0.000020 grad_norm=0.626631
Epoch 18/100 Iteration 14/234: loss=0.146035 lr=0.000020 grad_norm=0.606886
Epoch 18/100 Iteration 15/234: loss=0.116144 lr=0.000020 grad_norm=0.431562
Epoch 18/100 Iteration 16/234: loss=0.130463 lr=0.000020 grad_norm=0.507310
Epoch 18/100 Iteration 17/234: loss=0.136023 lr=0.000020 grad_norm=0.624535
Epoch 18/100 Iteration 18/234: loss=0.146551 lr=0.000020 grad_norm=0.480527
Epoch 18/100 Iteration 19/234: loss=0.141383 lr=0.000020 grad_norm=0.445955
Epoch 18/100 Iteration 20/234: loss=0.144848 lr=0.000020 grad_norm=0.451492
Epoch 18/100 Iteration 21/234: loss=0.145667 lr=0.000020 grad_norm=0.696380
Epoch 18/100 Iteration 22/234: loss=0.134443 lr=0.000020 grad_norm=0.550371
Epoch 18/100 Iteration 23/234: loss=0.154698 lr=0.000020 grad_norm=0.707148
Epoch 18/100 Iteration 24/234: loss=0.134593 lr=0.000020 grad_norm=1.107672
Epoch 18/100 Iteration 25/234: loss=0.139328 lr=0.000020 grad_norm=1.079202
Epoch 18/100 Iteration 26/234: loss=0.150616 lr=0.000020 grad_norm=0.820895
Epoch 18/100 Iteration 27/234: loss=0.137707 lr=0.000020 grad_norm=0.669079
Epoch 18/100 Iteration 28/234: loss=0.130154 lr=0.000020 grad_norm=0.587380
Epoch 18/100 Iteration 29/234: loss=0.141576 lr=0.000020 grad_norm=0.482475
Epoch 18/100 Iteration 30/234: loss=0.129644 lr=0.000020 grad_norm=0.589590
Epoch 18/100 Iteration 31/234: loss=0.147761 lr=0.000020 grad_norm=0.661989
Epoch 18/100 Iteration 32/234: loss=0.131861 lr=0.000020 grad_norm=0.798188
Epoch 18/100 Iteration 33/234: loss=0.145726 lr=0.000020 grad_norm=1.244394
Epoch 18/100 Iteration 34/234: loss=0.137002 lr=0.000020 grad_norm=0.921157
Epoch 18/100 Iteration 35/234: loss=0.130076 lr=0.000020 grad_norm=0.406729
Epoch 18/100 Iteration 36/234: loss=0.149535 lr=0.000020 grad_norm=1.218233
Epoch 18/100 Iteration 37/234: loss=0.137103 lr=0.000020 grad_norm=1.792064
Epoch 18/100 Iteration 38/234: loss=0.152743 lr=0.000020 grad_norm=1.024056
Epoch 18/100 Iteration 39/234: loss=0.154414 lr=0.000020 grad_norm=0.566158
Epoch 18/100 Iteration 40/234: loss=0.135307 lr=0.000020 grad_norm=0.968934
Epoch 18/100 Iteration 41/234: loss=0.135345 lr=0.000020 grad_norm=0.701992
Epoch 18/100 Iteration 42/234: loss=0.131823 lr=0.000020 grad_norm=0.520819
Epoch 18/100 Iteration 43/234: loss=0.127332 lr=0.000020 grad_norm=0.583847
Epoch 18/100 Iteration 44/234: loss=0.151373 lr=0.000020 grad_norm=0.616575
Epoch 18/100 Iteration 45/234: loss=0.148920 lr=0.000020 grad_norm=0.676761
Epoch 18/100 Iteration 46/234: loss=0.122091 lr=0.000020 grad_norm=0.580757
Epoch 18/100 Iteration 47/234: loss=0.131486 lr=0.000020 grad_norm=0.577256
Epoch 18/100 Iteration 48/234: loss=0.118034 lr=0.000020 grad_norm=0.779391
Epoch 18/100 Iteration 49/234: loss=0.128332 lr=0.000020 grad_norm=0.933128
Epoch 18/100 Iteration 50/234: loss=0.156183 lr=0.000020 grad_norm=0.584138
Epoch 18/100 Iteration 51/234: loss=0.122989 lr=0.000020 grad_norm=0.668145
Epoch 18/100 Iteration 52/234: loss=0.135671 lr=0.000020 grad_norm=1.162391
Epoch 18/100 Iteration 53/234: loss=0.138567 lr=0.000020 grad_norm=1.027133
Epoch 18/100 Iteration 54/234: loss=0.155872 lr=0.000020 grad_norm=0.474552
Epoch 18/100 Iteration 55/234: loss=0.135369 lr=0.000020 grad_norm=0.733730
Epoch 18/100 Iteration 56/234: loss=0.129660 lr=0.000020 grad_norm=0.723149
Epoch 18/100 Iteration 57/234: loss=0.128398 lr=0.000020 grad_norm=0.639878
Epoch 18/100 Iteration 58/234: loss=0.146518 lr=0.000020 grad_norm=0.680549
Epoch 18/100 Iteration 59/234: loss=0.125353 lr=0.000020 grad_norm=0.651771
Epoch 18/100 Iteration 60/234: loss=0.138839 lr=0.000020 grad_norm=0.501440
Epoch 18/100 Iteration 61/234: loss=0.139176 lr=0.000020 grad_norm=0.492239
Epoch 18/100 Iteration 62/234: loss=0.148023 lr=0.000020 grad_norm=0.648860
Epoch 18/100 Iteration 63/234: loss=0.127578 lr=0.000020 grad_norm=0.969137
Epoch 18/100 Iteration 64/234: loss=0.133914 lr=0.000020 grad_norm=0.998666
Epoch 18/100 Iteration 65/234: loss=0.152773 lr=0.000020 grad_norm=0.570342
Epoch 18/100 Iteration 66/234: loss=0.122585 lr=0.000020 grad_norm=0.283455
Epoch 18/100 Iteration 67/234: loss=0.127540 lr=0.000020 grad_norm=0.671589
Epoch 18/100 Iteration 68/234: loss=0.140189 lr=0.000020 grad_norm=0.749786
Epoch 18/100 Iteration 69/234: loss=0.150753 lr=0.000020 grad_norm=0.351922
Epoch 18/100 Iteration 70/234: loss=0.125903 lr=0.000020 grad_norm=0.462008
Epoch 18/100 Iteration 71/234: loss=0.116905 lr=0.000020 grad_norm=0.559927
Epoch 18/100 Iteration 72/234: loss=0.136802 lr=0.000020 grad_norm=0.503087
Epoch 18/100 Iteration 73/234: loss=0.132410 lr=0.000020 grad_norm=0.284409
Epoch 18/100 Iteration 74/234: loss=0.153987 lr=0.000020 grad_norm=0.773488
Epoch 18/100 Iteration 75/234: loss=0.133532 lr=0.000020 grad_norm=0.900735
Epoch 18/100 Iteration 76/234: loss=0.138074 lr=0.000020 grad_norm=0.475463
Epoch 18/100 Iteration 77/234: loss=0.141434 lr=0.000020 grad_norm=0.730199
Epoch 18/100 Iteration 78/234: loss=0.131438 lr=0.000020 grad_norm=0.993002
Epoch 18/100 Iteration 79/234: loss=0.136838 lr=0.000020 grad_norm=1.069389
Epoch 18/100 Iteration 80/234: loss=0.134914 lr=0.000020 grad_norm=0.651091
Epoch 18/100 Iteration 81/234: loss=0.143310 lr=0.000020 grad_norm=0.424652
Epoch 18/100 Iteration 82/234: loss=0.138201 lr=0.000020 grad_norm=0.437212
Epoch 18/100 Iteration 83/234: loss=0.134458 lr=0.000020 grad_norm=0.437002
Epoch 18/100 Iteration 84/234: loss=0.138160 lr=0.000020 grad_norm=0.744959
Epoch 18/100 Iteration 85/234: loss=0.137000 lr=0.000020 grad_norm=0.434382
Epoch 18/100 Iteration 86/234: loss=0.128638 lr=0.000020 grad_norm=0.489056
Epoch 18/100 Iteration 87/234: loss=0.121245 lr=0.000020 grad_norm=0.483266
Epoch 18/100 Iteration 88/234: loss=0.132958 lr=0.000020 grad_norm=0.398062
Epoch 18/100 Iteration 89/234: loss=0.116714 lr=0.000020 grad_norm=0.390896
Epoch 18/100 Iteration 90/234: loss=0.127482 lr=0.000020 grad_norm=0.497175
Epoch 18/100 Iteration 91/234: loss=0.130920 lr=0.000020 grad_norm=0.333543
Epoch 18/100 Iteration 92/234: loss=0.129439 lr=0.000020 grad_norm=0.396693
Epoch 18/100 Iteration 93/234: loss=0.128083 lr=0.000020 grad_norm=0.348394
Epoch 18/100 Iteration 94/234: loss=0.130566 lr=0.000020 grad_norm=0.556690
Epoch 18/100 Iteration 95/234: loss=0.138053 lr=0.000020 grad_norm=0.817634
Epoch 18/100 Iteration 96/234: loss=0.107738 lr=0.000020 grad_norm=0.977209
Epoch 18/100 Iteration 97/234: loss=0.133698 lr=0.000020 grad_norm=0.811942
Epoch 18/100 Iteration 98/234: loss=0.138917 lr=0.000020 grad_norm=0.498750
Epoch 18/100 Iteration 99/234: loss=0.147763 lr=0.000020 grad_norm=0.739802
Epoch 18/100 Iteration 100/234: loss=0.132625 lr=0.000020 grad_norm=0.798509
Epoch 18/100 Iteration 101/234: loss=0.133201 lr=0.000020 grad_norm=0.840120
Epoch 18/100 Iteration 102/234: loss=0.142388 lr=0.000020 grad_norm=0.780423
Epoch 18/100 Iteration 103/234: loss=0.142437 lr=0.000020 grad_norm=0.933412
Epoch 18/100 Iteration 104/234: loss=0.125065 lr=0.000020 grad_norm=0.826393
Epoch 18/100 Iteration 105/234: loss=0.125435 lr=0.000020 grad_norm=0.435440
Epoch 18/100 Iteration 106/234: loss=0.140056 lr=0.000020 grad_norm=0.625547
Epoch 18/100 Iteration 107/234: loss=0.136055 lr=0.000020 grad_norm=0.836632
Epoch 18/100 Iteration 108/234: loss=0.132697 lr=0.000020 grad_norm=0.582867
Epoch 18/100 Iteration 109/234: loss=0.130344 lr=0.000020 grad_norm=0.427941
Epoch 18/100 Iteration 110/234: loss=0.116612 lr=0.000020 grad_norm=0.427549
Epoch 18/100 Iteration 111/234: loss=0.140955 lr=0.000020 grad_norm=0.742024
Epoch 18/100 Iteration 112/234: loss=0.133778 lr=0.000020 grad_norm=0.871566
Epoch 18/100 Iteration 113/234: loss=0.135269 lr=0.000020 grad_norm=0.536595
Epoch 18/100 Iteration 114/234: loss=0.142447 lr=0.000020 grad_norm=0.416096
Epoch 18/100 Iteration 115/234: loss=0.142714 lr=0.000020 grad_norm=0.849890
Epoch 18/100 Iteration 116/234: loss=0.130572 lr=0.000020 grad_norm=1.021679
Epoch 18/100 Iteration 117/234: loss=0.155378 lr=0.000020 grad_norm=1.010824
Epoch 18/100 Iteration 118/234: loss=0.142704 lr=0.000020 grad_norm=0.722230
Epoch 18/100 Iteration 119/234: loss=0.129486 lr=0.000020 grad_norm=0.527063
Epoch 18/100 Iteration 120/234: loss=0.127301 lr=0.000020 grad_norm=0.497332
Epoch 18/100 Iteration 121/234: loss=0.132140 lr=0.000020 grad_norm=0.510028
Epoch 18/100 Iteration 122/234: loss=0.147999 lr=0.000020 grad_norm=0.526259
Epoch 18/100 Iteration 123/234: loss=0.144575 lr=0.000020 grad_norm=0.424132
Epoch 18/100 Iteration 124/234: loss=0.151436 lr=0.000020 grad_norm=0.572243
Epoch 18/100 Iteration 125/234: loss=0.119540 lr=0.000020 grad_norm=0.473078
Epoch 18/100 Iteration 126/234: loss=0.110052 lr=0.000020 grad_norm=0.513099
Epoch 18/100 Iteration 127/234: loss=0.130220 lr=0.000020 grad_norm=0.870491
Epoch 18/100 Iteration 128/234: loss=0.146446 lr=0.000020 grad_norm=0.870824
Epoch 18/100 Iteration 129/234: loss=0.138283 lr=0.000020 grad_norm=0.713333
Epoch 18/100 Iteration 130/234: loss=0.106341 lr=0.000020 grad_norm=0.388417
Epoch 18/100 Iteration 131/234: loss=0.131558 lr=0.000020 grad_norm=0.918279
Epoch 18/100 Iteration 132/234: loss=0.131431 lr=0.000020 grad_norm=1.041355
Epoch 18/100 Iteration 133/234: loss=0.120510 lr=0.000020 grad_norm=0.716225
Epoch 18/100 Iteration 134/234: loss=0.142835 lr=0.000020 grad_norm=0.388750
Epoch 18/100 Iteration 135/234: loss=0.128876 lr=0.000020 grad_norm=0.875966
Epoch 18/100 Iteration 136/234: loss=0.129904 lr=0.000020 grad_norm=1.008413
Epoch 18/100 Iteration 137/234: loss=0.115034 lr=0.000020 grad_norm=0.988610
Epoch 18/100 Iteration 138/234: loss=0.147802 lr=0.000020 grad_norm=0.990992
Epoch 18/100 Iteration 139/234: loss=0.129801 lr=0.000020 grad_norm=0.693797
Epoch 18/100 Iteration 140/234: loss=0.143699 lr=0.000020 grad_norm=0.973712
Epoch 18/100 Iteration 141/234: loss=0.129863 lr=0.000020 grad_norm=0.956954
Epoch 18/100 Iteration 142/234: loss=0.143340 lr=0.000020 grad_norm=0.721843
Epoch 18/100 Iteration 143/234: loss=0.147316 lr=0.000020 grad_norm=1.375588
Epoch 18/100 Iteration 144/234: loss=0.135310 lr=0.000020 grad_norm=1.312767
Epoch 18/100 Iteration 145/234: loss=0.130609 lr=0.000020 grad_norm=0.744028
Epoch 18/100 Iteration 146/234: loss=0.132675 lr=0.000020 grad_norm=0.435367
Epoch 18/100 Iteration 147/234: loss=0.149375 lr=0.000020 grad_norm=0.900411
Epoch 18/100 Iteration 148/234: loss=0.128628 lr=0.000020 grad_norm=0.600469
Epoch 18/100 Iteration 149/234: loss=0.128000 lr=0.000020 grad_norm=0.625848
Epoch 18/100 Iteration 150/234: loss=0.135552 lr=0.000020 grad_norm=0.968318
Epoch 18/100 Iteration 151/234: loss=0.125025 lr=0.000020 grad_norm=0.834186
Epoch 18/100 Iteration 152/234: loss=0.127566 lr=0.000020 grad_norm=0.596774
Epoch 18/100 Iteration 153/234: loss=0.116704 lr=0.000020 grad_norm=0.748839
Epoch 18/100 Iteration 154/234: loss=0.126421 lr=0.000020 grad_norm=0.625934
Epoch 18/100 Iteration 155/234: loss=0.133337 lr=0.000020 grad_norm=0.779190
Epoch 18/100 Iteration 156/234: loss=0.129044 lr=0.000020 grad_norm=1.068528
Epoch 18/100 Iteration 157/234: loss=0.148796 lr=0.000020 grad_norm=0.443290
Epoch 18/100 Iteration 158/234: loss=0.129815 lr=0.000020 grad_norm=0.916302
Epoch 18/100 Iteration 159/234: loss=0.141230 lr=0.000020 grad_norm=1.208933
Epoch 18/100 Iteration 160/234: loss=0.127601 lr=0.000020 grad_norm=0.836019
Epoch 18/100 Iteration 161/234: loss=0.126451 lr=0.000020 grad_norm=0.308619
Epoch 18/100 Iteration 162/234: loss=0.142825 lr=0.000020 grad_norm=0.696856
Epoch 18/100 Iteration 163/234: loss=0.130803 lr=0.000020 grad_norm=0.839053
Epoch 18/100 Iteration 164/234: loss=0.119323 lr=0.000020 grad_norm=0.477015
Epoch 18/100 Iteration 165/234: loss=0.135467 lr=0.000020 grad_norm=0.484913
Epoch 18/100 Iteration 166/234: loss=0.131791 lr=0.000020 grad_norm=0.972134
Epoch 18/100 Iteration 167/234: loss=0.123874 lr=0.000020 grad_norm=0.879508
Epoch 18/100 Iteration 168/234: loss=0.140748 lr=0.000020 grad_norm=0.440873
Epoch 18/100 Iteration 169/234: loss=0.132290 lr=0.000020 grad_norm=0.515285
Epoch 18/100 Iteration 170/234: loss=0.139666 lr=0.000020 grad_norm=0.778442
Epoch 18/100 Iteration 171/234: loss=0.138800 lr=0.000020 grad_norm=0.483739
Epoch 18/100 Iteration 172/234: loss=0.136221 lr=0.000020 grad_norm=0.688983
Epoch 18/100 Iteration 173/234: loss=0.139167 lr=0.000020 grad_norm=0.887945
Epoch 18/100 Iteration 174/234: loss=0.130957 lr=0.000020 grad_norm=0.453727
Epoch 18/100 Iteration 175/234: loss=0.135402 lr=0.000020 grad_norm=0.499571
Epoch 18/100 Iteration 176/234: loss=0.126360 lr=0.000020 grad_norm=0.755430
Epoch 18/100 Iteration 177/234: loss=0.124030 lr=0.000020 grad_norm=0.581170
Epoch 18/100 Iteration 178/234: loss=0.153003 lr=0.000020 grad_norm=0.563250
Epoch 18/100 Iteration 179/234: loss=0.134035 lr=0.000020 grad_norm=0.636688
Epoch 18/100 Iteration 180/234: loss=0.121015 lr=0.000020 grad_norm=0.533722
Epoch 18/100 Iteration 181/234: loss=0.117506 lr=0.000020 grad_norm=0.447426
Epoch 18/100 Iteration 182/234: loss=0.132496 lr=0.000020 grad_norm=0.416971
Epoch 18/100 Iteration 183/234: loss=0.139597 lr=0.000020 grad_norm=0.442936
Epoch 18/100 Iteration 184/234: loss=0.126473 lr=0.000020 grad_norm=0.568531
Epoch 18/100 Iteration 185/234: loss=0.121016 lr=0.000020 grad_norm=0.604681
Epoch 18/100 Iteration 186/234: loss=0.145284 lr=0.000020 grad_norm=0.480805
Epoch 18/100 Iteration 187/234: loss=0.123827 lr=0.000020 grad_norm=0.415662
Epoch 18/100 Iteration 188/234: loss=0.134329 lr=0.000020 grad_norm=0.549391
Epoch 18/100 Iteration 189/234: loss=0.130634 lr=0.000020 grad_norm=0.681054
Epoch 18/100 Iteration 190/234: loss=0.120761 lr=0.000020 grad_norm=0.473404
Epoch 18/100 Iteration 191/234: loss=0.130361 lr=0.000020 grad_norm=0.414531
Epoch 18/100 Iteration 192/234: loss=0.136443 lr=0.000020 grad_norm=0.773059
Epoch 18/100 Iteration 193/234: loss=0.142025 lr=0.000020 grad_norm=1.097539
Epoch 18/100 Iteration 194/234: loss=0.145521 lr=0.000020 grad_norm=1.186987
Epoch 18/100 Iteration 195/234: loss=0.118180 lr=0.000020 grad_norm=0.834384
Epoch 18/100 Iteration 196/234: loss=0.122616 lr=0.000020 grad_norm=0.619164
Epoch 18/100 Iteration 197/234: loss=0.124040 lr=0.000020 grad_norm=1.473405
Epoch 18/100 Iteration 198/234: loss=0.151219 lr=0.000020 grad_norm=1.472263
Epoch 18/100 Iteration 199/234: loss=0.132820 lr=0.000020 grad_norm=0.961064
Epoch 18/100 Iteration 200/234: loss=0.136682 lr=0.000020 grad_norm=0.326259
Epoch 18/100 Iteration 201/234: loss=0.126539 lr=0.000020 grad_norm=0.870600
Epoch 18/100 Iteration 202/234: loss=0.120271 lr=0.000020 grad_norm=0.683697
Epoch 18/100 Iteration 203/234: loss=0.137937 lr=0.000020 grad_norm=0.518106
Epoch 18/100 Iteration 204/234: loss=0.139192 lr=0.000020 grad_norm=0.592962
Epoch 18/100 Iteration 205/234: loss=0.134327 lr=0.000020 grad_norm=0.483846
Epoch 18/100 Iteration 206/234: loss=0.139016 lr=0.000020 grad_norm=0.488925
Epoch 18/100 Iteration 207/234: loss=0.117305 lr=0.000020 grad_norm=0.450425
Epoch 18/100 Iteration 208/234: loss=0.127397 lr=0.000020 grad_norm=0.510032
Epoch 18/100 Iteration 209/234: loss=0.117730 lr=0.000020 grad_norm=0.347041
Epoch 18/100 Iteration 210/234: loss=0.147499 lr=0.000020 grad_norm=0.565133
Epoch 18/100 Iteration 211/234: loss=0.137845 lr=0.000020 grad_norm=0.626950
Epoch 18/100 Iteration 212/234: loss=0.131658 lr=0.000020 grad_norm=0.460546
Epoch 18/100 Iteration 213/234: loss=0.113580 lr=0.000020 grad_norm=0.412348
Epoch 18/100 Iteration 214/234: loss=0.125289 lr=0.000020 grad_norm=0.522571
Epoch 18/100 Iteration 215/234: loss=0.131089 lr=0.000020 grad_norm=0.641479
Epoch 18/100 Iteration 216/234: loss=0.120586 lr=0.000020 grad_norm=0.793276
Epoch 18/100 Iteration 217/234: loss=0.143522 lr=0.000020 grad_norm=0.830989
Epoch 18/100 Iteration 218/234: loss=0.138311 lr=0.000020 grad_norm=0.767710
Epoch 18/100 Iteration 219/234: loss=0.130654 lr=0.000020 grad_norm=0.597088
Epoch 18/100 Iteration 220/234: loss=0.137411 lr=0.000020 grad_norm=0.921567
Epoch 18/100 Iteration 221/234: loss=0.120613 lr=0.000020 grad_norm=1.216281
Epoch 18/100 Iteration 222/234: loss=0.132492 lr=0.000020 grad_norm=0.742405
Epoch 18/100 Iteration 223/234: loss=0.115095 lr=0.000020 grad_norm=0.353911
Epoch 18/100 Iteration 224/234: loss=0.122460 lr=0.000020 grad_norm=0.926259
Epoch 18/100 Iteration 225/234: loss=0.144741 lr=0.000020 grad_norm=1.094826
Epoch 18/100 Iteration 226/234: loss=0.133834 lr=0.000020 grad_norm=0.829126
Epoch 18/100 Iteration 227/234: loss=0.133095 lr=0.000020 grad_norm=0.417560
Epoch 18/100 Iteration 228/234: loss=0.134308 lr=0.000020 grad_norm=1.085661
Epoch 18/100 Iteration 229/234: loss=0.134539 lr=0.000020 grad_norm=1.030721
Epoch 18/100 Iteration 230/234: loss=0.135850 lr=0.000020 grad_norm=0.317946
Epoch 18/100 Iteration 231/234: loss=0.138965 lr=0.000020 grad_norm=0.792756
Epoch 18/100 Iteration 232/234: loss=0.134535 lr=0.000020 grad_norm=0.704368
Epoch 18/100 Iteration 233/234: loss=0.120794 lr=0.000020 grad_norm=0.442972
Epoch 18/100 Iteration 234/234: loss=0.135191 lr=0.000020 grad_norm=0.587970
Epoch 18/100 finished. Avg Loss: 0.134287
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 19/100 Iteration 1/234: loss=0.126324 lr=0.000020 grad_norm=0.479495
Epoch 19/100 Iteration 2/234: loss=0.121481 lr=0.000020 grad_norm=0.367317
Epoch 19/100 Iteration 3/234: loss=0.131318 lr=0.000020 grad_norm=0.389019
Epoch 19/100 Iteration 4/234: loss=0.138408 lr=0.000020 grad_norm=0.444250
Epoch 19/100 Iteration 5/234: loss=0.131964 lr=0.000020 grad_norm=0.414720
Epoch 19/100 Iteration 6/234: loss=0.130268 lr=0.000020 grad_norm=0.447726
Epoch 19/100 Iteration 7/234: loss=0.135182 lr=0.000020 grad_norm=0.483173
Epoch 19/100 Iteration 8/234: loss=0.124325 lr=0.000020 grad_norm=0.560572
Epoch 19/100 Iteration 9/234: loss=0.129250 lr=0.000020 grad_norm=0.617627
Epoch 19/100 Iteration 10/234: loss=0.143914 lr=0.000020 grad_norm=0.626433
Epoch 19/100 Iteration 11/234: loss=0.149315 lr=0.000020 grad_norm=0.559413
Epoch 19/100 Iteration 12/234: loss=0.141046 lr=0.000020 grad_norm=0.923605
Epoch 19/100 Iteration 13/234: loss=0.115782 lr=0.000020 grad_norm=0.509214
Epoch 19/100 Iteration 14/234: loss=0.143628 lr=0.000020 grad_norm=0.407000
Epoch 19/100 Iteration 15/234: loss=0.130679 lr=0.000020 grad_norm=0.514481
Epoch 19/100 Iteration 16/234: loss=0.139737 lr=0.000020 grad_norm=0.814804
Epoch 19/100 Iteration 17/234: loss=0.134198 lr=0.000020 grad_norm=0.540738
Epoch 19/100 Iteration 18/234: loss=0.131704 lr=0.000020 grad_norm=0.713439
Epoch 19/100 Iteration 19/234: loss=0.117014 lr=0.000020 grad_norm=1.134251
Epoch 19/100 Iteration 20/234: loss=0.151115 lr=0.000020 grad_norm=0.532983
Epoch 19/100 Iteration 21/234: loss=0.139483 lr=0.000020 grad_norm=0.911410
Epoch 19/100 Iteration 22/234: loss=0.129148 lr=0.000020 grad_norm=1.506972
Epoch 19/100 Iteration 23/234: loss=0.149046 lr=0.000020 grad_norm=1.163596
Epoch 19/100 Iteration 24/234: loss=0.134830 lr=0.000020 grad_norm=0.461033
Epoch 19/100 Iteration 25/234: loss=0.133108 lr=0.000020 grad_norm=0.704012
Epoch 19/100 Iteration 26/234: loss=0.139437 lr=0.000020 grad_norm=0.582977
Epoch 19/100 Iteration 27/234: loss=0.129512 lr=0.000020 grad_norm=0.509783
Epoch 19/100 Iteration 28/234: loss=0.136857 lr=0.000020 grad_norm=0.708889
Epoch 19/100 Iteration 29/234: loss=0.120458 lr=0.000020 grad_norm=0.567166
Epoch 19/100 Iteration 30/234: loss=0.133753 lr=0.000020 grad_norm=0.753653
Epoch 19/100 Iteration 31/234: loss=0.136348 lr=0.000020 grad_norm=0.581883
Epoch 19/100 Iteration 32/234: loss=0.128486 lr=0.000020 grad_norm=0.942205
Epoch 19/100 Iteration 33/234: loss=0.123420 lr=0.000020 grad_norm=1.125877
Epoch 19/100 Iteration 34/234: loss=0.139172 lr=0.000020 grad_norm=0.521836
Epoch 19/100 Iteration 35/234: loss=0.133786 lr=0.000020 grad_norm=0.747957
Epoch 19/100 Iteration 36/234: loss=0.119561 lr=0.000020 grad_norm=1.201245
Epoch 19/100 Iteration 37/234: loss=0.123643 lr=0.000020 grad_norm=0.712681
Epoch 19/100 Iteration 38/234: loss=0.121176 lr=0.000020 grad_norm=0.501040
Epoch 19/100 Iteration 39/234: loss=0.135230 lr=0.000020 grad_norm=1.144751
Epoch 19/100 Iteration 40/234: loss=0.127631 lr=0.000020 grad_norm=1.103553
Epoch 19/100 Iteration 41/234: loss=0.126722 lr=0.000020 grad_norm=0.443708
Epoch 19/100 Iteration 42/234: loss=0.131399 lr=0.000020 grad_norm=0.514837
Epoch 19/100 Iteration 43/234: loss=0.113514 lr=0.000020 grad_norm=0.378672
Epoch 19/100 Iteration 44/234: loss=0.148571 lr=0.000020 grad_norm=0.542106
Epoch 19/100 Iteration 45/234: loss=0.122372 lr=0.000020 grad_norm=0.503754
Epoch 19/100 Iteration 46/234: loss=0.146614 lr=0.000020 grad_norm=0.548826
Epoch 19/100 Iteration 47/234: loss=0.137711 lr=0.000020 grad_norm=0.821112
Epoch 19/100 Iteration 48/234: loss=0.128555 lr=0.000020 grad_norm=0.359083
Epoch 19/100 Iteration 49/234: loss=0.129869 lr=0.000020 grad_norm=0.441073
Epoch 19/100 Iteration 50/234: loss=0.112778 lr=0.000020 grad_norm=0.419032
Epoch 19/100 Iteration 51/234: loss=0.139676 lr=0.000020 grad_norm=0.677027
Epoch 19/100 Iteration 52/234: loss=0.138519 lr=0.000020 grad_norm=0.930738
Epoch 19/100 Iteration 53/234: loss=0.120471 lr=0.000020 grad_norm=0.579987
Epoch 19/100 Iteration 54/234: loss=0.151459 lr=0.000020 grad_norm=0.401908
Epoch 19/100 Iteration 55/234: loss=0.129258 lr=0.000020 grad_norm=0.732714
Epoch 19/100 Iteration 56/234: loss=0.148269 lr=0.000020 grad_norm=0.710407
Epoch 19/100 Iteration 57/234: loss=0.132305 lr=0.000020 grad_norm=0.519142
Epoch 19/100 Iteration 58/234: loss=0.138083 lr=0.000020 grad_norm=0.378769
Epoch 19/100 Iteration 59/234: loss=0.136498 lr=0.000020 grad_norm=0.377986
Epoch 19/100 Iteration 60/234: loss=0.141780 lr=0.000020 grad_norm=0.528831
Epoch 19/100 Iteration 61/234: loss=0.121270 lr=0.000020 grad_norm=0.765334
Epoch 19/100 Iteration 62/234: loss=0.154340 lr=0.000020 grad_norm=0.873088
Epoch 19/100 Iteration 63/234: loss=0.140517 lr=0.000020 grad_norm=1.065500
Epoch 19/100 Iteration 64/234: loss=0.101574 lr=0.000020 grad_norm=0.858501
Epoch 19/100 Iteration 65/234: loss=0.128355 lr=0.000020 grad_norm=0.498679
Epoch 19/100 Iteration 66/234: loss=0.133960 lr=0.000020 grad_norm=0.334165
Epoch 19/100 Iteration 67/234: loss=0.146625 lr=0.000020 grad_norm=0.412063
Epoch 19/100 Iteration 68/234: loss=0.133924 lr=0.000020 grad_norm=0.526819
Epoch 19/100 Iteration 69/234: loss=0.141293 lr=0.000020 grad_norm=0.497568
Epoch 19/100 Iteration 70/234: loss=0.155401 lr=0.000020 grad_norm=0.359388
Epoch 19/100 Iteration 71/234: loss=0.138876 lr=0.000020 grad_norm=0.413459
Epoch 19/100 Iteration 72/234: loss=0.130922 lr=0.000020 grad_norm=0.437979
Epoch 19/100 Iteration 73/234: loss=0.126192 lr=0.000020 grad_norm=0.607905
Epoch 19/100 Iteration 74/234: loss=0.130269 lr=0.000020 grad_norm=0.815190
Epoch 19/100 Iteration 75/234: loss=0.127247 lr=0.000020 grad_norm=0.745285
Epoch 19/100 Iteration 76/234: loss=0.122565 lr=0.000020 grad_norm=0.324943
Epoch 19/100 Iteration 77/234: loss=0.115922 lr=0.000020 grad_norm=0.730911
Epoch 19/100 Iteration 78/234: loss=0.126463 lr=0.000020 grad_norm=0.822101
Epoch 19/100 Iteration 79/234: loss=0.146716 lr=0.000020 grad_norm=0.444143
Epoch 19/100 Iteration 80/234: loss=0.125428 lr=0.000020 grad_norm=0.984802
Epoch 19/100 Iteration 81/234: loss=0.115368 lr=0.000020 grad_norm=0.649964
Epoch 19/100 Iteration 82/234: loss=0.124078 lr=0.000020 grad_norm=0.728096
Epoch 19/100 Iteration 83/234: loss=0.141637 lr=0.000020 grad_norm=1.725977
Epoch 19/100 Iteration 84/234: loss=0.116154 lr=0.000020 grad_norm=1.411498
Epoch 19/100 Iteration 85/234: loss=0.126321 lr=0.000020 grad_norm=0.469039
Epoch 19/100 Iteration 86/234: loss=0.140902 lr=0.000020 grad_norm=1.380288
Epoch 19/100 Iteration 87/234: loss=0.127490 lr=0.000020 grad_norm=1.043570
Epoch 19/100 Iteration 88/234: loss=0.130715 lr=0.000020 grad_norm=0.457065
Epoch 19/100 Iteration 89/234: loss=0.141061 lr=0.000020 grad_norm=1.240985
Epoch 19/100 Iteration 90/234: loss=0.127078 lr=0.000020 grad_norm=0.808577
Epoch 19/100 Iteration 91/234: loss=0.120537 lr=0.000020 grad_norm=0.722487
Epoch 19/100 Iteration 92/234: loss=0.134396 lr=0.000020 grad_norm=1.081180
Epoch 19/100 Iteration 93/234: loss=0.128722 lr=0.000020 grad_norm=0.485618
Epoch 19/100 Iteration 94/234: loss=0.108710 lr=0.000020 grad_norm=0.729577
Epoch 19/100 Iteration 95/234: loss=0.129810 lr=0.000020 grad_norm=1.383366
Epoch 19/100 Iteration 96/234: loss=0.139259 lr=0.000020 grad_norm=1.161193
Epoch 19/100 Iteration 97/234: loss=0.124685 lr=0.000020 grad_norm=0.671853
Epoch 19/100 Iteration 98/234: loss=0.126615 lr=0.000020 grad_norm=1.377395
Epoch 19/100 Iteration 99/234: loss=0.137649 lr=0.000020 grad_norm=1.185877
Epoch 19/100 Iteration 100/234: loss=0.124589 lr=0.000020 grad_norm=0.920061
Epoch 19/100 Iteration 101/234: loss=0.117219 lr=0.000020 grad_norm=0.588805
Epoch 19/100 Iteration 102/234: loss=0.132051 lr=0.000020 grad_norm=0.505622
Epoch 19/100 Iteration 103/234: loss=0.136599 lr=0.000020 grad_norm=0.834621
Epoch 19/100 Iteration 104/234: loss=0.105740 lr=0.000020 grad_norm=0.586638
Epoch 19/100 Iteration 105/234: loss=0.131614 lr=0.000020 grad_norm=0.788388
Epoch 19/100 Iteration 106/234: loss=0.139122 lr=0.000020 grad_norm=0.734006
Epoch 19/100 Iteration 107/234: loss=0.115247 lr=0.000020 grad_norm=0.561833
Epoch 19/100 Iteration 108/234: loss=0.129652 lr=0.000020 grad_norm=0.867813
Epoch 19/100 Iteration 109/234: loss=0.137595 lr=0.000020 grad_norm=1.058685
Epoch 19/100 Iteration 110/234: loss=0.144443 lr=0.000020 grad_norm=0.511617
Epoch 19/100 Iteration 111/234: loss=0.125781 lr=0.000020 grad_norm=0.990894
Epoch 19/100 Iteration 112/234: loss=0.134597 lr=0.000020 grad_norm=1.459712
Epoch 19/100 Iteration 113/234: loss=0.137531 lr=0.000020 grad_norm=0.607131
Epoch 19/100 Iteration 114/234: loss=0.133659 lr=0.000020 grad_norm=0.987774
Epoch 19/100 Iteration 115/234: loss=0.129557 lr=0.000020 grad_norm=1.510898
Epoch 19/100 Iteration 116/234: loss=0.114364 lr=0.000020 grad_norm=0.668410
Epoch 19/100 Iteration 117/234: loss=0.135500 lr=0.000020 grad_norm=0.877770
Epoch 19/100 Iteration 118/234: loss=0.124918 lr=0.000020 grad_norm=1.157184
Epoch 19/100 Iteration 119/234: loss=0.107575 lr=0.000020 grad_norm=0.451646
Epoch 19/100 Iteration 120/234: loss=0.132381 lr=0.000020 grad_norm=0.991124
Epoch 19/100 Iteration 121/234: loss=0.134306 lr=0.000020 grad_norm=0.847501
Epoch 19/100 Iteration 122/234: loss=0.125313 lr=0.000020 grad_norm=0.752982
Epoch 19/100 Iteration 123/234: loss=0.128844 lr=0.000020 grad_norm=1.085864
Epoch 19/100 Iteration 124/234: loss=0.128743 lr=0.000020 grad_norm=0.698754
Epoch 19/100 Iteration 125/234: loss=0.136394 lr=0.000020 grad_norm=0.592806
Epoch 19/100 Iteration 126/234: loss=0.127091 lr=0.000020 grad_norm=0.645800
Epoch 19/100 Iteration 127/234: loss=0.129362 lr=0.000020 grad_norm=0.572390
Epoch 19/100 Iteration 128/234: loss=0.128960 lr=0.000020 grad_norm=0.563784
Epoch 19/100 Iteration 129/234: loss=0.127370 lr=0.000020 grad_norm=0.646664
Epoch 19/100 Iteration 130/234: loss=0.138006 lr=0.000020 grad_norm=0.697898
Epoch 19/100 Iteration 131/234: loss=0.123442 lr=0.000020 grad_norm=0.513359
Epoch 19/100 Iteration 132/234: loss=0.126410 lr=0.000020 grad_norm=0.444198
Epoch 19/100 Iteration 133/234: loss=0.143250 lr=0.000020 grad_norm=0.481929
Epoch 19/100 Iteration 134/234: loss=0.125160 lr=0.000020 grad_norm=0.568493
Epoch 19/100 Iteration 135/234: loss=0.131771 lr=0.000020 grad_norm=0.354678
Epoch 19/100 Iteration 136/234: loss=0.123447 lr=0.000020 grad_norm=0.701099
Epoch 19/100 Iteration 137/234: loss=0.130247 lr=0.000020 grad_norm=1.001986
Epoch 19/100 Iteration 138/234: loss=0.126508 lr=0.000020 grad_norm=0.949935
Epoch 19/100 Iteration 139/234: loss=0.115822 lr=0.000020 grad_norm=0.401872
Epoch 19/100 Iteration 140/234: loss=0.135409 lr=0.000020 grad_norm=0.915229
Epoch 19/100 Iteration 141/234: loss=0.128289 lr=0.000020 grad_norm=0.622153
Epoch 19/100 Iteration 142/234: loss=0.118181 lr=0.000020 grad_norm=0.725644
Epoch 19/100 Iteration 143/234: loss=0.141941 lr=0.000020 grad_norm=1.046075
Epoch 19/100 Iteration 144/234: loss=0.130018 lr=0.000020 grad_norm=0.732899
Epoch 19/100 Iteration 145/234: loss=0.128194 lr=0.000020 grad_norm=0.529892
Epoch 19/100 Iteration 146/234: loss=0.124396 lr=0.000020 grad_norm=0.829038
Epoch 19/100 Iteration 147/234: loss=0.126209 lr=0.000020 grad_norm=0.453934
Epoch 19/100 Iteration 148/234: loss=0.125357 lr=0.000020 grad_norm=1.492250
Epoch 19/100 Iteration 149/234: loss=0.144441 lr=0.000020 grad_norm=1.470679
Epoch 19/100 Iteration 150/234: loss=0.137292 lr=0.000020 grad_norm=0.477670
Epoch 19/100 Iteration 151/234: loss=0.141850 lr=0.000020 grad_norm=1.096018
Epoch 19/100 Iteration 152/234: loss=0.127456 lr=0.000020 grad_norm=0.738858
Epoch 19/100 Iteration 153/234: loss=0.139204 lr=0.000020 grad_norm=0.787270
Epoch 19/100 Iteration 154/234: loss=0.122043 lr=0.000020 grad_norm=1.279088
Epoch 19/100 Iteration 155/234: loss=0.137326 lr=0.000020 grad_norm=0.313949
Epoch 19/100 Iteration 156/234: loss=0.123540 lr=0.000020 grad_norm=1.031676
Epoch 19/100 Iteration 157/234: loss=0.135617 lr=0.000020 grad_norm=0.686697
Epoch 19/100 Iteration 158/234: loss=0.140608 lr=0.000020 grad_norm=0.767523
Epoch 19/100 Iteration 159/234: loss=0.129559 lr=0.000020 grad_norm=1.305273
Epoch 19/100 Iteration 160/234: loss=0.135763 lr=0.000020 grad_norm=0.807232
Epoch 19/100 Iteration 161/234: loss=0.126449 lr=0.000020 grad_norm=0.874227
Epoch 19/100 Iteration 162/234: loss=0.122587 lr=0.000020 grad_norm=1.056336
Epoch 19/100 Iteration 163/234: loss=0.136198 lr=0.000020 grad_norm=0.377138
Epoch 19/100 Iteration 164/234: loss=0.141486 lr=0.000020 grad_norm=0.907990
Epoch 19/100 Iteration 165/234: loss=0.118994 lr=0.000020 grad_norm=0.444084
Epoch 19/100 Iteration 166/234: loss=0.115441 lr=0.000020 grad_norm=0.703798
Epoch 19/100 Iteration 167/234: loss=0.125471 lr=0.000020 grad_norm=0.827824
Epoch 19/100 Iteration 168/234: loss=0.135182 lr=0.000020 grad_norm=0.607192
Epoch 19/100 Iteration 169/234: loss=0.125554 lr=0.000020 grad_norm=0.591477
Epoch 19/100 Iteration 170/234: loss=0.140361 lr=0.000020 grad_norm=0.729826
Epoch 19/100 Iteration 171/234: loss=0.135836 lr=0.000020 grad_norm=0.500991
Epoch 19/100 Iteration 172/234: loss=0.128228 lr=0.000020 grad_norm=0.583357
Epoch 19/100 Iteration 173/234: loss=0.145268 lr=0.000020 grad_norm=0.614288
Epoch 19/100 Iteration 174/234: loss=0.108546 lr=0.000020 grad_norm=0.601529
Epoch 19/100 Iteration 175/234: loss=0.129451 lr=0.000020 grad_norm=0.962068
Epoch 19/100 Iteration 176/234: loss=0.148132 lr=0.000020 grad_norm=0.525794
Epoch 19/100 Iteration 177/234: loss=0.134594 lr=0.000020 grad_norm=0.635220
Epoch 19/100 Iteration 178/234: loss=0.116266 lr=0.000020 grad_norm=0.825971
Epoch 19/100 Iteration 179/234: loss=0.132734 lr=0.000020 grad_norm=0.588434
Epoch 19/100 Iteration 180/234: loss=0.133039 lr=0.000020 grad_norm=0.485578
Epoch 19/100 Iteration 181/234: loss=0.115580 lr=0.000020 grad_norm=0.404947
Epoch 19/100 Iteration 182/234: loss=0.118142 lr=0.000020 grad_norm=0.394932
Epoch 19/100 Iteration 183/234: loss=0.120146 lr=0.000020 grad_norm=0.346719
Epoch 19/100 Iteration 184/234: loss=0.133326 lr=0.000020 grad_norm=0.426918
Epoch 19/100 Iteration 185/234: loss=0.131960 lr=0.000020 grad_norm=0.360301
Epoch 19/100 Iteration 186/234: loss=0.125384 lr=0.000020 grad_norm=0.430962
Epoch 19/100 Iteration 187/234: loss=0.131782 lr=0.000020 grad_norm=0.528243
Epoch 19/100 Iteration 188/234: loss=0.131232 lr=0.000020 grad_norm=0.873838
Epoch 19/100 Iteration 189/234: loss=0.128914 lr=0.000020 grad_norm=0.718302
Epoch 19/100 Iteration 190/234: loss=0.119235 lr=0.000020 grad_norm=0.321337
Epoch 19/100 Iteration 191/234: loss=0.122350 lr=0.000020 grad_norm=0.757132
Epoch 19/100 Iteration 192/234: loss=0.147580 lr=0.000020 grad_norm=0.845919
Epoch 19/100 Iteration 193/234: loss=0.129985 lr=0.000020 grad_norm=0.634804
Epoch 19/100 Iteration 194/234: loss=0.130668 lr=0.000020 grad_norm=0.668496
Epoch 19/100 Iteration 195/234: loss=0.119925 lr=0.000020 grad_norm=0.497870
Epoch 19/100 Iteration 196/234: loss=0.123878 lr=0.000020 grad_norm=0.548282
Epoch 19/100 Iteration 197/234: loss=0.117464 lr=0.000020 grad_norm=0.451057
Epoch 19/100 Iteration 198/234: loss=0.128930 lr=0.000020 grad_norm=0.515704
Epoch 19/100 Iteration 199/234: loss=0.119389 lr=0.000020 grad_norm=0.363816
Epoch 19/100 Iteration 200/234: loss=0.128258 lr=0.000020 grad_norm=0.565789
Epoch 19/100 Iteration 201/234: loss=0.146369 lr=0.000020 grad_norm=0.615115
Epoch 19/100 Iteration 202/234: loss=0.127758 lr=0.000020 grad_norm=0.784200
Epoch 19/100 Iteration 203/234: loss=0.118218 lr=0.000020 grad_norm=0.763407
Epoch 19/100 Iteration 204/234: loss=0.131246 lr=0.000020 grad_norm=0.716876
Epoch 19/100 Iteration 205/234: loss=0.122611 lr=0.000020 grad_norm=0.613204
Epoch 19/100 Iteration 206/234: loss=0.134253 lr=0.000020 grad_norm=0.485476
Epoch 19/100 Iteration 207/234: loss=0.125454 lr=0.000020 grad_norm=0.751678
Epoch 19/100 Iteration 208/234: loss=0.136186 lr=0.000020 grad_norm=0.682029
Epoch 19/100 Iteration 209/234: loss=0.136804 lr=0.000020 grad_norm=0.365410
Epoch 19/100 Iteration 210/234: loss=0.146112 lr=0.000020 grad_norm=0.766232
Epoch 19/100 Iteration 211/234: loss=0.129619 lr=0.000020 grad_norm=1.538503
Epoch 19/100 Iteration 212/234: loss=0.128636 lr=0.000020 grad_norm=1.746642
Epoch 19/100 Iteration 213/234: loss=0.138164 lr=0.000020 grad_norm=1.057516
Epoch 19/100 Iteration 214/234: loss=0.118637 lr=0.000020 grad_norm=0.623899
Epoch 19/100 Iteration 215/234: loss=0.127732 lr=0.000020 grad_norm=1.036736
Epoch 19/100 Iteration 216/234: loss=0.125016 lr=0.000020 grad_norm=0.688883
Epoch 19/100 Iteration 217/234: loss=0.136858 lr=0.000020 grad_norm=0.667725
Epoch 19/100 Iteration 218/234: loss=0.108819 lr=0.000020 grad_norm=0.596433
Epoch 19/100 Iteration 219/234: loss=0.120428 lr=0.000020 grad_norm=0.413391
Epoch 19/100 Iteration 220/234: loss=0.125581 lr=0.000020 grad_norm=0.564173
Epoch 19/100 Iteration 221/234: loss=0.127559 lr=0.000020 grad_norm=0.555638
Epoch 19/100 Iteration 222/234: loss=0.119494 lr=0.000020 grad_norm=0.504795
Epoch 19/100 Iteration 223/234: loss=0.137881 lr=0.000020 grad_norm=0.620834
Epoch 19/100 Iteration 224/234: loss=0.119961 lr=0.000020 grad_norm=0.712215
Epoch 19/100 Iteration 225/234: loss=0.109138 lr=0.000020 grad_norm=0.493746
Epoch 19/100 Iteration 226/234: loss=0.118874 lr=0.000020 grad_norm=0.411294
Epoch 19/100 Iteration 227/234: loss=0.125052 lr=0.000020 grad_norm=0.740996
Epoch 19/100 Iteration 228/234: loss=0.134341 lr=0.000020 grad_norm=0.698487
Epoch 19/100 Iteration 229/234: loss=0.122458 lr=0.000020 grad_norm=0.431265
Epoch 19/100 Iteration 230/234: loss=0.142114 lr=0.000020 grad_norm=1.074106
Epoch 19/100 Iteration 231/234: loss=0.136210 lr=0.000020 grad_norm=0.724875
Epoch 19/100 Iteration 232/234: loss=0.121016 lr=0.000020 grad_norm=0.543471
Epoch 19/100 Iteration 233/234: loss=0.129899 lr=0.000020 grad_norm=0.662827
Epoch 19/100 Iteration 234/234: loss=0.120077 lr=0.000020 grad_norm=0.474827
Epoch 19/100 finished. Avg Loss: 0.130126
Epoch 20/100 Iteration 1/234: loss=0.141860 lr=0.000020 grad_norm=0.574113
Epoch 20/100 Iteration 2/234: loss=0.122040 lr=0.000020 grad_norm=0.395468
Epoch 20/100 Iteration 3/234: loss=0.128909 lr=0.000020 grad_norm=0.408147
Epoch 20/100 Iteration 4/234: loss=0.116494 lr=0.000020 grad_norm=0.393991
Epoch 20/100 Iteration 5/234: loss=0.127380 lr=0.000020 grad_norm=0.384998
Epoch 20/100 Iteration 6/234: loss=0.150239 lr=0.000020 grad_norm=0.471595
Epoch 20/100 Iteration 7/234: loss=0.125438 lr=0.000020 grad_norm=0.394244
Epoch 20/100 Iteration 8/234: loss=0.107400 lr=0.000020 grad_norm=0.329819
Epoch 20/100 Iteration 9/234: loss=0.124788 lr=0.000020 grad_norm=0.454408
Epoch 20/100 Iteration 10/234: loss=0.132228 lr=0.000020 grad_norm=0.465028
Epoch 20/100 Iteration 11/234: loss=0.126214 lr=0.000020 grad_norm=0.705800
Epoch 20/100 Iteration 12/234: loss=0.123833 lr=0.000020 grad_norm=0.661040
Epoch 20/100 Iteration 13/234: loss=0.132307 lr=0.000020 grad_norm=0.381927
Epoch 20/100 Iteration 14/234: loss=0.122797 lr=0.000020 grad_norm=0.600568
Epoch 20/100 Iteration 15/234: loss=0.149841 lr=0.000020 grad_norm=0.527995
Epoch 20/100 Iteration 16/234: loss=0.128584 lr=0.000020 grad_norm=0.465728
Epoch 20/100 Iteration 17/234: loss=0.127832 lr=0.000020 grad_norm=0.479187
Epoch 20/100 Iteration 18/234: loss=0.129649 lr=0.000020 grad_norm=0.630486
Epoch 20/100 Iteration 19/234: loss=0.123042 lr=0.000020 grad_norm=0.719683
Epoch 20/100 Iteration 20/234: loss=0.132110 lr=0.000020 grad_norm=0.418244
Epoch 20/100 Iteration 21/234: loss=0.138647 lr=0.000020 grad_norm=0.581000
Epoch 20/100 Iteration 22/234: loss=0.114610 lr=0.000020 grad_norm=1.073172
Epoch 20/100 Iteration 23/234: loss=0.125292 lr=0.000020 grad_norm=1.158915
Epoch 20/100 Iteration 24/234: loss=0.128555 lr=0.000020 grad_norm=0.827498
Epoch 20/100 Iteration 25/234: loss=0.108711 lr=0.000020 grad_norm=0.470860
Epoch 20/100 Iteration 26/234: loss=0.112288 lr=0.000020 grad_norm=0.942435
Epoch 20/100 Iteration 27/234: loss=0.122718 lr=0.000020 grad_norm=1.171841
Epoch 20/100 Iteration 28/234: loss=0.123785 lr=0.000020 grad_norm=0.635196
Epoch 20/100 Iteration 29/234: loss=0.115473 lr=0.000020 grad_norm=0.973171
Epoch 20/100 Iteration 30/234: loss=0.125634 lr=0.000020 grad_norm=1.274236
Epoch 20/100 Iteration 31/234: loss=0.141027 lr=0.000020 grad_norm=1.147193
Epoch 20/100 Iteration 32/234: loss=0.137228 lr=0.000020 grad_norm=1.965292
Epoch 20/100 Iteration 33/234: loss=0.121302 lr=0.000020 grad_norm=2.372803
Epoch 20/100 Iteration 34/234: loss=0.139700 lr=0.000020 grad_norm=1.288819
Epoch 20/100 Iteration 35/234: loss=0.139514 lr=0.000020 grad_norm=0.919105
Epoch 20/100 Iteration 36/234: loss=0.113258 lr=0.000020 grad_norm=1.546207
Epoch 20/100 Iteration 37/234: loss=0.124452 lr=0.000020 grad_norm=1.189603
Epoch 20/100 Iteration 38/234: loss=0.124778 lr=0.000020 grad_norm=1.173864
Epoch 20/100 Iteration 39/234: loss=0.134758 lr=0.000020 grad_norm=0.901266
Epoch 20/100 Iteration 40/234: loss=0.134956 lr=0.000020 grad_norm=0.615072
Epoch 20/100 Iteration 41/234: loss=0.118308 lr=0.000020 grad_norm=1.049257
Epoch 20/100 Iteration 42/234: loss=0.119854 lr=0.000020 grad_norm=1.040972
Epoch 20/100 Iteration 43/234: loss=0.133481 lr=0.000020 grad_norm=0.819306
Epoch 20/100 Iteration 44/234: loss=0.137470 lr=0.000020 grad_norm=0.866704
Epoch 20/100 Iteration 45/234: loss=0.121414 lr=0.000020 grad_norm=0.826210
Epoch 20/100 Iteration 46/234: loss=0.131803 lr=0.000020 grad_norm=0.666443
Epoch 20/100 Iteration 47/234: loss=0.115845 lr=0.000020 grad_norm=0.514095
Epoch 20/100 Iteration 48/234: loss=0.112140 lr=0.000020 grad_norm=0.589498
Epoch 20/100 Iteration 49/234: loss=0.131259 lr=0.000020 grad_norm=0.537228
Epoch 20/100 Iteration 50/234: loss=0.136323 lr=0.000020 grad_norm=0.880867
Epoch 20/100 Iteration 51/234: loss=0.137251 lr=0.000020 grad_norm=0.615719
Epoch 20/100 Iteration 52/234: loss=0.116755 lr=0.000020 grad_norm=0.377603
Epoch 20/100 Iteration 53/234: loss=0.137691 lr=0.000020 grad_norm=0.441149
Epoch 20/100 Iteration 54/234: loss=0.137691 lr=0.000020 grad_norm=0.361659
Epoch 20/100 Iteration 55/234: loss=0.127687 lr=0.000020 grad_norm=0.655338
Epoch 20/100 Iteration 56/234: loss=0.126305 lr=0.000020 grad_norm=0.898332
Epoch 20/100 Iteration 57/234: loss=0.126976 lr=0.000020 grad_norm=0.370440
Epoch 20/100 Iteration 58/234: loss=0.144979 lr=0.000020 grad_norm=0.778686
Epoch 20/100 Iteration 59/234: loss=0.134885 lr=0.000020 grad_norm=1.008402
Epoch 20/100 Iteration 60/234: loss=0.121983 lr=0.000020 grad_norm=0.521271
Epoch 20/100 Iteration 61/234: loss=0.119092 lr=0.000020 grad_norm=0.839799
Epoch 20/100 Iteration 62/234: loss=0.149351 lr=0.000020 grad_norm=1.548228
Epoch 20/100 Iteration 63/234: loss=0.121662 lr=0.000020 grad_norm=1.142227
Epoch 20/100 Iteration 64/234: loss=0.119734 lr=0.000020 grad_norm=0.366249
Epoch 20/100 Iteration 65/234: loss=0.138402 lr=0.000020 grad_norm=0.845021
Epoch 20/100 Iteration 66/234: loss=0.116861 lr=0.000020 grad_norm=0.502947
Epoch 20/100 Iteration 67/234: loss=0.123165 lr=0.000020 grad_norm=0.991323
Epoch 20/100 Iteration 68/234: loss=0.129137 lr=0.000020 grad_norm=1.226401
Epoch 20/100 Iteration 69/234: loss=0.127855 lr=0.000020 grad_norm=0.564597
Epoch 20/100 Iteration 70/234: loss=0.138017 lr=0.000020 grad_norm=1.011820
Epoch 20/100 Iteration 71/234: loss=0.112838 lr=0.000020 grad_norm=0.603337
Epoch 20/100 Iteration 72/234: loss=0.119099 lr=0.000020 grad_norm=0.817519
Epoch 20/100 Iteration 73/234: loss=0.135361 lr=0.000020 grad_norm=1.183614
Epoch 20/100 Iteration 74/234: loss=0.129995 lr=0.000020 grad_norm=0.531955
Epoch 20/100 Iteration 75/234: loss=0.135964 lr=0.000020 grad_norm=0.583082
Epoch 20/100 Iteration 76/234: loss=0.124021 lr=0.000020 grad_norm=0.600584
Epoch 20/100 Iteration 77/234: loss=0.127648 lr=0.000020 grad_norm=0.270280
Epoch 20/100 Iteration 78/234: loss=0.128141 lr=0.000020 grad_norm=0.458331
Epoch 20/100 Iteration 79/234: loss=0.125616 lr=0.000020 grad_norm=0.474958
Epoch 20/100 Iteration 80/234: loss=0.120211 lr=0.000020 grad_norm=0.589688
Epoch 20/100 Iteration 81/234: loss=0.122976 lr=0.000020 grad_norm=0.814811
Epoch 20/100 Iteration 82/234: loss=0.117595 lr=0.000020 grad_norm=0.504434
Epoch 20/100 Iteration 83/234: loss=0.125762 lr=0.000020 grad_norm=0.366812
Epoch 20/100 Iteration 84/234: loss=0.138758 lr=0.000020 grad_norm=0.347847
Epoch 20/100 Iteration 85/234: loss=0.131513 lr=0.000020 grad_norm=0.391470
Epoch 20/100 Iteration 86/234: loss=0.120736 lr=0.000020 grad_norm=0.487545
Epoch 20/100 Iteration 87/234: loss=0.114891 lr=0.000020 grad_norm=0.839446
Epoch 20/100 Iteration 88/234: loss=0.134712 lr=0.000020 grad_norm=0.597645
Epoch 20/100 Iteration 89/234: loss=0.117251 lr=0.000020 grad_norm=0.400342
Epoch 20/100 Iteration 90/234: loss=0.117206 lr=0.000020 grad_norm=0.519914
Epoch 20/100 Iteration 91/234: loss=0.111951 lr=0.000020 grad_norm=0.431332
Epoch 20/100 Iteration 92/234: loss=0.118959 lr=0.000020 grad_norm=0.338781
Epoch 20/100 Iteration 93/234: loss=0.131140 lr=0.000020 grad_norm=0.480218
Epoch 20/100 Iteration 94/234: loss=0.128679 lr=0.000020 grad_norm=0.866067
Epoch 20/100 Iteration 95/234: loss=0.132003 lr=0.000020 grad_norm=0.958376
Epoch 20/100 Iteration 96/234: loss=0.120748 lr=0.000020 grad_norm=0.421125
Epoch 20/100 Iteration 97/234: loss=0.142040 lr=0.000020 grad_norm=0.890870
Epoch 20/100 Iteration 98/234: loss=0.126884 lr=0.000020 grad_norm=1.340045
Epoch 20/100 Iteration 99/234: loss=0.136116 lr=0.000020 grad_norm=0.786651
Epoch 20/100 Iteration 100/234: loss=0.126324 lr=0.000020 grad_norm=0.589324
Epoch 20/100 Iteration 101/234: loss=0.119915 lr=0.000020 grad_norm=0.908008
Epoch 20/100 Iteration 102/234: loss=0.124722 lr=0.000020 grad_norm=0.978253
Epoch 20/100 Iteration 103/234: loss=0.122402 lr=0.000020 grad_norm=0.403934
Epoch 20/100 Iteration 104/234: loss=0.126707 lr=0.000020 grad_norm=0.945749
Epoch 20/100 Iteration 105/234: loss=0.120547 lr=0.000020 grad_norm=1.035863
Epoch 20/100 Iteration 106/234: loss=0.137130 lr=0.000020 grad_norm=0.516973
Epoch 20/100 Iteration 107/234: loss=0.133179 lr=0.000020 grad_norm=0.695339
Epoch 20/100 Iteration 108/234: loss=0.108384 lr=0.000020 grad_norm=0.832960
Epoch 20/100 Iteration 109/234: loss=0.125638 lr=0.000020 grad_norm=0.801897
Epoch 20/100 Iteration 110/234: loss=0.137009 lr=0.000020 grad_norm=0.904043
Epoch 20/100 Iteration 111/234: loss=0.125391 lr=0.000020 grad_norm=0.622884
Epoch 20/100 Iteration 112/234: loss=0.125557 lr=0.000020 grad_norm=0.812764
Epoch 20/100 Iteration 113/234: loss=0.126350 lr=0.000020 grad_norm=0.585619
Epoch 20/100 Iteration 114/234: loss=0.112737 lr=0.000020 grad_norm=0.752714
Epoch 20/100 Iteration 115/234: loss=0.114521 lr=0.000020 grad_norm=0.568064
Epoch 20/100 Iteration 116/234: loss=0.129108 lr=0.000020 grad_norm=0.625651
Epoch 20/100 Iteration 117/234: loss=0.137728 lr=0.000020 grad_norm=0.908100
Epoch 20/100 Iteration 118/234: loss=0.126260 lr=0.000020 grad_norm=0.788874
Epoch 20/100 Iteration 119/234: loss=0.130736 lr=0.000020 grad_norm=0.434537
Epoch 20/100 Iteration 120/234: loss=0.141716 lr=0.000020 grad_norm=1.251629
Epoch 20/100 Iteration 121/234: loss=0.127369 lr=0.000020 grad_norm=1.290467
Epoch 20/100 Iteration 122/234: loss=0.133124 lr=0.000020 grad_norm=0.501739
Epoch 20/100 Iteration 123/234: loss=0.137213 lr=0.000020 grad_norm=0.912074
Epoch 20/100 Iteration 124/234: loss=0.110676 lr=0.000020 grad_norm=0.933788
Epoch 20/100 Iteration 125/234: loss=0.135692 lr=0.000020 grad_norm=0.419475
Epoch 20/100 Iteration 126/234: loss=0.129936 lr=0.000020 grad_norm=0.823699
Epoch 20/100 Iteration 127/234: loss=0.126859 lr=0.000020 grad_norm=0.709714
Epoch 20/100 Iteration 128/234: loss=0.125699 lr=0.000020 grad_norm=0.451363
Epoch 20/100 Iteration 129/234: loss=0.124743 lr=0.000020 grad_norm=0.853478
Epoch 20/100 Iteration 130/234: loss=0.122669 lr=0.000020 grad_norm=0.969118
Epoch 20/100 Iteration 131/234: loss=0.124526 lr=0.000020 grad_norm=0.358066
Epoch 20/100 Iteration 132/234: loss=0.118516 lr=0.000020 grad_norm=0.799521
Epoch 20/100 Iteration 133/234: loss=0.120853 lr=0.000020 grad_norm=1.185578
Epoch 20/100 Iteration 134/234: loss=0.130313 lr=0.000020 grad_norm=1.036002
Epoch 20/100 Iteration 135/234: loss=0.112292 lr=0.000020 grad_norm=0.360637
Epoch 20/100 Iteration 136/234: loss=0.127558 lr=0.000020 grad_norm=1.029943
Epoch 20/100 Iteration 137/234: loss=0.123106 lr=0.000020 grad_norm=0.975765
Epoch 20/100 Iteration 138/234: loss=0.133933 lr=0.000020 grad_norm=0.377785
Epoch 20/100 Iteration 139/234: loss=0.126023 lr=0.000020 grad_norm=0.945607
Epoch 20/100 Iteration 140/234: loss=0.117404 lr=0.000020 grad_norm=0.745322
Epoch 20/100 Iteration 141/234: loss=0.127636 lr=0.000020 grad_norm=0.405244
Epoch 20/100 Iteration 142/234: loss=0.129671 lr=0.000020 grad_norm=0.706320
Epoch 20/100 Iteration 143/234: loss=0.137927 lr=0.000020 grad_norm=0.697164
Epoch 20/100 Iteration 144/234: loss=0.124853 lr=0.000020 grad_norm=0.340374
Epoch 20/100 Iteration 145/234: loss=0.145393 lr=0.000020 grad_norm=0.766498
Epoch 20/100 Iteration 146/234: loss=0.121074 lr=0.000020 grad_norm=1.112417
Epoch 20/100 Iteration 147/234: loss=0.131128 lr=0.000020 grad_norm=0.727319
Epoch 20/100 Iteration 148/234: loss=0.122389 lr=0.000020 grad_norm=0.534250
Epoch 20/100 Iteration 149/234: loss=0.118865 lr=0.000020 grad_norm=0.435077
Epoch 20/100 Iteration 150/234: loss=0.121264 lr=0.000020 grad_norm=0.390607
Epoch 20/100 Iteration 151/234: loss=0.136444 lr=0.000020 grad_norm=0.534777
Epoch 20/100 Iteration 152/234: loss=0.131296 lr=0.000020 grad_norm=0.440412
Epoch 20/100 Iteration 153/234: loss=0.148456 lr=0.000020 grad_norm=0.352294
Epoch 20/100 Iteration 154/234: loss=0.128195 lr=0.000020 grad_norm=0.421782
Epoch 20/100 Iteration 155/234: loss=0.115128 lr=0.000020 grad_norm=0.423525
Epoch 20/100 Iteration 156/234: loss=0.140708 lr=0.000020 grad_norm=0.317642
Epoch 20/100 Iteration 157/234: loss=0.123526 lr=0.000020 grad_norm=0.391708
Epoch 20/100 Iteration 158/234: loss=0.125592 lr=0.000020 grad_norm=0.565928
Epoch 20/100 Iteration 159/234: loss=0.120636 lr=0.000020 grad_norm=0.626397
Epoch 20/100 Iteration 160/234: loss=0.125948 lr=0.000020 grad_norm=0.534376
Epoch 20/100 Iteration 161/234: loss=0.121581 lr=0.000020 grad_norm=0.499105
Epoch 20/100 Iteration 162/234: loss=0.128342 lr=0.000020 grad_norm=0.310770
Epoch 20/100 Iteration 163/234: loss=0.114423 lr=0.000020 grad_norm=0.570709
Epoch 20/100 Iteration 164/234: loss=0.130963 lr=0.000020 grad_norm=1.041197
Epoch 20/100 Iteration 165/234: loss=0.131727 lr=0.000020 grad_norm=1.061551
Epoch 20/100 Iteration 166/234: loss=0.122078 lr=0.000020 grad_norm=0.464746
Epoch 20/100 Iteration 167/234: loss=0.127137 lr=0.000020 grad_norm=0.620778
Epoch 20/100 Iteration 168/234: loss=0.118284 lr=0.000020 grad_norm=0.675357
Epoch 20/100 Iteration 169/234: loss=0.121772 lr=0.000020 grad_norm=0.301859
Epoch 20/100 Iteration 170/234: loss=0.127908 lr=0.000020 grad_norm=0.745195
Epoch 20/100 Iteration 171/234: loss=0.117746 lr=0.000020 grad_norm=0.924339
Epoch 20/100 Iteration 172/234: loss=0.129117 lr=0.000020 grad_norm=0.651480
Epoch 20/100 Iteration 173/234: loss=0.116568 lr=0.000020 grad_norm=0.320566
Epoch 20/100 Iteration 174/234: loss=0.128552 lr=0.000020 grad_norm=0.808840
Epoch 20/100 Iteration 175/234: loss=0.130178 lr=0.000020 grad_norm=0.701149
Epoch 20/100 Iteration 176/234: loss=0.126650 lr=0.000020 grad_norm=0.511837
Epoch 20/100 Iteration 177/234: loss=0.125204 lr=0.000020 grad_norm=1.394390
Epoch 20/100 Iteration 178/234: loss=0.138115 lr=0.000020 grad_norm=1.792046
Epoch 20/100 Iteration 179/234: loss=0.137940 lr=0.000020 grad_norm=1.234923
Epoch 20/100 Iteration 180/234: loss=0.123368 lr=0.000020 grad_norm=0.404543
Epoch 20/100 Iteration 181/234: loss=0.120945 lr=0.000020 grad_norm=1.117126
Epoch 20/100 Iteration 182/234: loss=0.131759 lr=0.000020 grad_norm=0.683527
Epoch 20/100 Iteration 183/234: loss=0.142295 lr=0.000020 grad_norm=0.516797
Epoch 20/100 Iteration 184/234: loss=0.103678 lr=0.000020 grad_norm=0.743770
Epoch 20/100 Iteration 185/234: loss=0.120999 lr=0.000020 grad_norm=0.696293
Epoch 20/100 Iteration 186/234: loss=0.136882 lr=0.000020 grad_norm=0.639015
Epoch 20/100 Iteration 187/234: loss=0.120612 lr=0.000020 grad_norm=0.452297
Epoch 20/100 Iteration 188/234: loss=0.128056 lr=0.000020 grad_norm=0.340087
Epoch 20/100 Iteration 189/234: loss=0.122445 lr=0.000020 grad_norm=0.617396
Epoch 20/100 Iteration 190/234: loss=0.129662 lr=0.000020 grad_norm=0.669269
Epoch 20/100 Iteration 191/234: loss=0.121981 lr=0.000020 grad_norm=0.444996
Epoch 20/100 Iteration 192/234: loss=0.112133 lr=0.000020 grad_norm=0.446943
Epoch 20/100 Iteration 193/234: loss=0.140558 lr=0.000020 grad_norm=0.531809
Epoch 20/100 Iteration 194/234: loss=0.129066 lr=0.000020 grad_norm=0.475410
Epoch 20/100 Iteration 195/234: loss=0.122391 lr=0.000020 grad_norm=0.539332
Epoch 20/100 Iteration 196/234: loss=0.121810 lr=0.000020 grad_norm=0.705727
Epoch 20/100 Iteration 197/234: loss=0.129254 lr=0.000020 grad_norm=0.403462
Epoch 20/100 Iteration 198/234: loss=0.111818 lr=0.000020 grad_norm=0.747696
Epoch 20/100 Iteration 199/234: loss=0.123947 lr=0.000020 grad_norm=0.742505
Epoch 20/100 Iteration 200/234: loss=0.121913 lr=0.000020 grad_norm=0.664508
Epoch 20/100 Iteration 201/234: loss=0.116505 lr=0.000020 grad_norm=0.395581
Epoch 20/100 Iteration 202/234: loss=0.109482 lr=0.000020 grad_norm=0.436154
Epoch 20/100 Iteration 203/234: loss=0.129532 lr=0.000020 grad_norm=0.364083
Epoch 20/100 Iteration 204/234: loss=0.127060 lr=0.000020 grad_norm=0.526583
Epoch 20/100 Iteration 205/234: loss=0.128818 lr=0.000020 grad_norm=0.411626
Epoch 20/100 Iteration 206/234: loss=0.126525 lr=0.000020 grad_norm=0.499623
Epoch 20/100 Iteration 207/234: loss=0.119149 lr=0.000020 grad_norm=0.511646
Epoch 20/100 Iteration 208/234: loss=0.120057 lr=0.000020 grad_norm=0.397621
Epoch 20/100 Iteration 209/234: loss=0.108540 lr=0.000020 grad_norm=0.439288
Epoch 20/100 Iteration 210/234: loss=0.118035 lr=0.000020 grad_norm=0.298374
Epoch 20/100 Iteration 211/234: loss=0.135780 lr=0.000020 grad_norm=0.729142
Epoch 20/100 Iteration 212/234: loss=0.102443 lr=0.000020 grad_norm=0.652096
Epoch 20/100 Iteration 213/234: loss=0.110936 lr=0.000020 grad_norm=0.360856
Epoch 20/100 Iteration 214/234: loss=0.117218 lr=0.000020 grad_norm=0.425967
Epoch 20/100 Iteration 215/234: loss=0.144521 lr=0.000020 grad_norm=0.328937
Epoch 20/100 Iteration 216/234: loss=0.122626 lr=0.000020 grad_norm=0.454864
Epoch 20/100 Iteration 217/234: loss=0.127430 lr=0.000020 grad_norm=0.669519
Epoch 20/100 Iteration 218/234: loss=0.124219 lr=0.000020 grad_norm=0.583795
Epoch 20/100 Iteration 219/234: loss=0.123989 lr=0.000020 grad_norm=0.462183
Epoch 20/100 Iteration 220/234: loss=0.129104 lr=0.000020 grad_norm=0.822726
Epoch 20/100 Iteration 221/234: loss=0.117921 lr=0.000020 grad_norm=1.053993
Epoch 20/100 Iteration 222/234: loss=0.127416 lr=0.000020 grad_norm=0.654059
Epoch 20/100 Iteration 223/234: loss=0.119822 lr=0.000020 grad_norm=0.287564
Epoch 20/100 Iteration 224/234: loss=0.119417 lr=0.000020 grad_norm=0.568672
Epoch 20/100 Iteration 225/234: loss=0.122789 lr=0.000020 grad_norm=0.561247
Epoch 20/100 Iteration 226/234: loss=0.113506 lr=0.000020 grad_norm=0.551735
Epoch 20/100 Iteration 227/234: loss=0.119446 lr=0.000020 grad_norm=0.757927
Epoch 20/100 Iteration 228/234: loss=0.140365 lr=0.000020 grad_norm=0.575014
Epoch 20/100 Iteration 229/234: loss=0.115874 lr=0.000020 grad_norm=1.084711
Epoch 20/100 Iteration 230/234: loss=0.112652 lr=0.000020 grad_norm=1.263459
Epoch 20/100 Iteration 231/234: loss=0.101289 lr=0.000020 grad_norm=0.685811
Epoch 20/100 Iteration 232/234: loss=0.133632 lr=0.000020 grad_norm=0.599562
Epoch 20/100 Iteration 233/234: loss=0.127768 lr=0.000020 grad_norm=1.441310
Epoch 20/100 Iteration 234/234: loss=0.137669 lr=0.000020 grad_norm=1.787656
Epoch 20/100 finished. Avg Loss: 0.126068
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 21/100 Iteration 1/234: loss=0.118960 lr=0.000020 grad_norm=0.624327
Epoch 21/100 Iteration 2/234: loss=0.131933 lr=0.000020 grad_norm=1.760117
Epoch 21/100 Iteration 3/234: loss=0.134823 lr=0.000020 grad_norm=1.223304
Epoch 21/100 Iteration 4/234: loss=0.132628 lr=0.000020 grad_norm=0.959550
Epoch 21/100 Iteration 5/234: loss=0.122968 lr=0.000020 grad_norm=1.701098
Epoch 21/100 Iteration 6/234: loss=0.124660 lr=0.000020 grad_norm=0.560076
Epoch 21/100 Iteration 7/234: loss=0.125004 lr=0.000020 grad_norm=1.145051
Epoch 21/100 Iteration 8/234: loss=0.110965 lr=0.000020 grad_norm=0.954308
Epoch 21/100 Iteration 9/234: loss=0.125434 lr=0.000020 grad_norm=0.568446
Epoch 21/100 Iteration 10/234: loss=0.124538 lr=0.000020 grad_norm=0.794697
Epoch 21/100 Iteration 11/234: loss=0.118416 lr=0.000020 grad_norm=0.796855
Epoch 21/100 Iteration 12/234: loss=0.114377 lr=0.000020 grad_norm=0.445836
Epoch 21/100 Iteration 13/234: loss=0.115920 lr=0.000020 grad_norm=0.583704
Epoch 21/100 Iteration 14/234: loss=0.108002 lr=0.000020 grad_norm=0.396253
Epoch 21/100 Iteration 15/234: loss=0.137358 lr=0.000020 grad_norm=0.578494
Epoch 21/100 Iteration 16/234: loss=0.116802 lr=0.000020 grad_norm=0.761216
Epoch 21/100 Iteration 17/234: loss=0.115510 lr=0.000020 grad_norm=0.546692
Epoch 21/100 Iteration 18/234: loss=0.131991 lr=0.000020 grad_norm=0.420869
Epoch 21/100 Iteration 19/234: loss=0.141220 lr=0.000020 grad_norm=0.809666
Epoch 21/100 Iteration 20/234: loss=0.127510 lr=0.000020 grad_norm=0.631541
Epoch 21/100 Iteration 21/234: loss=0.139053 lr=0.000020 grad_norm=0.439913
Epoch 21/100 Iteration 22/234: loss=0.140224 lr=0.000020 grad_norm=0.558542
Epoch 21/100 Iteration 23/234: loss=0.116875 lr=0.000020 grad_norm=0.378575
Epoch 21/100 Iteration 24/234: loss=0.116938 lr=0.000020 grad_norm=0.471057
Epoch 21/100 Iteration 25/234: loss=0.117003 lr=0.000020 grad_norm=0.391029
Epoch 21/100 Iteration 26/234: loss=0.122908 lr=0.000020 grad_norm=0.399418
Epoch 21/100 Iteration 27/234: loss=0.115175 lr=0.000020 grad_norm=0.578404
Epoch 21/100 Iteration 28/234: loss=0.142454 lr=0.000020 grad_norm=0.964126
Epoch 21/100 Iteration 29/234: loss=0.129211 lr=0.000020 grad_norm=0.567473
Epoch 21/100 Iteration 30/234: loss=0.110565 lr=0.000020 grad_norm=0.514510
Epoch 21/100 Iteration 31/234: loss=0.128906 lr=0.000020 grad_norm=0.805974
Epoch 21/100 Iteration 32/234: loss=0.124112 lr=0.000020 grad_norm=0.368318
Epoch 21/100 Iteration 33/234: loss=0.128745 lr=0.000020 grad_norm=0.573860
Epoch 21/100 Iteration 34/234: loss=0.123962 lr=0.000020 grad_norm=0.737164
Epoch 21/100 Iteration 35/234: loss=0.110743 lr=0.000020 grad_norm=0.823212
Epoch 21/100 Iteration 36/234: loss=0.116879 lr=0.000020 grad_norm=0.583071
Epoch 21/100 Iteration 37/234: loss=0.121178 lr=0.000020 grad_norm=0.391875
Epoch 21/100 Iteration 38/234: loss=0.119294 lr=0.000020 grad_norm=0.651683
Epoch 21/100 Iteration 39/234: loss=0.141585 lr=0.000020 grad_norm=0.494555
Epoch 21/100 Iteration 40/234: loss=0.111992 lr=0.000020 grad_norm=0.517267
Epoch 21/100 Iteration 41/234: loss=0.118298 lr=0.000020 grad_norm=0.534913
Epoch 21/100 Iteration 42/234: loss=0.115077 lr=0.000020 grad_norm=0.380780
Epoch 21/100 Iteration 43/234: loss=0.123773 lr=0.000020 grad_norm=0.418408
Epoch 21/100 Iteration 44/234: loss=0.127544 lr=0.000020 grad_norm=0.458963
Epoch 21/100 Iteration 45/234: loss=0.125184 lr=0.000020 grad_norm=0.463299
Epoch 21/100 Iteration 46/234: loss=0.121803 lr=0.000020 grad_norm=0.406312
Epoch 21/100 Iteration 47/234: loss=0.114146 lr=0.000020 grad_norm=0.391039
Epoch 21/100 Iteration 48/234: loss=0.115636 lr=0.000020 grad_norm=0.428955
Epoch 21/100 Iteration 49/234: loss=0.113186 lr=0.000020 grad_norm=0.425929
Epoch 21/100 Iteration 50/234: loss=0.118048 lr=0.000020 grad_norm=0.822509
Epoch 21/100 Iteration 51/234: loss=0.123647 lr=0.000020 grad_norm=0.859054
Epoch 21/100 Iteration 52/234: loss=0.136934 lr=0.000020 grad_norm=0.469080
Epoch 21/100 Iteration 53/234: loss=0.115120 lr=0.000020 grad_norm=0.372407
Epoch 21/100 Iteration 54/234: loss=0.133407 lr=0.000020 grad_norm=0.412967
Epoch 21/100 Iteration 55/234: loss=0.112462 lr=0.000020 grad_norm=0.352760
Epoch 21/100 Iteration 56/234: loss=0.111423 lr=0.000020 grad_norm=0.379427
Epoch 21/100 Iteration 57/234: loss=0.123682 lr=0.000020 grad_norm=0.479392
Epoch 21/100 Iteration 58/234: loss=0.124970 lr=0.000020 grad_norm=0.367184
Epoch 21/100 Iteration 59/234: loss=0.110401 lr=0.000020 grad_norm=0.575342
Epoch 21/100 Iteration 60/234: loss=0.124071 lr=0.000020 grad_norm=0.400125
Epoch 21/100 Iteration 61/234: loss=0.103503 lr=0.000020 grad_norm=0.588039
Epoch 21/100 Iteration 62/234: loss=0.116460 lr=0.000020 grad_norm=1.162844
Epoch 21/100 Iteration 63/234: loss=0.120532 lr=0.000020 grad_norm=1.256929
Epoch 21/100 Iteration 64/234: loss=0.123349 lr=0.000020 grad_norm=0.556237
Epoch 21/100 Iteration 65/234: loss=0.120329 lr=0.000020 grad_norm=0.947644
Epoch 21/100 Iteration 66/234: loss=0.115193 lr=0.000020 grad_norm=0.928168
Epoch 21/100 Iteration 67/234: loss=0.126702 lr=0.000020 grad_norm=0.408941
Epoch 21/100 Iteration 68/234: loss=0.139585 lr=0.000020 grad_norm=1.159505
Epoch 21/100 Iteration 69/234: loss=0.138210 lr=0.000020 grad_norm=1.220311
Epoch 21/100 Iteration 70/234: loss=0.124892 lr=0.000020 grad_norm=0.538519
Epoch 21/100 Iteration 71/234: loss=0.130464 lr=0.000020 grad_norm=0.739084
Epoch 21/100 Iteration 72/234: loss=0.126308 lr=0.000020 grad_norm=0.549199
Epoch 21/100 Iteration 73/234: loss=0.122567 lr=0.000020 grad_norm=0.443105
Epoch 21/100 Iteration 74/234: loss=0.131578 lr=0.000020 grad_norm=1.042894
Epoch 21/100 Iteration 75/234: loss=0.139776 lr=0.000020 grad_norm=1.064813
Epoch 21/100 Iteration 76/234: loss=0.124998 lr=0.000020 grad_norm=0.392825
Epoch 21/100 Iteration 77/234: loss=0.121671 lr=0.000020 grad_norm=0.998954
Epoch 21/100 Iteration 78/234: loss=0.140133 lr=0.000020 grad_norm=1.007165
Epoch 21/100 Iteration 79/234: loss=0.102840 lr=0.000020 grad_norm=0.782069
Epoch 21/100 Iteration 80/234: loss=0.107281 lr=0.000020 grad_norm=1.540684
Epoch 21/100 Iteration 81/234: loss=0.112991 lr=0.000020 grad_norm=1.256644
Epoch 21/100 Iteration 82/234: loss=0.111125 lr=0.000020 grad_norm=1.408667
Epoch 21/100 Iteration 83/234: loss=0.139969 lr=0.000020 grad_norm=2.465670
Epoch 21/100 Iteration 84/234: loss=0.119010 lr=0.000020 grad_norm=1.564991
Epoch 21/100 Iteration 85/234: loss=0.109685 lr=0.000020 grad_norm=1.016818
Epoch 21/100 Iteration 86/234: loss=0.119770 lr=0.000020 grad_norm=1.843773
Epoch 21/100 Iteration 87/234: loss=0.103963 lr=0.000020 grad_norm=0.597786
Epoch 21/100 Iteration 88/234: loss=0.114697 lr=0.000020 grad_norm=1.083341
Epoch 21/100 Iteration 89/234: loss=0.114150 lr=0.000020 grad_norm=0.722637
Epoch 21/100 Iteration 90/234: loss=0.123096 lr=0.000020 grad_norm=0.573977
Epoch 21/100 Iteration 91/234: loss=0.111437 lr=0.000020 grad_norm=0.640520
Epoch 21/100 Iteration 92/234: loss=0.121517 lr=0.000020 grad_norm=0.385939
Epoch 21/100 Iteration 93/234: loss=0.129255 lr=0.000020 grad_norm=0.692753
Epoch 21/100 Iteration 94/234: loss=0.118671 lr=0.000020 grad_norm=0.542570
Epoch 21/100 Iteration 95/234: loss=0.132892 lr=0.000020 grad_norm=0.678920
Epoch 21/100 Iteration 96/234: loss=0.131265 lr=0.000020 grad_norm=0.879476
Epoch 21/100 Iteration 97/234: loss=0.113976 lr=0.000020 grad_norm=0.748648
Epoch 21/100 Iteration 98/234: loss=0.103448 lr=0.000020 grad_norm=0.745320
Epoch 21/100 Iteration 99/234: loss=0.123151 lr=0.000020 grad_norm=0.992647
Epoch 21/100 Iteration 100/234: loss=0.112546 lr=0.000020 grad_norm=1.178266
Epoch 21/100 Iteration 101/234: loss=0.121836 lr=0.000020 grad_norm=0.622001
Epoch 21/100 Iteration 102/234: loss=0.118736 lr=0.000020 grad_norm=0.716751
Epoch 21/100 Iteration 103/234: loss=0.123066 lr=0.000020 grad_norm=0.876483
Epoch 21/100 Iteration 104/234: loss=0.117313 lr=0.000020 grad_norm=0.713115
Epoch 21/100 Iteration 105/234: loss=0.117309 lr=0.000020 grad_norm=0.610209
Epoch 21/100 Iteration 106/234: loss=0.125975 lr=0.000020 grad_norm=0.915293
Epoch 21/100 Iteration 107/234: loss=0.124989 lr=0.000020 grad_norm=0.788773
Epoch 21/100 Iteration 108/234: loss=0.112793 lr=0.000020 grad_norm=0.349743
Epoch 21/100 Iteration 109/234: loss=0.109095 lr=0.000020 grad_norm=0.550679
Epoch 21/100 Iteration 110/234: loss=0.136588 lr=0.000020 grad_norm=0.745175
Epoch 21/100 Iteration 111/234: loss=0.117946 lr=0.000020 grad_norm=0.519062
Epoch 21/100 Iteration 112/234: loss=0.114736 lr=0.000020 grad_norm=0.537011
Epoch 21/100 Iteration 113/234: loss=0.107516 lr=0.000020 grad_norm=0.828929
Epoch 21/100 Iteration 114/234: loss=0.123065 lr=0.000020 grad_norm=1.005582
Epoch 21/100 Iteration 115/234: loss=0.142454 lr=0.000020 grad_norm=0.667876
Epoch 21/100 Iteration 116/234: loss=0.118178 lr=0.000020 grad_norm=0.899224
Epoch 21/100 Iteration 117/234: loss=0.109821 lr=0.000020 grad_norm=0.701842
Epoch 21/100 Iteration 118/234: loss=0.117876 lr=0.000020 grad_norm=0.658771
Epoch 21/100 Iteration 119/234: loss=0.109377 lr=0.000020 grad_norm=0.615586
Epoch 21/100 Iteration 120/234: loss=0.114139 lr=0.000020 grad_norm=0.586885
Epoch 21/100 Iteration 121/234: loss=0.132047 lr=0.000020 grad_norm=0.459103
Epoch 21/100 Iteration 122/234: loss=0.126024 lr=0.000020 grad_norm=0.549628
Epoch 21/100 Iteration 123/234: loss=0.108276 lr=0.000020 grad_norm=0.692090
Epoch 21/100 Iteration 124/234: loss=0.114141 lr=0.000020 grad_norm=0.494191
Epoch 21/100 Iteration 125/234: loss=0.135958 lr=0.000020 grad_norm=0.828941
Epoch 21/100 Iteration 126/234: loss=0.117590 lr=0.000020 grad_norm=0.896215
Epoch 21/100 Iteration 127/234: loss=0.132144 lr=0.000020 grad_norm=0.576925
Epoch 21/100 Iteration 128/234: loss=0.119107 lr=0.000020 grad_norm=0.495362
Epoch 21/100 Iteration 129/234: loss=0.119600 lr=0.000020 grad_norm=0.573989
Epoch 21/100 Iteration 130/234: loss=0.118275 lr=0.000020 grad_norm=0.560926
Epoch 21/100 Iteration 131/234: loss=0.122726 lr=0.000020 grad_norm=0.693092
Epoch 21/100 Iteration 132/234: loss=0.131836 lr=0.000020 grad_norm=0.691765
Epoch 21/100 Iteration 133/234: loss=0.133337 lr=0.000020 grad_norm=0.571349
Epoch 21/100 Iteration 134/234: loss=0.118666 lr=0.000020 grad_norm=0.440905
Epoch 21/100 Iteration 135/234: loss=0.125083 lr=0.000020 grad_norm=0.553645
Epoch 21/100 Iteration 136/234: loss=0.112577 lr=0.000020 grad_norm=0.485889
Epoch 21/100 Iteration 137/234: loss=0.117364 lr=0.000020 grad_norm=0.524052
Epoch 21/100 Iteration 138/234: loss=0.111149 lr=0.000020 grad_norm=0.467203
Epoch 21/100 Iteration 139/234: loss=0.125776 lr=0.000020 grad_norm=0.677769
Epoch 21/100 Iteration 140/234: loss=0.126288 lr=0.000020 grad_norm=0.769192
Epoch 21/100 Iteration 141/234: loss=0.124873 lr=0.000020 grad_norm=0.840560
Epoch 21/100 Iteration 142/234: loss=0.107750 lr=0.000020 grad_norm=0.543243
Epoch 21/100 Iteration 143/234: loss=0.125948 lr=0.000020 grad_norm=0.976264
Epoch 21/100 Iteration 144/234: loss=0.120984 lr=0.000020 grad_norm=1.338090
Epoch 21/100 Iteration 145/234: loss=0.114639 lr=0.000020 grad_norm=0.640999
Epoch 21/100 Iteration 146/234: loss=0.119697 lr=0.000020 grad_norm=0.478721
Epoch 21/100 Iteration 147/234: loss=0.141708 lr=0.000020 grad_norm=0.835627
Epoch 21/100 Iteration 148/234: loss=0.123326 lr=0.000020 grad_norm=0.541956
Epoch 21/100 Iteration 149/234: loss=0.127490 lr=0.000020 grad_norm=0.713161
Epoch 21/100 Iteration 150/234: loss=0.123516 lr=0.000020 grad_norm=0.728194
Epoch 21/100 Iteration 151/234: loss=0.131014 lr=0.000020 grad_norm=0.522092
Epoch 21/100 Iteration 152/234: loss=0.141125 lr=0.000020 grad_norm=0.574047
Epoch 21/100 Iteration 153/234: loss=0.142260 lr=0.000020 grad_norm=0.694398
Epoch 21/100 Iteration 154/234: loss=0.126012 lr=0.000020 grad_norm=0.587687
Epoch 21/100 Iteration 155/234: loss=0.124464 lr=0.000020 grad_norm=0.531664
Epoch 21/100 Iteration 156/234: loss=0.119621 lr=0.000020 grad_norm=0.637973
Epoch 21/100 Iteration 157/234: loss=0.121622 lr=0.000020 grad_norm=0.772184
Epoch 21/100 Iteration 158/234: loss=0.127741 lr=0.000020 grad_norm=0.530896
Epoch 21/100 Iteration 159/234: loss=0.121418 lr=0.000020 grad_norm=0.651243
Epoch 21/100 Iteration 160/234: loss=0.135741 lr=0.000020 grad_norm=1.376738
Epoch 21/100 Iteration 161/234: loss=0.126430 lr=0.000020 grad_norm=1.937374
Epoch 21/100 Iteration 162/234: loss=0.127506 lr=0.000020 grad_norm=1.361791
Epoch 21/100 Iteration 163/234: loss=0.113114 lr=0.000020 grad_norm=0.406593
Epoch 21/100 Iteration 164/234: loss=0.133275 lr=0.000020 grad_norm=1.499240
Epoch 21/100 Iteration 165/234: loss=0.114160 lr=0.000020 grad_norm=1.984387
Epoch 21/100 Iteration 166/234: loss=0.137635 lr=0.000020 grad_norm=1.232632
Epoch 21/100 Iteration 167/234: loss=0.122346 lr=0.000020 grad_norm=0.660513
Epoch 21/100 Iteration 168/234: loss=0.123085 lr=0.000020 grad_norm=1.270641
Epoch 21/100 Iteration 169/234: loss=0.126667 lr=0.000020 grad_norm=0.854382
Epoch 21/100 Iteration 170/234: loss=0.119071 lr=0.000020 grad_norm=0.602841
Epoch 21/100 Iteration 171/234: loss=0.141490 lr=0.000020 grad_norm=0.671200
Epoch 21/100 Iteration 172/234: loss=0.113425 lr=0.000020 grad_norm=0.438186
Epoch 21/100 Iteration 173/234: loss=0.120406 lr=0.000020 grad_norm=0.638003
Epoch 21/100 Iteration 174/234: loss=0.116680 lr=0.000020 grad_norm=0.325137
Epoch 21/100 Iteration 175/234: loss=0.131733 lr=0.000020 grad_norm=0.962335
Epoch 21/100 Iteration 176/234: loss=0.109728 lr=0.000020 grad_norm=0.764153
Epoch 21/100 Iteration 177/234: loss=0.126787 lr=0.000020 grad_norm=0.356628
Epoch 21/100 Iteration 178/234: loss=0.112764 lr=0.000020 grad_norm=0.760338
Epoch 21/100 Iteration 179/234: loss=0.116239 lr=0.000020 grad_norm=0.463643
Epoch 21/100 Iteration 180/234: loss=0.117951 lr=0.000020 grad_norm=0.505384
Epoch 21/100 Iteration 181/234: loss=0.114576 lr=0.000020 grad_norm=0.329934
Epoch 21/100 Iteration 182/234: loss=0.133842 lr=0.000020 grad_norm=0.720813
Epoch 21/100 Iteration 183/234: loss=0.124528 lr=0.000020 grad_norm=0.874701
Epoch 21/100 Iteration 184/234: loss=0.132892 lr=0.000020 grad_norm=0.504636
Epoch 21/100 Iteration 185/234: loss=0.111384 lr=0.000020 grad_norm=0.511216
Epoch 21/100 Iteration 186/234: loss=0.127642 lr=0.000020 grad_norm=0.795468
Epoch 21/100 Iteration 187/234: loss=0.111531 lr=0.000020 grad_norm=0.490009
Epoch 21/100 Iteration 188/234: loss=0.122685 lr=0.000020 grad_norm=0.488344
Epoch 21/100 Iteration 189/234: loss=0.136467 lr=0.000020 grad_norm=0.494864
Epoch 21/100 Iteration 190/234: loss=0.119477 lr=0.000020 grad_norm=0.415657
Epoch 21/100 Iteration 191/234: loss=0.113394 lr=0.000020 grad_norm=0.389289
Epoch 21/100 Iteration 192/234: loss=0.116545 lr=0.000020 grad_norm=0.319592
Epoch 21/100 Iteration 193/234: loss=0.131809 lr=0.000020 grad_norm=0.439491
Epoch 21/100 Iteration 194/234: loss=0.120192 lr=0.000020 grad_norm=0.511878
Epoch 21/100 Iteration 195/234: loss=0.134182 lr=0.000020 grad_norm=0.550897
Epoch 21/100 Iteration 196/234: loss=0.120213 lr=0.000020 grad_norm=0.525074
Epoch 21/100 Iteration 197/234: loss=0.131673 lr=0.000020 grad_norm=0.478230
Epoch 21/100 Iteration 198/234: loss=0.112411 lr=0.000020 grad_norm=0.452161
Epoch 21/100 Iteration 199/234: loss=0.123355 lr=0.000020 grad_norm=0.471741
Epoch 21/100 Iteration 200/234: loss=0.118650 lr=0.000020 grad_norm=0.496047
Epoch 21/100 Iteration 201/234: loss=0.102281 lr=0.000020 grad_norm=0.350059
Epoch 21/100 Iteration 202/234: loss=0.109857 lr=0.000020 grad_norm=0.499153
Epoch 21/100 Iteration 203/234: loss=0.105342 lr=0.000020 grad_norm=0.448957
Epoch 21/100 Iteration 204/234: loss=0.128151 lr=0.000020 grad_norm=0.318710
Epoch 21/100 Iteration 205/234: loss=0.120127 lr=0.000020 grad_norm=0.459209
Epoch 21/100 Iteration 206/234: loss=0.110305 lr=0.000020 grad_norm=0.327824
Epoch 21/100 Iteration 207/234: loss=0.124452 lr=0.000020 grad_norm=0.352371
Epoch 21/100 Iteration 208/234: loss=0.116749 lr=0.000020 grad_norm=0.458715
Epoch 21/100 Iteration 209/234: loss=0.121734 lr=0.000020 grad_norm=0.741934
Epoch 21/100 Iteration 210/234: loss=0.109362 lr=0.000020 grad_norm=0.714281
Epoch 21/100 Iteration 211/234: loss=0.120757 lr=0.000020 grad_norm=0.359726
Epoch 21/100 Iteration 212/234: loss=0.115436 lr=0.000020 grad_norm=0.697767
Epoch 21/100 Iteration 213/234: loss=0.127072 lr=0.000020 grad_norm=0.699030
Epoch 21/100 Iteration 214/234: loss=0.117270 lr=0.000020 grad_norm=0.480896
Epoch 21/100 Iteration 215/234: loss=0.120532 lr=0.000020 grad_norm=0.386315
Epoch 21/100 Iteration 216/234: loss=0.104023 lr=0.000020 grad_norm=0.363906
Epoch 21/100 Iteration 217/234: loss=0.123462 lr=0.000020 grad_norm=0.629442
Epoch 21/100 Iteration 218/234: loss=0.136947 lr=0.000020 grad_norm=1.041324
Epoch 21/100 Iteration 219/234: loss=0.118213 lr=0.000020 grad_norm=0.779226
Epoch 21/100 Iteration 220/234: loss=0.133005 lr=0.000020 grad_norm=0.443280
Epoch 21/100 Iteration 221/234: loss=0.134036 lr=0.000020 grad_norm=0.762064
Epoch 21/100 Iteration 222/234: loss=0.113615 lr=0.000020 grad_norm=0.566115
Epoch 21/100 Iteration 223/234: loss=0.104782 lr=0.000020 grad_norm=0.328352
Epoch 21/100 Iteration 224/234: loss=0.102708 lr=0.000020 grad_norm=0.457275
Epoch 21/100 Iteration 225/234: loss=0.132297 lr=0.000020 grad_norm=0.452034
Epoch 21/100 Iteration 226/234: loss=0.115609 lr=0.000020 grad_norm=1.064582
Epoch 21/100 Iteration 227/234: loss=0.118154 lr=0.000020 grad_norm=1.424311
Epoch 21/100 Iteration 228/234: loss=0.127113 lr=0.000020 grad_norm=0.580638
Epoch 21/100 Iteration 229/234: loss=0.108868 lr=0.000020 grad_norm=1.023855
Epoch 21/100 Iteration 230/234: loss=0.117272 lr=0.000020 grad_norm=1.345764
Epoch 21/100 Iteration 231/234: loss=0.136047 lr=0.000020 grad_norm=0.376343
Epoch 21/100 Iteration 232/234: loss=0.107153 lr=0.000020 grad_norm=1.312596
Epoch 21/100 Iteration 233/234: loss=0.109337 lr=0.000020 grad_norm=1.253260
Epoch 21/100 Iteration 234/234: loss=0.096278 lr=0.000020 grad_norm=0.423443
Epoch 21/100 finished. Avg Loss: 0.121610
Epoch 22/100 Iteration 1/234: loss=0.122267 lr=0.000020 grad_norm=1.131559
Epoch 22/100 Iteration 2/234: loss=0.120777 lr=0.000020 grad_norm=1.078681
Epoch 22/100 Iteration 3/234: loss=0.110071 lr=0.000020 grad_norm=0.636087
Epoch 22/100 Iteration 4/234: loss=0.097101 lr=0.000020 grad_norm=0.488217
Epoch 22/100 Iteration 5/234: loss=0.105470 lr=0.000020 grad_norm=0.450376
Epoch 22/100 Iteration 6/234: loss=0.124368 lr=0.000020 grad_norm=0.391081
Epoch 22/100 Iteration 7/234: loss=0.103460 lr=0.000020 grad_norm=0.364104
Epoch 22/100 Iteration 8/234: loss=0.120852 lr=0.000020 grad_norm=0.360985
Epoch 22/100 Iteration 9/234: loss=0.127628 lr=0.000020 grad_norm=0.340232
Epoch 22/100 Iteration 10/234: loss=0.111448 lr=0.000020 grad_norm=0.362567
Epoch 22/100 Iteration 11/234: loss=0.120911 lr=0.000020 grad_norm=0.364245
Epoch 22/100 Iteration 12/234: loss=0.134784 lr=0.000020 grad_norm=0.468829
Epoch 22/100 Iteration 13/234: loss=0.108172 lr=0.000020 grad_norm=0.531488
Epoch 22/100 Iteration 14/234: loss=0.117982 lr=0.000020 grad_norm=0.442572
Epoch 22/100 Iteration 15/234: loss=0.115140 lr=0.000020 grad_norm=0.360066
Epoch 22/100 Iteration 16/234: loss=0.127499 lr=0.000020 grad_norm=0.447232
Epoch 22/100 Iteration 17/234: loss=0.132575 lr=0.000020 grad_norm=0.436711
Epoch 22/100 Iteration 18/234: loss=0.119948 lr=0.000020 grad_norm=0.867550
Epoch 22/100 Iteration 19/234: loss=0.118778 lr=0.000020 grad_norm=0.910088
Epoch 22/100 Iteration 20/234: loss=0.139793 lr=0.000020 grad_norm=0.599708
Epoch 22/100 Iteration 21/234: loss=0.111054 lr=0.000020 grad_norm=0.684427
Epoch 22/100 Iteration 22/234: loss=0.118714 lr=0.000020 grad_norm=1.443695
Epoch 22/100 Iteration 23/234: loss=0.122560 lr=0.000020 grad_norm=1.530318
Epoch 22/100 Iteration 24/234: loss=0.121230 lr=0.000020 grad_norm=0.778545
Epoch 22/100 Iteration 25/234: loss=0.127928 lr=0.000020 grad_norm=0.703196
Epoch 22/100 Iteration 26/234: loss=0.112317 lr=0.000020 grad_norm=0.617235
Epoch 22/100 Iteration 27/234: loss=0.104016 lr=0.000020 grad_norm=0.590969
Epoch 22/100 Iteration 28/234: loss=0.113059 lr=0.000020 grad_norm=0.581410
Epoch 22/100 Iteration 29/234: loss=0.121422 lr=0.000020 grad_norm=0.412383
Epoch 22/100 Iteration 30/234: loss=0.123209 lr=0.000020 grad_norm=0.535650
Epoch 22/100 Iteration 31/234: loss=0.104221 lr=0.000020 grad_norm=0.561775
Epoch 22/100 Iteration 32/234: loss=0.128160 lr=0.000020 grad_norm=0.349915
Epoch 22/100 Iteration 33/234: loss=0.116290 lr=0.000020 grad_norm=0.610229
Epoch 22/100 Iteration 34/234: loss=0.110654 lr=0.000020 grad_norm=0.718300
Epoch 22/100 Iteration 35/234: loss=0.123653 lr=0.000020 grad_norm=0.517029
Epoch 22/100 Iteration 36/234: loss=0.127210 lr=0.000020 grad_norm=0.427013
Epoch 22/100 Iteration 37/234: loss=0.124749 lr=0.000020 grad_norm=0.330686
Epoch 22/100 Iteration 38/234: loss=0.132689 lr=0.000020 grad_norm=0.422023
Epoch 22/100 Iteration 39/234: loss=0.121529 lr=0.000020 grad_norm=0.575430
Epoch 22/100 Iteration 40/234: loss=0.113655 lr=0.000020 grad_norm=0.700727
Epoch 22/100 Iteration 41/234: loss=0.118653 lr=0.000020 grad_norm=0.436686
Epoch 22/100 Iteration 42/234: loss=0.116825 lr=0.000020 grad_norm=0.371263
Epoch 22/100 Iteration 43/234: loss=0.121562 lr=0.000020 grad_norm=0.596404
Epoch 22/100 Iteration 44/234: loss=0.135582 lr=0.000020 grad_norm=0.767578
Epoch 22/100 Iteration 45/234: loss=0.107361 lr=0.000020 grad_norm=0.763178
Epoch 22/100 Iteration 46/234: loss=0.124309 lr=0.000020 grad_norm=0.472747
Epoch 22/100 Iteration 47/234: loss=0.114048 lr=0.000020 grad_norm=0.403120
Epoch 22/100 Iteration 48/234: loss=0.120147 lr=0.000020 grad_norm=0.630362
Epoch 22/100 Iteration 49/234: loss=0.123387 lr=0.000020 grad_norm=0.555877
Epoch 22/100 Iteration 50/234: loss=0.121648 lr=0.000020 grad_norm=0.542603
Epoch 22/100 Iteration 51/234: loss=0.136059 lr=0.000020 grad_norm=0.554090
Epoch 22/100 Iteration 52/234: loss=0.111070 lr=0.000020 grad_norm=0.567471
Epoch 22/100 Iteration 53/234: loss=0.121858 lr=0.000020 grad_norm=0.568742
Epoch 22/100 Iteration 54/234: loss=0.105214 lr=0.000020 grad_norm=0.560187
Epoch 22/100 Iteration 55/234: loss=0.129790 lr=0.000020 grad_norm=0.969335
Epoch 22/100 Iteration 56/234: loss=0.109263 lr=0.000020 grad_norm=1.050980
Epoch 22/100 Iteration 57/234: loss=0.111976 lr=0.000020 grad_norm=0.713915
Epoch 22/100 Iteration 58/234: loss=0.120766 lr=0.000020 grad_norm=0.826617
Epoch 22/100 Iteration 59/234: loss=0.123186 lr=0.000020 grad_norm=1.100021
Epoch 22/100 Iteration 60/234: loss=0.113888 lr=0.000020 grad_norm=0.681454
Epoch 22/100 Iteration 61/234: loss=0.128647 lr=0.000020 grad_norm=0.639579
Epoch 22/100 Iteration 62/234: loss=0.121443 lr=0.000020 grad_norm=0.821144
Epoch 22/100 Iteration 63/234: loss=0.128195 lr=0.000020 grad_norm=0.486312
Epoch 22/100 Iteration 64/234: loss=0.125188 lr=0.000020 grad_norm=0.508127
Epoch 22/100 Iteration 65/234: loss=0.115953 lr=0.000020 grad_norm=0.667994
Epoch 22/100 Iteration 66/234: loss=0.126815 lr=0.000020 grad_norm=0.615918
Epoch 22/100 Iteration 67/234: loss=0.122292 lr=0.000020 grad_norm=0.408365
Epoch 22/100 Iteration 68/234: loss=0.110437 lr=0.000020 grad_norm=1.025395
Epoch 22/100 Iteration 69/234: loss=0.101955 lr=0.000020 grad_norm=1.158436
Epoch 22/100 Iteration 70/234: loss=0.120626 lr=0.000020 grad_norm=0.476627
Epoch 22/100 Iteration 71/234: loss=0.144796 lr=0.000020 grad_norm=1.219202
Epoch 22/100 Iteration 72/234: loss=0.116861 lr=0.000020 grad_norm=1.214664
Epoch 22/100 Iteration 73/234: loss=0.118476 lr=0.000020 grad_norm=0.428839
Epoch 22/100 Iteration 74/234: loss=0.124820 lr=0.000020 grad_norm=0.941575
Epoch 22/100 Iteration 75/234: loss=0.112774 lr=0.000020 grad_norm=0.761864
Epoch 22/100 Iteration 76/234: loss=0.116719 lr=0.000020 grad_norm=0.499685
Epoch 22/100 Iteration 77/234: loss=0.099685 lr=0.000020 grad_norm=1.219686
Epoch 22/100 Iteration 78/234: loss=0.113585 lr=0.000020 grad_norm=0.723249
Epoch 22/100 Iteration 79/234: loss=0.121575 lr=0.000020 grad_norm=0.893301
Epoch 22/100 Iteration 80/234: loss=0.111156 lr=0.000020 grad_norm=1.360029
Epoch 22/100 Iteration 81/234: loss=0.111425 lr=0.000020 grad_norm=0.866593
Epoch 22/100 Iteration 82/234: loss=0.122240 lr=0.000020 grad_norm=0.457329
Epoch 22/100 Iteration 83/234: loss=0.126362 lr=0.000020 grad_norm=0.517146
Epoch 22/100 Iteration 84/234: loss=0.128244 lr=0.000020 grad_norm=0.445930
Epoch 22/100 Iteration 85/234: loss=0.102157 lr=0.000020 grad_norm=0.623177
Epoch 22/100 Iteration 86/234: loss=0.119397 lr=0.000020 grad_norm=0.464055
Epoch 22/100 Iteration 87/234: loss=0.103003 lr=0.000020 grad_norm=0.350823
Epoch 22/100 Iteration 88/234: loss=0.114923 lr=0.000020 grad_norm=0.634494
Epoch 22/100 Iteration 89/234: loss=0.138586 lr=0.000020 grad_norm=0.541632
Epoch 22/100 Iteration 90/234: loss=0.122022 lr=0.000020 grad_norm=1.114535
Epoch 22/100 Iteration 91/234: loss=0.113194 lr=0.000020 grad_norm=1.189251
Epoch 22/100 Iteration 92/234: loss=0.102057 lr=0.000020 grad_norm=0.438987
Epoch 22/100 Iteration 93/234: loss=0.130427 lr=0.000020 grad_norm=1.068296
Epoch 22/100 Iteration 94/234: loss=0.132695 lr=0.000020 grad_norm=1.317248
Epoch 22/100 Iteration 95/234: loss=0.124262 lr=0.000020 grad_norm=0.660026
Epoch 22/100 Iteration 96/234: loss=0.103526 lr=0.000020 grad_norm=0.636096
Epoch 22/100 Iteration 97/234: loss=0.119785 lr=0.000020 grad_norm=0.541016
Epoch 22/100 Iteration 98/234: loss=0.132415 lr=0.000020 grad_norm=0.446416
Epoch 22/100 Iteration 99/234: loss=0.111195 lr=0.000020 grad_norm=0.949324
Epoch 22/100 Iteration 100/234: loss=0.137314 lr=0.000020 grad_norm=1.407209
Epoch 22/100 Iteration 101/234: loss=0.120850 lr=0.000020 grad_norm=1.041705
Epoch 22/100 Iteration 102/234: loss=0.122377 lr=0.000020 grad_norm=0.603200
Epoch 22/100 Iteration 103/234: loss=0.115356 lr=0.000020 grad_norm=1.535940
Epoch 22/100 Iteration 104/234: loss=0.126750 lr=0.000020 grad_norm=1.271602
Epoch 22/100 Iteration 105/234: loss=0.115268 lr=0.000020 grad_norm=0.939237
Epoch 22/100 Iteration 106/234: loss=0.121762 lr=0.000020 grad_norm=0.909332
Epoch 22/100 Iteration 107/234: loss=0.105439 lr=0.000020 grad_norm=0.905584
Epoch 22/100 Iteration 108/234: loss=0.121456 lr=0.000020 grad_norm=0.651840
Epoch 22/100 Iteration 109/234: loss=0.138535 lr=0.000020 grad_norm=0.885375
Epoch 22/100 Iteration 110/234: loss=0.117598 lr=0.000020 grad_norm=0.948227
Epoch 22/100 Iteration 111/234: loss=0.114586 lr=0.000020 grad_norm=0.546783
Epoch 22/100 Iteration 112/234: loss=0.110593 lr=0.000020 grad_norm=0.735488
Epoch 22/100 Iteration 113/234: loss=0.125988 lr=0.000020 grad_norm=1.146029
Epoch 22/100 Iteration 114/234: loss=0.125255 lr=0.000020 grad_norm=0.993646
Epoch 22/100 Iteration 115/234: loss=0.119255 lr=0.000020 grad_norm=0.720515
Epoch 22/100 Iteration 116/234: loss=0.100554 lr=0.000020 grad_norm=0.461715
Epoch 22/100 Iteration 117/234: loss=0.110324 lr=0.000020 grad_norm=0.420475
Epoch 22/100 Iteration 118/234: loss=0.108211 lr=0.000020 grad_norm=0.657867
Epoch 22/100 Iteration 119/234: loss=0.117409 lr=0.000020 grad_norm=0.568482
Epoch 22/100 Iteration 120/234: loss=0.116638 lr=0.000020 grad_norm=0.418115
Epoch 22/100 Iteration 121/234: loss=0.129102 lr=0.000020 grad_norm=0.963057
Epoch 22/100 Iteration 122/234: loss=0.112315 lr=0.000020 grad_norm=1.906626
Epoch 22/100 Iteration 123/234: loss=0.122536 lr=0.000020 grad_norm=1.781622
Epoch 22/100 Iteration 124/234: loss=0.103476 lr=0.000020 grad_norm=0.494182
Epoch 22/100 Iteration 125/234: loss=0.109592 lr=0.000020 grad_norm=0.876634
Epoch 22/100 Iteration 126/234: loss=0.117970 lr=0.000020 grad_norm=0.964770
Epoch 22/100 Iteration 127/234: loss=0.126703 lr=0.000020 grad_norm=0.468135
Epoch 22/100 Iteration 128/234: loss=0.127181 lr=0.000020 grad_norm=0.710358
Epoch 22/100 Iteration 129/234: loss=0.119578 lr=0.000020 grad_norm=0.686858
Epoch 22/100 Iteration 130/234: loss=0.129778 lr=0.000020 grad_norm=0.392783
Epoch 22/100 Iteration 131/234: loss=0.134371 lr=0.000020 grad_norm=0.449690
Epoch 22/100 Iteration 132/234: loss=0.133551 lr=0.000020 grad_norm=0.420952
Epoch 22/100 Iteration 133/234: loss=0.118857 lr=0.000020 grad_norm=0.535650
Epoch 22/100 Iteration 134/234: loss=0.119847 lr=0.000020 grad_norm=1.224766
Epoch 22/100 Iteration 135/234: loss=0.114023 lr=0.000020 grad_norm=1.297652
Epoch 22/100 Iteration 136/234: loss=0.121951 lr=0.000020 grad_norm=0.494339
Epoch 22/100 Iteration 137/234: loss=0.116342 lr=0.000020 grad_norm=1.047310
Epoch 22/100 Iteration 138/234: loss=0.112847 lr=0.000020 grad_norm=1.346476
Epoch 22/100 Iteration 139/234: loss=0.114922 lr=0.000020 grad_norm=0.589669
Epoch 22/100 Iteration 140/234: loss=0.108668 lr=0.000020 grad_norm=1.020094
Epoch 22/100 Iteration 141/234: loss=0.127331 lr=0.000020 grad_norm=1.060631
Epoch 22/100 Iteration 142/234: loss=0.126617 lr=0.000020 grad_norm=0.369881
Epoch 22/100 Iteration 143/234: loss=0.116869 lr=0.000020 grad_norm=0.993250
Epoch 22/100 Iteration 144/234: loss=0.119209 lr=0.000020 grad_norm=1.182222
Epoch 22/100 Iteration 145/234: loss=0.121921 lr=0.000020 grad_norm=0.619929
Epoch 22/100 Iteration 146/234: loss=0.115758 lr=0.000020 grad_norm=0.599951
Epoch 22/100 Iteration 147/234: loss=0.111844 lr=0.000020 grad_norm=0.662021
Epoch 22/100 Iteration 148/234: loss=0.094171 lr=0.000020 grad_norm=0.461235
Epoch 22/100 Iteration 149/234: loss=0.111319 lr=0.000020 grad_norm=1.001729
Epoch 22/100 Iteration 150/234: loss=0.120316 lr=0.000020 grad_norm=0.593218
Epoch 22/100 Iteration 151/234: loss=0.108009 lr=0.000020 grad_norm=0.539614
Epoch 22/100 Iteration 152/234: loss=0.118192 lr=0.000020 grad_norm=0.825755
Epoch 22/100 Iteration 153/234: loss=0.098516 lr=0.000020 grad_norm=0.564525
Epoch 22/100 Iteration 154/234: loss=0.116898 lr=0.000020 grad_norm=0.681708
Epoch 22/100 Iteration 155/234: loss=0.136421 lr=0.000020 grad_norm=1.468650
Epoch 22/100 Iteration 156/234: loss=0.125129 lr=0.000020 grad_norm=1.237425
Epoch 22/100 Iteration 157/234: loss=0.116390 lr=0.000020 grad_norm=0.846966
Epoch 22/100 Iteration 158/234: loss=0.119597 lr=0.000020 grad_norm=0.895983
Epoch 22/100 Iteration 159/234: loss=0.115928 lr=0.000020 grad_norm=0.306902
Epoch 22/100 Iteration 160/234: loss=0.127987 lr=0.000020 grad_norm=0.761641
Epoch 22/100 Iteration 161/234: loss=0.105410 lr=0.000020 grad_norm=0.974218
Epoch 22/100 Iteration 162/234: loss=0.109302 lr=0.000020 grad_norm=1.027708
Epoch 22/100 Iteration 163/234: loss=0.117886 lr=0.000020 grad_norm=1.007973
Epoch 22/100 Iteration 164/234: loss=0.123119 lr=0.000020 grad_norm=0.724502
Epoch 22/100 Iteration 165/234: loss=0.113243 lr=0.000020 grad_norm=0.587682
Epoch 22/100 Iteration 166/234: loss=0.109072 lr=0.000020 grad_norm=0.770694
Epoch 22/100 Iteration 167/234: loss=0.121449 lr=0.000020 grad_norm=0.687819
Epoch 22/100 Iteration 168/234: loss=0.097319 lr=0.000020 grad_norm=0.510722
Epoch 22/100 Iteration 169/234: loss=0.132924 lr=0.000020 grad_norm=0.759967
Epoch 22/100 Iteration 170/234: loss=0.114906 lr=0.000020 grad_norm=1.013575
Epoch 22/100 Iteration 171/234: loss=0.123328 lr=0.000020 grad_norm=1.266873
Epoch 22/100 Iteration 172/234: loss=0.125721 lr=0.000020 grad_norm=0.868202
Epoch 22/100 Iteration 173/234: loss=0.121148 lr=0.000020 grad_norm=0.419910
Epoch 22/100 Iteration 174/234: loss=0.109884 lr=0.000020 grad_norm=0.987452
Epoch 22/100 Iteration 175/234: loss=0.122461 lr=0.000020 grad_norm=0.776417
Epoch 22/100 Iteration 176/234: loss=0.103565 lr=0.000020 grad_norm=0.340665
Epoch 22/100 Iteration 177/234: loss=0.113534 lr=0.000020 grad_norm=0.717116
Epoch 22/100 Iteration 178/234: loss=0.099966 lr=0.000020 grad_norm=0.793102
Epoch 22/100 Iteration 179/234: loss=0.111804 lr=0.000020 grad_norm=0.457164
Epoch 22/100 Iteration 180/234: loss=0.129479 lr=0.000020 grad_norm=0.440892
Epoch 22/100 Iteration 181/234: loss=0.112012 lr=0.000020 grad_norm=0.679898
Epoch 22/100 Iteration 182/234: loss=0.124004 lr=0.000020 grad_norm=0.658594
Epoch 22/100 Iteration 183/234: loss=0.129207 lr=0.000020 grad_norm=0.651553
Epoch 22/100 Iteration 184/234: loss=0.132175 lr=0.000020 grad_norm=0.544904
Epoch 22/100 Iteration 185/234: loss=0.117797 lr=0.000020 grad_norm=0.330185
Epoch 22/100 Iteration 186/234: loss=0.105058 lr=0.000020 grad_norm=0.643749
Epoch 22/100 Iteration 187/234: loss=0.107994 lr=0.000020 grad_norm=0.920103
Epoch 22/100 Iteration 188/234: loss=0.123048 lr=0.000020 grad_norm=0.519794
Epoch 22/100 Iteration 189/234: loss=0.118655 lr=0.000020 grad_norm=0.524166
Epoch 22/100 Iteration 190/234: loss=0.127795 lr=0.000020 grad_norm=0.796162
Epoch 22/100 Iteration 191/234: loss=0.107897 lr=0.000020 grad_norm=0.764566
Epoch 22/100 Iteration 192/234: loss=0.095113 lr=0.000020 grad_norm=0.389852
Epoch 22/100 Iteration 193/234: loss=0.113107 lr=0.000020 grad_norm=0.784197
Epoch 22/100 Iteration 194/234: loss=0.131275 lr=0.000020 grad_norm=0.887500
Epoch 22/100 Iteration 195/234: loss=0.120841 lr=0.000020 grad_norm=0.296551
Epoch 22/100 Iteration 196/234: loss=0.116615 lr=0.000020 grad_norm=0.773370
Epoch 22/100 Iteration 197/234: loss=0.119222 lr=0.000020 grad_norm=0.689717
Epoch 22/100 Iteration 198/234: loss=0.131215 lr=0.000020 grad_norm=0.660184
Epoch 22/100 Iteration 199/234: loss=0.110186 lr=0.000020 grad_norm=0.576706
Epoch 22/100 Iteration 200/234: loss=0.108984 lr=0.000020 grad_norm=0.416243
Epoch 22/100 Iteration 201/234: loss=0.117065 lr=0.000020 grad_norm=0.547191
Epoch 22/100 Iteration 202/234: loss=0.117506 lr=0.000020 grad_norm=0.711610
Epoch 22/100 Iteration 203/234: loss=0.126608 lr=0.000020 grad_norm=0.520945
Epoch 22/100 Iteration 204/234: loss=0.118633 lr=0.000020 grad_norm=0.367911
Epoch 22/100 Iteration 205/234: loss=0.117991 lr=0.000020 grad_norm=0.554967
Epoch 22/100 Iteration 206/234: loss=0.120393 lr=0.000020 grad_norm=0.520108
Epoch 22/100 Iteration 207/234: loss=0.107263 lr=0.000020 grad_norm=0.492187
Epoch 22/100 Iteration 208/234: loss=0.122586 lr=0.000020 grad_norm=0.493832
Epoch 22/100 Iteration 209/234: loss=0.111172 lr=0.000020 grad_norm=0.441799
Epoch 22/100 Iteration 210/234: loss=0.111500 lr=0.000020 grad_norm=0.293133
Epoch 22/100 Iteration 211/234: loss=0.126560 lr=0.000020 grad_norm=0.845540
Epoch 22/100 Iteration 212/234: loss=0.117358 lr=0.000020 grad_norm=1.526689
Epoch 22/100 Iteration 213/234: loss=0.126423 lr=0.000020 grad_norm=1.495726
Epoch 22/100 Iteration 214/234: loss=0.125764 lr=0.000020 grad_norm=1.048443
Epoch 22/100 Iteration 215/234: loss=0.116998 lr=0.000020 grad_norm=0.896163
Epoch 22/100 Iteration 216/234: loss=0.110510 lr=0.000020 grad_norm=0.976360
Epoch 22/100 Iteration 217/234: loss=0.106209 lr=0.000020 grad_norm=0.694489
Epoch 22/100 Iteration 218/234: loss=0.120222 lr=0.000020 grad_norm=0.583250
Epoch 22/100 Iteration 219/234: loss=0.133928 lr=0.000020 grad_norm=0.605102
Epoch 22/100 Iteration 220/234: loss=0.134188 lr=0.000020 grad_norm=0.563169
Epoch 22/100 Iteration 221/234: loss=0.115288 lr=0.000020 grad_norm=0.588881
Epoch 22/100 Iteration 222/234: loss=0.115383 lr=0.000020 grad_norm=0.508668
Epoch 22/100 Iteration 223/234: loss=0.127669 lr=0.000020 grad_norm=0.371771
Epoch 22/100 Iteration 224/234: loss=0.083193 lr=0.000020 grad_norm=0.785142
Epoch 22/100 Iteration 225/234: loss=0.123957 lr=0.000020 grad_norm=1.097401
Epoch 22/100 Iteration 226/234: loss=0.121006 lr=0.000020 grad_norm=0.813093
Epoch 22/100 Iteration 227/234: loss=0.111306 lr=0.000020 grad_norm=0.456558
Epoch 22/100 Iteration 228/234: loss=0.102213 lr=0.000020 grad_norm=0.500349
Epoch 22/100 Iteration 229/234: loss=0.117680 lr=0.000020 grad_norm=0.572246
Epoch 22/100 Iteration 230/234: loss=0.128400 lr=0.000020 grad_norm=0.946196
Epoch 22/100 Iteration 231/234: loss=0.106075 lr=0.000020 grad_norm=0.799115
Epoch 22/100 Iteration 232/234: loss=0.109373 lr=0.000020 grad_norm=0.469906
Epoch 22/100 Iteration 233/234: loss=0.103930 lr=0.000020 grad_norm=0.462399
Epoch 22/100 Iteration 234/234: loss=0.114182 lr=0.000020 grad_norm=0.630456
Epoch 22/100 finished. Avg Loss: 0.118135
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 23/100 Iteration 1/234: loss=0.107655 lr=0.000020 grad_norm=0.722518
Epoch 23/100 Iteration 2/234: loss=0.116036 lr=0.000020 grad_norm=0.614566
Epoch 23/100 Iteration 3/234: loss=0.133672 lr=0.000020 grad_norm=0.964025
Epoch 23/100 Iteration 4/234: loss=0.108664 lr=0.000020 grad_norm=1.662582
Epoch 23/100 Iteration 5/234: loss=0.124763 lr=0.000020 grad_norm=1.935453
Epoch 23/100 Iteration 6/234: loss=0.126512 lr=0.000020 grad_norm=0.904751
Epoch 23/100 Iteration 7/234: loss=0.110957 lr=0.000020 grad_norm=0.778602
Epoch 23/100 Iteration 8/234: loss=0.111204 lr=0.000020 grad_norm=0.969050
Epoch 23/100 Iteration 9/234: loss=0.125347 lr=0.000020 grad_norm=0.495595
Epoch 23/100 Iteration 10/234: loss=0.124800 lr=0.000020 grad_norm=1.145219
Epoch 23/100 Iteration 11/234: loss=0.113287 lr=0.000020 grad_norm=1.131595
Epoch 23/100 Iteration 12/234: loss=0.118518 lr=0.000020 grad_norm=0.608081
Epoch 23/100 Iteration 13/234: loss=0.108679 lr=0.000020 grad_norm=0.659962
Epoch 23/100 Iteration 14/234: loss=0.113117 lr=0.000020 grad_norm=0.485753
Epoch 23/100 Iteration 15/234: loss=0.130885 lr=0.000020 grad_norm=0.470546
Epoch 23/100 Iteration 16/234: loss=0.096413 lr=0.000020 grad_norm=0.656951
Epoch 23/100 Iteration 17/234: loss=0.098313 lr=0.000020 grad_norm=0.373221
Epoch 23/100 Iteration 18/234: loss=0.117360 lr=0.000020 grad_norm=0.933551
Epoch 23/100 Iteration 19/234: loss=0.119393 lr=0.000020 grad_norm=1.019539
Epoch 23/100 Iteration 20/234: loss=0.107475 lr=0.000020 grad_norm=0.816498
Epoch 23/100 Iteration 21/234: loss=0.107547 lr=0.000020 grad_norm=0.397543
Epoch 23/100 Iteration 22/234: loss=0.116810 lr=0.000020 grad_norm=0.702240
Epoch 23/100 Iteration 23/234: loss=0.120587 lr=0.000020 grad_norm=0.363236
Epoch 23/100 Iteration 24/234: loss=0.111634 lr=0.000020 grad_norm=0.673518
Epoch 23/100 Iteration 25/234: loss=0.113677 lr=0.000020 grad_norm=0.905328
Epoch 23/100 Iteration 26/234: loss=0.111281 lr=0.000020 grad_norm=0.535448
Epoch 23/100 Iteration 27/234: loss=0.114263 lr=0.000020 grad_norm=0.480665
Epoch 23/100 Iteration 28/234: loss=0.126534 lr=0.000020 grad_norm=0.775425
Epoch 23/100 Iteration 29/234: loss=0.119525 lr=0.000020 grad_norm=0.826996
Epoch 23/100 Iteration 30/234: loss=0.111457 lr=0.000020 grad_norm=0.486111
Epoch 23/100 Iteration 31/234: loss=0.119711 lr=0.000020 grad_norm=0.944332
Epoch 23/100 Iteration 32/234: loss=0.122087 lr=0.000020 grad_norm=0.788236
Epoch 23/100 Iteration 33/234: loss=0.111020 lr=0.000020 grad_norm=0.452342
Epoch 23/100 Iteration 34/234: loss=0.108588 lr=0.000020 grad_norm=0.465727
Epoch 23/100 Iteration 35/234: loss=0.128612 lr=0.000020 grad_norm=0.716145
Epoch 23/100 Iteration 36/234: loss=0.086004 lr=0.000020 grad_norm=1.380370
Epoch 23/100 Iteration 37/234: loss=0.108292 lr=0.000020 grad_norm=1.256210
Epoch 23/100 Iteration 38/234: loss=0.127511 lr=0.000020 grad_norm=0.418230
Epoch 23/100 Iteration 39/234: loss=0.127352 lr=0.000020 grad_norm=1.059430
Epoch 23/100 Iteration 40/234: loss=0.114788 lr=0.000020 grad_norm=1.431581
Epoch 23/100 Iteration 41/234: loss=0.129431 lr=0.000020 grad_norm=0.772450
Epoch 23/100 Iteration 42/234: loss=0.113561 lr=0.000020 grad_norm=0.580276
Epoch 23/100 Iteration 43/234: loss=0.115619 lr=0.000020 grad_norm=0.745178
Epoch 23/100 Iteration 44/234: loss=0.125693 lr=0.000020 grad_norm=0.383045
Epoch 23/100 Iteration 45/234: loss=0.117775 lr=0.000020 grad_norm=1.016732
Epoch 23/100 Iteration 46/234: loss=0.112359 lr=0.000020 grad_norm=1.070566
Epoch 23/100 Iteration 47/234: loss=0.122117 lr=0.000020 grad_norm=0.377256
Epoch 23/100 Iteration 48/234: loss=0.119317 lr=0.000020 grad_norm=1.036831
Epoch 23/100 Iteration 49/234: loss=0.107756 lr=0.000020 grad_norm=0.957449
Epoch 23/100 Iteration 50/234: loss=0.126331 lr=0.000020 grad_norm=0.308480
Epoch 23/100 Iteration 51/234: loss=0.121254 lr=0.000020 grad_norm=1.222401
Epoch 23/100 Iteration 52/234: loss=0.123216 lr=0.000020 grad_norm=1.207355
Epoch 23/100 Iteration 53/234: loss=0.114999 lr=0.000020 grad_norm=0.468491
Epoch 23/100 Iteration 54/234: loss=0.122886 lr=0.000020 grad_norm=1.554224
Epoch 23/100 Iteration 55/234: loss=0.116268 lr=0.000020 grad_norm=1.414228
Epoch 23/100 Iteration 56/234: loss=0.106606 lr=0.000020 grad_norm=0.506921
Epoch 23/100 Iteration 57/234: loss=0.101478 lr=0.000020 grad_norm=0.988925
Epoch 23/100 Iteration 58/234: loss=0.103108 lr=0.000020 grad_norm=0.544256
Epoch 23/100 Iteration 59/234: loss=0.115995 lr=0.000020 grad_norm=1.121466
Epoch 23/100 Iteration 60/234: loss=0.126655 lr=0.000020 grad_norm=1.304042
Epoch 23/100 Iteration 61/234: loss=0.110099 lr=0.000020 grad_norm=0.670495
Epoch 23/100 Iteration 62/234: loss=0.111957 lr=0.000020 grad_norm=0.997109
Epoch 23/100 Iteration 63/234: loss=0.112924 lr=0.000020 grad_norm=0.791653
Epoch 23/100 Iteration 64/234: loss=0.103284 lr=0.000020 grad_norm=0.518310
Epoch 23/100 Iteration 65/234: loss=0.110342 lr=0.000020 grad_norm=0.846753
Epoch 23/100 Iteration 66/234: loss=0.106062 lr=0.000020 grad_norm=0.512130
Epoch 23/100 Iteration 67/234: loss=0.111608 lr=0.000020 grad_norm=0.713820
Epoch 23/100 Iteration 68/234: loss=0.121480 lr=0.000020 grad_norm=0.778578
Epoch 23/100 Iteration 69/234: loss=0.101616 lr=0.000020 grad_norm=0.438064
Epoch 23/100 Iteration 70/234: loss=0.117564 lr=0.000020 grad_norm=1.209778
Epoch 23/100 Iteration 71/234: loss=0.087373 lr=0.000020 grad_norm=1.042912
Epoch 23/100 Iteration 72/234: loss=0.109185 lr=0.000020 grad_norm=0.513421
Epoch 23/100 Iteration 73/234: loss=0.122905 lr=0.000020 grad_norm=0.834859
Epoch 23/100 Iteration 74/234: loss=0.110592 lr=0.000020 grad_norm=0.733091
Epoch 23/100 Iteration 75/234: loss=0.113986 lr=0.000020 grad_norm=0.705003
Epoch 23/100 Iteration 76/234: loss=0.118356 lr=0.000020 grad_norm=0.616661
Epoch 23/100 Iteration 77/234: loss=0.110635 lr=0.000020 grad_norm=0.440771
Epoch 23/100 Iteration 78/234: loss=0.109654 lr=0.000020 grad_norm=0.440542
Epoch 23/100 Iteration 79/234: loss=0.133055 lr=0.000020 grad_norm=0.488655
Epoch 23/100 Iteration 80/234: loss=0.115844 lr=0.000020 grad_norm=0.645634
Epoch 23/100 Iteration 81/234: loss=0.124739 lr=0.000020 grad_norm=0.954240
Epoch 23/100 Iteration 82/234: loss=0.116925 lr=0.000020 grad_norm=0.907053
Epoch 23/100 Iteration 83/234: loss=0.130501 lr=0.000020 grad_norm=0.432960
Epoch 23/100 Iteration 84/234: loss=0.119656 lr=0.000020 grad_norm=0.872032
Epoch 23/100 Iteration 85/234: loss=0.112810 lr=0.000020 grad_norm=1.157803
Epoch 23/100 Iteration 86/234: loss=0.116419 lr=0.000020 grad_norm=0.725394
Epoch 23/100 Iteration 87/234: loss=0.123375 lr=0.000020 grad_norm=0.518727
Epoch 23/100 Iteration 88/234: loss=0.112600 lr=0.000020 grad_norm=0.767665
Epoch 23/100 Iteration 89/234: loss=0.116868 lr=0.000020 grad_norm=0.602538
Epoch 23/100 Iteration 90/234: loss=0.112177 lr=0.000020 grad_norm=0.481538
Epoch 23/100 Iteration 91/234: loss=0.111606 lr=0.000020 grad_norm=0.621501
Epoch 23/100 Iteration 92/234: loss=0.121700 lr=0.000020 grad_norm=0.453858
Epoch 23/100 Iteration 93/234: loss=0.106011 lr=0.000020 grad_norm=0.441220
Epoch 23/100 Iteration 94/234: loss=0.107866 lr=0.000020 grad_norm=0.531369
Epoch 23/100 Iteration 95/234: loss=0.109007 lr=0.000020 grad_norm=0.565504
Epoch 23/100 Iteration 96/234: loss=0.124524 lr=0.000020 grad_norm=0.438404
Epoch 23/100 Iteration 97/234: loss=0.116766 lr=0.000020 grad_norm=0.426210
Epoch 23/100 Iteration 98/234: loss=0.121844 lr=0.000020 grad_norm=0.679174
Epoch 23/100 Iteration 99/234: loss=0.125730 lr=0.000020 grad_norm=1.009001
Epoch 23/100 Iteration 100/234: loss=0.109761 lr=0.000020 grad_norm=0.605785
Epoch 23/100 Iteration 101/234: loss=0.110955 lr=0.000020 grad_norm=0.478310
Epoch 23/100 Iteration 102/234: loss=0.106619 lr=0.000020 grad_norm=0.956866
Epoch 23/100 Iteration 103/234: loss=0.122547 lr=0.000020 grad_norm=0.782981
Epoch 23/100 Iteration 104/234: loss=0.098727 lr=0.000020 grad_norm=0.546137
Epoch 23/100 Iteration 105/234: loss=0.109295 lr=0.000020 grad_norm=0.830550
Epoch 23/100 Iteration 106/234: loss=0.104522 lr=0.000020 grad_norm=1.258441
Epoch 23/100 Iteration 107/234: loss=0.120800 lr=0.000020 grad_norm=1.554924
Epoch 23/100 Iteration 108/234: loss=0.120072 lr=0.000020 grad_norm=1.168654
Epoch 23/100 Iteration 109/234: loss=0.122040 lr=0.000020 grad_norm=1.001692
Epoch 23/100 Iteration 110/234: loss=0.096926 lr=0.000020 grad_norm=1.066341
Epoch 23/100 Iteration 111/234: loss=0.102123 lr=0.000020 grad_norm=1.233642
Epoch 23/100 Iteration 112/234: loss=0.126853 lr=0.000020 grad_norm=0.798632
Epoch 23/100 Iteration 113/234: loss=0.118412 lr=0.000020 grad_norm=0.927022
Epoch 23/100 Iteration 114/234: loss=0.109700 lr=0.000020 grad_norm=1.144606
Epoch 23/100 Iteration 115/234: loss=0.115831 lr=0.000020 grad_norm=0.675212
Epoch 23/100 Iteration 116/234: loss=0.093900 lr=0.000020 grad_norm=0.444351
Epoch 23/100 Iteration 117/234: loss=0.113585 lr=0.000020 grad_norm=0.733925
Epoch 23/100 Iteration 118/234: loss=0.114853 lr=0.000020 grad_norm=0.534284
Epoch 23/100 Iteration 119/234: loss=0.106002 lr=0.000020 grad_norm=0.740377
Epoch 23/100 Iteration 120/234: loss=0.117082 lr=0.000020 grad_norm=0.784652
Epoch 23/100 Iteration 121/234: loss=0.110565 lr=0.000020 grad_norm=0.584670
Epoch 23/100 Iteration 122/234: loss=0.110218 lr=0.000020 grad_norm=0.431582
Epoch 23/100 Iteration 123/234: loss=0.124851 lr=0.000020 grad_norm=0.473555
Epoch 23/100 Iteration 124/234: loss=0.112523 lr=0.000020 grad_norm=0.411360
Epoch 23/100 Iteration 125/234: loss=0.127018 lr=0.000020 grad_norm=0.706816
Epoch 23/100 Iteration 126/234: loss=0.114563 lr=0.000020 grad_norm=0.456393
Epoch 23/100 Iteration 127/234: loss=0.121667 lr=0.000020 grad_norm=0.677616
Epoch 23/100 Iteration 128/234: loss=0.116941 lr=0.000020 grad_norm=1.064330
Epoch 23/100 Iteration 129/234: loss=0.112968 lr=0.000020 grad_norm=0.686729
Epoch 23/100 Iteration 130/234: loss=0.112758 lr=0.000020 grad_norm=0.339801
Epoch 23/100 Iteration 131/234: loss=0.113522 lr=0.000020 grad_norm=0.379244
Epoch 23/100 Iteration 132/234: loss=0.115053 lr=0.000020 grad_norm=0.664535
Epoch 23/100 Iteration 133/234: loss=0.132642 lr=0.000020 grad_norm=1.360374
Epoch 23/100 Iteration 134/234: loss=0.114778 lr=0.000020 grad_norm=1.522656
Epoch 23/100 Iteration 135/234: loss=0.116970 lr=0.000020 grad_norm=0.580156
Epoch 23/100 Iteration 136/234: loss=0.109816 lr=0.000020 grad_norm=0.899365
Epoch 23/100 Iteration 137/234: loss=0.107938 lr=0.000020 grad_norm=1.282682
Epoch 23/100 Iteration 138/234: loss=0.123109 lr=0.000020 grad_norm=0.872226
Epoch 23/100 Iteration 139/234: loss=0.127405 lr=0.000020 grad_norm=0.780442
Epoch 23/100 Iteration 140/234: loss=0.117751 lr=0.000020 grad_norm=1.020222
Epoch 23/100 Iteration 141/234: loss=0.110380 lr=0.000020 grad_norm=0.594695
Epoch 23/100 Iteration 142/234: loss=0.107915 lr=0.000020 grad_norm=0.688292
Epoch 23/100 Iteration 143/234: loss=0.099058 lr=0.000020 grad_norm=0.921927
Epoch 23/100 Iteration 144/234: loss=0.125196 lr=0.000020 grad_norm=0.604482
Epoch 23/100 Iteration 145/234: loss=0.112666 lr=0.000020 grad_norm=0.328037
Epoch 23/100 Iteration 146/234: loss=0.113587 lr=0.000020 grad_norm=0.630724
Epoch 23/100 Iteration 147/234: loss=0.119172 lr=0.000020 grad_norm=0.473981
Epoch 23/100 Iteration 148/234: loss=0.122000 lr=0.000020 grad_norm=0.507117
Epoch 23/100 Iteration 149/234: loss=0.119616 lr=0.000020 grad_norm=0.756130
Epoch 23/100 Iteration 150/234: loss=0.106096 lr=0.000020 grad_norm=0.553052
Epoch 23/100 Iteration 151/234: loss=0.130482 lr=0.000020 grad_norm=0.475347
Epoch 23/100 Iteration 152/234: loss=0.114445 lr=0.000020 grad_norm=0.537901
Epoch 23/100 Iteration 153/234: loss=0.128568 lr=0.000020 grad_norm=0.588328
Epoch 23/100 Iteration 154/234: loss=0.095443 lr=0.000020 grad_norm=0.294658
Epoch 23/100 Iteration 155/234: loss=0.097755 lr=0.000020 grad_norm=0.656297
Epoch 23/100 Iteration 156/234: loss=0.114043 lr=0.000020 grad_norm=0.681522
Epoch 23/100 Iteration 157/234: loss=0.123931 lr=0.000020 grad_norm=0.507530
Epoch 23/100 Iteration 158/234: loss=0.107453 lr=0.000020 grad_norm=0.333936
Epoch 23/100 Iteration 159/234: loss=0.111735 lr=0.000020 grad_norm=0.369035
Epoch 23/100 Iteration 160/234: loss=0.104959 lr=0.000020 grad_norm=0.506861
Epoch 23/100 Iteration 161/234: loss=0.113939 lr=0.000020 grad_norm=0.859459
Epoch 23/100 Iteration 162/234: loss=0.115529 lr=0.000020 grad_norm=0.742275
Epoch 23/100 Iteration 163/234: loss=0.114644 lr=0.000020 grad_norm=0.475557
Epoch 23/100 Iteration 164/234: loss=0.121596 lr=0.000020 grad_norm=1.218604
Epoch 23/100 Iteration 165/234: loss=0.106851 lr=0.000020 grad_norm=1.869733
Epoch 23/100 Iteration 166/234: loss=0.104973 lr=0.000020 grad_norm=1.560390
Epoch 23/100 Iteration 167/234: loss=0.117682 lr=0.000020 grad_norm=0.459668
Epoch 23/100 Iteration 168/234: loss=0.116578 lr=0.000020 grad_norm=1.247580
Epoch 23/100 Iteration 169/234: loss=0.091797 lr=0.000020 grad_norm=1.413167
Epoch 23/100 Iteration 170/234: loss=0.118594 lr=0.000020 grad_norm=0.447363
Epoch 23/100 Iteration 171/234: loss=0.115934 lr=0.000020 grad_norm=1.202853
Epoch 23/100 Iteration 172/234: loss=0.128028 lr=0.000020 grad_norm=1.309930
Epoch 23/100 Iteration 173/234: loss=0.111525 lr=0.000020 grad_norm=0.850529
Epoch 23/100 Iteration 174/234: loss=0.115432 lr=0.000020 grad_norm=0.569085
Epoch 23/100 Iteration 175/234: loss=0.105177 lr=0.000020 grad_norm=0.607528
Epoch 23/100 Iteration 176/234: loss=0.110372 lr=0.000020 grad_norm=0.476592
Epoch 23/100 Iteration 177/234: loss=0.105757 lr=0.000020 grad_norm=0.425034
Epoch 23/100 Iteration 178/234: loss=0.106758 lr=0.000020 grad_norm=0.656901
Epoch 23/100 Iteration 179/234: loss=0.104833 lr=0.000020 grad_norm=0.795901
Epoch 23/100 Iteration 180/234: loss=0.125184 lr=0.000020 grad_norm=0.434854
Epoch 23/100 Iteration 181/234: loss=0.126197 lr=0.000020 grad_norm=0.595656
Epoch 23/100 Iteration 182/234: loss=0.112243 lr=0.000020 grad_norm=0.702000
Epoch 23/100 Iteration 183/234: loss=0.107109 lr=0.000020 grad_norm=0.736790
Epoch 23/100 Iteration 184/234: loss=0.114031 lr=0.000020 grad_norm=0.611506
Epoch 23/100 Iteration 185/234: loss=0.113776 lr=0.000020 grad_norm=0.534842
Epoch 23/100 Iteration 186/234: loss=0.113454 lr=0.000020 grad_norm=0.520313
Epoch 23/100 Iteration 187/234: loss=0.099378 lr=0.000020 grad_norm=0.770861
Epoch 23/100 Iteration 188/234: loss=0.100265 lr=0.000020 grad_norm=0.628039
Epoch 23/100 Iteration 189/234: loss=0.118106 lr=0.000020 grad_norm=0.765000
Epoch 23/100 Iteration 190/234: loss=0.116142 lr=0.000020 grad_norm=1.031959
Epoch 23/100 Iteration 191/234: loss=0.113301 lr=0.000020 grad_norm=0.568708
Epoch 23/100 Iteration 192/234: loss=0.104432 lr=0.000020 grad_norm=0.730130
Epoch 23/100 Iteration 193/234: loss=0.113035 lr=0.000020 grad_norm=0.624082
Epoch 23/100 Iteration 194/234: loss=0.121719 lr=0.000020 grad_norm=0.695379
Epoch 23/100 Iteration 195/234: loss=0.120068 lr=0.000020 grad_norm=1.241459
Epoch 23/100 Iteration 196/234: loss=0.099591 lr=0.000020 grad_norm=0.821655
Epoch 23/100 Iteration 197/234: loss=0.115477 lr=0.000020 grad_norm=0.552040
Epoch 23/100 Iteration 198/234: loss=0.121984 lr=0.000020 grad_norm=0.668827
Epoch 23/100 Iteration 199/234: loss=0.105394 lr=0.000020 grad_norm=0.434727
Epoch 23/100 Iteration 200/234: loss=0.098093 lr=0.000020 grad_norm=0.704660
Epoch 23/100 Iteration 201/234: loss=0.105575 lr=0.000020 grad_norm=0.796007
Epoch 23/100 Iteration 202/234: loss=0.110170 lr=0.000020 grad_norm=0.502102
Epoch 23/100 Iteration 203/234: loss=0.110105 lr=0.000020 grad_norm=0.776478
Epoch 23/100 Iteration 204/234: loss=0.127195 lr=0.000020 grad_norm=1.066922
Epoch 23/100 Iteration 205/234: loss=0.118480 lr=0.000020 grad_norm=1.162318
Epoch 23/100 Iteration 206/234: loss=0.120981 lr=0.000020 grad_norm=0.427776
Epoch 23/100 Iteration 207/234: loss=0.117971 lr=0.000020 grad_norm=1.017610
Epoch 23/100 Iteration 208/234: loss=0.100128 lr=0.000020 grad_norm=1.202166
Epoch 23/100 Iteration 209/234: loss=0.110682 lr=0.000020 grad_norm=0.461265
Epoch 23/100 Iteration 210/234: loss=0.097768 lr=0.000020 grad_norm=0.755110
Epoch 23/100 Iteration 211/234: loss=0.099439 lr=0.000020 grad_norm=0.744640
Epoch 23/100 Iteration 212/234: loss=0.119438 lr=0.000020 grad_norm=0.501916
Epoch 23/100 Iteration 213/234: loss=0.122488 lr=0.000020 grad_norm=0.705390
Epoch 23/100 Iteration 214/234: loss=0.105078 lr=0.000020 grad_norm=0.647287
Epoch 23/100 Iteration 215/234: loss=0.114221 lr=0.000020 grad_norm=0.472029
Epoch 23/100 Iteration 216/234: loss=0.127400 lr=0.000020 grad_norm=0.508404
Epoch 23/100 Iteration 217/234: loss=0.093150 lr=0.000020 grad_norm=0.444977
Epoch 23/100 Iteration 218/234: loss=0.108384 lr=0.000020 grad_norm=0.351803
Epoch 23/100 Iteration 219/234: loss=0.110449 lr=0.000020 grad_norm=0.575836
Epoch 23/100 Iteration 220/234: loss=0.110790 lr=0.000020 grad_norm=0.717948
Epoch 23/100 Iteration 221/234: loss=0.120417 lr=0.000020 grad_norm=0.423130
Epoch 23/100 Iteration 222/234: loss=0.113111 lr=0.000020 grad_norm=0.775223
Epoch 23/100 Iteration 223/234: loss=0.097389 lr=0.000020 grad_norm=0.957485
Epoch 23/100 Iteration 224/234: loss=0.121932 lr=0.000020 grad_norm=0.914242
Epoch 23/100 Iteration 225/234: loss=0.111001 lr=0.000020 grad_norm=0.454391
Epoch 23/100 Iteration 226/234: loss=0.112421 lr=0.000020 grad_norm=0.949749
Epoch 23/100 Iteration 227/234: loss=0.104128 lr=0.000020 grad_norm=1.130339
Epoch 23/100 Iteration 228/234: loss=0.089312 lr=0.000020 grad_norm=0.691958
Epoch 23/100 Iteration 229/234: loss=0.111264 lr=0.000020 grad_norm=0.765232
Epoch 23/100 Iteration 230/234: loss=0.115707 lr=0.000020 grad_norm=0.711998
Epoch 23/100 Iteration 231/234: loss=0.128807 lr=0.000020 grad_norm=0.559513
Epoch 23/100 Iteration 232/234: loss=0.125307 lr=0.000020 grad_norm=0.996946
Epoch 23/100 Iteration 233/234: loss=0.121843 lr=0.000020 grad_norm=1.040264
Epoch 23/100 Iteration 234/234: loss=0.112299 lr=0.000020 grad_norm=0.511589
Epoch 23/100 finished. Avg Loss: 0.114019
Epoch 24/100 Iteration 1/234: loss=0.119743 lr=0.000020 grad_norm=1.207820
Epoch 24/100 Iteration 2/234: loss=0.096434 lr=0.000020 grad_norm=1.650463
Epoch 24/100 Iteration 3/234: loss=0.118003 lr=0.000020 grad_norm=1.070633
Epoch 24/100 Iteration 4/234: loss=0.107003 lr=0.000020 grad_norm=0.795154
Epoch 24/100 Iteration 5/234: loss=0.119006 lr=0.000020 grad_norm=1.475004
Epoch 24/100 Iteration 6/234: loss=0.098753 lr=0.000020 grad_norm=1.125662
Epoch 24/100 Iteration 7/234: loss=0.123579 lr=0.000020 grad_norm=0.748337
Epoch 24/100 Iteration 8/234: loss=0.103885 lr=0.000020 grad_norm=1.320196
Epoch 24/100 Iteration 9/234: loss=0.104422 lr=0.000020 grad_norm=0.856935
Epoch 24/100 Iteration 10/234: loss=0.117830 lr=0.000020 grad_norm=0.667787
Epoch 24/100 Iteration 11/234: loss=0.114629 lr=0.000020 grad_norm=0.813233
Epoch 24/100 Iteration 12/234: loss=0.116061 lr=0.000020 grad_norm=0.446481
Epoch 24/100 Iteration 13/234: loss=0.099096 lr=0.000020 grad_norm=0.644469
Epoch 24/100 Iteration 14/234: loss=0.121879 lr=0.000020 grad_norm=0.601417
Epoch 24/100 Iteration 15/234: loss=0.094393 lr=0.000020 grad_norm=0.304989
Epoch 24/100 Iteration 16/234: loss=0.108108 lr=0.000020 grad_norm=0.562971
Epoch 24/100 Iteration 17/234: loss=0.110167 lr=0.000020 grad_norm=0.468007
Epoch 24/100 Iteration 18/234: loss=0.107222 lr=0.000020 grad_norm=0.388988
Epoch 24/100 Iteration 19/234: loss=0.115816 lr=0.000020 grad_norm=0.481099
Epoch 24/100 Iteration 20/234: loss=0.119434 lr=0.000020 grad_norm=0.419824
Epoch 24/100 Iteration 21/234: loss=0.114483 lr=0.000020 grad_norm=0.568583
Epoch 24/100 Iteration 22/234: loss=0.112214 lr=0.000020 grad_norm=0.577286
Epoch 24/100 Iteration 23/234: loss=0.105243 lr=0.000020 grad_norm=0.615905
Epoch 24/100 Iteration 24/234: loss=0.112335 lr=0.000020 grad_norm=0.486206
Epoch 24/100 Iteration 25/234: loss=0.114893 lr=0.000020 grad_norm=0.612041
Epoch 24/100 Iteration 26/234: loss=0.114225 lr=0.000020 grad_norm=1.105866
Epoch 24/100 Iteration 27/234: loss=0.116579 lr=0.000020 grad_norm=1.316443
Epoch 24/100 Iteration 28/234: loss=0.113314 lr=0.000020 grad_norm=0.611907
Epoch 24/100 Iteration 29/234: loss=0.114137 lr=0.000020 grad_norm=0.832400
Epoch 24/100 Iteration 30/234: loss=0.103954 lr=0.000020 grad_norm=1.165070
Epoch 24/100 Iteration 31/234: loss=0.097282 lr=0.000020 grad_norm=0.714937
Epoch 24/100 Iteration 32/234: loss=0.118662 lr=0.000020 grad_norm=0.886593
Epoch 24/100 Iteration 33/234: loss=0.115260 lr=0.000020 grad_norm=0.945198
Epoch 24/100 Iteration 34/234: loss=0.115655 lr=0.000020 grad_norm=0.905752
Epoch 24/100 Iteration 35/234: loss=0.111800 lr=0.000020 grad_norm=0.675004
Epoch 24/100 Iteration 36/234: loss=0.097063 lr=0.000020 grad_norm=0.436834
Epoch 24/100 Iteration 37/234: loss=0.098227 lr=0.000020 grad_norm=0.481188
Epoch 24/100 Iteration 38/234: loss=0.110415 lr=0.000020 grad_norm=0.738151
Epoch 24/100 Iteration 39/234: loss=0.123034 lr=0.000020 grad_norm=0.675682
Epoch 24/100 Iteration 40/234: loss=0.091572 lr=0.000020 grad_norm=0.413433
Epoch 24/100 Iteration 41/234: loss=0.119932 lr=0.000020 grad_norm=0.657373
Epoch 24/100 Iteration 42/234: loss=0.108636 lr=0.000020 grad_norm=0.646120
Epoch 24/100 Iteration 43/234: loss=0.107316 lr=0.000020 grad_norm=0.659510
Epoch 24/100 Iteration 44/234: loss=0.123237 lr=0.000020 grad_norm=0.662504
Epoch 24/100 Iteration 45/234: loss=0.122920 lr=0.000020 grad_norm=0.713032
Epoch 24/100 Iteration 46/234: loss=0.103173 lr=0.000020 grad_norm=0.377206
Epoch 24/100 Iteration 47/234: loss=0.118931 lr=0.000020 grad_norm=0.700275
Epoch 24/100 Iteration 48/234: loss=0.113930 lr=0.000020 grad_norm=0.892283
Epoch 24/100 Iteration 49/234: loss=0.107132 lr=0.000020 grad_norm=0.951368
Epoch 24/100 Iteration 50/234: loss=0.097813 lr=0.000020 grad_norm=0.663602
Epoch 24/100 Iteration 51/234: loss=0.091480 lr=0.000020 grad_norm=0.672213
Epoch 24/100 Iteration 52/234: loss=0.125787 lr=0.000020 grad_norm=1.318013
Epoch 24/100 Iteration 53/234: loss=0.108472 lr=0.000020 grad_norm=1.046212
Epoch 24/100 Iteration 54/234: loss=0.099771 lr=0.000020 grad_norm=0.525989
Epoch 24/100 Iteration 55/234: loss=0.109847 lr=0.000020 grad_norm=0.995700
Epoch 24/100 Iteration 56/234: loss=0.115454 lr=0.000020 grad_norm=1.690236
Epoch 24/100 Iteration 57/234: loss=0.114661 lr=0.000020 grad_norm=1.707504
Epoch 24/100 Iteration 58/234: loss=0.100859 lr=0.000020 grad_norm=0.600209
Epoch 24/100 Iteration 59/234: loss=0.124808 lr=0.000020 grad_norm=1.186744
Epoch 24/100 Iteration 60/234: loss=0.105940 lr=0.000020 grad_norm=1.050662
Epoch 24/100 Iteration 61/234: loss=0.100431 lr=0.000020 grad_norm=0.615828
Epoch 24/100 Iteration 62/234: loss=0.090550 lr=0.000020 grad_norm=1.100662
Epoch 24/100 Iteration 63/234: loss=0.111731 lr=0.000020 grad_norm=0.665812
Epoch 24/100 Iteration 64/234: loss=0.100751 lr=0.000020 grad_norm=1.134712
Epoch 24/100 Iteration 65/234: loss=0.131978 lr=0.000020 grad_norm=1.294211
Epoch 24/100 Iteration 66/234: loss=0.106519 lr=0.000020 grad_norm=0.871768
Epoch 24/100 Iteration 67/234: loss=0.101988 lr=0.000020 grad_norm=0.619578
Epoch 24/100 Iteration 68/234: loss=0.109336 lr=0.000020 grad_norm=1.040444
Epoch 24/100 Iteration 69/234: loss=0.128493 lr=0.000020 grad_norm=1.070664
Epoch 24/100 Iteration 70/234: loss=0.098754 lr=0.000020 grad_norm=0.490814
Epoch 24/100 Iteration 71/234: loss=0.131129 lr=0.000020 grad_norm=0.569117
Epoch 24/100 Iteration 72/234: loss=0.107258 lr=0.000020 grad_norm=0.404856
Epoch 24/100 Iteration 73/234: loss=0.105045 lr=0.000020 grad_norm=0.604507
Epoch 24/100 Iteration 74/234: loss=0.116100 lr=0.000020 grad_norm=0.775929
Epoch 24/100 Iteration 75/234: loss=0.110875 lr=0.000020 grad_norm=0.556558
Epoch 24/100 Iteration 76/234: loss=0.113075 lr=0.000020 grad_norm=0.764441
Epoch 24/100 Iteration 77/234: loss=0.122976 lr=0.000020 grad_norm=0.760009
Epoch 24/100 Iteration 78/234: loss=0.096378 lr=0.000020 grad_norm=0.314582
Epoch 24/100 Iteration 79/234: loss=0.102362 lr=0.000020 grad_norm=0.598943
Epoch 24/100 Iteration 80/234: loss=0.111969 lr=0.000020 grad_norm=0.690519
Epoch 24/100 Iteration 81/234: loss=0.119546 lr=0.000020 grad_norm=0.706703
Epoch 24/100 Iteration 82/234: loss=0.113649 lr=0.000020 grad_norm=0.713335
Epoch 24/100 Iteration 83/234: loss=0.106910 lr=0.000020 grad_norm=0.639340
Epoch 24/100 Iteration 84/234: loss=0.117221 lr=0.000020 grad_norm=0.465466
Epoch 24/100 Iteration 85/234: loss=0.110620 lr=0.000020 grad_norm=1.022971
Epoch 24/100 Iteration 86/234: loss=0.100970 lr=0.000020 grad_norm=1.167607
Epoch 24/100 Iteration 87/234: loss=0.123747 lr=0.000020 grad_norm=1.295854
Epoch 24/100 Iteration 88/234: loss=0.117059 lr=0.000020 grad_norm=1.593167
Epoch 24/100 Iteration 89/234: loss=0.115364 lr=0.000020 grad_norm=1.072719
Epoch 24/100 Iteration 90/234: loss=0.097867 lr=0.000020 grad_norm=0.418566
Epoch 24/100 Iteration 91/234: loss=0.108862 lr=0.000020 grad_norm=0.880823
Epoch 24/100 Iteration 92/234: loss=0.110438 lr=0.000020 grad_norm=0.534145
Epoch 24/100 Iteration 93/234: loss=0.110100 lr=0.000020 grad_norm=1.376077
Epoch 24/100 Iteration 94/234: loss=0.103910 lr=0.000020 grad_norm=1.359107
Epoch 24/100 Iteration 95/234: loss=0.101018 lr=0.000020 grad_norm=0.442612
Epoch 24/100 Iteration 96/234: loss=0.111486 lr=0.000020 grad_norm=1.062974
Epoch 24/100 Iteration 97/234: loss=0.107134 lr=0.000020 grad_norm=0.502626
Epoch 24/100 Iteration 98/234: loss=0.122581 lr=0.000020 grad_norm=0.790325
Epoch 24/100 Iteration 99/234: loss=0.106231 lr=0.000020 grad_norm=0.885832
Epoch 24/100 Iteration 100/234: loss=0.111394 lr=0.000020 grad_norm=0.455877
Epoch 24/100 Iteration 101/234: loss=0.120185 lr=0.000020 grad_norm=0.727827
Epoch 24/100 Iteration 102/234: loss=0.103318 lr=0.000020 grad_norm=0.585901
Epoch 24/100 Iteration 103/234: loss=0.118668 lr=0.000020 grad_norm=0.637901
Epoch 24/100 Iteration 104/234: loss=0.106275 lr=0.000020 grad_norm=1.356680
Epoch 24/100 Iteration 105/234: loss=0.112171 lr=0.000020 grad_norm=1.419709
Epoch 24/100 Iteration 106/234: loss=0.111708 lr=0.000020 grad_norm=0.725999
Epoch 24/100 Iteration 107/234: loss=0.105519 lr=0.000020 grad_norm=0.537287
Epoch 24/100 Iteration 108/234: loss=0.117978 lr=0.000020 grad_norm=1.015829
Epoch 24/100 Iteration 109/234: loss=0.094541 lr=0.000020 grad_norm=0.592191
Epoch 24/100 Iteration 110/234: loss=0.105407 lr=0.000020 grad_norm=0.613061
Epoch 24/100 Iteration 111/234: loss=0.101343 lr=0.000020 grad_norm=1.056910
Epoch 24/100 Iteration 112/234: loss=0.112387 lr=0.000020 grad_norm=0.633819
Epoch 24/100 Iteration 113/234: loss=0.112517 lr=0.000020 grad_norm=0.666211
Epoch 24/100 Iteration 114/234: loss=0.110863 lr=0.000020 grad_norm=1.128561
Epoch 24/100 Iteration 115/234: loss=0.093230 lr=0.000020 grad_norm=0.824243
Epoch 24/100 Iteration 116/234: loss=0.096214 lr=0.000020 grad_norm=0.413221
Epoch 24/100 Iteration 117/234: loss=0.104163 lr=0.000020 grad_norm=0.758272
Epoch 24/100 Iteration 118/234: loss=0.101134 lr=0.000020 grad_norm=0.690006
Epoch 24/100 Iteration 119/234: loss=0.108536 lr=0.000020 grad_norm=0.407135
Epoch 24/100 Iteration 120/234: loss=0.097067 lr=0.000020 grad_norm=0.713122
Epoch 24/100 Iteration 121/234: loss=0.109317 lr=0.000020 grad_norm=0.508921
Epoch 24/100 Iteration 122/234: loss=0.114160 lr=0.000020 grad_norm=0.651849
Epoch 24/100 Iteration 123/234: loss=0.106729 lr=0.000020 grad_norm=0.709156
Epoch 24/100 Iteration 124/234: loss=0.109358 lr=0.000020 grad_norm=0.450732
Epoch 24/100 Iteration 125/234: loss=0.118356 lr=0.000020 grad_norm=0.885651
Epoch 24/100 Iteration 126/234: loss=0.115273 lr=0.000020 grad_norm=0.801844
Epoch 24/100 Iteration 127/234: loss=0.111460 lr=0.000020 grad_norm=0.632734
Epoch 24/100 Iteration 128/234: loss=0.108586 lr=0.000020 grad_norm=0.803571
Epoch 24/100 Iteration 129/234: loss=0.108414 lr=0.000020 grad_norm=1.226943
Epoch 24/100 Iteration 130/234: loss=0.113742 lr=0.000020 grad_norm=1.127326
Epoch 24/100 Iteration 131/234: loss=0.117303 lr=0.000020 grad_norm=0.510345
Epoch 24/100 Iteration 132/234: loss=0.110616 lr=0.000020 grad_norm=1.183692
Epoch 24/100 Iteration 133/234: loss=0.104184 lr=0.000020 grad_norm=1.860299
Epoch 24/100 Iteration 134/234: loss=0.109617 lr=0.000020 grad_norm=1.287676
Epoch 24/100 Iteration 135/234: loss=0.112245 lr=0.000020 grad_norm=0.566749
Epoch 24/100 Iteration 136/234: loss=0.132120 lr=0.000020 grad_norm=1.371750
Epoch 24/100 Iteration 137/234: loss=0.098396 lr=0.000020 grad_norm=1.142288
Epoch 24/100 Iteration 138/234: loss=0.106471 lr=0.000020 grad_norm=0.516245
Epoch 24/100 Iteration 139/234: loss=0.113604 lr=0.000020 grad_norm=0.838536
Epoch 24/100 Iteration 140/234: loss=0.109278 lr=0.000020 grad_norm=0.642059
Epoch 24/100 Iteration 141/234: loss=0.122968 lr=0.000020 grad_norm=0.531247
Epoch 24/100 Iteration 142/234: loss=0.117918 lr=0.000020 grad_norm=0.757116
Epoch 24/100 Iteration 143/234: loss=0.101656 lr=0.000020 grad_norm=0.674397
Epoch 24/100 Iteration 144/234: loss=0.095201 lr=0.000020 grad_norm=1.203560
Epoch 24/100 Iteration 145/234: loss=0.109928 lr=0.000020 grad_norm=1.016940
Epoch 24/100 Iteration 146/234: loss=0.115132 lr=0.000020 grad_norm=0.642964
Epoch 24/100 Iteration 147/234: loss=0.121092 lr=0.000020 grad_norm=2.040003
Epoch 24/100 Iteration 148/234: loss=0.119337 lr=0.000020 grad_norm=1.749919
Epoch 24/100 Iteration 149/234: loss=0.124195 lr=0.000020 grad_norm=0.698197
Epoch 24/100 Iteration 150/234: loss=0.118297 lr=0.000020 grad_norm=1.378969
Epoch 24/100 Iteration 151/234: loss=0.102091 lr=0.000020 grad_norm=1.441423
Epoch 24/100 Iteration 152/234: loss=0.131367 lr=0.000020 grad_norm=0.669116
Epoch 24/100 Iteration 153/234: loss=0.102591 lr=0.000020 grad_norm=1.049375
Epoch 24/100 Iteration 154/234: loss=0.121475 lr=0.000020 grad_norm=0.511428
Epoch 24/100 Iteration 155/234: loss=0.118821 lr=0.000020 grad_norm=0.872226
Epoch 24/100 Iteration 156/234: loss=0.098211 lr=0.000020 grad_norm=0.919818
Epoch 24/100 Iteration 157/234: loss=0.114928 lr=0.000020 grad_norm=0.457590
Epoch 24/100 Iteration 158/234: loss=0.111329 lr=0.000020 grad_norm=0.723927
Epoch 24/100 Iteration 159/234: loss=0.108624 lr=0.000020 grad_norm=0.566877
Epoch 24/100 Iteration 160/234: loss=0.113928 lr=0.000020 grad_norm=0.823501
Epoch 24/100 Iteration 161/234: loss=0.117854 lr=0.000020 grad_norm=1.276925
Epoch 24/100 Iteration 162/234: loss=0.115584 lr=0.000020 grad_norm=0.451213
Epoch 24/100 Iteration 163/234: loss=0.111398 lr=0.000020 grad_norm=1.117242
Epoch 24/100 Iteration 164/234: loss=0.103978 lr=0.000020 grad_norm=0.842838
Epoch 24/100 Iteration 165/234: loss=0.103873 lr=0.000020 grad_norm=0.546151
Epoch 24/100 Iteration 166/234: loss=0.107135 lr=0.000020 grad_norm=1.157905
Epoch 24/100 Iteration 167/234: loss=0.092464 lr=0.000020 grad_norm=0.712002
Epoch 24/100 Iteration 168/234: loss=0.121391 lr=0.000020 grad_norm=0.651361
Epoch 24/100 Iteration 169/234: loss=0.109168 lr=0.000020 grad_norm=1.064213
Epoch 24/100 Iteration 170/234: loss=0.107687 lr=0.000020 grad_norm=0.460642
Epoch 24/100 Iteration 171/234: loss=0.108299 lr=0.000020 grad_norm=0.832595
Epoch 24/100 Iteration 172/234: loss=0.109069 lr=0.000020 grad_norm=0.910422
Epoch 24/100 Iteration 173/234: loss=0.102601 lr=0.000020 grad_norm=0.405640
Epoch 24/100 Iteration 174/234: loss=0.109523 lr=0.000020 grad_norm=0.790582
Epoch 24/100 Iteration 175/234: loss=0.099487 lr=0.000020 grad_norm=0.581521
Epoch 24/100 Iteration 176/234: loss=0.128289 lr=0.000020 grad_norm=0.846749
Epoch 24/100 Iteration 177/234: loss=0.110303 lr=0.000020 grad_norm=1.196870
Epoch 24/100 Iteration 178/234: loss=0.121181 lr=0.000020 grad_norm=0.707292
Epoch 24/100 Iteration 179/234: loss=0.111072 lr=0.000020 grad_norm=0.753751
Epoch 24/100 Iteration 180/234: loss=0.111056 lr=0.000020 grad_norm=0.716215
Epoch 24/100 Iteration 181/234: loss=0.095590 lr=0.000020 grad_norm=0.426995
Epoch 24/100 Iteration 182/234: loss=0.121439 lr=0.000020 grad_norm=0.696747
Epoch 24/100 Iteration 183/234: loss=0.118600 lr=0.000020 grad_norm=0.822652
Epoch 24/100 Iteration 184/234: loss=0.103695 lr=0.000020 grad_norm=0.793810
Epoch 24/100 Iteration 185/234: loss=0.108401 lr=0.000020 grad_norm=0.564867
Epoch 24/100 Iteration 186/234: loss=0.113799 lr=0.000020 grad_norm=0.827027
Epoch 24/100 Iteration 187/234: loss=0.108825 lr=0.000020 grad_norm=0.822732
Epoch 24/100 Iteration 188/234: loss=0.120254 lr=0.000020 grad_norm=0.569756
Epoch 24/100 Iteration 189/234: loss=0.111452 lr=0.000020 grad_norm=0.643313
Epoch 24/100 Iteration 190/234: loss=0.111172 lr=0.000020 grad_norm=0.329414
Epoch 24/100 Iteration 191/234: loss=0.102858 lr=0.000020 grad_norm=0.470219
Epoch 24/100 Iteration 192/234: loss=0.108445 lr=0.000020 grad_norm=0.563152
Epoch 24/100 Iteration 193/234: loss=0.103614 lr=0.000020 grad_norm=0.525370
Epoch 24/100 Iteration 194/234: loss=0.094434 lr=0.000020 grad_norm=0.599503
Epoch 24/100 Iteration 195/234: loss=0.121538 lr=0.000020 grad_norm=0.640506
Epoch 24/100 Iteration 196/234: loss=0.108222 lr=0.000020 grad_norm=0.405374
Epoch 24/100 Iteration 197/234: loss=0.109426 lr=0.000020 grad_norm=0.428870
Epoch 24/100 Iteration 198/234: loss=0.100563 lr=0.000020 grad_norm=0.571018
Epoch 24/100 Iteration 199/234: loss=0.103312 lr=0.000020 grad_norm=0.648903
Epoch 24/100 Iteration 200/234: loss=0.116310 lr=0.000020 grad_norm=0.548077
Epoch 24/100 Iteration 201/234: loss=0.110755 lr=0.000020 grad_norm=0.709637
Epoch 24/100 Iteration 202/234: loss=0.109742 lr=0.000020 grad_norm=0.348801
Epoch 24/100 Iteration 203/234: loss=0.108497 lr=0.000020 grad_norm=0.624370
Epoch 24/100 Iteration 204/234: loss=0.100079 lr=0.000020 grad_norm=1.046132
Epoch 24/100 Iteration 205/234: loss=0.108602 lr=0.000020 grad_norm=1.027320
Epoch 24/100 Iteration 206/234: loss=0.118774 lr=0.000020 grad_norm=0.518902
Epoch 24/100 Iteration 207/234: loss=0.111929 lr=0.000020 grad_norm=0.774920
Epoch 24/100 Iteration 208/234: loss=0.099148 lr=0.000020 grad_norm=0.785425
Epoch 24/100 Iteration 209/234: loss=0.111904 lr=0.000020 grad_norm=0.547961
Epoch 24/100 Iteration 210/234: loss=0.110602 lr=0.000020 grad_norm=0.420970
Epoch 24/100 Iteration 211/234: loss=0.097858 lr=0.000020 grad_norm=0.561440
Epoch 24/100 Iteration 212/234: loss=0.105199 lr=0.000020 grad_norm=0.377099
Epoch 24/100 Iteration 213/234: loss=0.124197 lr=0.000020 grad_norm=0.459129
Epoch 24/100 Iteration 214/234: loss=0.104945 lr=0.000020 grad_norm=0.530491
Epoch 24/100 Iteration 215/234: loss=0.098564 lr=0.000020 grad_norm=0.721620
Epoch 24/100 Iteration 216/234: loss=0.104634 lr=0.000020 grad_norm=0.690440
Epoch 24/100 Iteration 217/234: loss=0.103939 lr=0.000020 grad_norm=0.445221
Epoch 24/100 Iteration 218/234: loss=0.106695 lr=0.000020 grad_norm=0.515756
Epoch 24/100 Iteration 219/234: loss=0.092317 lr=0.000020 grad_norm=0.694300
Epoch 24/100 Iteration 220/234: loss=0.113903 lr=0.000020 grad_norm=0.406040
Epoch 24/100 Iteration 221/234: loss=0.114419 lr=0.000020 grad_norm=0.679793
Epoch 24/100 Iteration 222/234: loss=0.107362 lr=0.000020 grad_norm=0.561997
Epoch 24/100 Iteration 223/234: loss=0.110643 lr=0.000020 grad_norm=0.565746
Epoch 24/100 Iteration 224/234: loss=0.109301 lr=0.000020 grad_norm=1.112614
Epoch 24/100 Iteration 225/234: loss=0.100587 lr=0.000020 grad_norm=0.719351
Epoch 24/100 Iteration 226/234: loss=0.117646 lr=0.000020 grad_norm=0.637680
Epoch 24/100 Iteration 227/234: loss=0.095791 lr=0.000020 grad_norm=1.275321
Epoch 24/100 Iteration 228/234: loss=0.119405 lr=0.000020 grad_norm=1.268715
Epoch 24/100 Iteration 229/234: loss=0.101391 lr=0.000020 grad_norm=0.603182
Epoch 24/100 Iteration 230/234: loss=0.106195 lr=0.000020 grad_norm=0.570448
Epoch 24/100 Iteration 231/234: loss=0.108776 lr=0.000020 grad_norm=0.830741
Epoch 24/100 Iteration 232/234: loss=0.104286 lr=0.000020 grad_norm=0.841208
Epoch 24/100 Iteration 233/234: loss=0.099614 lr=0.000020 grad_norm=0.592987
Epoch 24/100 Iteration 234/234: loss=0.117983 lr=0.000020 grad_norm=0.433514
Epoch 24/100 finished. Avg Loss: 0.109916
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 25/100 Iteration 1/234: loss=0.104589 lr=0.000020 grad_norm=0.551039
Epoch 25/100 Iteration 2/234: loss=0.108064 lr=0.000020 grad_norm=0.625311
Epoch 25/100 Iteration 3/234: loss=0.107095 lr=0.000020 grad_norm=0.373851
Epoch 25/100 Iteration 4/234: loss=0.107451 lr=0.000020 grad_norm=0.641971
Epoch 25/100 Iteration 5/234: loss=0.109449 lr=0.000020 grad_norm=1.203495
Epoch 25/100 Iteration 6/234: loss=0.086227 lr=0.000020 grad_norm=1.095932
Epoch 25/100 Iteration 7/234: loss=0.087366 lr=0.000020 grad_norm=0.453545
Epoch 25/100 Iteration 8/234: loss=0.115385 lr=0.000020 grad_norm=1.099928
Epoch 25/100 Iteration 9/234: loss=0.102350 lr=0.000020 grad_norm=1.464925
Epoch 25/100 Iteration 10/234: loss=0.108372 lr=0.000020 grad_norm=1.009892
Epoch 25/100 Iteration 11/234: loss=0.102753 lr=0.000020 grad_norm=0.654612
Epoch 25/100 Iteration 12/234: loss=0.099644 lr=0.000020 grad_norm=1.277345
Epoch 25/100 Iteration 13/234: loss=0.106319 lr=0.000020 grad_norm=0.774913
Epoch 25/100 Iteration 14/234: loss=0.116922 lr=0.000020 grad_norm=1.078681
Epoch 25/100 Iteration 15/234: loss=0.114957 lr=0.000020 grad_norm=1.627837
Epoch 25/100 Iteration 16/234: loss=0.107980 lr=0.000020 grad_norm=0.999349
Epoch 25/100 Iteration 17/234: loss=0.095916 lr=0.000020 grad_norm=0.817017
Epoch 25/100 Iteration 18/234: loss=0.111791 lr=0.000020 grad_norm=1.317317
Epoch 25/100 Iteration 19/234: loss=0.111835 lr=0.000020 grad_norm=1.067066
Epoch 25/100 Iteration 20/234: loss=0.107027 lr=0.000020 grad_norm=0.817350
Epoch 25/100 Iteration 21/234: loss=0.090489 lr=0.000020 grad_norm=0.929486
Epoch 25/100 Iteration 22/234: loss=0.099417 lr=0.000020 grad_norm=0.610790
Epoch 25/100 Iteration 23/234: loss=0.090646 lr=0.000020 grad_norm=1.362079
Epoch 25/100 Iteration 24/234: loss=0.113512 lr=0.000020 grad_norm=1.150815
Epoch 25/100 Iteration 25/234: loss=0.117747 lr=0.000020 grad_norm=0.573113
Epoch 25/100 Iteration 26/234: loss=0.109375 lr=0.000020 grad_norm=0.934536
Epoch 25/100 Iteration 27/234: loss=0.100284 lr=0.000020 grad_norm=0.351293
Epoch 25/100 Iteration 28/234: loss=0.097104 lr=0.000020 grad_norm=0.630793
Epoch 25/100 Iteration 29/234: loss=0.105446 lr=0.000020 grad_norm=0.430093
Epoch 25/100 Iteration 30/234: loss=0.115359 lr=0.000020 grad_norm=0.388940
Epoch 25/100 Iteration 31/234: loss=0.103445 lr=0.000020 grad_norm=0.358282
Epoch 25/100 Iteration 32/234: loss=0.108479 lr=0.000020 grad_norm=0.370299
Epoch 25/100 Iteration 33/234: loss=0.110966 lr=0.000020 grad_norm=0.355667
Epoch 25/100 Iteration 34/234: loss=0.112844 lr=0.000020 grad_norm=0.460420
Epoch 25/100 Iteration 35/234: loss=0.109536 lr=0.000020 grad_norm=0.426594
Epoch 25/100 Iteration 36/234: loss=0.104382 lr=0.000020 grad_norm=0.378834
Epoch 25/100 Iteration 37/234: loss=0.115529 lr=0.000020 grad_norm=0.634592
Epoch 25/100 Iteration 38/234: loss=0.096300 lr=0.000020 grad_norm=0.836580
Epoch 25/100 Iteration 39/234: loss=0.108196 lr=0.000020 grad_norm=0.795889
Epoch 25/100 Iteration 40/234: loss=0.114322 lr=0.000020 grad_norm=0.794110
Epoch 25/100 Iteration 41/234: loss=0.098704 lr=0.000020 grad_norm=0.447704
Epoch 25/100 Iteration 42/234: loss=0.118601 lr=0.000020 grad_norm=0.510832
Epoch 25/100 Iteration 43/234: loss=0.120104 lr=0.000020 grad_norm=0.818174
Epoch 25/100 Iteration 44/234: loss=0.108485 lr=0.000020 grad_norm=1.118959
Epoch 25/100 Iteration 45/234: loss=0.121529 lr=0.000020 grad_norm=0.933897
Epoch 25/100 Iteration 46/234: loss=0.101734 lr=0.000020 grad_norm=0.382136
Epoch 25/100 Iteration 47/234: loss=0.103413 lr=0.000020 grad_norm=0.840989
Epoch 25/100 Iteration 48/234: loss=0.109781 lr=0.000020 grad_norm=1.280787
Epoch 25/100 Iteration 49/234: loss=0.098107 lr=0.000020 grad_norm=0.574265
Epoch 25/100 Iteration 50/234: loss=0.111271 lr=0.000020 grad_norm=0.897732
Epoch 25/100 Iteration 51/234: loss=0.107834 lr=0.000020 grad_norm=1.513680
Epoch 25/100 Iteration 52/234: loss=0.108148 lr=0.000020 grad_norm=0.937117
Epoch 25/100 Iteration 53/234: loss=0.116632 lr=0.000020 grad_norm=0.613151
Epoch 25/100 Iteration 54/234: loss=0.116254 lr=0.000020 grad_norm=1.434877
Epoch 25/100 Iteration 55/234: loss=0.103031 lr=0.000020 grad_norm=0.948904
Epoch 25/100 Iteration 56/234: loss=0.122014 lr=0.000020 grad_norm=0.698711
Epoch 25/100 Iteration 57/234: loss=0.107755 lr=0.000020 grad_norm=1.253205
Epoch 25/100 Iteration 58/234: loss=0.102734 lr=0.000020 grad_norm=0.943769
Epoch 25/100 Iteration 59/234: loss=0.114040 lr=0.000020 grad_norm=0.632223
Epoch 25/100 Iteration 60/234: loss=0.105267 lr=0.000020 grad_norm=1.401175
Epoch 25/100 Iteration 61/234: loss=0.122136 lr=0.000020 grad_norm=0.960115
Epoch 25/100 Iteration 62/234: loss=0.101126 lr=0.000020 grad_norm=0.513061
Epoch 25/100 Iteration 63/234: loss=0.111908 lr=0.000020 grad_norm=1.240569
Epoch 25/100 Iteration 64/234: loss=0.095033 lr=0.000020 grad_norm=0.695144
Epoch 25/100 Iteration 65/234: loss=0.106487 lr=0.000020 grad_norm=1.006248
Epoch 25/100 Iteration 66/234: loss=0.102953 lr=0.000020 grad_norm=1.382134
Epoch 25/100 Iteration 67/234: loss=0.108212 lr=0.000020 grad_norm=0.628729
Epoch 25/100 Iteration 68/234: loss=0.103047 lr=0.000020 grad_norm=1.007280
Epoch 25/100 Iteration 69/234: loss=0.116532 lr=0.000020 grad_norm=1.402880
Epoch 25/100 Iteration 70/234: loss=0.127754 lr=0.000020 grad_norm=0.738619
Epoch 25/100 Iteration 71/234: loss=0.107045 lr=0.000020 grad_norm=0.726424
Epoch 25/100 Iteration 72/234: loss=0.105358 lr=0.000020 grad_norm=0.988200
Epoch 25/100 Iteration 73/234: loss=0.115974 lr=0.000020 grad_norm=0.605961
Epoch 25/100 Iteration 74/234: loss=0.105791 lr=0.000020 grad_norm=0.540060
Epoch 25/100 Iteration 75/234: loss=0.104010 lr=0.000020 grad_norm=0.416553
Epoch 25/100 Iteration 76/234: loss=0.113021 lr=0.000020 grad_norm=0.613708
Epoch 25/100 Iteration 77/234: loss=0.103686 lr=0.000020 grad_norm=0.729771
Epoch 25/100 Iteration 78/234: loss=0.107801 lr=0.000020 grad_norm=0.490385
Epoch 25/100 Iteration 79/234: loss=0.104602 lr=0.000020 grad_norm=0.497882
Epoch 25/100 Iteration 80/234: loss=0.116926 lr=0.000020 grad_norm=0.475173
Epoch 25/100 Iteration 81/234: loss=0.120398 lr=0.000020 grad_norm=0.584240
Epoch 25/100 Iteration 82/234: loss=0.121113 lr=0.000020 grad_norm=0.671296
Epoch 25/100 Iteration 83/234: loss=0.114808 lr=0.000020 grad_norm=0.629968
Epoch 25/100 Iteration 84/234: loss=0.107197 lr=0.000020 grad_norm=0.506109
Epoch 25/100 Iteration 85/234: loss=0.104772 lr=0.000020 grad_norm=0.665846
Epoch 25/100 Iteration 86/234: loss=0.106609 lr=0.000020 grad_norm=0.500457
Epoch 25/100 Iteration 87/234: loss=0.101302 lr=0.000020 grad_norm=0.416821
Epoch 25/100 Iteration 88/234: loss=0.108750 lr=0.000020 grad_norm=0.895442
Epoch 25/100 Iteration 89/234: loss=0.102622 lr=0.000020 grad_norm=0.878724
Epoch 25/100 Iteration 90/234: loss=0.117413 lr=0.000020 grad_norm=0.444924
Epoch 25/100 Iteration 91/234: loss=0.105064 lr=0.000020 grad_norm=0.856081
Epoch 25/100 Iteration 92/234: loss=0.108592 lr=0.000020 grad_norm=1.192743
Epoch 25/100 Iteration 93/234: loss=0.107694 lr=0.000020 grad_norm=0.516616
Epoch 25/100 Iteration 94/234: loss=0.114353 lr=0.000020 grad_norm=0.899475
Epoch 25/100 Iteration 95/234: loss=0.099419 lr=0.000020 grad_norm=1.214540
Epoch 25/100 Iteration 96/234: loss=0.107210 lr=0.000020 grad_norm=0.466935
Epoch 25/100 Iteration 97/234: loss=0.116281 lr=0.000020 grad_norm=1.016713
Epoch 25/100 Iteration 98/234: loss=0.111433 lr=0.000020 grad_norm=0.941094
Epoch 25/100 Iteration 99/234: loss=0.098377 lr=0.000020 grad_norm=0.447472
Epoch 25/100 Iteration 100/234: loss=0.119088 lr=0.000020 grad_norm=0.829613
Epoch 25/100 Iteration 101/234: loss=0.093275 lr=0.000020 grad_norm=0.954295
Epoch 25/100 Iteration 102/234: loss=0.102381 lr=0.000020 grad_norm=0.441641
Epoch 25/100 Iteration 103/234: loss=0.106960 lr=0.000020 grad_norm=1.216078
Epoch 25/100 Iteration 104/234: loss=0.107384 lr=0.000020 grad_norm=0.975632
Epoch 25/100 Iteration 105/234: loss=0.097007 lr=0.000020 grad_norm=0.553901
Epoch 25/100 Iteration 106/234: loss=0.107848 lr=0.000020 grad_norm=1.031860
Epoch 25/100 Iteration 107/234: loss=0.115829 lr=0.000020 grad_norm=0.988653
Epoch 25/100 Iteration 108/234: loss=0.102234 lr=0.000020 grad_norm=0.672128
Epoch 25/100 Iteration 109/234: loss=0.102754 lr=0.000020 grad_norm=1.168280
Epoch 25/100 Iteration 110/234: loss=0.098486 lr=0.000020 grad_norm=1.332105
Epoch 25/100 Iteration 111/234: loss=0.101451 lr=0.000020 grad_norm=0.685444
Epoch 25/100 Iteration 112/234: loss=0.110093 lr=0.000020 grad_norm=0.948408
Epoch 25/100 Iteration 113/234: loss=0.107650 lr=0.000020 grad_norm=0.879521
Epoch 25/100 Iteration 114/234: loss=0.104390 lr=0.000020 grad_norm=0.679682
Epoch 25/100 Iteration 115/234: loss=0.096893 lr=0.000020 grad_norm=0.714623
Epoch 25/100 Iteration 116/234: loss=0.108396 lr=0.000020 grad_norm=0.820650
Epoch 25/100 Iteration 117/234: loss=0.092583 lr=0.000020 grad_norm=1.000325
Epoch 25/100 Iteration 118/234: loss=0.102484 lr=0.000020 grad_norm=0.968816
Epoch 25/100 Iteration 119/234: loss=0.120833 lr=0.000020 grad_norm=0.641409
Epoch 25/100 Iteration 120/234: loss=0.107531 lr=0.000020 grad_norm=0.922037
Epoch 25/100 Iteration 121/234: loss=0.103785 lr=0.000020 grad_norm=0.870650
Epoch 25/100 Iteration 122/234: loss=0.099155 lr=0.000020 grad_norm=0.413685
Epoch 25/100 Iteration 123/234: loss=0.100330 lr=0.000020 grad_norm=0.897743
Epoch 25/100 Iteration 124/234: loss=0.109150 lr=0.000020 grad_norm=0.691929
Epoch 25/100 Iteration 125/234: loss=0.109867 lr=0.000020 grad_norm=0.694344
Epoch 25/100 Iteration 126/234: loss=0.090995 lr=0.000020 grad_norm=0.703562
Epoch 25/100 Iteration 127/234: loss=0.096556 lr=0.000020 grad_norm=0.640135
Epoch 25/100 Iteration 128/234: loss=0.093166 lr=0.000020 grad_norm=0.812968
Epoch 25/100 Iteration 129/234: loss=0.102636 lr=0.000020 grad_norm=0.462459
Epoch 25/100 Iteration 130/234: loss=0.109013 lr=0.000020 grad_norm=0.886301
Epoch 25/100 Iteration 131/234: loss=0.102530 lr=0.000020 grad_norm=0.663503
Epoch 25/100 Iteration 132/234: loss=0.109333 lr=0.000020 grad_norm=0.394383
Epoch 25/100 Iteration 133/234: loss=0.105168 lr=0.000020 grad_norm=0.645698
Epoch 25/100 Iteration 134/234: loss=0.100239 lr=0.000020 grad_norm=0.341700
Epoch 25/100 Iteration 135/234: loss=0.116069 lr=0.000020 grad_norm=0.726342
Epoch 25/100 Iteration 136/234: loss=0.099304 lr=0.000020 grad_norm=0.830965
Epoch 25/100 Iteration 137/234: loss=0.096350 lr=0.000020 grad_norm=0.373019
Epoch 25/100 Iteration 138/234: loss=0.116037 lr=0.000020 grad_norm=0.831966
Epoch 25/100 Iteration 139/234: loss=0.119755 lr=0.000020 grad_norm=0.924453
Epoch 25/100 Iteration 140/234: loss=0.099725 lr=0.000020 grad_norm=0.307388
Epoch 25/100 Iteration 141/234: loss=0.122724 lr=0.000020 grad_norm=1.049231
Epoch 25/100 Iteration 142/234: loss=0.106255 lr=0.000020 grad_norm=1.739914
Epoch 25/100 Iteration 143/234: loss=0.126284 lr=0.000020 grad_norm=2.131086
Epoch 25/100 Iteration 144/234: loss=0.097121 lr=0.000020 grad_norm=1.263936
Epoch 25/100 Iteration 145/234: loss=0.109470 lr=0.000020 grad_norm=0.857721
Epoch 25/100 Iteration 146/234: loss=0.114892 lr=0.000020 grad_norm=1.522983
Epoch 25/100 Iteration 147/234: loss=0.103861 lr=0.000020 grad_norm=1.147542
Epoch 25/100 Iteration 148/234: loss=0.103255 lr=0.000020 grad_norm=0.923964
Epoch 25/100 Iteration 149/234: loss=0.116922 lr=0.000020 grad_norm=1.243283
Epoch 25/100 Iteration 150/234: loss=0.099846 lr=0.000020 grad_norm=0.850050
Epoch 25/100 Iteration 151/234: loss=0.110545 lr=0.000020 grad_norm=0.863019
Epoch 25/100 Iteration 152/234: loss=0.111522 lr=0.000020 grad_norm=0.574715
Epoch 25/100 Iteration 153/234: loss=0.107614 lr=0.000020 grad_norm=0.650570
Epoch 25/100 Iteration 154/234: loss=0.096370 lr=0.000020 grad_norm=0.815380
Epoch 25/100 Iteration 155/234: loss=0.105919 lr=0.000020 grad_norm=0.571354
Epoch 25/100 Iteration 156/234: loss=0.099763 lr=0.000020 grad_norm=0.672899
Epoch 25/100 Iteration 157/234: loss=0.104802 lr=0.000020 grad_norm=1.022756
Epoch 25/100 Iteration 158/234: loss=0.116665 lr=0.000020 grad_norm=0.612229
Epoch 25/100 Iteration 159/234: loss=0.098783 lr=0.000020 grad_norm=0.653682
Epoch 25/100 Iteration 160/234: loss=0.106359 lr=0.000020 grad_norm=0.522963
Epoch 25/100 Iteration 161/234: loss=0.122562 lr=0.000020 grad_norm=0.455108
Epoch 25/100 Iteration 162/234: loss=0.102732 lr=0.000020 grad_norm=0.576547
Epoch 25/100 Iteration 163/234: loss=0.111286 lr=0.000020 grad_norm=0.358578
Epoch 25/100 Iteration 164/234: loss=0.109144 lr=0.000020 grad_norm=0.628831
Epoch 25/100 Iteration 165/234: loss=0.104024 lr=0.000020 grad_norm=0.461776
Epoch 25/100 Iteration 166/234: loss=0.098221 lr=0.000020 grad_norm=0.456727
Epoch 25/100 Iteration 167/234: loss=0.093344 lr=0.000020 grad_norm=0.422676
Epoch 25/100 Iteration 168/234: loss=0.095387 lr=0.000020 grad_norm=0.351169
Epoch 25/100 Iteration 169/234: loss=0.108134 lr=0.000020 grad_norm=0.430345
Epoch 25/100 Iteration 170/234: loss=0.103578 lr=0.000020 grad_norm=0.470604
Epoch 25/100 Iteration 171/234: loss=0.111510 lr=0.000020 grad_norm=0.590111
Epoch 25/100 Iteration 172/234: loss=0.101078 lr=0.000020 grad_norm=0.622483
Epoch 25/100 Iteration 173/234: loss=0.099761 lr=0.000020 grad_norm=0.388363
Epoch 25/100 Iteration 174/234: loss=0.108067 lr=0.000020 grad_norm=0.597833
Epoch 25/100 Iteration 175/234: loss=0.109449 lr=0.000020 grad_norm=0.779485
Epoch 25/100 Iteration 176/234: loss=0.123673 lr=0.000020 grad_norm=0.523792
Epoch 25/100 Iteration 177/234: loss=0.104840 lr=0.000020 grad_norm=0.542751
Epoch 25/100 Iteration 178/234: loss=0.105670 lr=0.000020 grad_norm=0.869794
Epoch 25/100 Iteration 179/234: loss=0.109795 lr=0.000020 grad_norm=0.647888
Epoch 25/100 Iteration 180/234: loss=0.127150 lr=0.000020 grad_norm=0.547872
Epoch 25/100 Iteration 181/234: loss=0.113359 lr=0.000020 grad_norm=0.484881
Epoch 25/100 Iteration 182/234: loss=0.089270 lr=0.000020 grad_norm=0.396016
Epoch 25/100 Iteration 183/234: loss=0.110833 lr=0.000020 grad_norm=0.760837
Epoch 25/100 Iteration 184/234: loss=0.112552 lr=0.000020 grad_norm=0.757789
Epoch 25/100 Iteration 185/234: loss=0.101531 lr=0.000020 grad_norm=0.318171
Epoch 25/100 Iteration 186/234: loss=0.099119 lr=0.000020 grad_norm=0.705845
Epoch 25/100 Iteration 187/234: loss=0.093180 lr=0.000020 grad_norm=0.869087
Epoch 25/100 Iteration 188/234: loss=0.094911 lr=0.000020 grad_norm=0.593942
Epoch 25/100 Iteration 189/234: loss=0.110288 lr=0.000020 grad_norm=1.083967
Epoch 25/100 Iteration 190/234: loss=0.109437 lr=0.000020 grad_norm=1.557196
Epoch 25/100 Iteration 191/234: loss=0.112750 lr=0.000020 grad_norm=0.993216
Epoch 25/100 Iteration 192/234: loss=0.107669 lr=0.000020 grad_norm=0.444276
Epoch 25/100 Iteration 193/234: loss=0.094648 lr=0.000020 grad_norm=1.020755
Epoch 25/100 Iteration 194/234: loss=0.109665 lr=0.000020 grad_norm=1.040152
Epoch 25/100 Iteration 195/234: loss=0.108045 lr=0.000020 grad_norm=0.600529
Epoch 25/100 Iteration 196/234: loss=0.120297 lr=0.000020 grad_norm=0.740025
Epoch 25/100 Iteration 197/234: loss=0.092907 lr=0.000020 grad_norm=0.579651
Epoch 25/100 Iteration 198/234: loss=0.104588 lr=0.000020 grad_norm=0.619640
Epoch 25/100 Iteration 199/234: loss=0.102121 lr=0.000020 grad_norm=0.705466
Epoch 25/100 Iteration 200/234: loss=0.115241 lr=0.000020 grad_norm=0.695298
Epoch 25/100 Iteration 201/234: loss=0.093989 lr=0.000020 grad_norm=0.421927
Epoch 25/100 Iteration 202/234: loss=0.118437 lr=0.000020 grad_norm=0.894469
Epoch 25/100 Iteration 203/234: loss=0.102363 lr=0.000020 grad_norm=0.667208
Epoch 25/100 Iteration 204/234: loss=0.097631 lr=0.000020 grad_norm=0.784758
Epoch 25/100 Iteration 205/234: loss=0.090592 lr=0.000020 grad_norm=1.562195
Epoch 25/100 Iteration 206/234: loss=0.113275 lr=0.000020 grad_norm=1.241537
Epoch 25/100 Iteration 207/234: loss=0.093649 lr=0.000020 grad_norm=0.654119
Epoch 25/100 Iteration 208/234: loss=0.097389 lr=0.000020 grad_norm=1.211609
Epoch 25/100 Iteration 209/234: loss=0.116054 lr=0.000020 grad_norm=1.063147
Epoch 25/100 Iteration 210/234: loss=0.103905 lr=0.000020 grad_norm=0.720568
Epoch 25/100 Iteration 211/234: loss=0.107054 lr=0.000020 grad_norm=0.691870
Epoch 25/100 Iteration 212/234: loss=0.118294 lr=0.000020 grad_norm=0.651436
Epoch 25/100 Iteration 213/234: loss=0.110129 lr=0.000020 grad_norm=0.715647
Epoch 25/100 Iteration 214/234: loss=0.103701 lr=0.000020 grad_norm=0.651298
Epoch 25/100 Iteration 215/234: loss=0.102484 lr=0.000020 grad_norm=0.482249
Epoch 25/100 Iteration 216/234: loss=0.103291 lr=0.000020 grad_norm=0.498278
Epoch 25/100 Iteration 217/234: loss=0.095904 lr=0.000020 grad_norm=0.650514
Epoch 25/100 Iteration 218/234: loss=0.113599 lr=0.000020 grad_norm=0.814686
Epoch 25/100 Iteration 219/234: loss=0.110941 lr=0.000020 grad_norm=0.603187
Epoch 25/100 Iteration 220/234: loss=0.104271 lr=0.000020 grad_norm=0.487453
Epoch 25/100 Iteration 221/234: loss=0.111181 lr=0.000020 grad_norm=1.225916
Epoch 25/100 Iteration 222/234: loss=0.119350 lr=0.000020 grad_norm=1.535792
Epoch 25/100 Iteration 223/234: loss=0.098384 lr=0.000020 grad_norm=0.904004
Epoch 25/100 Iteration 224/234: loss=0.112201 lr=0.000020 grad_norm=0.518285
Epoch 25/100 Iteration 225/234: loss=0.099483 lr=0.000020 grad_norm=1.209038
Epoch 25/100 Iteration 226/234: loss=0.112086 lr=0.000020 grad_norm=1.196716
Epoch 25/100 Iteration 227/234: loss=0.108717 lr=0.000020 grad_norm=0.556623
Epoch 25/100 Iteration 228/234: loss=0.099487 lr=0.000020 grad_norm=0.767971
Epoch 25/100 Iteration 229/234: loss=0.111608 lr=0.000020 grad_norm=0.962455
Epoch 25/100 Iteration 230/234: loss=0.103264 lr=0.000020 grad_norm=0.505532
Epoch 25/100 Iteration 231/234: loss=0.103486 lr=0.000020 grad_norm=0.700150
Epoch 25/100 Iteration 232/234: loss=0.108083 lr=0.000020 grad_norm=0.621472
Epoch 25/100 Iteration 233/234: loss=0.084478 lr=0.000020 grad_norm=0.537370
Epoch 25/100 Iteration 234/234: loss=0.103847 lr=0.000020 grad_norm=0.635146
Epoch 25/100 finished. Avg Loss: 0.106629
Epoch 26/100 Iteration 1/234: loss=0.094240 lr=0.000020 grad_norm=0.579604
Epoch 26/100 Iteration 2/234: loss=0.108290 lr=0.000020 grad_norm=0.552470
Epoch 26/100 Iteration 3/234: loss=0.101961 lr=0.000020 grad_norm=0.524381
Epoch 26/100 Iteration 4/234: loss=0.103014 lr=0.000020 grad_norm=0.853056
Epoch 26/100 Iteration 5/234: loss=0.105588 lr=0.000020 grad_norm=1.164658
Epoch 26/100 Iteration 6/234: loss=0.097500 lr=0.000020 grad_norm=0.751486
Epoch 26/100 Iteration 7/234: loss=0.090972 lr=0.000020 grad_norm=0.719806
Epoch 26/100 Iteration 8/234: loss=0.098827 lr=0.000020 grad_norm=0.734139
Epoch 26/100 Iteration 9/234: loss=0.109331 lr=0.000020 grad_norm=0.775429
Epoch 26/100 Iteration 10/234: loss=0.095596 lr=0.000020 grad_norm=1.127173
Epoch 26/100 Iteration 11/234: loss=0.102817 lr=0.000020 grad_norm=0.731418
Epoch 26/100 Iteration 12/234: loss=0.092312 lr=0.000020 grad_norm=0.799086
Epoch 26/100 Iteration 13/234: loss=0.103452 lr=0.000020 grad_norm=0.862327
Epoch 26/100 Iteration 14/234: loss=0.097041 lr=0.000020 grad_norm=1.078750
Epoch 26/100 Iteration 15/234: loss=0.089598 lr=0.000020 grad_norm=0.718833
Epoch 26/100 Iteration 16/234: loss=0.123236 lr=0.000020 grad_norm=1.076610
Epoch 26/100 Iteration 17/234: loss=0.106788 lr=0.000020 grad_norm=1.547029
Epoch 26/100 Iteration 18/234: loss=0.120757 lr=0.000020 grad_norm=1.269353
Epoch 26/100 Iteration 19/234: loss=0.109057 lr=0.000020 grad_norm=0.927111
Epoch 26/100 Iteration 20/234: loss=0.120279 lr=0.000020 grad_norm=0.929383
Epoch 26/100 Iteration 21/234: loss=0.110973 lr=0.000020 grad_norm=1.240741
Epoch 26/100 Iteration 22/234: loss=0.101589 lr=0.000020 grad_norm=0.681473
Epoch 26/100 Iteration 23/234: loss=0.092455 lr=0.000020 grad_norm=0.647353
Epoch 26/100 Iteration 24/234: loss=0.111015 lr=0.000020 grad_norm=0.656836
Epoch 26/100 Iteration 25/234: loss=0.118080 lr=0.000020 grad_norm=0.680239
Epoch 26/100 Iteration 26/234: loss=0.107704 lr=0.000020 grad_norm=0.401864
Epoch 26/100 Iteration 27/234: loss=0.111353 lr=0.000020 grad_norm=0.653458
Epoch 26/100 Iteration 28/234: loss=0.105586 lr=0.000020 grad_norm=0.601065
Epoch 26/100 Iteration 29/234: loss=0.113017 lr=0.000020 grad_norm=0.950629
Epoch 26/100 Iteration 30/234: loss=0.107153 lr=0.000020 grad_norm=0.952815
Epoch 26/100 Iteration 31/234: loss=0.096168 lr=0.000020 grad_norm=0.825999
Epoch 26/100 Iteration 32/234: loss=0.093418 lr=0.000020 grad_norm=0.455620
Epoch 26/100 Iteration 33/234: loss=0.114491 lr=0.000020 grad_norm=0.819707
Epoch 26/100 Iteration 34/234: loss=0.099053 lr=0.000020 grad_norm=0.442808
Epoch 26/100 Iteration 35/234: loss=0.092919 lr=0.000020 grad_norm=0.533201
Epoch 26/100 Iteration 36/234: loss=0.105983 lr=0.000020 grad_norm=0.399913
Epoch 26/100 Iteration 37/234: loss=0.115470 lr=0.000020 grad_norm=0.545419
Epoch 26/100 Iteration 38/234: loss=0.095441 lr=0.000020 grad_norm=0.357621
Epoch 26/100 Iteration 39/234: loss=0.108711 lr=0.000020 grad_norm=0.343412
Epoch 26/100 Iteration 40/234: loss=0.097112 lr=0.000020 grad_norm=0.318752
Epoch 26/100 Iteration 41/234: loss=0.120734 lr=0.000020 grad_norm=0.779619
Epoch 26/100 Iteration 42/234: loss=0.099803 lr=0.000020 grad_norm=0.737684
Epoch 26/100 Iteration 43/234: loss=0.088744 lr=0.000020 grad_norm=0.327421
Epoch 26/100 Iteration 44/234: loss=0.103002 lr=0.000020 grad_norm=0.822821
Epoch 26/100 Iteration 45/234: loss=0.112825 lr=0.000020 grad_norm=1.033777
Epoch 26/100 Iteration 46/234: loss=0.089986 lr=0.000020 grad_norm=0.493745
Epoch 26/100 Iteration 47/234: loss=0.108749 lr=0.000020 grad_norm=0.568684
Epoch 26/100 Iteration 48/234: loss=0.102775 lr=0.000020 grad_norm=1.048052
Epoch 26/100 Iteration 49/234: loss=0.096010 lr=0.000020 grad_norm=0.926686
Epoch 26/100 Iteration 50/234: loss=0.102314 lr=0.000020 grad_norm=0.278672
Epoch 26/100 Iteration 51/234: loss=0.092005 lr=0.000020 grad_norm=0.784693
Epoch 26/100 Iteration 52/234: loss=0.104466 lr=0.000020 grad_norm=0.546074
Epoch 26/100 Iteration 53/234: loss=0.105211 lr=0.000020 grad_norm=0.586236
Epoch 26/100 Iteration 54/234: loss=0.103581 lr=0.000020 grad_norm=0.549621
Epoch 26/100 Iteration 55/234: loss=0.113142 lr=0.000020 grad_norm=0.747127
Epoch 26/100 Iteration 56/234: loss=0.100135 lr=0.000020 grad_norm=1.499311
Epoch 26/100 Iteration 57/234: loss=0.095841 lr=0.000020 grad_norm=1.114362
Epoch 26/100 Iteration 58/234: loss=0.101963 lr=0.000020 grad_norm=0.686627
Epoch 26/100 Iteration 59/234: loss=0.099738 lr=0.000020 grad_norm=0.971544
Epoch 26/100 Iteration 60/234: loss=0.104331 lr=0.000020 grad_norm=0.989794
Epoch 26/100 Iteration 61/234: loss=0.090428 lr=0.000020 grad_norm=0.490405
Epoch 26/100 Iteration 62/234: loss=0.108119 lr=0.000020 grad_norm=0.741400
Epoch 26/100 Iteration 63/234: loss=0.098192 lr=0.000020 grad_norm=1.128303
Epoch 26/100 Iteration 64/234: loss=0.093682 lr=0.000020 grad_norm=0.748225
Epoch 26/100 Iteration 65/234: loss=0.111368 lr=0.000020 grad_norm=0.477809
Epoch 26/100 Iteration 66/234: loss=0.091657 lr=0.000020 grad_norm=1.114886
Epoch 26/100 Iteration 67/234: loss=0.098168 lr=0.000020 grad_norm=1.094387
Epoch 26/100 Iteration 68/234: loss=0.098085 lr=0.000020 grad_norm=0.426185
Epoch 26/100 Iteration 69/234: loss=0.103627 lr=0.000020 grad_norm=0.609891
Epoch 26/100 Iteration 70/234: loss=0.104697 lr=0.000020 grad_norm=0.838493
Epoch 26/100 Iteration 71/234: loss=0.091553 lr=0.000020 grad_norm=0.320293
Epoch 26/100 Iteration 72/234: loss=0.114151 lr=0.000020 grad_norm=0.844271
Epoch 26/100 Iteration 73/234: loss=0.092679 lr=0.000020 grad_norm=0.903274
Epoch 26/100 Iteration 74/234: loss=0.108498 lr=0.000020 grad_norm=0.539258
Epoch 26/100 Iteration 75/234: loss=0.094613 lr=0.000020 grad_norm=0.410154
Epoch 26/100 Iteration 76/234: loss=0.089981 lr=0.000020 grad_norm=0.668477
Epoch 26/100 Iteration 77/234: loss=0.106074 lr=0.000020 grad_norm=0.381858
Epoch 26/100 Iteration 78/234: loss=0.119186 lr=0.000020 grad_norm=0.481137
Epoch 26/100 Iteration 79/234: loss=0.098714 lr=0.000020 grad_norm=0.737490
Epoch 26/100 Iteration 80/234: loss=0.092383 lr=0.000020 grad_norm=0.588648
Epoch 26/100 Iteration 81/234: loss=0.103390 lr=0.000020 grad_norm=0.767453
Epoch 26/100 Iteration 82/234: loss=0.081611 lr=0.000020 grad_norm=0.860349
Epoch 26/100 Iteration 83/234: loss=0.107482 lr=0.000020 grad_norm=0.732575
Epoch 26/100 Iteration 84/234: loss=0.108605 lr=0.000020 grad_norm=0.603161
Epoch 26/100 Iteration 85/234: loss=0.102952 lr=0.000020 grad_norm=0.417713
Epoch 26/100 Iteration 86/234: loss=0.103652 lr=0.000020 grad_norm=1.007948
Epoch 26/100 Iteration 87/234: loss=0.089449 lr=0.000020 grad_norm=1.015297
Epoch 26/100 Iteration 88/234: loss=0.094016 lr=0.000020 grad_norm=0.508100
Epoch 26/100 Iteration 89/234: loss=0.110803 lr=0.000020 grad_norm=0.567499
Epoch 26/100 Iteration 90/234: loss=0.107577 lr=0.000020 grad_norm=0.626206
Epoch 26/100 Iteration 91/234: loss=0.111190 lr=0.000020 grad_norm=0.592810
Epoch 26/100 Iteration 92/234: loss=0.108925 lr=0.000020 grad_norm=0.456571
Epoch 26/100 Iteration 93/234: loss=0.107294 lr=0.000020 grad_norm=0.554967
Epoch 26/100 Iteration 94/234: loss=0.090594 lr=0.000020 grad_norm=0.638253
Epoch 26/100 Iteration 95/234: loss=0.093022 lr=0.000020 grad_norm=0.717393
Epoch 26/100 Iteration 96/234: loss=0.110512 lr=0.000020 grad_norm=0.916864
Epoch 26/100 Iteration 97/234: loss=0.102853 lr=0.000020 grad_norm=0.744186
Epoch 26/100 Iteration 98/234: loss=0.100952 lr=0.000020 grad_norm=0.378669
Epoch 26/100 Iteration 99/234: loss=0.095121 lr=0.000020 grad_norm=0.562666
Epoch 26/100 Iteration 100/234: loss=0.102918 lr=0.000020 grad_norm=0.433298
Epoch 26/100 Iteration 101/234: loss=0.112826 lr=0.000020 grad_norm=0.394962
Epoch 26/100 Iteration 102/234: loss=0.093704 lr=0.000020 grad_norm=0.779662
Epoch 26/100 Iteration 103/234: loss=0.107409 lr=0.000020 grad_norm=1.094745
Epoch 26/100 Iteration 104/234: loss=0.094036 lr=0.000020 grad_norm=0.831328
Epoch 26/100 Iteration 105/234: loss=0.099305 lr=0.000020 grad_norm=0.395941
Epoch 26/100 Iteration 106/234: loss=0.117229 lr=0.000020 grad_norm=1.390983
Epoch 26/100 Iteration 107/234: loss=0.099414 lr=0.000020 grad_norm=2.132235
Epoch 26/100 Iteration 108/234: loss=0.111644 lr=0.000020 grad_norm=1.579117
Epoch 26/100 Iteration 109/234: loss=0.113687 lr=0.000020 grad_norm=0.439461
Epoch 26/100 Iteration 110/234: loss=0.094012 lr=0.000020 grad_norm=1.207022
Epoch 26/100 Iteration 111/234: loss=0.093740 lr=0.000020 grad_norm=0.822802
Epoch 26/100 Iteration 112/234: loss=0.110961 lr=0.000020 grad_norm=0.502370
Epoch 26/100 Iteration 113/234: loss=0.102536 lr=0.000020 grad_norm=0.489538
Epoch 26/100 Iteration 114/234: loss=0.097679 lr=0.000020 grad_norm=0.630797
Epoch 26/100 Iteration 115/234: loss=0.100114 lr=0.000020 grad_norm=0.516281
Epoch 26/100 Iteration 116/234: loss=0.119150 lr=0.000020 grad_norm=0.454282
Epoch 26/100 Iteration 117/234: loss=0.106704 lr=0.000020 grad_norm=0.718156
Epoch 26/100 Iteration 118/234: loss=0.086904 lr=0.000020 grad_norm=0.509852
Epoch 26/100 Iteration 119/234: loss=0.094816 lr=0.000020 grad_norm=0.523172
Epoch 26/100 Iteration 120/234: loss=0.107488 lr=0.000020 grad_norm=0.834021
Epoch 26/100 Iteration 121/234: loss=0.099792 lr=0.000020 grad_norm=0.760569
Epoch 26/100 Iteration 122/234: loss=0.101374 lr=0.000020 grad_norm=0.368425
Epoch 26/100 Iteration 123/234: loss=0.099581 lr=0.000020 grad_norm=0.693559
Epoch 26/100 Iteration 124/234: loss=0.099795 lr=0.000020 grad_norm=0.747919
Epoch 26/100 Iteration 125/234: loss=0.113459 lr=0.000020 grad_norm=0.361728
Epoch 26/100 Iteration 126/234: loss=0.105099 lr=0.000020 grad_norm=0.449364
Epoch 26/100 Iteration 127/234: loss=0.091330 lr=0.000020 grad_norm=0.620974
Epoch 26/100 Iteration 128/234: loss=0.092362 lr=0.000020 grad_norm=0.602342
Epoch 26/100 Iteration 129/234: loss=0.111696 lr=0.000020 grad_norm=0.433324
Epoch 26/100 Iteration 130/234: loss=0.094590 lr=0.000020 grad_norm=0.436791
Epoch 26/100 Iteration 131/234: loss=0.107587 lr=0.000020 grad_norm=0.575805
Epoch 26/100 Iteration 132/234: loss=0.110288 lr=0.000020 grad_norm=0.564185
Epoch 26/100 Iteration 133/234: loss=0.101454 lr=0.000020 grad_norm=0.612689
Epoch 26/100 Iteration 134/234: loss=0.108166 lr=0.000020 grad_norm=0.523365
Epoch 26/100 Iteration 135/234: loss=0.102429 lr=0.000020 grad_norm=0.371582
Epoch 26/100 Iteration 136/234: loss=0.114867 lr=0.000020 grad_norm=0.351015
Epoch 26/100 Iteration 137/234: loss=0.112555 lr=0.000020 grad_norm=0.542618
Epoch 26/100 Iteration 138/234: loss=0.103332 lr=0.000020 grad_norm=0.528539
Epoch 26/100 Iteration 139/234: loss=0.105551 lr=0.000020 grad_norm=0.332910
Epoch 26/100 Iteration 140/234: loss=0.102896 lr=0.000020 grad_norm=0.478382
Epoch 26/100 Iteration 141/234: loss=0.100554 lr=0.000020 grad_norm=0.586393
Epoch 26/100 Iteration 142/234: loss=0.099041 lr=0.000020 grad_norm=0.661328
Epoch 26/100 Iteration 143/234: loss=0.101503 lr=0.000020 grad_norm=0.944489
Epoch 26/100 Iteration 144/234: loss=0.105765 lr=0.000020 grad_norm=0.641824
Epoch 26/100 Iteration 145/234: loss=0.110003 lr=0.000020 grad_norm=0.580879
Epoch 26/100 Iteration 146/234: loss=0.110431 lr=0.000020 grad_norm=1.075607
Epoch 26/100 Iteration 147/234: loss=0.117992 lr=0.000020 grad_norm=0.977557
Epoch 26/100 Iteration 148/234: loss=0.110631 lr=0.000020 grad_norm=0.408750
Epoch 26/100 Iteration 149/234: loss=0.100607 lr=0.000020 grad_norm=1.206524
Epoch 26/100 Iteration 150/234: loss=0.098331 lr=0.000020 grad_norm=1.091062
Epoch 26/100 Iteration 151/234: loss=0.102986 lr=0.000020 grad_norm=0.335498
Epoch 26/100 Iteration 152/234: loss=0.098588 lr=0.000020 grad_norm=0.835734
Epoch 26/100 Iteration 153/234: loss=0.103425 lr=0.000020 grad_norm=1.115625
Epoch 26/100 Iteration 154/234: loss=0.120997 lr=0.000020 grad_norm=0.703627
Epoch 26/100 Iteration 155/234: loss=0.104197 lr=0.000020 grad_norm=0.528446
Epoch 26/100 Iteration 156/234: loss=0.101636 lr=0.000020 grad_norm=0.937728
Epoch 26/100 Iteration 157/234: loss=0.097908 lr=0.000020 grad_norm=0.719707
Epoch 26/100 Iteration 158/234: loss=0.104453 lr=0.000020 grad_norm=0.432885
Epoch 26/100 Iteration 159/234: loss=0.103929 lr=0.000020 grad_norm=0.616250
Epoch 26/100 Iteration 160/234: loss=0.111953 lr=0.000020 grad_norm=0.930980
Epoch 26/100 Iteration 161/234: loss=0.113880 lr=0.000020 grad_norm=0.835890
Epoch 26/100 Iteration 162/234: loss=0.103809 lr=0.000020 grad_norm=0.343369
Epoch 26/100 Iteration 163/234: loss=0.103076 lr=0.000020 grad_norm=1.049529
Epoch 26/100 Iteration 164/234: loss=0.102714 lr=0.000020 grad_norm=1.280688
Epoch 26/100 Iteration 165/234: loss=0.103141 lr=0.000020 grad_norm=0.725585
Epoch 26/100 Iteration 166/234: loss=0.107051 lr=0.000020 grad_norm=1.163773
Epoch 26/100 Iteration 167/234: loss=0.087546 lr=0.000020 grad_norm=1.093737
Epoch 26/100 Iteration 168/234: loss=0.113793 lr=0.000020 grad_norm=0.639632
Epoch 26/100 Iteration 169/234: loss=0.096967 lr=0.000020 grad_norm=1.767657
Epoch 26/100 Iteration 170/234: loss=0.117133 lr=0.000020 grad_norm=1.676802
Epoch 26/100 Iteration 171/234: loss=0.109634 lr=0.000020 grad_norm=0.538744
Epoch 26/100 Iteration 172/234: loss=0.113851 lr=0.000020 grad_norm=1.603822
Epoch 26/100 Iteration 173/234: loss=0.093735 lr=0.000020 grad_norm=1.629271
Epoch 26/100 Iteration 174/234: loss=0.120261 lr=0.000020 grad_norm=0.420177
Epoch 26/100 Iteration 175/234: loss=0.090426 lr=0.000020 grad_norm=1.008324
Epoch 26/100 Iteration 176/234: loss=0.105083 lr=0.000020 grad_norm=1.002669
Epoch 26/100 Iteration 177/234: loss=0.103734 lr=0.000020 grad_norm=0.583806
Epoch 26/100 Iteration 178/234: loss=0.089294 lr=0.000020 grad_norm=0.475055
Epoch 26/100 Iteration 179/234: loss=0.102868 lr=0.000020 grad_norm=0.608001
Epoch 26/100 Iteration 180/234: loss=0.099864 lr=0.000020 grad_norm=0.616407
Epoch 26/100 Iteration 181/234: loss=0.086172 lr=0.000020 grad_norm=0.641421
Epoch 26/100 Iteration 182/234: loss=0.108276 lr=0.000020 grad_norm=0.451397
Epoch 26/100 Iteration 183/234: loss=0.098623 lr=0.000020 grad_norm=0.452554
Epoch 26/100 Iteration 184/234: loss=0.097048 lr=0.000020 grad_norm=0.490109
Epoch 26/100 Iteration 185/234: loss=0.086034 lr=0.000020 grad_norm=0.508183
Epoch 26/100 Iteration 186/234: loss=0.102639 lr=0.000020 grad_norm=0.461374
Epoch 26/100 Iteration 187/234: loss=0.110575 lr=0.000020 grad_norm=0.557235
Epoch 26/100 Iteration 188/234: loss=0.112368 lr=0.000020 grad_norm=0.599503
Epoch 26/100 Iteration 189/234: loss=0.103665 lr=0.000020 grad_norm=0.527530
Epoch 26/100 Iteration 190/234: loss=0.109526 lr=0.000020 grad_norm=0.671709
Epoch 26/100 Iteration 191/234: loss=0.103645 lr=0.000020 grad_norm=0.606721
Epoch 26/100 Iteration 192/234: loss=0.095255 lr=0.000020 grad_norm=0.497568
Epoch 26/100 Iteration 193/234: loss=0.110500 lr=0.000020 grad_norm=0.611633
Epoch 26/100 Iteration 194/234: loss=0.104600 lr=0.000020 grad_norm=0.835293
Epoch 26/100 Iteration 195/234: loss=0.097032 lr=0.000020 grad_norm=0.849820
Epoch 26/100 Iteration 196/234: loss=0.089360 lr=0.000020 grad_norm=0.699596
Epoch 26/100 Iteration 197/234: loss=0.110491 lr=0.000020 grad_norm=0.400575
Epoch 26/100 Iteration 198/234: loss=0.115852 lr=0.000020 grad_norm=0.562228
Epoch 26/100 Iteration 199/234: loss=0.113306 lr=0.000020 grad_norm=0.593290
Epoch 26/100 Iteration 200/234: loss=0.122678 lr=0.000020 grad_norm=0.751377
Epoch 26/100 Iteration 201/234: loss=0.096063 lr=0.000020 grad_norm=0.640998
Epoch 26/100 Iteration 202/234: loss=0.110250 lr=0.000020 grad_norm=0.487066
Epoch 26/100 Iteration 203/234: loss=0.098872 lr=0.000020 grad_norm=0.906106
Epoch 26/100 Iteration 204/234: loss=0.109010 lr=0.000020 grad_norm=1.235902
Epoch 26/100 Iteration 205/234: loss=0.107543 lr=0.000020 grad_norm=0.935376
Epoch 26/100 Iteration 206/234: loss=0.109933 lr=0.000020 grad_norm=0.477692
Epoch 26/100 Iteration 207/234: loss=0.089623 lr=0.000020 grad_norm=0.620716
Epoch 26/100 Iteration 208/234: loss=0.107798 lr=0.000020 grad_norm=0.575647
Epoch 26/100 Iteration 209/234: loss=0.119291 lr=0.000020 grad_norm=0.715538
Epoch 26/100 Iteration 210/234: loss=0.098336 lr=0.000020 grad_norm=0.862176
Epoch 26/100 Iteration 211/234: loss=0.106611 lr=0.000020 grad_norm=1.066597
Epoch 26/100 Iteration 212/234: loss=0.102912 lr=0.000020 grad_norm=0.529015
Epoch 26/100 Iteration 213/234: loss=0.102192 lr=0.000020 grad_norm=0.890917
Epoch 26/100 Iteration 214/234: loss=0.108661 lr=0.000020 grad_norm=1.369683
Epoch 26/100 Iteration 215/234: loss=0.109751 lr=0.000020 grad_norm=0.848733
Epoch 26/100 Iteration 216/234: loss=0.093646 lr=0.000020 grad_norm=0.557716
Epoch 26/100 Iteration 217/234: loss=0.103533 lr=0.000020 grad_norm=0.947774
Epoch 26/100 Iteration 218/234: loss=0.097138 lr=0.000020 grad_norm=1.125404
Epoch 26/100 Iteration 219/234: loss=0.111767 lr=0.000020 grad_norm=0.947902
Epoch 26/100 Iteration 220/234: loss=0.106777 lr=0.000020 grad_norm=0.661975
Epoch 26/100 Iteration 221/234: loss=0.110912 lr=0.000020 grad_norm=0.537061
Epoch 26/100 Iteration 222/234: loss=0.093127 lr=0.000020 grad_norm=0.578265
Epoch 26/100 Iteration 223/234: loss=0.091758 lr=0.000020 grad_norm=0.806009
Epoch 26/100 Iteration 224/234: loss=0.106094 lr=0.000020 grad_norm=0.835227
Epoch 26/100 Iteration 225/234: loss=0.104758 lr=0.000020 grad_norm=0.558558
Epoch 26/100 Iteration 226/234: loss=0.119097 lr=0.000020 grad_norm=0.521776
Epoch 26/100 Iteration 227/234: loss=0.104580 lr=0.000020 grad_norm=1.120938
Epoch 26/100 Iteration 228/234: loss=0.112080 lr=0.000020 grad_norm=1.116657
Epoch 26/100 Iteration 229/234: loss=0.113115 lr=0.000020 grad_norm=0.896959
Epoch 26/100 Iteration 230/234: loss=0.098229 lr=0.000020 grad_norm=0.853634
Epoch 26/100 Iteration 231/234: loss=0.101765 lr=0.000020 grad_norm=0.713292
Epoch 26/100 Iteration 232/234: loss=0.113459 lr=0.000020 grad_norm=0.575308
Epoch 26/100 Iteration 233/234: loss=0.111127 lr=0.000020 grad_norm=0.784084
Epoch 26/100 Iteration 234/234: loss=0.104912 lr=0.000020 grad_norm=1.087789
Epoch 26/100 finished. Avg Loss: 0.103544
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 27/100 Iteration 1/234: loss=0.092236 lr=0.000020 grad_norm=1.119651
Epoch 27/100 Iteration 2/234: loss=0.100761 lr=0.000020 grad_norm=0.858033
Epoch 27/100 Iteration 3/234: loss=0.108072 lr=0.000020 grad_norm=0.829970
Epoch 27/100 Iteration 4/234: loss=0.093244 lr=0.000020 grad_norm=1.165386
Epoch 27/100 Iteration 5/234: loss=0.093446 lr=0.000020 grad_norm=0.973676
Epoch 27/100 Iteration 6/234: loss=0.102385 lr=0.000020 grad_norm=0.696121
Epoch 27/100 Iteration 7/234: loss=0.095848 lr=0.000020 grad_norm=0.685588
Epoch 27/100 Iteration 8/234: loss=0.109522 lr=0.000020 grad_norm=1.016006
Epoch 27/100 Iteration 9/234: loss=0.101591 lr=0.000020 grad_norm=0.922888
Epoch 27/100 Iteration 10/234: loss=0.105278 lr=0.000020 grad_norm=0.607461
Epoch 27/100 Iteration 11/234: loss=0.105197 lr=0.000020 grad_norm=0.533596
Epoch 27/100 Iteration 12/234: loss=0.109906 lr=0.000020 grad_norm=0.982453
Epoch 27/100 Iteration 13/234: loss=0.091802 lr=0.000020 grad_norm=0.770771
Epoch 27/100 Iteration 14/234: loss=0.105121 lr=0.000020 grad_norm=0.573041
Epoch 27/100 Iteration 15/234: loss=0.118259 lr=0.000020 grad_norm=1.284507
Epoch 27/100 Iteration 16/234: loss=0.100580 lr=0.000020 grad_norm=1.359714
Epoch 27/100 Iteration 17/234: loss=0.112487 lr=0.000020 grad_norm=0.652370
Epoch 27/100 Iteration 18/234: loss=0.101580 lr=0.000020 grad_norm=1.773446
Epoch 27/100 Iteration 19/234: loss=0.091577 lr=0.000020 grad_norm=0.926529
Epoch 27/100 Iteration 20/234: loss=0.103998 lr=0.000020 grad_norm=1.318291
Epoch 27/100 Iteration 21/234: loss=0.106269 lr=0.000020 grad_norm=2.202269
Epoch 27/100 Iteration 22/234: loss=0.093188 lr=0.000020 grad_norm=1.059126
Epoch 27/100 Iteration 23/234: loss=0.090766 lr=0.000020 grad_norm=0.991989
Epoch 27/100 Iteration 24/234: loss=0.112727 lr=0.000020 grad_norm=0.901485
Epoch 27/100 Iteration 25/234: loss=0.094487 lr=0.000020 grad_norm=0.442939
Epoch 27/100 Iteration 26/234: loss=0.098116 lr=0.000020 grad_norm=0.927163
Epoch 27/100 Iteration 27/234: loss=0.107146 lr=0.000020 grad_norm=0.626870
Epoch 27/100 Iteration 28/234: loss=0.102124 lr=0.000020 grad_norm=0.400096
Epoch 27/100 Iteration 29/234: loss=0.093678 lr=0.000020 grad_norm=0.609063
Epoch 27/100 Iteration 30/234: loss=0.100036 lr=0.000020 grad_norm=0.692365
Epoch 27/100 Iteration 31/234: loss=0.102962 lr=0.000020 grad_norm=1.260757
Epoch 27/100 Iteration 32/234: loss=0.102243 lr=0.000020 grad_norm=1.260293
Epoch 27/100 Iteration 33/234: loss=0.095730 lr=0.000020 grad_norm=0.456667
Epoch 27/100 Iteration 34/234: loss=0.120843 lr=0.000020 grad_norm=1.234169
Epoch 27/100 Iteration 35/234: loss=0.113261 lr=0.000020 grad_norm=1.432266
Epoch 27/100 Iteration 36/234: loss=0.108818 lr=0.000020 grad_norm=0.724692
Epoch 27/100 Iteration 37/234: loss=0.087817 lr=0.000020 grad_norm=0.701339
Epoch 27/100 Iteration 38/234: loss=0.087687 lr=0.000020 grad_norm=0.521955
Epoch 27/100 Iteration 39/234: loss=0.104455 lr=0.000020 grad_norm=0.527540
Epoch 27/100 Iteration 40/234: loss=0.094543 lr=0.000020 grad_norm=0.557021
Epoch 27/100 Iteration 41/234: loss=0.118487 lr=0.000020 grad_norm=0.702907
Epoch 27/100 Iteration 42/234: loss=0.100362 lr=0.000020 grad_norm=0.389533
Epoch 27/100 Iteration 43/234: loss=0.125653 lr=0.000020 grad_norm=0.709025
Epoch 27/100 Iteration 44/234: loss=0.099091 lr=0.000020 grad_norm=1.307181
Epoch 27/100 Iteration 45/234: loss=0.093428 lr=0.000020 grad_norm=1.177783
Epoch 27/100 Iteration 46/234: loss=0.098730 lr=0.000020 grad_norm=0.352694
Epoch 27/100 Iteration 47/234: loss=0.104516 lr=0.000020 grad_norm=1.666730
Epoch 27/100 Iteration 48/234: loss=0.106429 lr=0.000020 grad_norm=2.020384
Epoch 27/100 Iteration 49/234: loss=0.104629 lr=0.000020 grad_norm=0.954802
Epoch 27/100 Iteration 50/234: loss=0.120780 lr=0.000020 grad_norm=0.911263
Epoch 27/100 Iteration 51/234: loss=0.111333 lr=0.000020 grad_norm=1.205284
Epoch 27/100 Iteration 52/234: loss=0.089122 lr=0.000020 grad_norm=0.510277
Epoch 27/100 Iteration 53/234: loss=0.096790 lr=0.000020 grad_norm=1.445703
Epoch 27/100 Iteration 54/234: loss=0.091653 lr=0.000020 grad_norm=1.057222
Epoch 27/100 Iteration 55/234: loss=0.097490 lr=0.000020 grad_norm=0.775048
Epoch 27/100 Iteration 56/234: loss=0.101492 lr=0.000020 grad_norm=1.011394
Epoch 27/100 Iteration 57/234: loss=0.105954 lr=0.000020 grad_norm=0.653658
Epoch 27/100 Iteration 58/234: loss=0.106781 lr=0.000020 grad_norm=0.670483
Epoch 27/100 Iteration 59/234: loss=0.089739 lr=0.000020 grad_norm=0.579504
Epoch 27/100 Iteration 60/234: loss=0.095989 lr=0.000020 grad_norm=0.616799
Epoch 27/100 Iteration 61/234: loss=0.114924 lr=0.000020 grad_norm=0.584400
Epoch 27/100 Iteration 62/234: loss=0.101551 lr=0.000020 grad_norm=0.754064
Epoch 27/100 Iteration 63/234: loss=0.101919 lr=0.000020 grad_norm=0.882355
Epoch 27/100 Iteration 64/234: loss=0.105643 lr=0.000020 grad_norm=0.801049
Epoch 27/100 Iteration 65/234: loss=0.097647 lr=0.000020 grad_norm=0.599042
Epoch 27/100 Iteration 66/234: loss=0.104019 lr=0.000020 grad_norm=0.476860
Epoch 27/100 Iteration 67/234: loss=0.105695 lr=0.000020 grad_norm=0.446368
Epoch 27/100 Iteration 68/234: loss=0.097881 lr=0.000020 grad_norm=0.609223
Epoch 27/100 Iteration 69/234: loss=0.097153 lr=0.000020 grad_norm=0.629980
Epoch 27/100 Iteration 70/234: loss=0.096031 lr=0.000020 grad_norm=0.692566
Epoch 27/100 Iteration 71/234: loss=0.110244 lr=0.000020 grad_norm=0.701534
Epoch 27/100 Iteration 72/234: loss=0.102067 lr=0.000020 grad_norm=0.530872
Epoch 27/100 Iteration 73/234: loss=0.113582 lr=0.000020 grad_norm=0.338049
Epoch 27/100 Iteration 74/234: loss=0.102285 lr=0.000020 grad_norm=0.505208
Epoch 27/100 Iteration 75/234: loss=0.094049 lr=0.000020 grad_norm=0.646328
Epoch 27/100 Iteration 76/234: loss=0.115433 lr=0.000020 grad_norm=1.107030
Epoch 27/100 Iteration 77/234: loss=0.103049 lr=0.000020 grad_norm=1.235304
Epoch 27/100 Iteration 78/234: loss=0.104579 lr=0.000020 grad_norm=0.639474
Epoch 27/100 Iteration 79/234: loss=0.101046 lr=0.000020 grad_norm=0.457979
Epoch 27/100 Iteration 80/234: loss=0.096994 lr=0.000020 grad_norm=0.657880
Epoch 27/100 Iteration 81/234: loss=0.098013 lr=0.000020 grad_norm=0.374938
Epoch 27/100 Iteration 82/234: loss=0.110228 lr=0.000020 grad_norm=0.891760
Epoch 27/100 Iteration 83/234: loss=0.109940 lr=0.000020 grad_norm=0.938876
Epoch 27/100 Iteration 84/234: loss=0.095792 lr=0.000020 grad_norm=0.617257
Epoch 27/100 Iteration 85/234: loss=0.107010 lr=0.000020 grad_norm=0.669659
Epoch 27/100 Iteration 86/234: loss=0.097988 lr=0.000020 grad_norm=0.654177
Epoch 27/100 Iteration 87/234: loss=0.090574 lr=0.000020 grad_norm=0.567444
Epoch 27/100 Iteration 88/234: loss=0.103886 lr=0.000020 grad_norm=0.536201
Epoch 27/100 Iteration 89/234: loss=0.100471 lr=0.000020 grad_norm=0.416110
Epoch 27/100 Iteration 90/234: loss=0.098709 lr=0.000020 grad_norm=0.590458
Epoch 27/100 Iteration 91/234: loss=0.091360 lr=0.000020 grad_norm=0.464570
Epoch 27/100 Iteration 92/234: loss=0.110148 lr=0.000020 grad_norm=0.380774
Epoch 27/100 Iteration 93/234: loss=0.090603 lr=0.000020 grad_norm=0.471742
Epoch 27/100 Iteration 94/234: loss=0.107874 lr=0.000020 grad_norm=0.406167
Epoch 27/100 Iteration 95/234: loss=0.089322 lr=0.000020 grad_norm=0.414221
Epoch 27/100 Iteration 96/234: loss=0.115495 lr=0.000020 grad_norm=0.502564
Epoch 27/100 Iteration 97/234: loss=0.093629 lr=0.000020 grad_norm=0.532816
Epoch 27/100 Iteration 98/234: loss=0.105138 lr=0.000020 grad_norm=0.823455
Epoch 27/100 Iteration 99/234: loss=0.096485 lr=0.000020 grad_norm=1.150567
Epoch 27/100 Iteration 100/234: loss=0.101657 lr=0.000020 grad_norm=1.173782
Epoch 27/100 Iteration 101/234: loss=0.101979 lr=0.000020 grad_norm=0.473003
Epoch 27/100 Iteration 102/234: loss=0.100363 lr=0.000020 grad_norm=0.659005
Epoch 27/100 Iteration 103/234: loss=0.108375 lr=0.000020 grad_norm=0.685098
Epoch 27/100 Iteration 104/234: loss=0.090211 lr=0.000020 grad_norm=0.590978
Epoch 27/100 Iteration 105/234: loss=0.094089 lr=0.000020 grad_norm=0.766814
Epoch 27/100 Iteration 106/234: loss=0.105838 lr=0.000020 grad_norm=0.567592
Epoch 27/100 Iteration 107/234: loss=0.101908 lr=0.000020 grad_norm=0.423041
Epoch 27/100 Iteration 108/234: loss=0.099989 lr=0.000020 grad_norm=0.677854
Epoch 27/100 Iteration 109/234: loss=0.092709 lr=0.000020 grad_norm=0.836854
Epoch 27/100 Iteration 110/234: loss=0.096462 lr=0.000020 grad_norm=0.809442
Epoch 27/100 Iteration 111/234: loss=0.097479 lr=0.000020 grad_norm=0.734486
Epoch 27/100 Iteration 112/234: loss=0.104569 lr=0.000020 grad_norm=0.756413
Epoch 27/100 Iteration 113/234: loss=0.102797 lr=0.000020 grad_norm=0.691765
Epoch 27/100 Iteration 114/234: loss=0.094203 lr=0.000020 grad_norm=0.800548
Epoch 27/100 Iteration 115/234: loss=0.092100 lr=0.000020 grad_norm=0.694519
Epoch 27/100 Iteration 116/234: loss=0.095956 lr=0.000020 grad_norm=0.579895
Epoch 27/100 Iteration 117/234: loss=0.096692 lr=0.000020 grad_norm=0.814609
Epoch 27/100 Iteration 118/234: loss=0.111740 lr=0.000020 grad_norm=0.880299
Epoch 27/100 Iteration 119/234: loss=0.100719 lr=0.000020 grad_norm=0.731715
Epoch 27/100 Iteration 120/234: loss=0.099751 lr=0.000020 grad_norm=0.669500
Epoch 27/100 Iteration 121/234: loss=0.097900 lr=0.000020 grad_norm=0.686712
Epoch 27/100 Iteration 122/234: loss=0.101953 lr=0.000020 grad_norm=0.520345
Epoch 27/100 Iteration 123/234: loss=0.103778 lr=0.000020 grad_norm=0.387963
Epoch 27/100 Iteration 124/234: loss=0.105243 lr=0.000020 grad_norm=0.364932
Epoch 27/100 Iteration 125/234: loss=0.095184 lr=0.000020 grad_norm=0.513590
Epoch 27/100 Iteration 126/234: loss=0.092613 lr=0.000020 grad_norm=0.592840
Epoch 27/100 Iteration 127/234: loss=0.105785 lr=0.000020 grad_norm=0.647034
Epoch 27/100 Iteration 128/234: loss=0.108130 lr=0.000020 grad_norm=0.589822
Epoch 27/100 Iteration 129/234: loss=0.100344 lr=0.000020 grad_norm=0.639717
Epoch 27/100 Iteration 130/234: loss=0.101807 lr=0.000020 grad_norm=0.556204
Epoch 27/100 Iteration 131/234: loss=0.087744 lr=0.000020 grad_norm=0.660215
Epoch 27/100 Iteration 132/234: loss=0.109061 lr=0.000020 grad_norm=0.618657
Epoch 27/100 Iteration 133/234: loss=0.108201 lr=0.000020 grad_norm=0.982569
Epoch 27/100 Iteration 134/234: loss=0.103179 lr=0.000020 grad_norm=1.234454
Epoch 27/100 Iteration 135/234: loss=0.102979 lr=0.000020 grad_norm=1.389762
Epoch 27/100 Iteration 136/234: loss=0.111004 lr=0.000020 grad_norm=1.221521
Epoch 27/100 Iteration 137/234: loss=0.099017 lr=0.000020 grad_norm=0.828963
Epoch 27/100 Iteration 138/234: loss=0.117238 lr=0.000020 grad_norm=0.530169
Epoch 27/100 Iteration 139/234: loss=0.101730 lr=0.000020 grad_norm=0.672181
Epoch 27/100 Iteration 140/234: loss=0.120091 lr=0.000020 grad_norm=0.832814
Epoch 27/100 Iteration 141/234: loss=0.091640 lr=0.000020 grad_norm=0.330544
Epoch 27/100 Iteration 142/234: loss=0.084532 lr=0.000020 grad_norm=0.901063
Epoch 27/100 Iteration 143/234: loss=0.104804 lr=0.000020 grad_norm=0.828875
Epoch 27/100 Iteration 144/234: loss=0.097321 lr=0.000020 grad_norm=0.378327
Epoch 27/100 Iteration 145/234: loss=0.107076 lr=0.000020 grad_norm=1.255902
Epoch 27/100 Iteration 146/234: loss=0.097430 lr=0.000020 grad_norm=1.230599
Epoch 27/100 Iteration 147/234: loss=0.090741 lr=0.000020 grad_norm=0.504482
Epoch 27/100 Iteration 148/234: loss=0.098845 lr=0.000020 grad_norm=0.714899
Epoch 27/100 Iteration 149/234: loss=0.093176 lr=0.000020 grad_norm=0.668516
Epoch 27/100 Iteration 150/234: loss=0.097929 lr=0.000020 grad_norm=0.480435
Epoch 27/100 Iteration 151/234: loss=0.101463 lr=0.000020 grad_norm=0.703083
Epoch 27/100 Iteration 152/234: loss=0.112820 lr=0.000020 grad_norm=0.797268
Epoch 27/100 Iteration 153/234: loss=0.089017 lr=0.000020 grad_norm=0.531840
Epoch 27/100 Iteration 154/234: loss=0.092344 lr=0.000020 grad_norm=0.406653
Epoch 27/100 Iteration 155/234: loss=0.100758 lr=0.000020 grad_norm=0.537119
Epoch 27/100 Iteration 156/234: loss=0.105440 lr=0.000020 grad_norm=0.849278
Epoch 27/100 Iteration 157/234: loss=0.095580 lr=0.000020 grad_norm=0.689172
Epoch 27/100 Iteration 158/234: loss=0.090261 lr=0.000020 grad_norm=0.507585
Epoch 27/100 Iteration 159/234: loss=0.086228 lr=0.000020 grad_norm=0.398030
Epoch 27/100 Iteration 160/234: loss=0.100135 lr=0.000020 grad_norm=0.296989
Epoch 27/100 Iteration 161/234: loss=0.103658 lr=0.000020 grad_norm=0.440980
Epoch 27/100 Iteration 162/234: loss=0.101519 lr=0.000020 grad_norm=0.756963
Epoch 27/100 Iteration 163/234: loss=0.101652 lr=0.000020 grad_norm=0.891470
Epoch 27/100 Iteration 164/234: loss=0.104317 lr=0.000020 grad_norm=0.502004
Epoch 27/100 Iteration 165/234: loss=0.092070 lr=0.000020 grad_norm=0.512820
Epoch 27/100 Iteration 166/234: loss=0.102872 lr=0.000020 grad_norm=0.644359
Epoch 27/100 Iteration 167/234: loss=0.107450 lr=0.000020 grad_norm=0.375230
Epoch 27/100 Iteration 168/234: loss=0.104076 lr=0.000020 grad_norm=0.908175
Epoch 27/100 Iteration 169/234: loss=0.103721 lr=0.000020 grad_norm=1.174859
Epoch 27/100 Iteration 170/234: loss=0.099567 lr=0.000020 grad_norm=1.322080
Epoch 27/100 Iteration 171/234: loss=0.098462 lr=0.000020 grad_norm=1.069162
Epoch 27/100 Iteration 172/234: loss=0.097710 lr=0.000020 grad_norm=0.597750
Epoch 27/100 Iteration 173/234: loss=0.092490 lr=0.000020 grad_norm=0.744638
Epoch 27/100 Iteration 174/234: loss=0.090589 lr=0.000020 grad_norm=0.652009
Epoch 27/100 Iteration 175/234: loss=0.074031 lr=0.000020 grad_norm=0.578810
Epoch 27/100 Iteration 176/234: loss=0.102844 lr=0.000020 grad_norm=0.586623
Epoch 27/100 Iteration 177/234: loss=0.100534 lr=0.000020 grad_norm=0.404995
Epoch 27/100 Iteration 178/234: loss=0.105806 lr=0.000020 grad_norm=0.790698
Epoch 27/100 Iteration 179/234: loss=0.084674 lr=0.000020 grad_norm=0.650258
Epoch 27/100 Iteration 180/234: loss=0.105763 lr=0.000020 grad_norm=0.547845
Epoch 27/100 Iteration 181/234: loss=0.097604 lr=0.000020 grad_norm=0.934003
Epoch 27/100 Iteration 182/234: loss=0.102863 lr=0.000020 grad_norm=1.298291
Epoch 27/100 Iteration 183/234: loss=0.095333 lr=0.000020 grad_norm=1.256806
Epoch 27/100 Iteration 184/234: loss=0.088995 lr=0.000020 grad_norm=0.764624
Epoch 27/100 Iteration 185/234: loss=0.090308 lr=0.000020 grad_norm=0.770259
Epoch 27/100 Iteration 186/234: loss=0.107156 lr=0.000020 grad_norm=1.342651
Epoch 27/100 Iteration 187/234: loss=0.093951 lr=0.000020 grad_norm=1.206557
Epoch 27/100 Iteration 188/234: loss=0.098726 lr=0.000020 grad_norm=0.945513
Epoch 27/100 Iteration 189/234: loss=0.107492 lr=0.000020 grad_norm=0.874891
Epoch 27/100 Iteration 190/234: loss=0.096257 lr=0.000020 grad_norm=0.365965
Epoch 27/100 Iteration 191/234: loss=0.106528 lr=0.000020 grad_norm=0.795794
Epoch 27/100 Iteration 192/234: loss=0.104153 lr=0.000020 grad_norm=0.611162
Epoch 27/100 Iteration 193/234: loss=0.095942 lr=0.000020 grad_norm=0.454392
Epoch 27/100 Iteration 194/234: loss=0.088723 lr=0.000020 grad_norm=0.525797
Epoch 27/100 Iteration 195/234: loss=0.109962 lr=0.000020 grad_norm=0.821172
Epoch 27/100 Iteration 196/234: loss=0.087907 lr=0.000020 grad_norm=0.763944
Epoch 27/100 Iteration 197/234: loss=0.093503 lr=0.000020 grad_norm=0.614741
Epoch 27/100 Iteration 198/234: loss=0.096678 lr=0.000020 grad_norm=0.570615
Epoch 27/100 Iteration 199/234: loss=0.099367 lr=0.000020 grad_norm=1.024225
Epoch 27/100 Iteration 200/234: loss=0.103821 lr=0.000020 grad_norm=1.198958
Epoch 27/100 Iteration 201/234: loss=0.100751 lr=0.000020 grad_norm=0.722313
Epoch 27/100 Iteration 202/234: loss=0.104844 lr=0.000020 grad_norm=0.804667
Epoch 27/100 Iteration 203/234: loss=0.096452 lr=0.000020 grad_norm=1.425091
Epoch 27/100 Iteration 204/234: loss=0.086531 lr=0.000020 grad_norm=0.978989
Epoch 27/100 Iteration 205/234: loss=0.095362 lr=0.000020 grad_norm=0.609278
Epoch 27/100 Iteration 206/234: loss=0.097218 lr=0.000020 grad_norm=0.357890
Epoch 27/100 Iteration 207/234: loss=0.098518 lr=0.000020 grad_norm=0.332910
Epoch 27/100 Iteration 208/234: loss=0.100872 lr=0.000020 grad_norm=0.478542
Epoch 27/100 Iteration 209/234: loss=0.103457 lr=0.000020 grad_norm=0.362590
Epoch 27/100 Iteration 210/234: loss=0.103713 lr=0.000020 grad_norm=0.373060
Epoch 27/100 Iteration 211/234: loss=0.107082 lr=0.000020 grad_norm=0.652634
Epoch 27/100 Iteration 212/234: loss=0.094369 lr=0.000020 grad_norm=0.845576
Epoch 27/100 Iteration 213/234: loss=0.100679 lr=0.000020 grad_norm=0.837708
Epoch 27/100 Iteration 214/234: loss=0.108147 lr=0.000020 grad_norm=0.575480
Epoch 27/100 Iteration 215/234: loss=0.099041 lr=0.000020 grad_norm=0.560673
Epoch 27/100 Iteration 216/234: loss=0.101755 lr=0.000020 grad_norm=0.408862
Epoch 27/100 Iteration 217/234: loss=0.094287 lr=0.000020 grad_norm=0.404297
Epoch 27/100 Iteration 218/234: loss=0.099223 lr=0.000020 grad_norm=0.427458
Epoch 27/100 Iteration 219/234: loss=0.099089 lr=0.000020 grad_norm=0.368686
Epoch 27/100 Iteration 220/234: loss=0.093247 lr=0.000020 grad_norm=0.461883
Epoch 27/100 Iteration 221/234: loss=0.111382 lr=0.000020 grad_norm=0.926259
Epoch 27/100 Iteration 222/234: loss=0.098812 lr=0.000020 grad_norm=1.211301
Epoch 27/100 Iteration 223/234: loss=0.092571 lr=0.000020 grad_norm=1.255913
Epoch 27/100 Iteration 224/234: loss=0.098376 lr=0.000020 grad_norm=0.997349
Epoch 27/100 Iteration 225/234: loss=0.092466 lr=0.000020 grad_norm=0.680737
Epoch 27/100 Iteration 226/234: loss=0.101014 lr=0.000020 grad_norm=0.416804
Epoch 27/100 Iteration 227/234: loss=0.093068 lr=0.000020 grad_norm=0.377544
Epoch 27/100 Iteration 228/234: loss=0.098673 lr=0.000020 grad_norm=0.496877
Epoch 27/100 Iteration 229/234: loss=0.098487 lr=0.000020 grad_norm=0.415400
Epoch 27/100 Iteration 230/234: loss=0.089286 lr=0.000020 grad_norm=0.378887
Epoch 27/100 Iteration 231/234: loss=0.091473 lr=0.000020 grad_norm=0.399536
Epoch 27/100 Iteration 232/234: loss=0.100826 lr=0.000020 grad_norm=0.923345
Epoch 27/100 Iteration 233/234: loss=0.098794 lr=0.000020 grad_norm=1.119701
Epoch 27/100 Iteration 234/234: loss=0.107884 lr=0.000020 grad_norm=0.900119
Epoch 27/100 finished. Avg Loss: 0.100400
Epoch 28/100 Iteration 1/234: loss=0.100066 lr=0.000020 grad_norm=0.540924
Epoch 28/100 Iteration 2/234: loss=0.092190 lr=0.000020 grad_norm=0.580799
Epoch 28/100 Iteration 3/234: loss=0.083965 lr=0.000020 grad_norm=0.730179
Epoch 28/100 Iteration 4/234: loss=0.102480 lr=0.000020 grad_norm=0.924218
Epoch 28/100 Iteration 5/234: loss=0.088956 lr=0.000020 grad_norm=0.994498
Epoch 28/100 Iteration 6/234: loss=0.087473 lr=0.000020 grad_norm=0.813251
Epoch 28/100 Iteration 7/234: loss=0.092657 lr=0.000020 grad_norm=0.741882
Epoch 28/100 Iteration 8/234: loss=0.110099 lr=0.000020 grad_norm=1.387182
Epoch 28/100 Iteration 9/234: loss=0.098743 lr=0.000020 grad_norm=1.641235
Epoch 28/100 Iteration 10/234: loss=0.105005 lr=0.000020 grad_norm=1.346690
Epoch 28/100 Iteration 11/234: loss=0.102315 lr=0.000020 grad_norm=0.958260
Epoch 28/100 Iteration 12/234: loss=0.104833 lr=0.000020 grad_norm=1.104989
Epoch 28/100 Iteration 13/234: loss=0.092207 lr=0.000020 grad_norm=1.225625
Epoch 28/100 Iteration 14/234: loss=0.101793 lr=0.000020 grad_norm=1.197888
Epoch 28/100 Iteration 15/234: loss=0.110233 lr=0.000020 grad_norm=1.031538
Epoch 28/100 Iteration 16/234: loss=0.107445 lr=0.000020 grad_norm=0.585774
Epoch 28/100 Iteration 17/234: loss=0.103054 lr=0.000020 grad_norm=1.091840
Epoch 28/100 Iteration 18/234: loss=0.102401 lr=0.000020 grad_norm=0.991642
Epoch 28/100 Iteration 19/234: loss=0.099509 lr=0.000020 grad_norm=0.798581
Epoch 28/100 Iteration 20/234: loss=0.113498 lr=0.000020 grad_norm=1.247070
Epoch 28/100 Iteration 21/234: loss=0.086688 lr=0.000020 grad_norm=1.083978
Epoch 28/100 Iteration 22/234: loss=0.103363 lr=0.000020 grad_norm=0.764672
Epoch 28/100 Iteration 23/234: loss=0.108771 lr=0.000020 grad_norm=1.164270
Epoch 28/100 Iteration 24/234: loss=0.094816 lr=0.000020 grad_norm=2.073903
Epoch 28/100 Iteration 25/234: loss=0.095588 lr=0.000020 grad_norm=2.006038
Epoch 28/100 Iteration 26/234: loss=0.099020 lr=0.000020 grad_norm=0.685242
Epoch 28/100 Iteration 27/234: loss=0.108472 lr=0.000020 grad_norm=1.576597
Epoch 28/100 Iteration 28/234: loss=0.097184 lr=0.000020 grad_norm=1.851594
Epoch 28/100 Iteration 29/234: loss=0.094607 lr=0.000020 grad_norm=0.676086
Epoch 28/100 Iteration 30/234: loss=0.094182 lr=0.000020 grad_norm=1.928180
Epoch 28/100 Iteration 31/234: loss=0.105825 lr=0.000020 grad_norm=0.897678
Epoch 28/100 Iteration 32/234: loss=0.086283 lr=0.000020 grad_norm=1.281562
Epoch 28/100 Iteration 33/234: loss=0.123354 lr=0.000020 grad_norm=1.388491
Epoch 28/100 Iteration 34/234: loss=0.092407 lr=0.000020 grad_norm=0.968478
Epoch 28/100 Iteration 35/234: loss=0.104664 lr=0.000020 grad_norm=1.618881
Epoch 28/100 Iteration 36/234: loss=0.086080 lr=0.000020 grad_norm=0.776340
Epoch 28/100 Iteration 37/234: loss=0.082059 lr=0.000020 grad_norm=1.807004
Epoch 28/100 Iteration 38/234: loss=0.113647 lr=0.000020 grad_norm=1.797247
Epoch 28/100 Iteration 39/234: loss=0.093907 lr=0.000020 grad_norm=0.488555
Epoch 28/100 Iteration 40/234: loss=0.111855 lr=0.000020 grad_norm=1.721935
Epoch 28/100 Iteration 41/234: loss=0.101993 lr=0.000020 grad_norm=1.194210
Epoch 28/100 Iteration 42/234: loss=0.101340 lr=0.000020 grad_norm=0.792963
Epoch 28/100 Iteration 43/234: loss=0.086454 lr=0.000020 grad_norm=1.377502
Epoch 28/100 Iteration 44/234: loss=0.089048 lr=0.000020 grad_norm=0.755359
Epoch 28/100 Iteration 45/234: loss=0.095759 lr=0.000020 grad_norm=0.583136
Epoch 28/100 Iteration 46/234: loss=0.090466 lr=0.000020 grad_norm=0.512655
Epoch 28/100 Iteration 47/234: loss=0.092713 lr=0.000020 grad_norm=0.611926
Epoch 28/100 Iteration 48/234: loss=0.097084 lr=0.000020 grad_norm=0.504574
Epoch 28/100 Iteration 49/234: loss=0.100636 lr=0.000020 grad_norm=0.435910
Epoch 28/100 Iteration 50/234: loss=0.094968 lr=0.000020 grad_norm=0.399821
Epoch 28/100 Iteration 51/234: loss=0.107592 lr=0.000020 grad_norm=0.454753
Epoch 28/100 Iteration 52/234: loss=0.111139 lr=0.000020 grad_norm=0.829144
Epoch 28/100 Iteration 53/234: loss=0.088931 lr=0.000020 grad_norm=0.690072
Epoch 28/100 Iteration 54/234: loss=0.103729 lr=0.000020 grad_norm=0.510254
Epoch 28/100 Iteration 55/234: loss=0.107084 lr=0.000020 grad_norm=0.570455
Epoch 28/100 Iteration 56/234: loss=0.097836 lr=0.000020 grad_norm=0.333263
Epoch 28/100 Iteration 57/234: loss=0.096239 lr=0.000020 grad_norm=0.534988
Epoch 28/100 Iteration 58/234: loss=0.094809 lr=0.000020 grad_norm=0.467653
Epoch 28/100 Iteration 59/234: loss=0.095428 lr=0.000020 grad_norm=0.330332
Epoch 28/100 Iteration 60/234: loss=0.105334 lr=0.000020 grad_norm=0.816146
Epoch 28/100 Iteration 61/234: loss=0.104904 lr=0.000020 grad_norm=1.138054
Epoch 28/100 Iteration 62/234: loss=0.097055 lr=0.000020 grad_norm=0.668438
Epoch 28/100 Iteration 63/234: loss=0.123130 lr=0.000020 grad_norm=0.628219
Epoch 28/100 Iteration 64/234: loss=0.087416 lr=0.000020 grad_norm=1.008680
Epoch 28/100 Iteration 65/234: loss=0.099951 lr=0.000020 grad_norm=0.502463
Epoch 28/100 Iteration 66/234: loss=0.083966 lr=0.000020 grad_norm=0.584979
Epoch 28/100 Iteration 67/234: loss=0.104464 lr=0.000020 grad_norm=1.111084
Epoch 28/100 Iteration 68/234: loss=0.109165 lr=0.000020 grad_norm=0.740168
Epoch 28/100 Iteration 69/234: loss=0.086553 lr=0.000020 grad_norm=0.318613
Epoch 28/100 Iteration 70/234: loss=0.087053 lr=0.000020 grad_norm=0.740562
Epoch 28/100 Iteration 71/234: loss=0.095252 lr=0.000020 grad_norm=0.547595
Epoch 28/100 Iteration 72/234: loss=0.097112 lr=0.000020 grad_norm=0.555810
Epoch 28/100 Iteration 73/234: loss=0.101468 lr=0.000020 grad_norm=0.553711
Epoch 28/100 Iteration 74/234: loss=0.086008 lr=0.000020 grad_norm=0.381317
Epoch 28/100 Iteration 75/234: loss=0.092791 lr=0.000020 grad_norm=0.451839
Epoch 28/100 Iteration 76/234: loss=0.095910 lr=0.000020 grad_norm=0.620539
Epoch 28/100 Iteration 77/234: loss=0.098067 lr=0.000020 grad_norm=0.544606
Epoch 28/100 Iteration 78/234: loss=0.098863 lr=0.000020 grad_norm=0.649219
Epoch 28/100 Iteration 79/234: loss=0.107315 lr=0.000020 grad_norm=0.754999
Epoch 28/100 Iteration 80/234: loss=0.101354 lr=0.000020 grad_norm=0.622630
Epoch 28/100 Iteration 81/234: loss=0.106434 lr=0.000020 grad_norm=0.620570
Epoch 28/100 Iteration 82/234: loss=0.101398 lr=0.000020 grad_norm=0.552142
Epoch 28/100 Iteration 83/234: loss=0.092892 lr=0.000020 grad_norm=0.537190
Epoch 28/100 Iteration 84/234: loss=0.094707 lr=0.000020 grad_norm=0.524052
Epoch 28/100 Iteration 85/234: loss=0.090465 lr=0.000020 grad_norm=0.679480
Epoch 28/100 Iteration 86/234: loss=0.092800 lr=0.000020 grad_norm=0.953573
Epoch 28/100 Iteration 87/234: loss=0.100192 lr=0.000020 grad_norm=0.826766
Epoch 28/100 Iteration 88/234: loss=0.098032 lr=0.000020 grad_norm=0.666818
Epoch 28/100 Iteration 89/234: loss=0.091441 lr=0.000020 grad_norm=1.708553
Epoch 28/100 Iteration 90/234: loss=0.107075 lr=0.000020 grad_norm=2.125264
Epoch 28/100 Iteration 91/234: loss=0.094665 lr=0.000020 grad_norm=0.794919
Epoch 28/100 Iteration 92/234: loss=0.111006 lr=0.000020 grad_norm=1.380071
Epoch 28/100 Iteration 93/234: loss=0.095963 lr=0.000020 grad_norm=1.622472
Epoch 28/100 Iteration 94/234: loss=0.090797 lr=0.000020 grad_norm=0.411347
Epoch 28/100 Iteration 95/234: loss=0.098378 lr=0.000020 grad_norm=1.802804
Epoch 28/100 Iteration 96/234: loss=0.083190 lr=0.000020 grad_norm=1.770519
Epoch 28/100 Iteration 97/234: loss=0.097030 lr=0.000020 grad_norm=0.763341
Epoch 28/100 Iteration 98/234: loss=0.100711 lr=0.000020 grad_norm=1.853333
Epoch 28/100 Iteration 99/234: loss=0.106303 lr=0.000020 grad_norm=0.522420
Epoch 28/100 Iteration 100/234: loss=0.091241 lr=0.000020 grad_norm=1.533770
Epoch 28/100 Iteration 101/234: loss=0.072665 lr=0.000020 grad_norm=0.447637
Epoch 28/100 Iteration 102/234: loss=0.091907 lr=0.000020 grad_norm=1.796464
Epoch 28/100 Iteration 103/234: loss=0.098906 lr=0.000020 grad_norm=1.416152
Epoch 28/100 Iteration 104/234: loss=0.108270 lr=0.000020 grad_norm=0.859369
Epoch 28/100 Iteration 105/234: loss=0.098754 lr=0.000020 grad_norm=1.499362
Epoch 28/100 Iteration 106/234: loss=0.100094 lr=0.000020 grad_norm=0.551349
Epoch 28/100 Iteration 107/234: loss=0.101810 lr=0.000020 grad_norm=1.095718
Epoch 28/100 Iteration 108/234: loss=0.088867 lr=0.000020 grad_norm=0.698654
Epoch 28/100 Iteration 109/234: loss=0.104154 lr=0.000020 grad_norm=1.015651
Epoch 28/100 Iteration 110/234: loss=0.089945 lr=0.000020 grad_norm=0.961254
Epoch 28/100 Iteration 111/234: loss=0.099259 lr=0.000020 grad_norm=0.510200
Epoch 28/100 Iteration 112/234: loss=0.095246 lr=0.000020 grad_norm=0.808656
Epoch 28/100 Iteration 113/234: loss=0.093482 lr=0.000020 grad_norm=0.407326
Epoch 28/100 Iteration 114/234: loss=0.102623 lr=0.000020 grad_norm=0.440755
Epoch 28/100 Iteration 115/234: loss=0.091982 lr=0.000020 grad_norm=0.542178
Epoch 28/100 Iteration 116/234: loss=0.097583 lr=0.000020 grad_norm=0.387979
Epoch 28/100 Iteration 117/234: loss=0.089363 lr=0.000020 grad_norm=0.565121
Epoch 28/100 Iteration 118/234: loss=0.103571 lr=0.000020 grad_norm=0.436651
Epoch 28/100 Iteration 119/234: loss=0.096799 lr=0.000020 grad_norm=0.421887
Epoch 28/100 Iteration 120/234: loss=0.099749 lr=0.000020 grad_norm=0.690300
Epoch 28/100 Iteration 121/234: loss=0.079435 lr=0.000020 grad_norm=0.398294
Epoch 28/100 Iteration 122/234: loss=0.110938 lr=0.000020 grad_norm=0.455676
Epoch 28/100 Iteration 123/234: loss=0.109582 lr=0.000020 grad_norm=0.568891
Epoch 28/100 Iteration 124/234: loss=0.110323 lr=0.000020 grad_norm=0.662947
Epoch 28/100 Iteration 125/234: loss=0.104681 lr=0.000020 grad_norm=0.512418
Epoch 28/100 Iteration 126/234: loss=0.105915 lr=0.000020 grad_norm=0.430691
Epoch 28/100 Iteration 127/234: loss=0.086870 lr=0.000020 grad_norm=0.584735
Epoch 28/100 Iteration 128/234: loss=0.081423 lr=0.000020 grad_norm=0.473423
Epoch 28/100 Iteration 129/234: loss=0.102814 lr=0.000020 grad_norm=0.615994
Epoch 28/100 Iteration 130/234: loss=0.113223 lr=0.000020 grad_norm=0.672519
Epoch 28/100 Iteration 131/234: loss=0.098874 lr=0.000020 grad_norm=0.698912
Epoch 28/100 Iteration 132/234: loss=0.090593 lr=0.000020 grad_norm=0.548922
Epoch 28/100 Iteration 133/234: loss=0.094256 lr=0.000020 grad_norm=0.495253
Epoch 28/100 Iteration 134/234: loss=0.095601 lr=0.000020 grad_norm=0.488094
Epoch 28/100 Iteration 135/234: loss=0.105202 lr=0.000020 grad_norm=0.997718
Epoch 28/100 Iteration 136/234: loss=0.108822 lr=0.000020 grad_norm=1.153640
Epoch 28/100 Iteration 137/234: loss=0.082428 lr=0.000020 grad_norm=0.468444
Epoch 28/100 Iteration 138/234: loss=0.077155 lr=0.000020 grad_norm=0.633692
Epoch 28/100 Iteration 139/234: loss=0.089557 lr=0.000020 grad_norm=0.618759
Epoch 28/100 Iteration 140/234: loss=0.105968 lr=0.000020 grad_norm=0.764558
Epoch 28/100 Iteration 141/234: loss=0.096351 lr=0.000020 grad_norm=1.477965
Epoch 28/100 Iteration 142/234: loss=0.104357 lr=0.000020 grad_norm=0.970282
Epoch 28/100 Iteration 143/234: loss=0.097476 lr=0.000020 grad_norm=0.879068
Epoch 28/100 Iteration 144/234: loss=0.098317 lr=0.000020 grad_norm=1.956729
Epoch 28/100 Iteration 145/234: loss=0.093760 lr=0.000020 grad_norm=1.489543
Epoch 28/100 Iteration 146/234: loss=0.097647 lr=0.000020 grad_norm=1.126534
Epoch 28/100 Iteration 147/234: loss=0.097855 lr=0.000020 grad_norm=1.836264
Epoch 28/100 Iteration 148/234: loss=0.095864 lr=0.000020 grad_norm=0.606397
Epoch 28/100 Iteration 149/234: loss=0.097884 lr=0.000020 grad_norm=1.800212
Epoch 28/100 Iteration 150/234: loss=0.098799 lr=0.000020 grad_norm=0.514790
Epoch 28/100 Iteration 151/234: loss=0.087360 lr=0.000020 grad_norm=1.518465
Epoch 28/100 Iteration 152/234: loss=0.083128 lr=0.000020 grad_norm=0.993926
Epoch 28/100 Iteration 153/234: loss=0.091212 lr=0.000020 grad_norm=0.986024
Epoch 28/100 Iteration 154/234: loss=0.097609 lr=0.000020 grad_norm=0.923138
Epoch 28/100 Iteration 155/234: loss=0.094272 lr=0.000020 grad_norm=0.889284
Epoch 28/100 Iteration 156/234: loss=0.090192 lr=0.000020 grad_norm=0.964765
Epoch 28/100 Iteration 157/234: loss=0.092887 lr=0.000020 grad_norm=0.905077
Epoch 28/100 Iteration 158/234: loss=0.106003 lr=0.000020 grad_norm=1.668291
Epoch 28/100 Iteration 159/234: loss=0.100037 lr=0.000020 grad_norm=0.767674
Epoch 28/100 Iteration 160/234: loss=0.100949 lr=0.000020 grad_norm=0.931120
Epoch 28/100 Iteration 161/234: loss=0.098925 lr=0.000020 grad_norm=1.203881
Epoch 28/100 Iteration 162/234: loss=0.092944 lr=0.000020 grad_norm=0.532995
Epoch 28/100 Iteration 163/234: loss=0.088709 lr=0.000020 grad_norm=0.936027
Epoch 28/100 Iteration 164/234: loss=0.094122 lr=0.000020 grad_norm=0.818613
Epoch 28/100 Iteration 165/234: loss=0.098398 lr=0.000020 grad_norm=0.360571
Epoch 28/100 Iteration 166/234: loss=0.099310 lr=0.000020 grad_norm=0.467694
Epoch 28/100 Iteration 167/234: loss=0.092629 lr=0.000020 grad_norm=0.391081
Epoch 28/100 Iteration 168/234: loss=0.093738 lr=0.000020 grad_norm=0.703975
Epoch 28/100 Iteration 169/234: loss=0.098356 lr=0.000020 grad_norm=0.704745
Epoch 28/100 Iteration 170/234: loss=0.090114 lr=0.000020 grad_norm=0.383898
Epoch 28/100 Iteration 171/234: loss=0.091534 lr=0.000020 grad_norm=0.461386
Epoch 28/100 Iteration 172/234: loss=0.097904 lr=0.000020 grad_norm=0.439746
Epoch 28/100 Iteration 173/234: loss=0.101861 lr=0.000020 grad_norm=0.436798
Epoch 28/100 Iteration 174/234: loss=0.097756 lr=0.000020 grad_norm=0.549581
Epoch 28/100 Iteration 175/234: loss=0.095703 lr=0.000020 grad_norm=0.369400
Epoch 28/100 Iteration 176/234: loss=0.085155 lr=0.000020 grad_norm=0.480661
Epoch 28/100 Iteration 177/234: loss=0.097822 lr=0.000020 grad_norm=0.446542
Epoch 28/100 Iteration 178/234: loss=0.105809 lr=0.000020 grad_norm=0.628188
Epoch 28/100 Iteration 179/234: loss=0.082283 lr=0.000020 grad_norm=0.405761
Epoch 28/100 Iteration 180/234: loss=0.096976 lr=0.000020 grad_norm=0.843140
Epoch 28/100 Iteration 181/234: loss=0.100625 lr=0.000020 grad_norm=1.521276
Epoch 28/100 Iteration 182/234: loss=0.118722 lr=0.000020 grad_norm=1.411301
Epoch 28/100 Iteration 183/234: loss=0.095392 lr=0.000020 grad_norm=0.542045
Epoch 28/100 Iteration 184/234: loss=0.107317 lr=0.000020 grad_norm=1.228266
Epoch 28/100 Iteration 185/234: loss=0.104572 lr=0.000020 grad_norm=0.774998
Epoch 28/100 Iteration 186/234: loss=0.103199 lr=0.000020 grad_norm=0.834785
Epoch 28/100 Iteration 187/234: loss=0.087253 lr=0.000020 grad_norm=0.879178
Epoch 28/100 Iteration 188/234: loss=0.102313 lr=0.000020 grad_norm=0.437827
Epoch 28/100 Iteration 189/234: loss=0.106630 lr=0.000020 grad_norm=1.025963
Epoch 28/100 Iteration 190/234: loss=0.101738 lr=0.000020 grad_norm=0.765195
Epoch 28/100 Iteration 191/234: loss=0.095263 lr=0.000020 grad_norm=1.054122
Epoch 28/100 Iteration 192/234: loss=0.085688 lr=0.000020 grad_norm=0.931703
Epoch 28/100 Iteration 193/234: loss=0.099940 lr=0.000020 grad_norm=0.468298
Epoch 28/100 Iteration 194/234: loss=0.093090 lr=0.000020 grad_norm=1.119235
Epoch 28/100 Iteration 195/234: loss=0.085038 lr=0.000020 grad_norm=0.714589
Epoch 28/100 Iteration 196/234: loss=0.092545 lr=0.000020 grad_norm=0.870450
Epoch 28/100 Iteration 197/234: loss=0.090214 lr=0.000020 grad_norm=1.036535
Epoch 28/100 Iteration 198/234: loss=0.096099 lr=0.000020 grad_norm=0.492006
Epoch 28/100 Iteration 199/234: loss=0.085011 lr=0.000020 grad_norm=0.687880
Epoch 28/100 Iteration 200/234: loss=0.093252 lr=0.000020 grad_norm=0.765555
Epoch 28/100 Iteration 201/234: loss=0.099130 lr=0.000020 grad_norm=0.671758
Epoch 28/100 Iteration 202/234: loss=0.088786 lr=0.000020 grad_norm=0.416131
Epoch 28/100 Iteration 203/234: loss=0.092102 lr=0.000020 grad_norm=0.651885
Epoch 28/100 Iteration 204/234: loss=0.104480 lr=0.000020 grad_norm=0.593668
Epoch 28/100 Iteration 205/234: loss=0.098133 lr=0.000020 grad_norm=0.510150
Epoch 28/100 Iteration 206/234: loss=0.090148 lr=0.000020 grad_norm=0.425604
Epoch 28/100 Iteration 207/234: loss=0.095139 lr=0.000020 grad_norm=0.602643
Epoch 28/100 Iteration 208/234: loss=0.114489 lr=0.000020 grad_norm=0.810062
Epoch 28/100 Iteration 209/234: loss=0.090059 lr=0.000020 grad_norm=0.640044
Epoch 28/100 Iteration 210/234: loss=0.096387 lr=0.000020 grad_norm=0.487752
Epoch 28/100 Iteration 211/234: loss=0.091512 lr=0.000020 grad_norm=0.733575
Epoch 28/100 Iteration 212/234: loss=0.104847 lr=0.000020 grad_norm=0.686431
Epoch 28/100 Iteration 213/234: loss=0.100944 lr=0.000020 grad_norm=0.524202
Epoch 28/100 Iteration 214/234: loss=0.095515 lr=0.000020 grad_norm=0.730094
Epoch 28/100 Iteration 215/234: loss=0.100101 lr=0.000020 grad_norm=0.525824
Epoch 28/100 Iteration 216/234: loss=0.093657 lr=0.000020 grad_norm=0.482668
Epoch 28/100 Iteration 217/234: loss=0.092015 lr=0.000020 grad_norm=0.694145
Epoch 28/100 Iteration 218/234: loss=0.093775 lr=0.000020 grad_norm=1.139666
Epoch 28/100 Iteration 219/234: loss=0.108356 lr=0.000020 grad_norm=0.613057
Epoch 28/100 Iteration 220/234: loss=0.106677 lr=0.000020 grad_norm=0.812016
Epoch 28/100 Iteration 221/234: loss=0.101278 lr=0.000020 grad_norm=0.886340
Epoch 28/100 Iteration 222/234: loss=0.092778 lr=0.000020 grad_norm=0.777546
Epoch 28/100 Iteration 223/234: loss=0.093869 lr=0.000020 grad_norm=0.589399
Epoch 28/100 Iteration 224/234: loss=0.094307 lr=0.000020 grad_norm=0.757698
Epoch 28/100 Iteration 225/234: loss=0.087178 lr=0.000020 grad_norm=0.511796
Epoch 28/100 Iteration 226/234: loss=0.100716 lr=0.000020 grad_norm=0.672511
Epoch 28/100 Iteration 227/234: loss=0.091525 lr=0.000020 grad_norm=0.755009
Epoch 28/100 Iteration 228/234: loss=0.080719 lr=0.000020 grad_norm=0.458336
Epoch 28/100 Iteration 229/234: loss=0.108545 lr=0.000020 grad_norm=0.728133
Epoch 28/100 Iteration 230/234: loss=0.102495 lr=0.000020 grad_norm=1.017333
Epoch 28/100 Iteration 231/234: loss=0.107492 lr=0.000020 grad_norm=1.246087
Epoch 28/100 Iteration 232/234: loss=0.094104 lr=0.000020 grad_norm=1.001168
Epoch 28/100 Iteration 233/234: loss=0.097757 lr=0.000020 grad_norm=0.677488
Epoch 28/100 Iteration 234/234: loss=0.099548 lr=0.000020 grad_norm=0.919027
Epoch 28/100 finished. Avg Loss: 0.097419
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 29/100 Iteration 1/234: loss=0.093955 lr=0.000020 grad_norm=0.831801
Epoch 29/100 Iteration 2/234: loss=0.094183 lr=0.000020 grad_norm=0.718007
Epoch 29/100 Iteration 3/234: loss=0.085205 lr=0.000020 grad_norm=1.040140
Epoch 29/100 Iteration 4/234: loss=0.101557 lr=0.000020 grad_norm=0.770127
Epoch 29/100 Iteration 5/234: loss=0.101845 lr=0.000020 grad_norm=0.567898
Epoch 29/100 Iteration 6/234: loss=0.093264 lr=0.000020 grad_norm=0.425072
Epoch 29/100 Iteration 7/234: loss=0.105166 lr=0.000020 grad_norm=0.858786
Epoch 29/100 Iteration 8/234: loss=0.098581 lr=0.000020 grad_norm=1.187717
Epoch 29/100 Iteration 9/234: loss=0.108608 lr=0.000020 grad_norm=0.645687
Epoch 29/100 Iteration 10/234: loss=0.086543 lr=0.000020 grad_norm=0.770961
Epoch 29/100 Iteration 11/234: loss=0.097241 lr=0.000020 grad_norm=1.201618
Epoch 29/100 Iteration 12/234: loss=0.096482 lr=0.000020 grad_norm=0.610192
Epoch 29/100 Iteration 13/234: loss=0.097768 lr=0.000020 grad_norm=1.045955
Epoch 29/100 Iteration 14/234: loss=0.102405 lr=0.000020 grad_norm=1.523002
Epoch 29/100 Iteration 15/234: loss=0.088852 lr=0.000020 grad_norm=0.886105
Epoch 29/100 Iteration 16/234: loss=0.083010 lr=0.000020 grad_norm=0.619252
Epoch 29/100 Iteration 17/234: loss=0.095038 lr=0.000020 grad_norm=0.623817
Epoch 29/100 Iteration 18/234: loss=0.081399 lr=0.000020 grad_norm=0.530952
Epoch 29/100 Iteration 19/234: loss=0.092016 lr=0.000020 grad_norm=0.516969
Epoch 29/100 Iteration 20/234: loss=0.080753 lr=0.000020 grad_norm=0.289665
Epoch 29/100 Iteration 21/234: loss=0.104870 lr=0.000020 grad_norm=0.737200
Epoch 29/100 Iteration 22/234: loss=0.109439 lr=0.000020 grad_norm=1.019763
Epoch 29/100 Iteration 23/234: loss=0.103941 lr=0.000020 grad_norm=0.456497
Epoch 29/100 Iteration 24/234: loss=0.086811 lr=0.000020 grad_norm=0.560302
Epoch 29/100 Iteration 25/234: loss=0.095619 lr=0.000020 grad_norm=0.734437
Epoch 29/100 Iteration 26/234: loss=0.100821 lr=0.000020 grad_norm=0.604989
Epoch 29/100 Iteration 27/234: loss=0.090734 lr=0.000020 grad_norm=0.594247
Epoch 29/100 Iteration 28/234: loss=0.102220 lr=0.000020 grad_norm=0.746446
Epoch 29/100 Iteration 29/234: loss=0.077884 lr=0.000020 grad_norm=0.662073
Epoch 29/100 Iteration 30/234: loss=0.095682 lr=0.000020 grad_norm=0.562242
Epoch 29/100 Iteration 31/234: loss=0.099295 lr=0.000020 grad_norm=0.612357
Epoch 29/100 Iteration 32/234: loss=0.093284 lr=0.000020 grad_norm=0.554344
Epoch 29/100 Iteration 33/234: loss=0.086193 lr=0.000020 grad_norm=0.609767
Epoch 29/100 Iteration 34/234: loss=0.094345 lr=0.000020 grad_norm=0.415942
Epoch 29/100 Iteration 35/234: loss=0.098907 lr=0.000020 grad_norm=0.561046
Epoch 29/100 Iteration 36/234: loss=0.107611 lr=0.000020 grad_norm=0.428222
Epoch 29/100 Iteration 37/234: loss=0.089477 lr=0.000020 grad_norm=0.547578
Epoch 29/100 Iteration 38/234: loss=0.084093 lr=0.000020 grad_norm=0.668827
Epoch 29/100 Iteration 39/234: loss=0.111368 lr=0.000020 grad_norm=0.386316
Epoch 29/100 Iteration 40/234: loss=0.093016 lr=0.000020 grad_norm=0.881041
Epoch 29/100 Iteration 41/234: loss=0.086976 lr=0.000020 grad_norm=0.609517
Epoch 29/100 Iteration 42/234: loss=0.089484 lr=0.000020 grad_norm=0.691565
Epoch 29/100 Iteration 43/234: loss=0.098294 lr=0.000020 grad_norm=1.029735
Epoch 29/100 Iteration 44/234: loss=0.092546 lr=0.000020 grad_norm=0.782442
Epoch 29/100 Iteration 45/234: loss=0.090585 lr=0.000020 grad_norm=0.310866
Epoch 29/100 Iteration 46/234: loss=0.096690 lr=0.000020 grad_norm=0.484912
Epoch 29/100 Iteration 47/234: loss=0.099932 lr=0.000020 grad_norm=0.330296
Epoch 29/100 Iteration 48/234: loss=0.102329 lr=0.000020 grad_norm=0.629711
Epoch 29/100 Iteration 49/234: loss=0.094247 lr=0.000020 grad_norm=0.494581
Epoch 29/100 Iteration 50/234: loss=0.092630 lr=0.000020 grad_norm=0.535681
Epoch 29/100 Iteration 51/234: loss=0.104388 lr=0.000020 grad_norm=0.894333
Epoch 29/100 Iteration 52/234: loss=0.105887 lr=0.000020 grad_norm=0.795214
Epoch 29/100 Iteration 53/234: loss=0.087196 lr=0.000020 grad_norm=0.582242
Epoch 29/100 Iteration 54/234: loss=0.099475 lr=0.000020 grad_norm=1.064787
Epoch 29/100 Iteration 55/234: loss=0.089563 lr=0.000020 grad_norm=1.171063
Epoch 29/100 Iteration 56/234: loss=0.102363 lr=0.000020 grad_norm=0.810078
Epoch 29/100 Iteration 57/234: loss=0.093977 lr=0.000020 grad_norm=0.409507
Epoch 29/100 Iteration 58/234: loss=0.090298 lr=0.000020 grad_norm=0.738861
Epoch 29/100 Iteration 59/234: loss=0.086447 lr=0.000020 grad_norm=1.121805
Epoch 29/100 Iteration 60/234: loss=0.090535 lr=0.000020 grad_norm=1.120987
Epoch 29/100 Iteration 61/234: loss=0.088525 lr=0.000020 grad_norm=1.026183
Epoch 29/100 Iteration 62/234: loss=0.101887 lr=0.000020 grad_norm=0.887722
Epoch 29/100 Iteration 63/234: loss=0.093825 lr=0.000020 grad_norm=0.797859
Epoch 29/100 Iteration 64/234: loss=0.088693 lr=0.000020 grad_norm=0.517861
Epoch 29/100 Iteration 65/234: loss=0.107926 lr=0.000020 grad_norm=0.574692
Epoch 29/100 Iteration 66/234: loss=0.092444 lr=0.000020 grad_norm=0.758763
Epoch 29/100 Iteration 67/234: loss=0.082902 lr=0.000020 grad_norm=1.148136
Epoch 29/100 Iteration 68/234: loss=0.082577 lr=0.000020 grad_norm=0.988164
Epoch 29/100 Iteration 69/234: loss=0.103499 lr=0.000020 grad_norm=0.484563
Epoch 29/100 Iteration 70/234: loss=0.101647 lr=0.000020 grad_norm=1.427989
Epoch 29/100 Iteration 71/234: loss=0.087455 lr=0.000020 grad_norm=1.473007
Epoch 29/100 Iteration 72/234: loss=0.087242 lr=0.000020 grad_norm=0.659226
Epoch 29/100 Iteration 73/234: loss=0.099236 lr=0.000020 grad_norm=0.472821
Epoch 29/100 Iteration 74/234: loss=0.090853 lr=0.000020 grad_norm=0.721985
Epoch 29/100 Iteration 75/234: loss=0.103219 lr=0.000020 grad_norm=0.516192
Epoch 29/100 Iteration 76/234: loss=0.090017 lr=0.000020 grad_norm=0.424105
Epoch 29/100 Iteration 77/234: loss=0.094921 lr=0.000020 grad_norm=0.368216
Epoch 29/100 Iteration 78/234: loss=0.091852 lr=0.000020 grad_norm=0.532322
Epoch 29/100 Iteration 79/234: loss=0.104037 lr=0.000020 grad_norm=0.742466
Epoch 29/100 Iteration 80/234: loss=0.082033 lr=0.000020 grad_norm=0.568502
Epoch 29/100 Iteration 81/234: loss=0.094154 lr=0.000020 grad_norm=0.602436
Epoch 29/100 Iteration 82/234: loss=0.084844 lr=0.000020 grad_norm=1.083786
Epoch 29/100 Iteration 83/234: loss=0.090775 lr=0.000020 grad_norm=0.731823
Epoch 29/100 Iteration 84/234: loss=0.087813 lr=0.000020 grad_norm=0.709245
Epoch 29/100 Iteration 85/234: loss=0.102433 lr=0.000020 grad_norm=1.428978
Epoch 29/100 Iteration 86/234: loss=0.085676 lr=0.000020 grad_norm=0.681902
Epoch 29/100 Iteration 87/234: loss=0.091232 lr=0.000020 grad_norm=1.247176
Epoch 29/100 Iteration 88/234: loss=0.096874 lr=0.000020 grad_norm=1.980036
Epoch 29/100 Iteration 89/234: loss=0.096660 lr=0.000020 grad_norm=1.178301
Epoch 29/100 Iteration 90/234: loss=0.099926 lr=0.000020 grad_norm=0.751102
Epoch 29/100 Iteration 91/234: loss=0.104595 lr=0.000020 grad_norm=1.643881
Epoch 29/100 Iteration 92/234: loss=0.102836 lr=0.000020 grad_norm=1.106394
Epoch 29/100 Iteration 93/234: loss=0.088468 lr=0.000020 grad_norm=0.782240
Epoch 29/100 Iteration 94/234: loss=0.075088 lr=0.000020 grad_norm=1.440321
Epoch 29/100 Iteration 95/234: loss=0.099789 lr=0.000020 grad_norm=0.953573
Epoch 29/100 Iteration 96/234: loss=0.092411 lr=0.000020 grad_norm=1.072549
Epoch 29/100 Iteration 97/234: loss=0.098078 lr=0.000020 grad_norm=1.015483
Epoch 29/100 Iteration 98/234: loss=0.108672 lr=0.000020 grad_norm=0.693848
Epoch 29/100 Iteration 99/234: loss=0.094671 lr=0.000020 grad_norm=1.154412
Epoch 29/100 Iteration 100/234: loss=0.095928 lr=0.000020 grad_norm=0.797528
Epoch 29/100 Iteration 101/234: loss=0.100050 lr=0.000020 grad_norm=0.840817
Epoch 29/100 Iteration 102/234: loss=0.101876 lr=0.000020 grad_norm=0.728191
Epoch 29/100 Iteration 103/234: loss=0.098634 lr=0.000020 grad_norm=0.544570
Epoch 29/100 Iteration 104/234: loss=0.098722 lr=0.000020 grad_norm=0.657625
Epoch 29/100 Iteration 105/234: loss=0.088817 lr=0.000020 grad_norm=0.598536
Epoch 29/100 Iteration 106/234: loss=0.105165 lr=0.000020 grad_norm=1.178882
Epoch 29/100 Iteration 107/234: loss=0.083421 lr=0.000020 grad_norm=0.910760
Epoch 29/100 Iteration 108/234: loss=0.103958 lr=0.000020 grad_norm=0.721011
Epoch 29/100 Iteration 109/234: loss=0.078634 lr=0.000020 grad_norm=1.371976
Epoch 29/100 Iteration 110/234: loss=0.110328 lr=0.000020 grad_norm=1.532153
Epoch 29/100 Iteration 111/234: loss=0.081346 lr=0.000020 grad_norm=0.516156
Epoch 29/100 Iteration 112/234: loss=0.084025 lr=0.000020 grad_norm=1.177800
Epoch 29/100 Iteration 113/234: loss=0.098038 lr=0.000020 grad_norm=1.571774
Epoch 29/100 Iteration 114/234: loss=0.093660 lr=0.000020 grad_norm=0.716154
Epoch 29/100 Iteration 115/234: loss=0.077979 lr=0.000020 grad_norm=0.915202
Epoch 29/100 Iteration 116/234: loss=0.099849 lr=0.000020 grad_norm=0.790804
Epoch 29/100 Iteration 117/234: loss=0.087419 lr=0.000020 grad_norm=0.606926
Epoch 29/100 Iteration 118/234: loss=0.093707 lr=0.000020 grad_norm=0.634828
Epoch 29/100 Iteration 119/234: loss=0.090305 lr=0.000020 grad_norm=0.413653
Epoch 29/100 Iteration 120/234: loss=0.094800 lr=0.000020 grad_norm=0.396217
Epoch 29/100 Iteration 121/234: loss=0.095215 lr=0.000020 grad_norm=0.640292
Epoch 29/100 Iteration 122/234: loss=0.089112 lr=0.000020 grad_norm=0.516016
Epoch 29/100 Iteration 123/234: loss=0.108132 lr=0.000020 grad_norm=0.586393
Epoch 29/100 Iteration 124/234: loss=0.100796 lr=0.000020 grad_norm=0.452289
Epoch 29/100 Iteration 125/234: loss=0.083768 lr=0.000020 grad_norm=0.627194
Epoch 29/100 Iteration 126/234: loss=0.086987 lr=0.000020 grad_norm=0.783947
Epoch 29/100 Iteration 127/234: loss=0.107938 lr=0.000020 grad_norm=0.542355
Epoch 29/100 Iteration 128/234: loss=0.088566 lr=0.000020 grad_norm=0.545482
Epoch 29/100 Iteration 129/234: loss=0.094911 lr=0.000020 grad_norm=0.605978
Epoch 29/100 Iteration 130/234: loss=0.087299 lr=0.000020 grad_norm=0.482938
Epoch 29/100 Iteration 131/234: loss=0.098735 lr=0.000020 grad_norm=0.432016
Epoch 29/100 Iteration 132/234: loss=0.083045 lr=0.000020 grad_norm=0.498031
Epoch 29/100 Iteration 133/234: loss=0.096009 lr=0.000020 grad_norm=0.469944
Epoch 29/100 Iteration 134/234: loss=0.094323 lr=0.000020 grad_norm=0.406093
Epoch 29/100 Iteration 135/234: loss=0.098561 lr=0.000020 grad_norm=0.447576
Epoch 29/100 Iteration 136/234: loss=0.098771 lr=0.000020 grad_norm=0.510366
Epoch 29/100 Iteration 137/234: loss=0.097152 lr=0.000020 grad_norm=0.371464
Epoch 29/100 Iteration 138/234: loss=0.088191 lr=0.000020 grad_norm=0.479254
Epoch 29/100 Iteration 139/234: loss=0.092787 lr=0.000020 grad_norm=0.478736
Epoch 29/100 Iteration 140/234: loss=0.098437 lr=0.000020 grad_norm=0.388403
Epoch 29/100 Iteration 141/234: loss=0.077191 lr=0.000020 grad_norm=0.579081
Epoch 29/100 Iteration 142/234: loss=0.097126 lr=0.000020 grad_norm=0.845266
Epoch 29/100 Iteration 143/234: loss=0.090121 lr=0.000020 grad_norm=0.844259
Epoch 29/100 Iteration 144/234: loss=0.093719 lr=0.000020 grad_norm=0.390127
Epoch 29/100 Iteration 145/234: loss=0.089208 lr=0.000020 grad_norm=0.650618
Epoch 29/100 Iteration 146/234: loss=0.097343 lr=0.000020 grad_norm=0.896512
Epoch 29/100 Iteration 147/234: loss=0.114013 lr=0.000020 grad_norm=0.996794
Epoch 29/100 Iteration 148/234: loss=0.101180 lr=0.000020 grad_norm=1.001065
Epoch 29/100 Iteration 149/234: loss=0.111589 lr=0.000020 grad_norm=0.963102
Epoch 29/100 Iteration 150/234: loss=0.084610 lr=0.000020 grad_norm=0.710000
Epoch 29/100 Iteration 151/234: loss=0.106254 lr=0.000020 grad_norm=0.449777
Epoch 29/100 Iteration 152/234: loss=0.091366 lr=0.000020 grad_norm=1.066011
Epoch 29/100 Iteration 153/234: loss=0.089119 lr=0.000020 grad_norm=1.364390
Epoch 29/100 Iteration 154/234: loss=0.095718 lr=0.000020 grad_norm=1.071783
Epoch 29/100 Iteration 155/234: loss=0.085200 lr=0.000020 grad_norm=0.404694
Epoch 29/100 Iteration 156/234: loss=0.092299 lr=0.000020 grad_norm=1.396582
Epoch 29/100 Iteration 157/234: loss=0.110758 lr=0.000020 grad_norm=1.848618
Epoch 29/100 Iteration 158/234: loss=0.093756 lr=0.000020 grad_norm=1.212765
Epoch 29/100 Iteration 159/234: loss=0.091309 lr=0.000020 grad_norm=0.506990
Epoch 29/100 Iteration 160/234: loss=0.082879 lr=0.000020 grad_norm=1.050724
Epoch 29/100 Iteration 161/234: loss=0.101490 lr=0.000020 grad_norm=0.435870
Epoch 29/100 Iteration 162/234: loss=0.101312 lr=0.000020 grad_norm=1.856987
Epoch 29/100 Iteration 163/234: loss=0.086750 lr=0.000020 grad_norm=1.794559
Epoch 29/100 Iteration 164/234: loss=0.097784 lr=0.000020 grad_norm=0.625094
Epoch 29/100 Iteration 165/234: loss=0.102209 lr=0.000020 grad_norm=2.194037
Epoch 29/100 Iteration 166/234: loss=0.109684 lr=0.000020 grad_norm=1.575073
Epoch 29/100 Iteration 167/234: loss=0.086833 lr=0.000020 grad_norm=0.907725
Epoch 29/100 Iteration 168/234: loss=0.084544 lr=0.000020 grad_norm=1.306525
Epoch 29/100 Iteration 169/234: loss=0.090495 lr=0.000020 grad_norm=0.753824
Epoch 29/100 Iteration 170/234: loss=0.088266 lr=0.000020 grad_norm=0.692523
Epoch 29/100 Iteration 171/234: loss=0.102517 lr=0.000020 grad_norm=0.620362
Epoch 29/100 Iteration 172/234: loss=0.093594 lr=0.000020 grad_norm=0.576973
Epoch 29/100 Iteration 173/234: loss=0.109127 lr=0.000020 grad_norm=0.671650
Epoch 29/100 Iteration 174/234: loss=0.100734 lr=0.000020 grad_norm=0.358983
Epoch 29/100 Iteration 175/234: loss=0.097890 lr=0.000020 grad_norm=0.888628
Epoch 29/100 Iteration 176/234: loss=0.078073 lr=0.000020 grad_norm=0.853076
Epoch 29/100 Iteration 177/234: loss=0.095343 lr=0.000020 grad_norm=0.335718
Epoch 29/100 Iteration 178/234: loss=0.084366 lr=0.000020 grad_norm=0.698346
Epoch 29/100 Iteration 179/234: loss=0.092626 lr=0.000020 grad_norm=0.585580
Epoch 29/100 Iteration 180/234: loss=0.102877 lr=0.000020 grad_norm=0.390003
Epoch 29/100 Iteration 181/234: loss=0.095338 lr=0.000020 grad_norm=0.401957
Epoch 29/100 Iteration 182/234: loss=0.095541 lr=0.000020 grad_norm=0.348240
Epoch 29/100 Iteration 183/234: loss=0.093642 lr=0.000020 grad_norm=0.388838
Epoch 29/100 Iteration 184/234: loss=0.097190 lr=0.000020 grad_norm=0.445018
Epoch 29/100 Iteration 185/234: loss=0.100453 lr=0.000020 grad_norm=0.395495
Epoch 29/100 Iteration 186/234: loss=0.100111 lr=0.000020 grad_norm=0.503269
Epoch 29/100 Iteration 187/234: loss=0.081565 lr=0.000020 grad_norm=0.418649
Epoch 29/100 Iteration 188/234: loss=0.094506 lr=0.000020 grad_norm=0.741349
Epoch 29/100 Iteration 189/234: loss=0.108355 lr=0.000020 grad_norm=0.770234
Epoch 29/100 Iteration 190/234: loss=0.090617 lr=0.000020 grad_norm=0.478422
Epoch 29/100 Iteration 191/234: loss=0.091416 lr=0.000020 grad_norm=0.634937
Epoch 29/100 Iteration 192/234: loss=0.088251 lr=0.000020 grad_norm=0.585108
Epoch 29/100 Iteration 193/234: loss=0.094343 lr=0.000020 grad_norm=0.475985
Epoch 29/100 Iteration 194/234: loss=0.094182 lr=0.000020 grad_norm=0.656486
Epoch 29/100 Iteration 195/234: loss=0.088126 lr=0.000020 grad_norm=0.339924
Epoch 29/100 Iteration 196/234: loss=0.092457 lr=0.000020 grad_norm=0.797022
Epoch 29/100 Iteration 197/234: loss=0.091875 lr=0.000020 grad_norm=0.635539
Epoch 29/100 Iteration 198/234: loss=0.098993 lr=0.000020 grad_norm=0.762451
Epoch 29/100 Iteration 199/234: loss=0.100616 lr=0.000020 grad_norm=1.232793
Epoch 29/100 Iteration 200/234: loss=0.104865 lr=0.000020 grad_norm=0.620856
Epoch 29/100 Iteration 201/234: loss=0.095129 lr=0.000020 grad_norm=0.809299
Epoch 29/100 Iteration 202/234: loss=0.099326 lr=0.000020 grad_norm=1.639584
Epoch 29/100 Iteration 203/234: loss=0.091432 lr=0.000020 grad_norm=1.621449
Epoch 29/100 Iteration 204/234: loss=0.095617 lr=0.000020 grad_norm=0.801694
Epoch 29/100 Iteration 205/234: loss=0.092576 lr=0.000020 grad_norm=1.153315
Epoch 29/100 Iteration 206/234: loss=0.098348 lr=0.000020 grad_norm=1.339112
Epoch 29/100 Iteration 207/234: loss=0.082075 lr=0.000020 grad_norm=0.712073
Epoch 29/100 Iteration 208/234: loss=0.094258 lr=0.000020 grad_norm=0.891279
Epoch 29/100 Iteration 209/234: loss=0.086047 lr=0.000020 grad_norm=0.666451
Epoch 29/100 Iteration 210/234: loss=0.087038 lr=0.000020 grad_norm=0.537314
Epoch 29/100 Iteration 211/234: loss=0.093747 lr=0.000020 grad_norm=1.015985
Epoch 29/100 Iteration 212/234: loss=0.078458 lr=0.000020 grad_norm=0.538638
Epoch 29/100 Iteration 213/234: loss=0.094995 lr=0.000020 grad_norm=1.788482
Epoch 29/100 Iteration 214/234: loss=0.088936 lr=0.000020 grad_norm=1.218180
Epoch 29/100 Iteration 215/234: loss=0.101498 lr=0.000020 grad_norm=0.925321
Epoch 29/100 Iteration 216/234: loss=0.102397 lr=0.000020 grad_norm=1.534291
Epoch 29/100 Iteration 217/234: loss=0.091680 lr=0.000020 grad_norm=0.856190
Epoch 29/100 Iteration 218/234: loss=0.092050 lr=0.000020 grad_norm=0.688715
Epoch 29/100 Iteration 219/234: loss=0.085776 lr=0.000020 grad_norm=1.189667
Epoch 29/100 Iteration 220/234: loss=0.097716 lr=0.000020 grad_norm=0.874979
Epoch 29/100 Iteration 221/234: loss=0.102980 lr=0.000020 grad_norm=0.654273
Epoch 29/100 Iteration 222/234: loss=0.091122 lr=0.000020 grad_norm=0.595740
Epoch 29/100 Iteration 223/234: loss=0.088067 lr=0.000020 grad_norm=0.382825
Epoch 29/100 Iteration 224/234: loss=0.094844 lr=0.000020 grad_norm=0.666262
Epoch 29/100 Iteration 225/234: loss=0.081514 lr=0.000020 grad_norm=0.471637
Epoch 29/100 Iteration 226/234: loss=0.088285 lr=0.000020 grad_norm=0.597867
Epoch 29/100 Iteration 227/234: loss=0.090547 lr=0.000020 grad_norm=0.753547
Epoch 29/100 Iteration 228/234: loss=0.098882 lr=0.000020 grad_norm=0.736849
Epoch 29/100 Iteration 229/234: loss=0.101094 lr=0.000020 grad_norm=0.522080
Epoch 29/100 Iteration 230/234: loss=0.109955 lr=0.000020 grad_norm=1.600344
Epoch 29/100 Iteration 231/234: loss=0.081517 lr=0.000020 grad_norm=1.659153
Epoch 29/100 Iteration 232/234: loss=0.097478 lr=0.000020 grad_norm=0.708842
Epoch 29/100 Iteration 233/234: loss=0.090998 lr=0.000020 grad_norm=1.660598
Epoch 29/100 Iteration 234/234: loss=0.093079 lr=0.000020 grad_norm=1.272286
Epoch 29/100 finished. Avg Loss: 0.094409
Epoch 30/100 Iteration 1/234: loss=0.098199 lr=0.000020 grad_norm=0.970126
Epoch 30/100 Iteration 2/234: loss=0.098922 lr=0.000020 grad_norm=1.420393
Epoch 30/100 Iteration 3/234: loss=0.086251 lr=0.000020 grad_norm=1.173528
Epoch 30/100 Iteration 4/234: loss=0.101317 lr=0.000020 grad_norm=0.979665
Epoch 30/100 Iteration 5/234: loss=0.094487 lr=0.000020 grad_norm=0.804699
Epoch 30/100 Iteration 6/234: loss=0.096938 lr=0.000020 grad_norm=0.450068
Epoch 30/100 Iteration 7/234: loss=0.093288 lr=0.000020 grad_norm=0.509759
Epoch 30/100 Iteration 8/234: loss=0.098508 lr=0.000020 grad_norm=0.376527
Epoch 30/100 Iteration 9/234: loss=0.093137 lr=0.000020 grad_norm=0.412482
Epoch 30/100 Iteration 10/234: loss=0.088149 lr=0.000020 grad_norm=0.543926
Epoch 30/100 Iteration 11/234: loss=0.092475 lr=0.000020 grad_norm=0.830165
Epoch 30/100 Iteration 12/234: loss=0.091927 lr=0.000020 grad_norm=0.757721
Epoch 30/100 Iteration 13/234: loss=0.093871 lr=0.000020 grad_norm=0.507327
Epoch 30/100 Iteration 14/234: loss=0.079510 lr=0.000020 grad_norm=0.819538
Epoch 30/100 Iteration 15/234: loss=0.094680 lr=0.000020 grad_norm=0.659240
Epoch 30/100 Iteration 16/234: loss=0.086554 lr=0.000020 grad_norm=0.454407
Epoch 30/100 Iteration 17/234: loss=0.098886 lr=0.000020 grad_norm=0.770304
Epoch 30/100 Iteration 18/234: loss=0.104150 lr=0.000020 grad_norm=0.703258
Epoch 30/100 Iteration 19/234: loss=0.091274 lr=0.000020 grad_norm=0.649949
Epoch 30/100 Iteration 20/234: loss=0.094450 lr=0.000020 grad_norm=0.697399
Epoch 30/100 Iteration 21/234: loss=0.097883 lr=0.000020 grad_norm=0.662449
Epoch 30/100 Iteration 22/234: loss=0.088746 lr=0.000020 grad_norm=0.649669
Epoch 30/100 Iteration 23/234: loss=0.081000 lr=0.000020 grad_norm=0.681919
Epoch 30/100 Iteration 24/234: loss=0.090882 lr=0.000020 grad_norm=0.455067
Epoch 30/100 Iteration 25/234: loss=0.103006 lr=0.000020 grad_norm=0.348072
Epoch 30/100 Iteration 26/234: loss=0.078935 lr=0.000020 grad_norm=0.475270
Epoch 30/100 Iteration 27/234: loss=0.092668 lr=0.000020 grad_norm=0.416012
Epoch 30/100 Iteration 28/234: loss=0.093865 lr=0.000020 grad_norm=0.530693
Epoch 30/100 Iteration 29/234: loss=0.078041 lr=0.000020 grad_norm=0.530005
Epoch 30/100 Iteration 30/234: loss=0.084233 lr=0.000020 grad_norm=0.596454
Epoch 30/100 Iteration 31/234: loss=0.089172 lr=0.000020 grad_norm=0.657540
Epoch 30/100 Iteration 32/234: loss=0.088148 lr=0.000020 grad_norm=0.591643
Epoch 30/100 Iteration 33/234: loss=0.091727 lr=0.000020 grad_norm=1.094510
Epoch 30/100 Iteration 34/234: loss=0.097687 lr=0.000020 grad_norm=1.104635
Epoch 30/100 Iteration 35/234: loss=0.093751 lr=0.000020 grad_norm=0.496358
Epoch 30/100 Iteration 36/234: loss=0.095882 lr=0.000020 grad_norm=1.342383
Epoch 30/100 Iteration 37/234: loss=0.099331 lr=0.000020 grad_norm=0.995507
Epoch 30/100 Iteration 38/234: loss=0.086361 lr=0.000020 grad_norm=0.687671
Epoch 30/100 Iteration 39/234: loss=0.083623 lr=0.000020 grad_norm=0.772479
Epoch 30/100 Iteration 40/234: loss=0.091076 lr=0.000020 grad_norm=0.561347
Epoch 30/100 Iteration 41/234: loss=0.094449 lr=0.000020 grad_norm=0.888623
Epoch 30/100 Iteration 42/234: loss=0.096911 lr=0.000020 grad_norm=1.058453
Epoch 30/100 Iteration 43/234: loss=0.080870 lr=0.000020 grad_norm=0.498274
Epoch 30/100 Iteration 44/234: loss=0.102255 lr=0.000020 grad_norm=0.636626
Epoch 30/100 Iteration 45/234: loss=0.089780 lr=0.000020 grad_norm=0.621436
Epoch 30/100 Iteration 46/234: loss=0.079305 lr=0.000020 grad_norm=0.453810
Epoch 30/100 Iteration 47/234: loss=0.100500 lr=0.000020 grad_norm=0.883292
Epoch 30/100 Iteration 48/234: loss=0.080648 lr=0.000020 grad_norm=0.671457
Epoch 30/100 Iteration 49/234: loss=0.090251 lr=0.000020 grad_norm=0.441432
Epoch 30/100 Iteration 50/234: loss=0.099381 lr=0.000020 grad_norm=0.775661
Epoch 30/100 Iteration 51/234: loss=0.098989 lr=0.000020 grad_norm=0.591423
Epoch 30/100 Iteration 52/234: loss=0.080181 lr=0.000020 grad_norm=0.557773
Epoch 30/100 Iteration 53/234: loss=0.092145 lr=0.000020 grad_norm=0.937692
Epoch 30/100 Iteration 54/234: loss=0.087362 lr=0.000020 grad_norm=0.491007
Epoch 30/100 Iteration 55/234: loss=0.094648 lr=0.000020 grad_norm=0.684838
Epoch 30/100 Iteration 56/234: loss=0.093706 lr=0.000020 grad_norm=1.018202
Epoch 30/100 Iteration 57/234: loss=0.094168 lr=0.000020 grad_norm=1.061071
Epoch 30/100 Iteration 58/234: loss=0.083848 lr=0.000020 grad_norm=0.969269
Epoch 30/100 Iteration 59/234: loss=0.095738 lr=0.000020 grad_norm=0.537997
Epoch 30/100 Iteration 60/234: loss=0.089653 lr=0.000020 grad_norm=0.529390
Epoch 30/100 Iteration 61/234: loss=0.098751 lr=0.000020 grad_norm=1.132286
Epoch 30/100 Iteration 62/234: loss=0.095795 lr=0.000020 grad_norm=1.839269
Epoch 30/100 Iteration 63/234: loss=0.087644 lr=0.000020 grad_norm=1.573700
Epoch 30/100 Iteration 64/234: loss=0.104757 lr=0.000020 grad_norm=0.479432
Epoch 30/100 Iteration 65/234: loss=0.089694 lr=0.000020 grad_norm=1.696568
Epoch 30/100 Iteration 66/234: loss=0.079894 lr=0.000020 grad_norm=1.470092
Epoch 30/100 Iteration 67/234: loss=0.081886 lr=0.000020 grad_norm=0.790091
Epoch 30/100 Iteration 68/234: loss=0.097183 lr=0.000020 grad_norm=2.063563
Epoch 30/100 Iteration 69/234: loss=0.085095 lr=0.000020 grad_norm=1.572622
Epoch 30/100 Iteration 70/234: loss=0.081341 lr=0.000020 grad_norm=0.797825
Epoch 30/100 Iteration 71/234: loss=0.094698 lr=0.000020 grad_norm=0.750494
Epoch 30/100 Iteration 72/234: loss=0.090973 lr=0.000020 grad_norm=0.621741
Epoch 30/100 Iteration 73/234: loss=0.090779 lr=0.000020 grad_norm=1.063556
Epoch 30/100 Iteration 74/234: loss=0.096039 lr=0.000020 grad_norm=0.673045
Epoch 30/100 Iteration 75/234: loss=0.087132 lr=0.000020 grad_norm=0.983749
Epoch 30/100 Iteration 76/234: loss=0.091744 lr=0.000020 grad_norm=1.196268
Epoch 30/100 Iteration 77/234: loss=0.086449 lr=0.000020 grad_norm=0.353556
Epoch 30/100 Iteration 78/234: loss=0.082097 lr=0.000020 grad_norm=1.030044
Epoch 30/100 Iteration 79/234: loss=0.077863 lr=0.000020 grad_norm=0.460633
Epoch 30/100 Iteration 80/234: loss=0.089354 lr=0.000020 grad_norm=0.792111
Epoch 30/100 Iteration 81/234: loss=0.088826 lr=0.000020 grad_norm=0.696898
Epoch 30/100 Iteration 82/234: loss=0.091370 lr=0.000020 grad_norm=0.510853
Epoch 30/100 Iteration 83/234: loss=0.088635 lr=0.000020 grad_norm=0.980472
Epoch 30/100 Iteration 84/234: loss=0.089491 lr=0.000020 grad_norm=0.439367
Epoch 30/100 Iteration 85/234: loss=0.099375 lr=0.000020 grad_norm=0.683602
Epoch 30/100 Iteration 86/234: loss=0.098176 lr=0.000020 grad_norm=0.810131
Epoch 30/100 Iteration 87/234: loss=0.097358 lr=0.000020 grad_norm=0.446591
Epoch 30/100 Iteration 88/234: loss=0.084763 lr=0.000020 grad_norm=0.512592
Epoch 30/100 Iteration 89/234: loss=0.102076 lr=0.000020 grad_norm=0.476686
Epoch 30/100 Iteration 90/234: loss=0.089709 lr=0.000020 grad_norm=0.452893
Epoch 30/100 Iteration 91/234: loss=0.098283 lr=0.000020 grad_norm=0.603563
Epoch 30/100 Iteration 92/234: loss=0.089113 lr=0.000020 grad_norm=0.464141
Epoch 30/100 Iteration 93/234: loss=0.092349 lr=0.000020 grad_norm=0.656002
Epoch 30/100 Iteration 94/234: loss=0.089327 lr=0.000020 grad_norm=1.101549
Epoch 30/100 Iteration 95/234: loss=0.092258 lr=0.000020 grad_norm=1.093958
Epoch 30/100 Iteration 96/234: loss=0.098340 lr=0.000020 grad_norm=0.441067
Epoch 30/100 Iteration 97/234: loss=0.078843 lr=0.000020 grad_norm=0.781815
Epoch 30/100 Iteration 98/234: loss=0.094671 lr=0.000020 grad_norm=1.052735
Epoch 30/100 Iteration 99/234: loss=0.080453 lr=0.000020 grad_norm=0.918215
Epoch 30/100 Iteration 100/234: loss=0.091650 lr=0.000020 grad_norm=0.576584
Epoch 30/100 Iteration 101/234: loss=0.105429 lr=0.000020 grad_norm=0.698606
Epoch 30/100 Iteration 102/234: loss=0.097167 lr=0.000020 grad_norm=0.678131
Epoch 30/100 Iteration 103/234: loss=0.090146 lr=0.000020 grad_norm=0.909879
Epoch 30/100 Iteration 104/234: loss=0.108663 lr=0.000020 grad_norm=0.909010
Epoch 30/100 Iteration 105/234: loss=0.092444 lr=0.000020 grad_norm=1.123209
Epoch 30/100 Iteration 106/234: loss=0.094821 lr=0.000020 grad_norm=0.975849
Epoch 30/100 Iteration 107/234: loss=0.073715 lr=0.000020 grad_norm=0.499331
Epoch 30/100 Iteration 108/234: loss=0.098205 lr=0.000020 grad_norm=0.446550
Epoch 30/100 Iteration 109/234: loss=0.089890 lr=0.000020 grad_norm=0.815430
Epoch 30/100 Iteration 110/234: loss=0.092767 lr=0.000020 grad_norm=0.688329
Epoch 30/100 Iteration 111/234: loss=0.085982 lr=0.000020 grad_norm=0.444129
Epoch 30/100 Iteration 112/234: loss=0.086826 lr=0.000020 grad_norm=0.657308
Epoch 30/100 Iteration 113/234: loss=0.078900 lr=0.000020 grad_norm=0.583162
Epoch 30/100 Iteration 114/234: loss=0.089821 lr=0.000020 grad_norm=0.584956
Epoch 30/100 Iteration 115/234: loss=0.080693 lr=0.000020 grad_norm=0.495327
Epoch 30/100 Iteration 116/234: loss=0.100842 lr=0.000020 grad_norm=0.469403
Epoch 30/100 Iteration 117/234: loss=0.101566 lr=0.000020 grad_norm=0.579489
Epoch 30/100 Iteration 118/234: loss=0.092761 lr=0.000020 grad_norm=0.959445
Epoch 30/100 Iteration 119/234: loss=0.094334 lr=0.000020 grad_norm=1.298040
Epoch 30/100 Iteration 120/234: loss=0.088564 lr=0.000020 grad_norm=1.161454
Epoch 30/100 Iteration 121/234: loss=0.086883 lr=0.000020 grad_norm=0.575110
Epoch 30/100 Iteration 122/234: loss=0.105470 lr=0.000020 grad_norm=0.522797
Epoch 30/100 Iteration 123/234: loss=0.096567 lr=0.000020 grad_norm=0.722635
Epoch 30/100 Iteration 124/234: loss=0.098346 lr=0.000020 grad_norm=0.745006
Epoch 30/100 Iteration 125/234: loss=0.087394 lr=0.000020 grad_norm=0.698634
Epoch 30/100 Iteration 126/234: loss=0.088707 lr=0.000020 grad_norm=0.696971
Epoch 30/100 Iteration 127/234: loss=0.098898 lr=0.000020 grad_norm=0.573230
Epoch 30/100 Iteration 128/234: loss=0.096844 lr=0.000020 grad_norm=0.467518
Epoch 30/100 Iteration 129/234: loss=0.086526 lr=0.000020 grad_norm=0.471679
Epoch 30/100 Iteration 130/234: loss=0.089518 lr=0.000020 grad_norm=0.472525
Epoch 30/100 Iteration 131/234: loss=0.091689 lr=0.000020 grad_norm=0.808418
Epoch 30/100 Iteration 132/234: loss=0.086147 lr=0.000020 grad_norm=0.944469
Epoch 30/100 Iteration 133/234: loss=0.097151 lr=0.000020 grad_norm=0.941960
Epoch 30/100 Iteration 134/234: loss=0.092636 lr=0.000020 grad_norm=0.600113
Epoch 30/100 Iteration 135/234: loss=0.085493 lr=0.000020 grad_norm=0.358086
Epoch 30/100 Iteration 136/234: loss=0.092167 lr=0.000020 grad_norm=0.411277
Epoch 30/100 Iteration 137/234: loss=0.092451 lr=0.000020 grad_norm=0.367093
Epoch 30/100 Iteration 138/234: loss=0.080698 lr=0.000020 grad_norm=0.570870
Epoch 30/100 Iteration 139/234: loss=0.085776 lr=0.000020 grad_norm=0.696203
Epoch 30/100 Iteration 140/234: loss=0.090184 lr=0.000020 grad_norm=0.457461
Epoch 30/100 Iteration 141/234: loss=0.091142 lr=0.000020 grad_norm=0.353078
Epoch 30/100 Iteration 142/234: loss=0.090797 lr=0.000020 grad_norm=0.570335
Epoch 30/100 Iteration 143/234: loss=0.082171 lr=0.000020 grad_norm=0.573513
Epoch 30/100 Iteration 144/234: loss=0.083814 lr=0.000020 grad_norm=0.368939
Epoch 30/100 Iteration 145/234: loss=0.100867 lr=0.000020 grad_norm=0.547013
Epoch 30/100 Iteration 146/234: loss=0.109018 lr=0.000020 grad_norm=1.172615
Epoch 30/100 Iteration 147/234: loss=0.088497 lr=0.000020 grad_norm=2.163524
Epoch 30/100 Iteration 148/234: loss=0.093125 lr=0.000020 grad_norm=1.971559
Epoch 30/100 Iteration 149/234: loss=0.105811 lr=0.000020 grad_norm=0.611363
Epoch 30/100 Iteration 150/234: loss=0.085248 lr=0.000020 grad_norm=1.210683
Epoch 30/100 Iteration 151/234: loss=0.101673 lr=0.000020 grad_norm=0.750272
Epoch 30/100 Iteration 152/234: loss=0.088273 lr=0.000020 grad_norm=0.959574
Epoch 30/100 Iteration 153/234: loss=0.089462 lr=0.000020 grad_norm=1.409789
Epoch 30/100 Iteration 154/234: loss=0.096821 lr=0.000020 grad_norm=0.607015
Epoch 30/100 Iteration 155/234: loss=0.096861 lr=0.000020 grad_norm=1.130453
Epoch 30/100 Iteration 156/234: loss=0.097624 lr=0.000020 grad_norm=0.687610
Epoch 30/100 Iteration 157/234: loss=0.095625 lr=0.000020 grad_norm=1.132260
Epoch 30/100 Iteration 158/234: loss=0.073942 lr=0.000020 grad_norm=1.662587
Epoch 30/100 Iteration 159/234: loss=0.081141 lr=0.000020 grad_norm=0.450688
Epoch 30/100 Iteration 160/234: loss=0.092226 lr=0.000020 grad_norm=1.566582
Epoch 30/100 Iteration 161/234: loss=0.094418 lr=0.000020 grad_norm=1.005243
Epoch 30/100 Iteration 162/234: loss=0.078301 lr=0.000020 grad_norm=0.905067
Epoch 30/100 Iteration 163/234: loss=0.091681 lr=0.000020 grad_norm=1.609492
Epoch 30/100 Iteration 164/234: loss=0.100544 lr=0.000020 grad_norm=1.197449
Epoch 30/100 Iteration 165/234: loss=0.097789 lr=0.000020 grad_norm=0.795553
Epoch 30/100 Iteration 166/234: loss=0.091164 lr=0.000020 grad_norm=1.144942
Epoch 30/100 Iteration 167/234: loss=0.093254 lr=0.000020 grad_norm=0.594379
Epoch 30/100 Iteration 168/234: loss=0.086596 lr=0.000020 grad_norm=1.363418
Epoch 30/100 Iteration 169/234: loss=0.077855 lr=0.000020 grad_norm=0.680426
Epoch 30/100 Iteration 170/234: loss=0.094859 lr=0.000020 grad_norm=1.308780
Epoch 30/100 Iteration 171/234: loss=0.091780 lr=0.000020 grad_norm=1.863295
Epoch 30/100 Iteration 172/234: loss=0.080869 lr=0.000020 grad_norm=0.437358
Epoch 30/100 Iteration 173/234: loss=0.085883 lr=0.000020 grad_norm=1.838985
Epoch 30/100 Iteration 174/234: loss=0.100887 lr=0.000020 grad_norm=0.761418
Epoch 30/100 Iteration 175/234: loss=0.091846 lr=0.000020 grad_norm=1.500075
Epoch 30/100 Iteration 176/234: loss=0.089018 lr=0.000020 grad_norm=1.794811
Epoch 30/100 Iteration 177/234: loss=0.087893 lr=0.000020 grad_norm=0.703516
Epoch 30/100 Iteration 178/234: loss=0.099357 lr=0.000020 grad_norm=1.211949
Epoch 30/100 Iteration 179/234: loss=0.086620 lr=0.000020 grad_norm=0.847930
Epoch 30/100 Iteration 180/234: loss=0.096930 lr=0.000020 grad_norm=0.729175
Epoch 30/100 Iteration 181/234: loss=0.090148 lr=0.000020 grad_norm=1.011035
Epoch 30/100 Iteration 182/234: loss=0.092500 lr=0.000020 grad_norm=0.828678
Epoch 30/100 Iteration 183/234: loss=0.079165 lr=0.000020 grad_norm=1.310571
Epoch 30/100 Iteration 184/234: loss=0.091400 lr=0.000020 grad_norm=0.494126
Epoch 30/100 Iteration 185/234: loss=0.087227 lr=0.000020 grad_norm=1.259562
Epoch 30/100 Iteration 186/234: loss=0.078984 lr=0.000020 grad_norm=1.217815
Epoch 30/100 Iteration 187/234: loss=0.086157 lr=0.000020 grad_norm=0.372104
Epoch 30/100 Iteration 188/234: loss=0.100476 lr=0.000020 grad_norm=1.102601
Epoch 30/100 Iteration 189/234: loss=0.089536 lr=0.000020 grad_norm=1.280664
Epoch 30/100 Iteration 190/234: loss=0.091225 lr=0.000020 grad_norm=0.631569
Epoch 30/100 Iteration 191/234: loss=0.082626 lr=0.000020 grad_norm=0.688763
Epoch 30/100 Iteration 192/234: loss=0.095308 lr=0.000020 grad_norm=1.080437
Epoch 30/100 Iteration 193/234: loss=0.082221 lr=0.000020 grad_norm=1.053254
Epoch 30/100 Iteration 194/234: loss=0.098312 lr=0.000020 grad_norm=0.587386
Epoch 30/100 Iteration 195/234: loss=0.093210 lr=0.000020 grad_norm=0.648605
Epoch 30/100 Iteration 196/234: loss=0.098341 lr=0.000020 grad_norm=0.954315
Epoch 30/100 Iteration 197/234: loss=0.080033 lr=0.000020 grad_norm=0.683362
Epoch 30/100 Iteration 198/234: loss=0.100404 lr=0.000020 grad_norm=0.440400
Epoch 30/100 Iteration 199/234: loss=0.092839 lr=0.000020 grad_norm=1.048246
Epoch 30/100 Iteration 200/234: loss=0.102607 lr=0.000020 grad_norm=1.480773
Epoch 30/100 Iteration 201/234: loss=0.098268 lr=0.000020 grad_norm=1.000260
Epoch 30/100 Iteration 202/234: loss=0.102673 lr=0.000020 grad_norm=0.780733
Epoch 30/100 Iteration 203/234: loss=0.100539 lr=0.000020 grad_norm=1.654005
Epoch 30/100 Iteration 204/234: loss=0.094633 lr=0.000020 grad_norm=1.117220
Epoch 30/100 Iteration 205/234: loss=0.101010 lr=0.000020 grad_norm=0.519850
Epoch 30/100 Iteration 206/234: loss=0.084009 lr=0.000020 grad_norm=1.376354
Epoch 30/100 Iteration 207/234: loss=0.081734 lr=0.000020 grad_norm=1.050411
Epoch 30/100 Iteration 208/234: loss=0.099006 lr=0.000020 grad_norm=0.735217
Epoch 30/100 Iteration 209/234: loss=0.094232 lr=0.000020 grad_norm=1.814270
Epoch 30/100 Iteration 210/234: loss=0.073173 lr=0.000020 grad_norm=1.108658
Epoch 30/100 Iteration 211/234: loss=0.083370 lr=0.000020 grad_norm=0.903389
Epoch 30/100 Iteration 212/234: loss=0.094860 lr=0.000020 grad_norm=1.274432
Epoch 30/100 Iteration 213/234: loss=0.100026 lr=0.000020 grad_norm=0.409704
Epoch 30/100 Iteration 214/234: loss=0.102961 lr=0.000020 grad_norm=1.203722
Epoch 30/100 Iteration 215/234: loss=0.087468 lr=0.000020 grad_norm=0.874259
Epoch 30/100 Iteration 216/234: loss=0.088427 lr=0.000020 grad_norm=0.728522
Epoch 30/100 Iteration 217/234: loss=0.077073 lr=0.000020 grad_norm=0.729027
Epoch 30/100 Iteration 218/234: loss=0.078343 lr=0.000020 grad_norm=0.574509
Epoch 30/100 Iteration 219/234: loss=0.092250 lr=0.000020 grad_norm=1.018857
Epoch 30/100 Iteration 220/234: loss=0.086490 lr=0.000020 grad_norm=0.741294
Epoch 30/100 Iteration 221/234: loss=0.087242 lr=0.000020 grad_norm=0.557520
Epoch 30/100 Iteration 222/234: loss=0.086881 lr=0.000020 grad_norm=0.370548
Epoch 30/100 Iteration 223/234: loss=0.099367 lr=0.000020 grad_norm=0.605742
Epoch 30/100 Iteration 224/234: loss=0.090280 lr=0.000020 grad_norm=0.474583
Epoch 30/100 Iteration 225/234: loss=0.088204 lr=0.000020 grad_norm=0.442735
Epoch 30/100 Iteration 226/234: loss=0.093658 lr=0.000020 grad_norm=0.631227
Epoch 30/100 Iteration 227/234: loss=0.086860 lr=0.000020 grad_norm=0.601200
Epoch 30/100 Iteration 228/234: loss=0.105015 lr=0.000020 grad_norm=0.586361
Epoch 30/100 Iteration 229/234: loss=0.085536 lr=0.000020 grad_norm=0.566130
Epoch 30/100 Iteration 230/234: loss=0.091744 lr=0.000020 grad_norm=0.535992
Epoch 30/100 Iteration 231/234: loss=0.082881 lr=0.000020 grad_norm=0.386122
Epoch 30/100 Iteration 232/234: loss=0.080329 lr=0.000020 grad_norm=0.719690
Epoch 30/100 Iteration 233/234: loss=0.092335 lr=0.000020 grad_norm=0.920786
Epoch 30/100 Iteration 234/234: loss=0.098308 lr=0.000020 grad_norm=0.915699
Epoch 30/100 finished. Avg Loss: 0.091319
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 31/100 Iteration 1/234: loss=0.078115 lr=0.000020 grad_norm=0.495694
Epoch 31/100 Iteration 2/234: loss=0.083659 lr=0.000020 grad_norm=0.608334
Epoch 31/100 Iteration 3/234: loss=0.084953 lr=0.000020 grad_norm=0.790763
Epoch 31/100 Iteration 4/234: loss=0.098425 lr=0.000020 grad_norm=0.668714
Epoch 31/100 Iteration 5/234: loss=0.093855 lr=0.000020 grad_norm=1.095871
Epoch 31/100 Iteration 6/234: loss=0.083018 lr=0.000020 grad_norm=0.887349
Epoch 31/100 Iteration 7/234: loss=0.090133 lr=0.000020 grad_norm=0.866544
Epoch 31/100 Iteration 8/234: loss=0.074836 lr=0.000020 grad_norm=0.673198
Epoch 31/100 Iteration 9/234: loss=0.093382 lr=0.000020 grad_norm=0.842406
Epoch 31/100 Iteration 10/234: loss=0.098938 lr=0.000020 grad_norm=0.681250
Epoch 31/100 Iteration 11/234: loss=0.098183 lr=0.000020 grad_norm=0.633338
Epoch 31/100 Iteration 12/234: loss=0.094996 lr=0.000020 grad_norm=0.501906
Epoch 31/100 Iteration 13/234: loss=0.097769 lr=0.000020 grad_norm=0.604380
Epoch 31/100 Iteration 14/234: loss=0.086869 lr=0.000020 grad_norm=0.661239
Epoch 31/100 Iteration 15/234: loss=0.078645 lr=0.000020 grad_norm=0.759717
Epoch 31/100 Iteration 16/234: loss=0.081702 lr=0.000020 grad_norm=1.059963
Epoch 31/100 Iteration 17/234: loss=0.108156 lr=0.000020 grad_norm=0.962485
Epoch 31/100 Iteration 18/234: loss=0.091873 lr=0.000020 grad_norm=0.433376
Epoch 31/100 Iteration 19/234: loss=0.092544 lr=0.000020 grad_norm=0.645433
Epoch 31/100 Iteration 20/234: loss=0.100920 lr=0.000020 grad_norm=0.800766
Epoch 31/100 Iteration 21/234: loss=0.092601 lr=0.000020 grad_norm=0.977628
Epoch 31/100 Iteration 22/234: loss=0.085169 lr=0.000020 grad_norm=0.520374
Epoch 31/100 Iteration 23/234: loss=0.090025 lr=0.000020 grad_norm=0.472554
Epoch 31/100 Iteration 24/234: loss=0.079967 lr=0.000020 grad_norm=0.378012
Epoch 31/100 Iteration 25/234: loss=0.095624 lr=0.000020 grad_norm=0.591520
Epoch 31/100 Iteration 26/234: loss=0.080896 lr=0.000020 grad_norm=1.156616
Epoch 31/100 Iteration 27/234: loss=0.090844 lr=0.000020 grad_norm=1.628958
Epoch 31/100 Iteration 28/234: loss=0.091188 lr=0.000020 grad_norm=1.243661
Epoch 31/100 Iteration 29/234: loss=0.099911 lr=0.000020 grad_norm=0.435424
Epoch 31/100 Iteration 30/234: loss=0.085396 lr=0.000020 grad_norm=1.182864
Epoch 31/100 Iteration 31/234: loss=0.092837 lr=0.000020 grad_norm=1.124531
Epoch 31/100 Iteration 32/234: loss=0.089186 lr=0.000020 grad_norm=0.401841
Epoch 31/100 Iteration 33/234: loss=0.092402 lr=0.000020 grad_norm=1.116628
Epoch 31/100 Iteration 34/234: loss=0.093714 lr=0.000020 grad_norm=1.012863
Epoch 31/100 Iteration 35/234: loss=0.087905 lr=0.000020 grad_norm=0.460474
Epoch 31/100 Iteration 36/234: loss=0.088818 lr=0.000020 grad_norm=0.801275
Epoch 31/100 Iteration 37/234: loss=0.092953 lr=0.000020 grad_norm=0.685852
Epoch 31/100 Iteration 38/234: loss=0.070982 lr=0.000020 grad_norm=0.693755
Epoch 31/100 Iteration 39/234: loss=0.088278 lr=0.000020 grad_norm=0.405246
Epoch 31/100 Iteration 40/234: loss=0.097388 lr=0.000020 grad_norm=0.766851
Epoch 31/100 Iteration 41/234: loss=0.079612 lr=0.000020 grad_norm=0.529505
Epoch 31/100 Iteration 42/234: loss=0.085606 lr=0.000020 grad_norm=0.457818
Epoch 31/100 Iteration 43/234: loss=0.087650 lr=0.000020 grad_norm=0.564337
Epoch 31/100 Iteration 44/234: loss=0.085900 lr=0.000020 grad_norm=0.582333
Epoch 31/100 Iteration 45/234: loss=0.078860 lr=0.000020 grad_norm=0.380901
Epoch 31/100 Iteration 46/234: loss=0.096080 lr=0.000020 grad_norm=0.395975
Epoch 31/100 Iteration 47/234: loss=0.087583 lr=0.000020 grad_norm=0.519046
Epoch 31/100 Iteration 48/234: loss=0.077921 lr=0.000020 grad_norm=0.422115
Epoch 31/100 Iteration 49/234: loss=0.098586 lr=0.000020 grad_norm=0.534557
Epoch 31/100 Iteration 50/234: loss=0.091732 lr=0.000020 grad_norm=0.721621
Epoch 31/100 Iteration 51/234: loss=0.106855 lr=0.000020 grad_norm=0.795883
Epoch 31/100 Iteration 52/234: loss=0.090978 lr=0.000020 grad_norm=1.045559
Epoch 31/100 Iteration 53/234: loss=0.088204 lr=0.000020 grad_norm=0.894968
Epoch 31/100 Iteration 54/234: loss=0.083563 lr=0.000020 grad_norm=0.504680
Epoch 31/100 Iteration 55/234: loss=0.084527 lr=0.000020 grad_norm=0.818507
Epoch 31/100 Iteration 56/234: loss=0.085675 lr=0.000020 grad_norm=0.829916
Epoch 31/100 Iteration 57/234: loss=0.081075 lr=0.000020 grad_norm=0.412861
Epoch 31/100 Iteration 58/234: loss=0.098252 lr=0.000020 grad_norm=0.456101
Epoch 31/100 Iteration 59/234: loss=0.078123 lr=0.000020 grad_norm=0.552788
Epoch 31/100 Iteration 60/234: loss=0.089307 lr=0.000020 grad_norm=0.664830
Epoch 31/100 Iteration 61/234: loss=0.087962 lr=0.000020 grad_norm=0.485452
Epoch 31/100 Iteration 62/234: loss=0.099946 lr=0.000020 grad_norm=0.482250
Epoch 31/100 Iteration 63/234: loss=0.094020 lr=0.000020 grad_norm=0.577308
Epoch 31/100 Iteration 64/234: loss=0.085605 lr=0.000020 grad_norm=0.624898
Epoch 31/100 Iteration 65/234: loss=0.074150 lr=0.000020 grad_norm=0.447765
Epoch 31/100 Iteration 66/234: loss=0.087186 lr=0.000020 grad_norm=0.482632
Epoch 31/100 Iteration 67/234: loss=0.084005 lr=0.000020 grad_norm=0.968940
Epoch 31/100 Iteration 68/234: loss=0.086243 lr=0.000020 grad_norm=1.303864
Epoch 31/100 Iteration 69/234: loss=0.095979 lr=0.000020 grad_norm=0.790534
Epoch 31/100 Iteration 70/234: loss=0.091501 lr=0.000020 grad_norm=0.512260
Epoch 31/100 Iteration 71/234: loss=0.097416 lr=0.000020 grad_norm=0.461606
Epoch 31/100 Iteration 72/234: loss=0.088375 lr=0.000020 grad_norm=0.839129
Epoch 31/100 Iteration 73/234: loss=0.087423 lr=0.000020 grad_norm=0.913488
Epoch 31/100 Iteration 74/234: loss=0.079723 lr=0.000020 grad_norm=0.874222
Epoch 31/100 Iteration 75/234: loss=0.101699 lr=0.000020 grad_norm=0.710878
Epoch 31/100 Iteration 76/234: loss=0.092488 lr=0.000020 grad_norm=0.498447
Epoch 31/100 Iteration 77/234: loss=0.084147 lr=0.000020 grad_norm=0.352579
Epoch 31/100 Iteration 78/234: loss=0.096859 lr=0.000020 grad_norm=0.477746
Epoch 31/100 Iteration 79/234: loss=0.089600 lr=0.000020 grad_norm=0.475056
Epoch 31/100 Iteration 80/234: loss=0.087941 lr=0.000020 grad_norm=0.587140
Epoch 31/100 Iteration 81/234: loss=0.092063 lr=0.000020 grad_norm=0.680085
Epoch 31/100 Iteration 82/234: loss=0.089112 lr=0.000020 grad_norm=0.422010
Epoch 31/100 Iteration 83/234: loss=0.080602 lr=0.000020 grad_norm=0.627547
Epoch 31/100 Iteration 84/234: loss=0.085602 lr=0.000020 grad_norm=0.830802
Epoch 31/100 Iteration 85/234: loss=0.078349 lr=0.000020 grad_norm=0.496343
Epoch 31/100 Iteration 86/234: loss=0.090011 lr=0.000020 grad_norm=0.689627
Epoch 31/100 Iteration 87/234: loss=0.086659 lr=0.000020 grad_norm=1.435104
Epoch 31/100 Iteration 88/234: loss=0.092165 lr=0.000020 grad_norm=1.234555
Epoch 31/100 Iteration 89/234: loss=0.094495 lr=0.000020 grad_norm=0.387842
Epoch 31/100 Iteration 90/234: loss=0.083455 lr=0.000020 grad_norm=1.153255
Epoch 31/100 Iteration 91/234: loss=0.096269 lr=0.000020 grad_norm=1.290424
Epoch 31/100 Iteration 92/234: loss=0.088448 lr=0.000020 grad_norm=0.726131
Epoch 31/100 Iteration 93/234: loss=0.091427 lr=0.000020 grad_norm=0.503170
Epoch 31/100 Iteration 94/234: loss=0.079332 lr=0.000020 grad_norm=0.637776
Epoch 31/100 Iteration 95/234: loss=0.093511 lr=0.000020 grad_norm=0.417044
Epoch 31/100 Iteration 96/234: loss=0.092693 lr=0.000020 grad_norm=0.402798
Epoch 31/100 Iteration 97/234: loss=0.075949 lr=0.000020 grad_norm=0.438847
Epoch 31/100 Iteration 98/234: loss=0.079672 lr=0.000020 grad_norm=0.484932
Epoch 31/100 Iteration 99/234: loss=0.081215 lr=0.000020 grad_norm=0.438389
Epoch 31/100 Iteration 100/234: loss=0.100849 lr=0.000020 grad_norm=0.367475
Epoch 31/100 Iteration 101/234: loss=0.086868 lr=0.000020 grad_norm=0.707436
Epoch 31/100 Iteration 102/234: loss=0.091791 lr=0.000020 grad_norm=0.726709
Epoch 31/100 Iteration 103/234: loss=0.080012 lr=0.000020 grad_norm=0.580564
Epoch 31/100 Iteration 104/234: loss=0.097514 lr=0.000020 grad_norm=0.450261
Epoch 31/100 Iteration 105/234: loss=0.082610 lr=0.000020 grad_norm=0.939612
Epoch 31/100 Iteration 106/234: loss=0.085470 lr=0.000020 grad_norm=0.990637
Epoch 31/100 Iteration 107/234: loss=0.079790 lr=0.000020 grad_norm=0.720763
Epoch 31/100 Iteration 108/234: loss=0.093859 lr=0.000020 grad_norm=0.594799
Epoch 31/100 Iteration 109/234: loss=0.088369 lr=0.000020 grad_norm=0.857459
Epoch 31/100 Iteration 110/234: loss=0.088958 lr=0.000020 grad_norm=1.936371
Epoch 31/100 Iteration 111/234: loss=0.073494 lr=0.000020 grad_norm=2.124898
Epoch 31/100 Iteration 112/234: loss=0.085963 lr=0.000020 grad_norm=0.874520
Epoch 31/100 Iteration 113/234: loss=0.086689 lr=0.000020 grad_norm=1.084916
Epoch 31/100 Iteration 114/234: loss=0.080153 lr=0.000020 grad_norm=1.480718
Epoch 31/100 Iteration 115/234: loss=0.084492 lr=0.000020 grad_norm=0.693477
Epoch 31/100 Iteration 116/234: loss=0.092492 lr=0.000020 grad_norm=1.037221
Epoch 31/100 Iteration 117/234: loss=0.093258 lr=0.000020 grad_norm=1.549966
Epoch 31/100 Iteration 118/234: loss=0.082015 lr=0.000020 grad_norm=1.240010
Epoch 31/100 Iteration 119/234: loss=0.100617 lr=0.000020 grad_norm=0.628591
Epoch 31/100 Iteration 120/234: loss=0.087559 lr=0.000020 grad_norm=0.985672
Epoch 31/100 Iteration 121/234: loss=0.090891 lr=0.000020 grad_norm=1.146687
Epoch 31/100 Iteration 122/234: loss=0.078994 lr=0.000020 grad_norm=0.803336
Epoch 31/100 Iteration 123/234: loss=0.087183 lr=0.000020 grad_norm=0.630629
Epoch 31/100 Iteration 124/234: loss=0.089245 lr=0.000020 grad_norm=1.012010
Epoch 31/100 Iteration 125/234: loss=0.079646 lr=0.000020 grad_norm=1.023579
Epoch 31/100 Iteration 126/234: loss=0.084110 lr=0.000020 grad_norm=0.760913
Epoch 31/100 Iteration 127/234: loss=0.095720 lr=0.000020 grad_norm=0.983164
Epoch 31/100 Iteration 128/234: loss=0.097099 lr=0.000020 grad_norm=1.546794
Epoch 31/100 Iteration 129/234: loss=0.090804 lr=0.000020 grad_norm=1.728462
Epoch 31/100 Iteration 130/234: loss=0.096513 lr=0.000020 grad_norm=0.854892
Epoch 31/100 Iteration 131/234: loss=0.086238 lr=0.000020 grad_norm=0.856420
Epoch 31/100 Iteration 132/234: loss=0.097999 lr=0.000020 grad_norm=0.767988
Epoch 31/100 Iteration 133/234: loss=0.089367 lr=0.000020 grad_norm=0.606541
Epoch 31/100 Iteration 134/234: loss=0.080286 lr=0.000020 grad_norm=0.880680
Epoch 31/100 Iteration 135/234: loss=0.093159 lr=0.000020 grad_norm=0.987702
Epoch 31/100 Iteration 136/234: loss=0.087824 lr=0.000020 grad_norm=0.399150
Epoch 31/100 Iteration 137/234: loss=0.093850 lr=0.000020 grad_norm=0.715178
Epoch 31/100 Iteration 138/234: loss=0.080196 lr=0.000020 grad_norm=0.537898
Epoch 31/100 Iteration 139/234: loss=0.085776 lr=0.000020 grad_norm=0.888836
Epoch 31/100 Iteration 140/234: loss=0.092294 lr=0.000020 grad_norm=1.205766
Epoch 31/100 Iteration 141/234: loss=0.089333 lr=0.000020 grad_norm=0.725004
Epoch 31/100 Iteration 142/234: loss=0.094192 lr=0.000020 grad_norm=0.984042
Epoch 31/100 Iteration 143/234: loss=0.096482 lr=0.000020 grad_norm=0.807286
Epoch 31/100 Iteration 144/234: loss=0.095561 lr=0.000020 grad_norm=0.637521
Epoch 31/100 Iteration 145/234: loss=0.095042 lr=0.000020 grad_norm=0.590624
Epoch 31/100 Iteration 146/234: loss=0.099054 lr=0.000020 grad_norm=0.475917
Epoch 31/100 Iteration 147/234: loss=0.093793 lr=0.000020 grad_norm=0.550039
Epoch 31/100 Iteration 148/234: loss=0.103164 lr=0.000020 grad_norm=0.745088
Epoch 31/100 Iteration 149/234: loss=0.089584 lr=0.000020 grad_norm=0.566987
Epoch 31/100 Iteration 150/234: loss=0.085724 lr=0.000020 grad_norm=0.604383
Epoch 31/100 Iteration 151/234: loss=0.080993 lr=0.000020 grad_norm=0.824970
Epoch 31/100 Iteration 152/234: loss=0.089418 lr=0.000020 grad_norm=0.794154
Epoch 31/100 Iteration 153/234: loss=0.085413 lr=0.000020 grad_norm=0.912986
Epoch 31/100 Iteration 154/234: loss=0.106986 lr=0.000020 grad_norm=0.888654
Epoch 31/100 Iteration 155/234: loss=0.083572 lr=0.000020 grad_norm=0.554778
Epoch 31/100 Iteration 156/234: loss=0.090752 lr=0.000020 grad_norm=0.666270
Epoch 31/100 Iteration 157/234: loss=0.089778 lr=0.000020 grad_norm=1.020150
Epoch 31/100 Iteration 158/234: loss=0.080058 lr=0.000020 grad_norm=1.093042
Epoch 31/100 Iteration 159/234: loss=0.094496 lr=0.000020 grad_norm=0.679790
Epoch 31/100 Iteration 160/234: loss=0.086489 lr=0.000020 grad_norm=0.420694
Epoch 31/100 Iteration 161/234: loss=0.081132 lr=0.000020 grad_norm=0.892368
Epoch 31/100 Iteration 162/234: loss=0.091991 lr=0.000020 grad_norm=0.909407
Epoch 31/100 Iteration 163/234: loss=0.094785 lr=0.000020 grad_norm=0.555365
Epoch 31/100 Iteration 164/234: loss=0.082413 lr=0.000020 grad_norm=0.438046
Epoch 31/100 Iteration 165/234: loss=0.082787 lr=0.000020 grad_norm=0.505920
Epoch 31/100 Iteration 166/234: loss=0.082174 lr=0.000020 grad_norm=0.749233
Epoch 31/100 Iteration 167/234: loss=0.096387 lr=0.000020 grad_norm=0.643156
Epoch 31/100 Iteration 168/234: loss=0.088700 lr=0.000020 grad_norm=0.561637
Epoch 31/100 Iteration 169/234: loss=0.090337 lr=0.000020 grad_norm=0.514555
Epoch 31/100 Iteration 170/234: loss=0.099071 lr=0.000020 grad_norm=0.942500
Epoch 31/100 Iteration 171/234: loss=0.083397 lr=0.000020 grad_norm=1.722496
Epoch 31/100 Iteration 172/234: loss=0.089619 lr=0.000020 grad_norm=1.688817
Epoch 31/100 Iteration 173/234: loss=0.097292 lr=0.000020 grad_norm=0.681438
Epoch 31/100 Iteration 174/234: loss=0.089637 lr=0.000020 grad_norm=1.252742
Epoch 31/100 Iteration 175/234: loss=0.088503 lr=0.000020 grad_norm=1.898714
Epoch 31/100 Iteration 176/234: loss=0.081518 lr=0.000020 grad_norm=0.926871
Epoch 31/100 Iteration 177/234: loss=0.097408 lr=0.000020 grad_norm=1.379069
Epoch 31/100 Iteration 178/234: loss=0.082678 lr=0.000020 grad_norm=2.051031
Epoch 31/100 Iteration 179/234: loss=0.089529 lr=0.000020 grad_norm=1.440848
Epoch 31/100 Iteration 180/234: loss=0.090214 lr=0.000020 grad_norm=0.727751
Epoch 31/100 Iteration 181/234: loss=0.089085 lr=0.000020 grad_norm=1.104030
Epoch 31/100 Iteration 182/234: loss=0.104904 lr=0.000020 grad_norm=1.143466
Epoch 31/100 Iteration 183/234: loss=0.082448 lr=0.000020 grad_norm=0.954058
Epoch 31/100 Iteration 184/234: loss=0.089580 lr=0.000020 grad_norm=1.028773
Epoch 31/100 Iteration 185/234: loss=0.086119 lr=0.000020 grad_norm=1.380020
Epoch 31/100 Iteration 186/234: loss=0.083663 lr=0.000020 grad_norm=0.574535
Epoch 31/100 Iteration 187/234: loss=0.078567 lr=0.000020 grad_norm=0.709235
Epoch 31/100 Iteration 188/234: loss=0.093574 lr=0.000020 grad_norm=0.843654
Epoch 31/100 Iteration 189/234: loss=0.097584 lr=0.000020 grad_norm=0.852506
Epoch 31/100 Iteration 190/234: loss=0.090209 lr=0.000020 grad_norm=0.940025
Epoch 31/100 Iteration 191/234: loss=0.095866 lr=0.000020 grad_norm=0.613609
Epoch 31/100 Iteration 192/234: loss=0.093490 lr=0.000020 grad_norm=0.448438
Epoch 31/100 Iteration 193/234: loss=0.074976 lr=0.000020 grad_norm=0.820004
Epoch 31/100 Iteration 194/234: loss=0.090510 lr=0.000020 grad_norm=0.784182
Epoch 31/100 Iteration 195/234: loss=0.081705 lr=0.000020 grad_norm=0.399693
Epoch 31/100 Iteration 196/234: loss=0.093174 lr=0.000020 grad_norm=0.966016
Epoch 31/100 Iteration 197/234: loss=0.078542 lr=0.000020 grad_norm=0.615524
Epoch 31/100 Iteration 198/234: loss=0.090794 lr=0.000020 grad_norm=0.742724
Epoch 31/100 Iteration 199/234: loss=0.078761 lr=0.000020 grad_norm=1.276659
Epoch 31/100 Iteration 200/234: loss=0.090752 lr=0.000020 grad_norm=0.441233
Epoch 31/100 Iteration 201/234: loss=0.086447 lr=0.000020 grad_norm=1.270473
Epoch 31/100 Iteration 202/234: loss=0.074782 lr=0.000020 grad_norm=1.056071
Epoch 31/100 Iteration 203/234: loss=0.081951 lr=0.000020 grad_norm=0.702656
Epoch 31/100 Iteration 204/234: loss=0.085937 lr=0.000020 grad_norm=1.210462
Epoch 31/100 Iteration 205/234: loss=0.081152 lr=0.000020 grad_norm=1.139090
Epoch 31/100 Iteration 206/234: loss=0.088525 lr=0.000020 grad_norm=0.625400
Epoch 31/100 Iteration 207/234: loss=0.078506 lr=0.000020 grad_norm=0.679879
Epoch 31/100 Iteration 208/234: loss=0.092303 lr=0.000020 grad_norm=0.764847
Epoch 31/100 Iteration 209/234: loss=0.091210 lr=0.000020 grad_norm=0.562768
Epoch 31/100 Iteration 210/234: loss=0.082594 lr=0.000020 grad_norm=0.972478
Epoch 31/100 Iteration 211/234: loss=0.084496 lr=0.000020 grad_norm=0.837571
Epoch 31/100 Iteration 212/234: loss=0.086059 lr=0.000020 grad_norm=0.477187
Epoch 31/100 Iteration 213/234: loss=0.084216 lr=0.000020 grad_norm=0.709877
Epoch 31/100 Iteration 214/234: loss=0.076654 lr=0.000020 grad_norm=1.110529
Epoch 31/100 Iteration 215/234: loss=0.081005 lr=0.000020 grad_norm=0.898917
Epoch 31/100 Iteration 216/234: loss=0.097389 lr=0.000020 grad_norm=0.535973
Epoch 31/100 Iteration 217/234: loss=0.100336 lr=0.000020 grad_norm=0.775574
Epoch 31/100 Iteration 218/234: loss=0.092398 lr=0.000020 grad_norm=0.754468
Epoch 31/100 Iteration 219/234: loss=0.087749 lr=0.000020 grad_norm=0.690085
Epoch 31/100 Iteration 220/234: loss=0.087558 lr=0.000020 grad_norm=0.420440
Epoch 31/100 Iteration 221/234: loss=0.074832 lr=0.000020 grad_norm=0.531333
Epoch 31/100 Iteration 222/234: loss=0.084556 lr=0.000020 grad_norm=0.492105
Epoch 31/100 Iteration 223/234: loss=0.085003 lr=0.000020 grad_norm=1.252388
Epoch 31/100 Iteration 224/234: loss=0.088506 lr=0.000020 grad_norm=1.587854
Epoch 31/100 Iteration 225/234: loss=0.089164 lr=0.000020 grad_norm=0.558056
Epoch 31/100 Iteration 226/234: loss=0.089036 lr=0.000020 grad_norm=0.960162
Epoch 31/100 Iteration 227/234: loss=0.079254 lr=0.000020 grad_norm=1.043078
Epoch 31/100 Iteration 228/234: loss=0.089338 lr=0.000020 grad_norm=0.572938
Epoch 31/100 Iteration 229/234: loss=0.085455 lr=0.000020 grad_norm=0.400068
Epoch 31/100 Iteration 230/234: loss=0.093705 lr=0.000020 grad_norm=0.590200
Epoch 31/100 Iteration 231/234: loss=0.081646 lr=0.000020 grad_norm=0.499817
Epoch 31/100 Iteration 232/234: loss=0.084491 lr=0.000020 grad_norm=1.241981
Epoch 31/100 Iteration 233/234: loss=0.083840 lr=0.000020 grad_norm=1.578598
Epoch 31/100 Iteration 234/234: loss=0.084353 lr=0.000020 grad_norm=1.085219
Epoch 31/100 finished. Avg Loss: 0.088481
Epoch 32/100 Iteration 1/234: loss=0.075451 lr=0.000020 grad_norm=0.401502
Epoch 32/100 Iteration 2/234: loss=0.097442 lr=0.000020 grad_norm=0.956866
Epoch 32/100 Iteration 3/234: loss=0.091730 lr=0.000020 grad_norm=0.812444
Epoch 32/100 Iteration 4/234: loss=0.084522 lr=0.000020 grad_norm=0.487294
Epoch 32/100 Iteration 5/234: loss=0.083689 lr=0.000020 grad_norm=0.992629
Epoch 32/100 Iteration 6/234: loss=0.082138 lr=0.000020 grad_norm=0.706316
Epoch 32/100 Iteration 7/234: loss=0.093208 lr=0.000020 grad_norm=0.594385
Epoch 32/100 Iteration 8/234: loss=0.083699 lr=0.000020 grad_norm=1.195846
Epoch 32/100 Iteration 9/234: loss=0.078104 lr=0.000020 grad_norm=0.945187
Epoch 32/100 Iteration 10/234: loss=0.098654 lr=0.000020 grad_norm=0.564727
Epoch 32/100 Iteration 11/234: loss=0.076670 lr=0.000020 grad_norm=0.553630
Epoch 32/100 Iteration 12/234: loss=0.085342 lr=0.000020 grad_norm=0.487969
Epoch 32/100 Iteration 13/234: loss=0.092076 lr=0.000020 grad_norm=0.407938
Epoch 32/100 Iteration 14/234: loss=0.080719 lr=0.000020 grad_norm=0.418891
Epoch 32/100 Iteration 15/234: loss=0.079907 lr=0.000020 grad_norm=0.369729
Epoch 32/100 Iteration 16/234: loss=0.084487 lr=0.000020 grad_norm=0.488010
Epoch 32/100 Iteration 17/234: loss=0.097971 lr=0.000020 grad_norm=0.615528
Epoch 32/100 Iteration 18/234: loss=0.081762 lr=0.000020 grad_norm=0.520223
Epoch 32/100 Iteration 19/234: loss=0.095484 lr=0.000020 grad_norm=0.548912
Epoch 32/100 Iteration 20/234: loss=0.102513 lr=0.000020 grad_norm=0.840420
Epoch 32/100 Iteration 21/234: loss=0.092412 lr=0.000020 grad_norm=0.903957
Epoch 32/100 Iteration 22/234: loss=0.088184 lr=0.000020 grad_norm=0.638546
Epoch 32/100 Iteration 23/234: loss=0.089929 lr=0.000020 grad_norm=0.758886
Epoch 32/100 Iteration 24/234: loss=0.090702 lr=0.000020 grad_norm=0.807074
Epoch 32/100 Iteration 25/234: loss=0.090773 lr=0.000020 grad_norm=0.815100
Epoch 32/100 Iteration 26/234: loss=0.083942 lr=0.000020 grad_norm=0.631380
Epoch 32/100 Iteration 27/234: loss=0.071842 lr=0.000020 grad_norm=0.378072
Epoch 32/100 Iteration 28/234: loss=0.088300 lr=0.000020 grad_norm=0.467788
Epoch 32/100 Iteration 29/234: loss=0.088526 lr=0.000020 grad_norm=0.487379
Epoch 32/100 Iteration 30/234: loss=0.087688 lr=0.000020 grad_norm=0.609507
Epoch 32/100 Iteration 31/234: loss=0.088359 lr=0.000020 grad_norm=0.670631
Epoch 32/100 Iteration 32/234: loss=0.092044 lr=0.000020 grad_norm=0.650159
Epoch 32/100 Iteration 33/234: loss=0.091311 lr=0.000020 grad_norm=0.383183
Epoch 32/100 Iteration 34/234: loss=0.094297 lr=0.000020 grad_norm=0.455349
Epoch 32/100 Iteration 35/234: loss=0.087756 lr=0.000020 grad_norm=0.705584
Epoch 32/100 Iteration 36/234: loss=0.087360 lr=0.000020 grad_norm=1.002797
Epoch 32/100 Iteration 37/234: loss=0.085976 lr=0.000020 grad_norm=0.611991
Epoch 32/100 Iteration 38/234: loss=0.091785 lr=0.000020 grad_norm=0.771040
Epoch 32/100 Iteration 39/234: loss=0.085075 lr=0.000020 grad_norm=1.169178
Epoch 32/100 Iteration 40/234: loss=0.083324 lr=0.000020 grad_norm=0.785244
Epoch 32/100 Iteration 41/234: loss=0.091260 lr=0.000020 grad_norm=0.781490
Epoch 32/100 Iteration 42/234: loss=0.095297 lr=0.000020 grad_norm=1.681170
Epoch 32/100 Iteration 43/234: loss=0.090210 lr=0.000020 grad_norm=1.291311
Epoch 32/100 Iteration 44/234: loss=0.089599 lr=0.000020 grad_norm=0.430716
Epoch 32/100 Iteration 45/234: loss=0.087829 lr=0.000020 grad_norm=1.210891
Epoch 32/100 Iteration 46/234: loss=0.080399 lr=0.000020 grad_norm=0.929236
Epoch 32/100 Iteration 47/234: loss=0.092033 lr=0.000020 grad_norm=0.548594
Epoch 32/100 Iteration 48/234: loss=0.095005 lr=0.000020 grad_norm=0.500524
Epoch 32/100 Iteration 49/234: loss=0.077290 lr=0.000020 grad_norm=0.388198
Epoch 32/100 Iteration 50/234: loss=0.100489 lr=0.000020 grad_norm=0.685785
Epoch 32/100 Iteration 51/234: loss=0.090700 lr=0.000020 grad_norm=1.064648
Epoch 32/100 Iteration 52/234: loss=0.082260 lr=0.000020 grad_norm=1.044511
Epoch 32/100 Iteration 53/234: loss=0.097503 lr=0.000020 grad_norm=0.801215
Epoch 32/100 Iteration 54/234: loss=0.085883 lr=0.000020 grad_norm=0.490047
Epoch 32/100 Iteration 55/234: loss=0.073161 lr=0.000020 grad_norm=1.097575
Epoch 32/100 Iteration 56/234: loss=0.080028 lr=0.000020 grad_norm=0.584058
Epoch 32/100 Iteration 57/234: loss=0.098206 lr=0.000020 grad_norm=0.723148
Epoch 32/100 Iteration 58/234: loss=0.084062 lr=0.000020 grad_norm=0.991740
Epoch 32/100 Iteration 59/234: loss=0.084938 lr=0.000020 grad_norm=0.964389
Epoch 32/100 Iteration 60/234: loss=0.083503 lr=0.000020 grad_norm=0.795127
Epoch 32/100 Iteration 61/234: loss=0.088788 lr=0.000020 grad_norm=0.599231
Epoch 32/100 Iteration 62/234: loss=0.093879 lr=0.000020 grad_norm=0.518970
Epoch 32/100 Iteration 63/234: loss=0.095943 lr=0.000020 grad_norm=0.751985
Epoch 32/100 Iteration 64/234: loss=0.087509 lr=0.000020 grad_norm=0.940950
Epoch 32/100 Iteration 65/234: loss=0.075112 lr=0.000020 grad_norm=0.784274
Epoch 32/100 Iteration 66/234: loss=0.080236 lr=0.000020 grad_norm=0.585121
Epoch 32/100 Iteration 67/234: loss=0.090627 lr=0.000020 grad_norm=0.483476
Epoch 32/100 Iteration 68/234: loss=0.086279 lr=0.000020 grad_norm=0.546524
Epoch 32/100 Iteration 69/234: loss=0.081568 lr=0.000020 grad_norm=0.343250
Epoch 32/100 Iteration 70/234: loss=0.078846 lr=0.000020 grad_norm=0.438598
Epoch 32/100 Iteration 71/234: loss=0.092485 lr=0.000020 grad_norm=0.494274
Epoch 32/100 Iteration 72/234: loss=0.086748 lr=0.000020 grad_norm=0.664048
Epoch 32/100 Iteration 73/234: loss=0.092391 lr=0.000020 grad_norm=1.215909
Epoch 32/100 Iteration 74/234: loss=0.079532 lr=0.000020 grad_norm=1.633855
Epoch 32/100 Iteration 75/234: loss=0.076644 lr=0.000020 grad_norm=1.007420
Epoch 32/100 Iteration 76/234: loss=0.082874 lr=0.000020 grad_norm=0.617625
Epoch 32/100 Iteration 77/234: loss=0.088583 lr=0.000020 grad_norm=1.420054
Epoch 32/100 Iteration 78/234: loss=0.100364 lr=0.000020 grad_norm=1.191686
Epoch 32/100 Iteration 79/234: loss=0.084574 lr=0.000020 grad_norm=0.556938
Epoch 32/100 Iteration 80/234: loss=0.102803 lr=0.000020 grad_norm=1.353414
Epoch 32/100 Iteration 81/234: loss=0.084968 lr=0.000020 grad_norm=1.236665
Epoch 32/100 Iteration 82/234: loss=0.075903 lr=0.000020 grad_norm=0.804621
Epoch 32/100 Iteration 83/234: loss=0.092541 lr=0.000020 grad_norm=1.058724
Epoch 32/100 Iteration 84/234: loss=0.084673 lr=0.000020 grad_norm=1.178043
Epoch 32/100 Iteration 85/234: loss=0.091632 lr=0.000020 grad_norm=0.986775
Epoch 32/100 Iteration 86/234: loss=0.080137 lr=0.000020 grad_norm=0.726575
Epoch 32/100 Iteration 87/234: loss=0.076684 lr=0.000020 grad_norm=0.600666
Epoch 32/100 Iteration 88/234: loss=0.087167 lr=0.000020 grad_norm=0.799120
Epoch 32/100 Iteration 89/234: loss=0.090445 lr=0.000020 grad_norm=0.750632
Epoch 32/100 Iteration 90/234: loss=0.090203 lr=0.000020 grad_norm=1.021334
Epoch 32/100 Iteration 91/234: loss=0.091409 lr=0.000020 grad_norm=1.278499
Epoch 32/100 Iteration 92/234: loss=0.093870 lr=0.000020 grad_norm=0.850999
Epoch 32/100 Iteration 93/234: loss=0.078310 lr=0.000020 grad_norm=0.462939
Epoch 32/100 Iteration 94/234: loss=0.080304 lr=0.000020 grad_norm=1.181753
Epoch 32/100 Iteration 95/234: loss=0.086043 lr=0.000020 grad_norm=1.804829
Epoch 32/100 Iteration 96/234: loss=0.082901 lr=0.000020 grad_norm=1.911440
Epoch 32/100 Iteration 97/234: loss=0.092096 lr=0.000020 grad_norm=0.922144
Epoch 32/100 Iteration 98/234: loss=0.079283 lr=0.000020 grad_norm=1.330224
Epoch 32/100 Iteration 99/234: loss=0.083948 lr=0.000020 grad_norm=1.682070
Epoch 32/100 Iteration 100/234: loss=0.092887 lr=0.000020 grad_norm=0.533767
Epoch 32/100 Iteration 101/234: loss=0.096923 lr=0.000020 grad_norm=3.079055
Epoch 32/100 Iteration 102/234: loss=0.085623 lr=0.000020 grad_norm=2.523053
Epoch 32/100 Iteration 103/234: loss=0.091876 lr=0.000020 grad_norm=1.390177
Epoch 32/100 Iteration 104/234: loss=0.095078 lr=0.000020 grad_norm=2.510382
Epoch 32/100 Iteration 105/234: loss=0.076474 lr=0.000020 grad_norm=0.500318
Epoch 32/100 Iteration 106/234: loss=0.094005 lr=0.000020 grad_norm=2.175479
Epoch 32/100 Iteration 107/234: loss=0.080814 lr=0.000020 grad_norm=0.474174
Epoch 32/100 Iteration 108/234: loss=0.092974 lr=0.000020 grad_norm=2.230232
Epoch 32/100 Iteration 109/234: loss=0.094202 lr=0.000020 grad_norm=1.219380
Epoch 32/100 Iteration 110/234: loss=0.077188 lr=0.000020 grad_norm=0.944666
Epoch 32/100 Iteration 111/234: loss=0.085145 lr=0.000020 grad_norm=0.671211
Epoch 32/100 Iteration 112/234: loss=0.080662 lr=0.000020 grad_norm=1.048178
Epoch 32/100 Iteration 113/234: loss=0.085266 lr=0.000020 grad_norm=0.813238
Epoch 32/100 Iteration 114/234: loss=0.081070 lr=0.000020 grad_norm=0.732355
Epoch 32/100 Iteration 115/234: loss=0.078716 lr=0.000020 grad_norm=0.733275
Epoch 32/100 Iteration 116/234: loss=0.086125 lr=0.000020 grad_norm=0.633227
Epoch 32/100 Iteration 117/234: loss=0.078907 lr=0.000020 grad_norm=0.901194
Epoch 32/100 Iteration 118/234: loss=0.079264 lr=0.000020 grad_norm=0.370796
Epoch 32/100 Iteration 119/234: loss=0.083722 lr=0.000020 grad_norm=0.930473
Epoch 32/100 Iteration 120/234: loss=0.084485 lr=0.000020 grad_norm=0.846629
Epoch 32/100 Iteration 121/234: loss=0.078959 lr=0.000020 grad_norm=0.507504
Epoch 32/100 Iteration 122/234: loss=0.078936 lr=0.000020 grad_norm=0.574030
Epoch 32/100 Iteration 123/234: loss=0.081373 lr=0.000020 grad_norm=0.468213
Epoch 32/100 Iteration 124/234: loss=0.085177 lr=0.000020 grad_norm=0.675050
Epoch 32/100 Iteration 125/234: loss=0.078083 lr=0.000020 grad_norm=0.534735
Epoch 32/100 Iteration 126/234: loss=0.090891 lr=0.000020 grad_norm=0.525403
Epoch 32/100 Iteration 127/234: loss=0.082664 lr=0.000020 grad_norm=0.561832
Epoch 32/100 Iteration 128/234: loss=0.094825 lr=0.000020 grad_norm=0.606483
Epoch 32/100 Iteration 129/234: loss=0.075509 lr=0.000020 grad_norm=0.663069
Epoch 32/100 Iteration 130/234: loss=0.083795 lr=0.000020 grad_norm=0.511484
Epoch 32/100 Iteration 131/234: loss=0.088266 lr=0.000020 grad_norm=0.355514
Epoch 32/100 Iteration 132/234: loss=0.073849 lr=0.000020 grad_norm=0.686280
Epoch 32/100 Iteration 133/234: loss=0.076360 lr=0.000020 grad_norm=0.536250
Epoch 32/100 Iteration 134/234: loss=0.078547 lr=0.000020 grad_norm=0.500521
Epoch 32/100 Iteration 135/234: loss=0.097531 lr=0.000020 grad_norm=0.636135
Epoch 32/100 Iteration 136/234: loss=0.073581 lr=0.000020 grad_norm=0.376842
Epoch 32/100 Iteration 137/234: loss=0.080974 lr=0.000020 grad_norm=0.547905
Epoch 32/100 Iteration 138/234: loss=0.090001 lr=0.000020 grad_norm=0.879192
Epoch 32/100 Iteration 139/234: loss=0.092757 lr=0.000020 grad_norm=0.522473
Epoch 32/100 Iteration 140/234: loss=0.092588 lr=0.000020 grad_norm=0.795233
Epoch 32/100 Iteration 141/234: loss=0.090030 lr=0.000020 grad_norm=1.714341
Epoch 32/100 Iteration 142/234: loss=0.087709 lr=0.000020 grad_norm=1.458599
Epoch 32/100 Iteration 143/234: loss=0.070794 lr=0.000020 grad_norm=0.607304
Epoch 32/100 Iteration 144/234: loss=0.090686 lr=0.000020 grad_norm=1.712949
Epoch 32/100 Iteration 145/234: loss=0.079986 lr=0.000020 grad_norm=0.943237
Epoch 32/100 Iteration 146/234: loss=0.080341 lr=0.000020 grad_norm=0.906395
Epoch 32/100 Iteration 147/234: loss=0.089046 lr=0.000020 grad_norm=0.866781
Epoch 32/100 Iteration 148/234: loss=0.096060 lr=0.000020 grad_norm=0.535640
Epoch 32/100 Iteration 149/234: loss=0.097087 lr=0.000020 grad_norm=0.629098
Epoch 32/100 Iteration 150/234: loss=0.095198 lr=0.000020 grad_norm=0.503576
Epoch 32/100 Iteration 151/234: loss=0.080493 lr=0.000020 grad_norm=0.466697
Epoch 32/100 Iteration 152/234: loss=0.082487 lr=0.000020 grad_norm=0.910505
Epoch 32/100 Iteration 153/234: loss=0.092144 lr=0.000020 grad_norm=0.773394
Epoch 32/100 Iteration 154/234: loss=0.091743 lr=0.000020 grad_norm=0.830796
Epoch 32/100 Iteration 155/234: loss=0.095000 lr=0.000020 grad_norm=0.635001
Epoch 32/100 Iteration 156/234: loss=0.091523 lr=0.000020 grad_norm=0.645700
Epoch 32/100 Iteration 157/234: loss=0.067339 lr=0.000020 grad_norm=0.527328
Epoch 32/100 Iteration 158/234: loss=0.080448 lr=0.000020 grad_norm=0.339791
Epoch 32/100 Iteration 159/234: loss=0.080514 lr=0.000020 grad_norm=0.476388
Epoch 32/100 Iteration 160/234: loss=0.087991 lr=0.000020 grad_norm=0.512939
Epoch 32/100 Iteration 161/234: loss=0.100560 lr=0.000020 grad_norm=0.396907
Epoch 32/100 Iteration 162/234: loss=0.088818 lr=0.000020 grad_norm=0.415739
Epoch 32/100 Iteration 163/234: loss=0.086210 lr=0.000020 grad_norm=0.357850
Epoch 32/100 Iteration 164/234: loss=0.076127 lr=0.000020 grad_norm=0.360261
Epoch 32/100 Iteration 165/234: loss=0.093807 lr=0.000020 grad_norm=0.413459
Epoch 32/100 Iteration 166/234: loss=0.079145 lr=0.000020 grad_norm=0.538407
Epoch 32/100 Iteration 167/234: loss=0.092469 lr=0.000020 grad_norm=0.676119
Epoch 32/100 Iteration 168/234: loss=0.089971 lr=0.000020 grad_norm=0.562335
Epoch 32/100 Iteration 169/234: loss=0.096805 lr=0.000020 grad_norm=0.527364
Epoch 32/100 Iteration 170/234: loss=0.093459 lr=0.000020 grad_norm=0.336238
Epoch 32/100 Iteration 171/234: loss=0.089799 lr=0.000020 grad_norm=0.406931
Epoch 32/100 Iteration 172/234: loss=0.085759 lr=0.000020 grad_norm=0.382630
Epoch 32/100 Iteration 173/234: loss=0.092539 lr=0.000020 grad_norm=0.378962
Epoch 32/100 Iteration 174/234: loss=0.081444 lr=0.000020 grad_norm=0.735593
Epoch 32/100 Iteration 175/234: loss=0.085194 lr=0.000020 grad_norm=0.931812
Epoch 32/100 Iteration 176/234: loss=0.082157 lr=0.000020 grad_norm=0.579502
Epoch 32/100 Iteration 177/234: loss=0.088742 lr=0.000020 grad_norm=0.786589
Epoch 32/100 Iteration 178/234: loss=0.089081 lr=0.000020 grad_norm=0.811721
Epoch 32/100 Iteration 179/234: loss=0.093985 lr=0.000020 grad_norm=0.583120
Epoch 32/100 Iteration 180/234: loss=0.081342 lr=0.000020 grad_norm=0.714952
Epoch 32/100 Iteration 181/234: loss=0.084709 lr=0.000020 grad_norm=0.623132
Epoch 32/100 Iteration 182/234: loss=0.074331 lr=0.000020 grad_norm=0.447042
Epoch 32/100 Iteration 183/234: loss=0.088484 lr=0.000020 grad_norm=0.444739
Epoch 32/100 Iteration 184/234: loss=0.073988 lr=0.000020 grad_norm=0.528355
Epoch 32/100 Iteration 185/234: loss=0.090213 lr=0.000020 grad_norm=0.575533
Epoch 32/100 Iteration 186/234: loss=0.088569 lr=0.000020 grad_norm=0.842675
Epoch 32/100 Iteration 187/234: loss=0.092801 lr=0.000020 grad_norm=0.567872
Epoch 32/100 Iteration 188/234: loss=0.083860 lr=0.000020 grad_norm=0.594254
Epoch 32/100 Iteration 189/234: loss=0.090301 lr=0.000020 grad_norm=1.240166
Epoch 32/100 Iteration 190/234: loss=0.095199 lr=0.000020 grad_norm=1.110489
Epoch 32/100 Iteration 191/234: loss=0.088511 lr=0.000020 grad_norm=0.539922
Epoch 32/100 Iteration 192/234: loss=0.092370 lr=0.000020 grad_norm=1.738302
Epoch 32/100 Iteration 193/234: loss=0.086859 lr=0.000020 grad_norm=1.759504
Epoch 32/100 Iteration 194/234: loss=0.082515 lr=0.000020 grad_norm=0.766750
Epoch 32/100 Iteration 195/234: loss=0.084503 lr=0.000020 grad_norm=1.795236
Epoch 32/100 Iteration 196/234: loss=0.081262 lr=0.000020 grad_norm=1.040464
Epoch 32/100 Iteration 197/234: loss=0.088800 lr=0.000020 grad_norm=0.778094
Epoch 32/100 Iteration 198/234: loss=0.081190 lr=0.000020 grad_norm=1.221588
Epoch 32/100 Iteration 199/234: loss=0.090545 lr=0.000020 grad_norm=0.492182
Epoch 32/100 Iteration 200/234: loss=0.081871 lr=0.000020 grad_norm=1.169505
Epoch 32/100 Iteration 201/234: loss=0.080147 lr=0.000020 grad_norm=0.868126
Epoch 32/100 Iteration 202/234: loss=0.093367 lr=0.000020 grad_norm=0.624512
Epoch 32/100 Iteration 203/234: loss=0.072772 lr=0.000020 grad_norm=0.910628
Epoch 32/100 Iteration 204/234: loss=0.082449 lr=0.000020 grad_norm=0.958300
Epoch 32/100 Iteration 205/234: loss=0.084778 lr=0.000020 grad_norm=2.175031
Epoch 32/100 Iteration 206/234: loss=0.077606 lr=0.000020 grad_norm=1.485589
Epoch 32/100 Iteration 207/234: loss=0.085991 lr=0.000020 grad_norm=0.691875
Epoch 32/100 Iteration 208/234: loss=0.087634 lr=0.000020 grad_norm=1.756926
Epoch 32/100 Iteration 209/234: loss=0.090582 lr=0.000020 grad_norm=1.052954
Epoch 32/100 Iteration 210/234: loss=0.084466 lr=0.000020 grad_norm=1.179667
Epoch 32/100 Iteration 211/234: loss=0.080183 lr=0.000020 grad_norm=1.720057
Epoch 32/100 Iteration 212/234: loss=0.084906 lr=0.000020 grad_norm=0.703855
Epoch 32/100 Iteration 213/234: loss=0.080483 lr=0.000020 grad_norm=1.073963
Epoch 32/100 Iteration 214/234: loss=0.073715 lr=0.000020 grad_norm=0.414722
Epoch 32/100 Iteration 215/234: loss=0.088811 lr=0.000020 grad_norm=1.073183
Epoch 32/100 Iteration 216/234: loss=0.084480 lr=0.000020 grad_norm=0.559069
Epoch 32/100 Iteration 217/234: loss=0.069909 lr=0.000020 grad_norm=0.794060
Epoch 32/100 Iteration 218/234: loss=0.091614 lr=0.000020 grad_norm=0.501001
Epoch 32/100 Iteration 219/234: loss=0.088737 lr=0.000020 grad_norm=0.720571
Epoch 32/100 Iteration 220/234: loss=0.077310 lr=0.000020 grad_norm=0.665522
Epoch 32/100 Iteration 221/234: loss=0.083612 lr=0.000020 grad_norm=0.503418
Epoch 32/100 Iteration 222/234: loss=0.094770 lr=0.000020 grad_norm=0.914601
Epoch 32/100 Iteration 223/234: loss=0.078941 lr=0.000020 grad_norm=0.549779
Epoch 32/100 Iteration 224/234: loss=0.085849 lr=0.000020 grad_norm=0.532814
Epoch 32/100 Iteration 225/234: loss=0.094079 lr=0.000020 grad_norm=0.692120
Epoch 32/100 Iteration 226/234: loss=0.091873 lr=0.000020 grad_norm=0.456698
Epoch 32/100 Iteration 227/234: loss=0.077348 lr=0.000020 grad_norm=0.520820
Epoch 32/100 Iteration 228/234: loss=0.084707 lr=0.000020 grad_norm=0.339152
Epoch 32/100 Iteration 229/234: loss=0.079625 lr=0.000020 grad_norm=0.601088
Epoch 32/100 Iteration 230/234: loss=0.080334 lr=0.000020 grad_norm=0.571386
Epoch 32/100 Iteration 231/234: loss=0.091355 lr=0.000020 grad_norm=0.467625
Epoch 32/100 Iteration 232/234: loss=0.084133 lr=0.000020 grad_norm=0.698393
Epoch 32/100 Iteration 233/234: loss=0.077573 lr=0.000020 grad_norm=0.431941
Epoch 32/100 Iteration 234/234: loss=0.082047 lr=0.000020 grad_norm=0.399116
Epoch 32/100 finished. Avg Loss: 0.086198
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 33/100 Iteration 1/234: loss=0.082783 lr=0.000020 grad_norm=0.518921
Epoch 33/100 Iteration 2/234: loss=0.091970 lr=0.000020 grad_norm=0.847603
Epoch 33/100 Iteration 3/234: loss=0.084352 lr=0.000020 grad_norm=0.822451
Epoch 33/100 Iteration 4/234: loss=0.074269 lr=0.000020 grad_norm=0.557902
Epoch 33/100 Iteration 5/234: loss=0.085397 lr=0.000020 grad_norm=0.507671
Epoch 33/100 Iteration 6/234: loss=0.094536 lr=0.000020 grad_norm=1.094509
Epoch 33/100 Iteration 7/234: loss=0.086051 lr=0.000020 grad_norm=1.025871
Epoch 33/100 Iteration 8/234: loss=0.081936 lr=0.000020 grad_norm=0.414454
Epoch 33/100 Iteration 9/234: loss=0.085168 lr=0.000020 grad_norm=0.982968
Epoch 33/100 Iteration 10/234: loss=0.084195 lr=0.000020 grad_norm=0.831195
Epoch 33/100 Iteration 11/234: loss=0.080940 lr=0.000020 grad_norm=0.565776
Epoch 33/100 Iteration 12/234: loss=0.081948 lr=0.000020 grad_norm=0.447336
Epoch 33/100 Iteration 13/234: loss=0.094744 lr=0.000020 grad_norm=0.519325
Epoch 33/100 Iteration 14/234: loss=0.086430 lr=0.000020 grad_norm=0.557158
Epoch 33/100 Iteration 15/234: loss=0.087769 lr=0.000020 grad_norm=0.597717
Epoch 33/100 Iteration 16/234: loss=0.076349 lr=0.000020 grad_norm=0.864201
Epoch 33/100 Iteration 17/234: loss=0.089463 lr=0.000020 grad_norm=1.281813
Epoch 33/100 Iteration 18/234: loss=0.078181 lr=0.000020 grad_norm=0.663708
Epoch 33/100 Iteration 19/234: loss=0.093743 lr=0.000020 grad_norm=0.830603
Epoch 33/100 Iteration 20/234: loss=0.085607 lr=0.000020 grad_norm=1.465313
Epoch 33/100 Iteration 21/234: loss=0.089342 lr=0.000020 grad_norm=0.963131
Epoch 33/100 Iteration 22/234: loss=0.084902 lr=0.000020 grad_norm=0.595634
Epoch 33/100 Iteration 23/234: loss=0.085338 lr=0.000020 grad_norm=0.694209
Epoch 33/100 Iteration 24/234: loss=0.085272 lr=0.000020 grad_norm=0.504823
Epoch 33/100 Iteration 25/234: loss=0.077804 lr=0.000020 grad_norm=0.671573
Epoch 33/100 Iteration 26/234: loss=0.088853 lr=0.000020 grad_norm=0.916470
Epoch 33/100 Iteration 27/234: loss=0.078587 lr=0.000020 grad_norm=0.746698
Epoch 33/100 Iteration 28/234: loss=0.090407 lr=0.000020 grad_norm=0.772859
Epoch 33/100 Iteration 29/234: loss=0.077285 lr=0.000020 grad_norm=0.746786
Epoch 33/100 Iteration 30/234: loss=0.082821 lr=0.000020 grad_norm=0.518312
Epoch 33/100 Iteration 31/234: loss=0.082574 lr=0.000020 grad_norm=0.764597
Epoch 33/100 Iteration 32/234: loss=0.078750 lr=0.000020 grad_norm=0.940103
Epoch 33/100 Iteration 33/234: loss=0.079157 lr=0.000020 grad_norm=0.448756
Epoch 33/100 Iteration 34/234: loss=0.072047 lr=0.000020 grad_norm=0.654417
Epoch 33/100 Iteration 35/234: loss=0.093448 lr=0.000020 grad_norm=0.834803
Epoch 33/100 Iteration 36/234: loss=0.091573 lr=0.000020 grad_norm=2.121873
Epoch 33/100 Iteration 37/234: loss=0.086861 lr=0.000020 grad_norm=3.128511
Epoch 33/100 Iteration 38/234: loss=0.083243 lr=0.000020 grad_norm=1.655619
Epoch 33/100 Iteration 39/234: loss=0.074543 lr=0.000020 grad_norm=1.130249
Epoch 33/100 Iteration 40/234: loss=0.087878 lr=0.000020 grad_norm=1.735528
Epoch 33/100 Iteration 41/234: loss=0.095638 lr=0.000020 grad_norm=0.558797
Epoch 33/100 Iteration 42/234: loss=0.085275 lr=0.000020 grad_norm=1.593433
Epoch 33/100 Iteration 43/234: loss=0.085071 lr=0.000020 grad_norm=1.102210
Epoch 33/100 Iteration 44/234: loss=0.070605 lr=0.000020 grad_norm=0.829824
Epoch 33/100 Iteration 45/234: loss=0.095974 lr=0.000020 grad_norm=1.239326
Epoch 33/100 Iteration 46/234: loss=0.090681 lr=0.000020 grad_norm=0.569334
Epoch 33/100 Iteration 47/234: loss=0.088645 lr=0.000020 grad_norm=0.613637
Epoch 33/100 Iteration 48/234: loss=0.075723 lr=0.000020 grad_norm=0.656727
Epoch 33/100 Iteration 49/234: loss=0.092316 lr=0.000020 grad_norm=0.601255
Epoch 33/100 Iteration 50/234: loss=0.078814 lr=0.000020 grad_norm=0.859376
Epoch 33/100 Iteration 51/234: loss=0.083662 lr=0.000020 grad_norm=0.536335
Epoch 33/100 Iteration 52/234: loss=0.095499 lr=0.000020 grad_norm=0.582919
Epoch 33/100 Iteration 53/234: loss=0.083075 lr=0.000020 grad_norm=0.722893
Epoch 33/100 Iteration 54/234: loss=0.075118 lr=0.000020 grad_norm=0.489723
Epoch 33/100 Iteration 55/234: loss=0.088312 lr=0.000020 grad_norm=0.676605
Epoch 33/100 Iteration 56/234: loss=0.087422 lr=0.000020 grad_norm=0.613147
Epoch 33/100 Iteration 57/234: loss=0.080098 lr=0.000020 grad_norm=0.578832
Epoch 33/100 Iteration 58/234: loss=0.084843 lr=0.000020 grad_norm=0.984007
Epoch 33/100 Iteration 59/234: loss=0.089028 lr=0.000020 grad_norm=0.698330
Epoch 33/100 Iteration 60/234: loss=0.081412 lr=0.000020 grad_norm=0.541663
Epoch 33/100 Iteration 61/234: loss=0.088027 lr=0.000020 grad_norm=0.552097
Epoch 33/100 Iteration 62/234: loss=0.094428 lr=0.000020 grad_norm=0.427255
Epoch 33/100 Iteration 63/234: loss=0.079714 lr=0.000020 grad_norm=0.659477
Epoch 33/100 Iteration 64/234: loss=0.095693 lr=0.000020 grad_norm=0.741677
Epoch 33/100 Iteration 65/234: loss=0.089544 lr=0.000020 grad_norm=0.760723
Epoch 33/100 Iteration 66/234: loss=0.080336 lr=0.000020 grad_norm=0.616144
Epoch 33/100 Iteration 67/234: loss=0.076266 lr=0.000020 grad_norm=0.443169
Epoch 33/100 Iteration 68/234: loss=0.083016 lr=0.000020 grad_norm=0.456666
Epoch 33/100 Iteration 69/234: loss=0.093453 lr=0.000020 grad_norm=0.489740
Epoch 33/100 Iteration 70/234: loss=0.077407 lr=0.000020 grad_norm=0.645575
Epoch 33/100 Iteration 71/234: loss=0.086359 lr=0.000020 grad_norm=0.437659
Epoch 33/100 Iteration 72/234: loss=0.070644 lr=0.000020 grad_norm=0.794839
Epoch 33/100 Iteration 73/234: loss=0.078430 lr=0.000020 grad_norm=0.944915
Epoch 33/100 Iteration 74/234: loss=0.086833 lr=0.000020 grad_norm=0.801476
Epoch 33/100 Iteration 75/234: loss=0.079086 lr=0.000020 grad_norm=0.362765
Epoch 33/100 Iteration 76/234: loss=0.092804 lr=0.000020 grad_norm=0.719621
Epoch 33/100 Iteration 77/234: loss=0.081206 lr=0.000020 grad_norm=1.249347
Epoch 33/100 Iteration 78/234: loss=0.092471 lr=0.000020 grad_norm=1.277343
Epoch 33/100 Iteration 79/234: loss=0.084312 lr=0.000020 grad_norm=0.816460
Epoch 33/100 Iteration 80/234: loss=0.083056 lr=0.000020 grad_norm=0.805533
Epoch 33/100 Iteration 81/234: loss=0.089492 lr=0.000020 grad_norm=1.182538
Epoch 33/100 Iteration 82/234: loss=0.093821 lr=0.000020 grad_norm=1.597561
Epoch 33/100 Iteration 83/234: loss=0.089412 lr=0.000020 grad_norm=1.602696
Epoch 33/100 Iteration 84/234: loss=0.090890 lr=0.000020 grad_norm=1.059382
Epoch 33/100 Iteration 85/234: loss=0.082622 lr=0.000020 grad_norm=0.626904
Epoch 33/100 Iteration 86/234: loss=0.074294 lr=0.000020 grad_norm=1.138093
Epoch 33/100 Iteration 87/234: loss=0.090884 lr=0.000020 grad_norm=1.084573
Epoch 33/100 Iteration 88/234: loss=0.085742 lr=0.000020 grad_norm=0.604926
Epoch 33/100 Iteration 89/234: loss=0.073800 lr=0.000020 grad_norm=1.028721
Epoch 33/100 Iteration 90/234: loss=0.094648 lr=0.000020 grad_norm=0.674314
Epoch 33/100 Iteration 91/234: loss=0.081601 lr=0.000020 grad_norm=0.640213
Epoch 33/100 Iteration 92/234: loss=0.084656 lr=0.000020 grad_norm=1.134298
Epoch 33/100 Iteration 93/234: loss=0.076498 lr=0.000020 grad_norm=0.660850
Epoch 33/100 Iteration 94/234: loss=0.090800 lr=0.000020 grad_norm=0.873235
Epoch 33/100 Iteration 95/234: loss=0.082396 lr=0.000020 grad_norm=1.502952
Epoch 33/100 Iteration 96/234: loss=0.085127 lr=0.000020 grad_norm=1.080647
Epoch 33/100 Iteration 97/234: loss=0.090109 lr=0.000020 grad_norm=0.449072
Epoch 33/100 Iteration 98/234: loss=0.082889 lr=0.000020 grad_norm=0.913110
Epoch 33/100 Iteration 99/234: loss=0.083905 lr=0.000020 grad_norm=1.001910
Epoch 33/100 Iteration 100/234: loss=0.076645 lr=0.000020 grad_norm=0.591828
Epoch 33/100 Iteration 101/234: loss=0.086056 lr=0.000020 grad_norm=0.832855
Epoch 33/100 Iteration 102/234: loss=0.095768 lr=0.000020 grad_norm=0.941373
Epoch 33/100 Iteration 103/234: loss=0.091187 lr=0.000020 grad_norm=0.651169
Epoch 33/100 Iteration 104/234: loss=0.086311 lr=0.000020 grad_norm=1.020830
Epoch 33/100 Iteration 105/234: loss=0.079628 lr=0.000020 grad_norm=1.601989
Epoch 33/100 Iteration 106/234: loss=0.081828 lr=0.000020 grad_norm=1.308091
Epoch 33/100 Iteration 107/234: loss=0.081120 lr=0.000020 grad_norm=0.436953
Epoch 33/100 Iteration 108/234: loss=0.079600 lr=0.000020 grad_norm=1.202891
Epoch 33/100 Iteration 109/234: loss=0.084111 lr=0.000020 grad_norm=1.253296
Epoch 33/100 Iteration 110/234: loss=0.078249 lr=0.000020 grad_norm=0.523362
Epoch 33/100 Iteration 111/234: loss=0.081010 lr=0.000020 grad_norm=1.025977
Epoch 33/100 Iteration 112/234: loss=0.074051 lr=0.000020 grad_norm=1.282017
Epoch 33/100 Iteration 113/234: loss=0.082538 lr=0.000020 grad_norm=0.732784
Epoch 33/100 Iteration 114/234: loss=0.070026 lr=0.000020 grad_norm=0.930866
Epoch 33/100 Iteration 115/234: loss=0.091481 lr=0.000020 grad_norm=1.544301
Epoch 33/100 Iteration 116/234: loss=0.089601 lr=0.000020 grad_norm=1.511885
Epoch 33/100 Iteration 117/234: loss=0.089564 lr=0.000020 grad_norm=0.634176
Epoch 33/100 Iteration 118/234: loss=0.079300 lr=0.000020 grad_norm=1.045104
Epoch 33/100 Iteration 119/234: loss=0.087298 lr=0.000020 grad_norm=1.483954
Epoch 33/100 Iteration 120/234: loss=0.083109 lr=0.000020 grad_norm=0.754557
Epoch 33/100 Iteration 121/234: loss=0.095740 lr=0.000020 grad_norm=1.051970
Epoch 33/100 Iteration 122/234: loss=0.079802 lr=0.000020 grad_norm=1.419014
Epoch 33/100 Iteration 123/234: loss=0.085729 lr=0.000020 grad_norm=1.115817
Epoch 33/100 Iteration 124/234: loss=0.082528 lr=0.000020 grad_norm=0.687431
Epoch 33/100 Iteration 125/234: loss=0.095333 lr=0.000020 grad_norm=1.358148
Epoch 33/100 Iteration 126/234: loss=0.088119 lr=0.000020 grad_norm=0.557985
Epoch 33/100 Iteration 127/234: loss=0.079152 lr=0.000020 grad_norm=0.941883
Epoch 33/100 Iteration 128/234: loss=0.096656 lr=0.000020 grad_norm=0.871932
Epoch 33/100 Iteration 129/234: loss=0.084153 lr=0.000020 grad_norm=0.651695
Epoch 33/100 Iteration 130/234: loss=0.094402 lr=0.000020 grad_norm=1.392104
Epoch 33/100 Iteration 131/234: loss=0.076960 lr=0.000020 grad_norm=0.621708
Epoch 33/100 Iteration 132/234: loss=0.088440 lr=0.000020 grad_norm=1.031876
Epoch 33/100 Iteration 133/234: loss=0.074188 lr=0.000020 grad_norm=1.238070
Epoch 33/100 Iteration 134/234: loss=0.081012 lr=0.000020 grad_norm=0.398480
Epoch 33/100 Iteration 135/234: loss=0.089019 lr=0.000020 grad_norm=1.563328
Epoch 33/100 Iteration 136/234: loss=0.075293 lr=0.000020 grad_norm=1.696977
Epoch 33/100 Iteration 137/234: loss=0.081542 lr=0.000020 grad_norm=0.485808
Epoch 33/100 Iteration 138/234: loss=0.085950 lr=0.000020 grad_norm=2.236037
Epoch 33/100 Iteration 139/234: loss=0.084064 lr=0.000020 grad_norm=1.704316
Epoch 33/100 Iteration 140/234: loss=0.086336 lr=0.000020 grad_norm=1.061209
Epoch 33/100 Iteration 141/234: loss=0.094071 lr=0.000020 grad_norm=2.444650
Epoch 33/100 Iteration 142/234: loss=0.084473 lr=0.000020 grad_norm=1.222344
Epoch 33/100 Iteration 143/234: loss=0.088999 lr=0.000020 grad_norm=1.330812
Epoch 33/100 Iteration 144/234: loss=0.079631 lr=0.000020 grad_norm=1.378612
Epoch 33/100 Iteration 145/234: loss=0.093935 lr=0.000020 grad_norm=0.707024
Epoch 33/100 Iteration 146/234: loss=0.084746 lr=0.000020 grad_norm=1.053164
Epoch 33/100 Iteration 147/234: loss=0.079349 lr=0.000020 grad_norm=0.532419
Epoch 33/100 Iteration 148/234: loss=0.082861 lr=0.000020 grad_norm=1.538526
Epoch 33/100 Iteration 149/234: loss=0.077387 lr=0.000020 grad_norm=0.862446
Epoch 33/100 Iteration 150/234: loss=0.071483 lr=0.000020 grad_norm=0.783067
Epoch 33/100 Iteration 151/234: loss=0.091138 lr=0.000020 grad_norm=0.774280
Epoch 33/100 Iteration 152/234: loss=0.099981 lr=0.000020 grad_norm=0.653618
Epoch 33/100 Iteration 153/234: loss=0.078942 lr=0.000020 grad_norm=0.939759
Epoch 33/100 Iteration 154/234: loss=0.087059 lr=0.000020 grad_norm=0.387189
Epoch 33/100 Iteration 155/234: loss=0.082992 lr=0.000020 grad_norm=1.106366
Epoch 33/100 Iteration 156/234: loss=0.102602 lr=0.000020 grad_norm=1.405495
Epoch 33/100 Iteration 157/234: loss=0.093513 lr=0.000020 grad_norm=0.556807
Epoch 33/100 Iteration 158/234: loss=0.090230 lr=0.000020 grad_norm=0.819918
Epoch 33/100 Iteration 159/234: loss=0.086978 lr=0.000020 grad_norm=0.790350
Epoch 33/100 Iteration 160/234: loss=0.093970 lr=0.000020 grad_norm=0.596006
Epoch 33/100 Iteration 161/234: loss=0.078100 lr=0.000020 grad_norm=0.828396
Epoch 33/100 Iteration 162/234: loss=0.082270 lr=0.000020 grad_norm=0.493742
Epoch 33/100 Iteration 163/234: loss=0.084829 lr=0.000020 grad_norm=0.744670
Epoch 33/100 Iteration 164/234: loss=0.091889 lr=0.000020 grad_norm=0.644507
Epoch 33/100 Iteration 165/234: loss=0.079078 lr=0.000020 grad_norm=0.617331
Epoch 33/100 Iteration 166/234: loss=0.072752 lr=0.000020 grad_norm=0.932341
Epoch 33/100 Iteration 167/234: loss=0.091666 lr=0.000020 grad_norm=0.509134
Epoch 33/100 Iteration 168/234: loss=0.088282 lr=0.000020 grad_norm=0.551908
Epoch 33/100 Iteration 169/234: loss=0.088029 lr=0.000020 grad_norm=0.432198
Epoch 33/100 Iteration 170/234: loss=0.085321 lr=0.000020 grad_norm=0.466633
Epoch 33/100 Iteration 171/234: loss=0.084120 lr=0.000020 grad_norm=0.466251
Epoch 33/100 Iteration 172/234: loss=0.079743 lr=0.000020 grad_norm=0.457353
Epoch 33/100 Iteration 173/234: loss=0.085583 lr=0.000020 grad_norm=0.347358
Epoch 33/100 Iteration 174/234: loss=0.082702 lr=0.000020 grad_norm=0.621069
Epoch 33/100 Iteration 175/234: loss=0.074168 lr=0.000020 grad_norm=0.523438
Epoch 33/100 Iteration 176/234: loss=0.081059 lr=0.000020 grad_norm=0.635778
Epoch 33/100 Iteration 177/234: loss=0.087571 lr=0.000020 grad_norm=0.945725
Epoch 33/100 Iteration 178/234: loss=0.084750 lr=0.000020 grad_norm=0.340059
Epoch 33/100 Iteration 179/234: loss=0.081207 lr=0.000020 grad_norm=1.032630
Epoch 33/100 Iteration 180/234: loss=0.081250 lr=0.000020 grad_norm=0.994178
Epoch 33/100 Iteration 181/234: loss=0.076507 lr=0.000020 grad_norm=0.422515
Epoch 33/100 Iteration 182/234: loss=0.086188 lr=0.000020 grad_norm=1.239963
Epoch 33/100 Iteration 183/234: loss=0.081935 lr=0.000020 grad_norm=0.882434
Epoch 33/100 Iteration 184/234: loss=0.096974 lr=0.000020 grad_norm=0.662706
Epoch 33/100 Iteration 185/234: loss=0.095068 lr=0.000020 grad_norm=1.368700
Epoch 33/100 Iteration 186/234: loss=0.079651 lr=0.000020 grad_norm=0.647279
Epoch 33/100 Iteration 187/234: loss=0.095088 lr=0.000020 grad_norm=0.826300
Epoch 33/100 Iteration 188/234: loss=0.070857 lr=0.000020 grad_norm=1.192048
Epoch 33/100 Iteration 189/234: loss=0.085156 lr=0.000020 grad_norm=0.614534
Epoch 33/100 Iteration 190/234: loss=0.077291 lr=0.000020 grad_norm=0.693804
Epoch 33/100 Iteration 191/234: loss=0.085275 lr=0.000020 grad_norm=0.616330
Epoch 33/100 Iteration 192/234: loss=0.091017 lr=0.000020 grad_norm=0.554456
Epoch 33/100 Iteration 193/234: loss=0.079048 lr=0.000020 grad_norm=0.807564
Epoch 33/100 Iteration 194/234: loss=0.077145 lr=0.000020 grad_norm=0.634882
Epoch 33/100 Iteration 195/234: loss=0.086245 lr=0.000020 grad_norm=0.764723
Epoch 33/100 Iteration 196/234: loss=0.074651 lr=0.000020 grad_norm=1.088237
Epoch 33/100 Iteration 197/234: loss=0.079399 lr=0.000020 grad_norm=0.309933
Epoch 33/100 Iteration 198/234: loss=0.091840 lr=0.000020 grad_norm=0.916346
Epoch 33/100 Iteration 199/234: loss=0.072301 lr=0.000020 grad_norm=0.991601
Epoch 33/100 Iteration 200/234: loss=0.073727 lr=0.000020 grad_norm=1.217459
Epoch 33/100 Iteration 201/234: loss=0.084756 lr=0.000020 grad_norm=0.826696
Epoch 33/100 Iteration 202/234: loss=0.075246 lr=0.000020 grad_norm=0.449820
Epoch 33/100 Iteration 203/234: loss=0.081721 lr=0.000020 grad_norm=0.936640
Epoch 33/100 Iteration 204/234: loss=0.072730 lr=0.000020 grad_norm=0.673686
Epoch 33/100 Iteration 205/234: loss=0.087299 lr=0.000020 grad_norm=0.629512
Epoch 33/100 Iteration 206/234: loss=0.083482 lr=0.000020 grad_norm=1.338596
Epoch 33/100 Iteration 207/234: loss=0.084162 lr=0.000020 grad_norm=1.020760
Epoch 33/100 Iteration 208/234: loss=0.084861 lr=0.000020 grad_norm=0.355648
Epoch 33/100 Iteration 209/234: loss=0.082444 lr=0.000020 grad_norm=1.191040
Epoch 33/100 Iteration 210/234: loss=0.076883 lr=0.000020 grad_norm=1.119483
Epoch 33/100 Iteration 211/234: loss=0.081435 lr=0.000020 grad_norm=0.481497
Epoch 33/100 Iteration 212/234: loss=0.080768 lr=0.000020 grad_norm=1.060482
Epoch 33/100 Iteration 213/234: loss=0.081975 lr=0.000020 grad_norm=0.929342
Epoch 33/100 Iteration 214/234: loss=0.076195 lr=0.000020 grad_norm=0.643290
Epoch 33/100 Iteration 215/234: loss=0.088951 lr=0.000020 grad_norm=0.438731
Epoch 33/100 Iteration 216/234: loss=0.078039 lr=0.000020 grad_norm=0.543657
Epoch 33/100 Iteration 217/234: loss=0.082313 lr=0.000020 grad_norm=0.447439
Epoch 33/100 Iteration 218/234: loss=0.069903 lr=0.000020 grad_norm=0.400684
Epoch 33/100 Iteration 219/234: loss=0.080927 lr=0.000020 grad_norm=0.578667
Epoch 33/100 Iteration 220/234: loss=0.076569 lr=0.000020 grad_norm=0.693399
Epoch 33/100 Iteration 221/234: loss=0.087605 lr=0.000020 grad_norm=0.687551
Epoch 33/100 Iteration 222/234: loss=0.074629 lr=0.000020 grad_norm=1.742011
Epoch 33/100 Iteration 223/234: loss=0.090772 lr=0.000020 grad_norm=1.921845
Epoch 33/100 Iteration 224/234: loss=0.080411 lr=0.000020 grad_norm=0.585859
Epoch 33/100 Iteration 225/234: loss=0.080086 lr=0.000020 grad_norm=1.477126
Epoch 33/100 Iteration 226/234: loss=0.078348 lr=0.000020 grad_norm=1.449848
Epoch 33/100 Iteration 227/234: loss=0.094270 lr=0.000020 grad_norm=0.731171
Epoch 33/100 Iteration 228/234: loss=0.078422 lr=0.000020 grad_norm=0.914185
Epoch 33/100 Iteration 229/234: loss=0.084204 lr=0.000020 grad_norm=0.639932
Epoch 33/100 Iteration 230/234: loss=0.074200 lr=0.000020 grad_norm=0.460826
Epoch 33/100 Iteration 231/234: loss=0.099557 lr=0.000020 grad_norm=0.746790
Epoch 33/100 Iteration 232/234: loss=0.092833 lr=0.000020 grad_norm=0.853580
Epoch 33/100 Iteration 233/234: loss=0.086155 lr=0.000020 grad_norm=0.716961
Epoch 33/100 Iteration 234/234: loss=0.079028 lr=0.000020 grad_norm=0.708207
Epoch 33/100 finished. Avg Loss: 0.084225
Epoch 34/100 Iteration 1/234: loss=0.076240 lr=0.000020 grad_norm=0.592628
Epoch 34/100 Iteration 2/234: loss=0.084704 lr=0.000020 grad_norm=0.701685
Epoch 34/100 Iteration 3/234: loss=0.089373 lr=0.000020 grad_norm=0.799490
Epoch 34/100 Iteration 4/234: loss=0.083680 lr=0.000020 grad_norm=0.527125
Epoch 34/100 Iteration 5/234: loss=0.087736 lr=0.000020 grad_norm=0.872986
Epoch 34/100 Iteration 6/234: loss=0.086802 lr=0.000020 grad_norm=1.391300
Epoch 34/100 Iteration 7/234: loss=0.083146 lr=0.000020 grad_norm=0.999490
Epoch 34/100 Iteration 8/234: loss=0.093250 lr=0.000020 grad_norm=0.373519
Epoch 34/100 Iteration 9/234: loss=0.093254 lr=0.000020 grad_norm=1.216851
Epoch 34/100 Iteration 10/234: loss=0.073160 lr=0.000020 grad_norm=1.520366
Epoch 34/100 Iteration 11/234: loss=0.090544 lr=0.000020 grad_norm=1.054404
Epoch 34/100 Iteration 12/234: loss=0.088467 lr=0.000020 grad_norm=0.497250
Epoch 34/100 Iteration 13/234: loss=0.076349 lr=0.000020 grad_norm=1.487438
Epoch 34/100 Iteration 14/234: loss=0.090579 lr=0.000020 grad_norm=1.026555
Epoch 34/100 Iteration 15/234: loss=0.091623 lr=0.000020 grad_norm=0.620689
Epoch 34/100 Iteration 16/234: loss=0.074691 lr=0.000020 grad_norm=0.712595
Epoch 34/100 Iteration 17/234: loss=0.075159 lr=0.000020 grad_norm=0.694635
Epoch 34/100 Iteration 18/234: loss=0.082321 lr=0.000020 grad_norm=1.110594
Epoch 34/100 Iteration 19/234: loss=0.082758 lr=0.000020 grad_norm=1.295011
Epoch 34/100 Iteration 20/234: loss=0.083865 lr=0.000020 grad_norm=0.843359
Epoch 34/100 Iteration 21/234: loss=0.083721 lr=0.000020 grad_norm=0.567387
Epoch 34/100 Iteration 22/234: loss=0.082437 lr=0.000020 grad_norm=0.877671
Epoch 34/100 Iteration 23/234: loss=0.090501 lr=0.000020 grad_norm=0.659658
Epoch 34/100 Iteration 24/234: loss=0.082247 lr=0.000020 grad_norm=0.336062
Epoch 34/100 Iteration 25/234: loss=0.091351 lr=0.000020 grad_norm=0.637144
Epoch 34/100 Iteration 26/234: loss=0.078190 lr=0.000020 grad_norm=0.890024
Epoch 34/100 Iteration 27/234: loss=0.080222 lr=0.000020 grad_norm=1.216437
Epoch 34/100 Iteration 28/234: loss=0.083937 lr=0.000020 grad_norm=1.212425
Epoch 34/100 Iteration 29/234: loss=0.079532 lr=0.000020 grad_norm=0.606473
Epoch 34/100 Iteration 30/234: loss=0.081122 lr=0.000020 grad_norm=1.055299
Epoch 34/100 Iteration 31/234: loss=0.081328 lr=0.000020 grad_norm=1.187879
Epoch 34/100 Iteration 32/234: loss=0.081914 lr=0.000020 grad_norm=0.470047
Epoch 34/100 Iteration 33/234: loss=0.073729 lr=0.000020 grad_norm=0.964860
Epoch 34/100 Iteration 34/234: loss=0.082693 lr=0.000020 grad_norm=0.649986
Epoch 34/100 Iteration 35/234: loss=0.079779 lr=0.000020 grad_norm=0.462138
Epoch 34/100 Iteration 36/234: loss=0.078677 lr=0.000020 grad_norm=0.351386
Epoch 34/100 Iteration 37/234: loss=0.071471 lr=0.000020 grad_norm=0.591854
Epoch 34/100 Iteration 38/234: loss=0.076894 lr=0.000020 grad_norm=1.115672
Epoch 34/100 Iteration 39/234: loss=0.078542 lr=0.000020 grad_norm=0.965360
Epoch 34/100 Iteration 40/234: loss=0.074077 lr=0.000020 grad_norm=0.299282
Epoch 34/100 Iteration 41/234: loss=0.079895 lr=0.000020 grad_norm=1.021936
Epoch 34/100 Iteration 42/234: loss=0.073058 lr=0.000020 grad_norm=0.800037
Epoch 34/100 Iteration 43/234: loss=0.073332 lr=0.000020 grad_norm=0.393373
Epoch 34/100 Iteration 44/234: loss=0.086083 lr=0.000020 grad_norm=0.702904
Epoch 34/100 Iteration 45/234: loss=0.087544 lr=0.000020 grad_norm=0.495417
Epoch 34/100 Iteration 46/234: loss=0.075001 lr=0.000020 grad_norm=0.437103
Epoch 34/100 Iteration 47/234: loss=0.080240 lr=0.000020 grad_norm=0.695538
Epoch 34/100 Iteration 48/234: loss=0.075565 lr=0.000020 grad_norm=0.543509
Epoch 34/100 Iteration 49/234: loss=0.096963 lr=0.000020 grad_norm=0.918657
Epoch 34/100 Iteration 50/234: loss=0.084771 lr=0.000020 grad_norm=0.684184
Epoch 34/100 Iteration 51/234: loss=0.074416 lr=0.000020 grad_norm=0.469652
Epoch 34/100 Iteration 52/234: loss=0.081933 lr=0.000020 grad_norm=0.949309
Epoch 34/100 Iteration 53/234: loss=0.079068 lr=0.000020 grad_norm=1.004824
Epoch 34/100 Iteration 54/234: loss=0.090622 lr=0.000020 grad_norm=0.996972
Epoch 34/100 Iteration 55/234: loss=0.063456 lr=0.000020 grad_norm=0.733537
Epoch 34/100 Iteration 56/234: loss=0.077191 lr=0.000020 grad_norm=0.671371
Epoch 34/100 Iteration 57/234: loss=0.072451 lr=0.000020 grad_norm=0.968276
Epoch 34/100 Iteration 58/234: loss=0.082595 lr=0.000020 grad_norm=0.666920
Epoch 34/100 Iteration 59/234: loss=0.101996 lr=0.000020 grad_norm=0.518945
Epoch 34/100 Iteration 60/234: loss=0.080794 lr=0.000020 grad_norm=0.963354
Epoch 34/100 Iteration 61/234: loss=0.083628 lr=0.000020 grad_norm=1.311030
Epoch 34/100 Iteration 62/234: loss=0.079733 lr=0.000020 grad_norm=1.216373
Epoch 34/100 Iteration 63/234: loss=0.078562 lr=0.000020 grad_norm=0.634627
Epoch 34/100 Iteration 64/234: loss=0.097649 lr=0.000020 grad_norm=1.200255
Epoch 34/100 Iteration 65/234: loss=0.076000 lr=0.000020 grad_norm=1.532110
Epoch 34/100 Iteration 66/234: loss=0.082264 lr=0.000020 grad_norm=1.030766
Epoch 34/100 Iteration 67/234: loss=0.080742 lr=0.000020 grad_norm=1.015106
Epoch 34/100 Iteration 68/234: loss=0.080365 lr=0.000020 grad_norm=1.878118
Epoch 34/100 Iteration 69/234: loss=0.085136 lr=0.000020 grad_norm=1.039319
Epoch 34/100 Iteration 70/234: loss=0.076071 lr=0.000020 grad_norm=0.862880
Epoch 34/100 Iteration 71/234: loss=0.084185 lr=0.000020 grad_norm=1.458255
Epoch 34/100 Iteration 72/234: loss=0.071740 lr=0.000020 grad_norm=1.036032
Epoch 34/100 Iteration 73/234: loss=0.077010 lr=0.000020 grad_norm=0.592151
Epoch 34/100 Iteration 74/234: loss=0.086180 lr=0.000020 grad_norm=0.826447
Epoch 34/100 Iteration 75/234: loss=0.074118 lr=0.000020 grad_norm=0.945534
Epoch 34/100 Iteration 76/234: loss=0.083443 lr=0.000020 grad_norm=0.460366
Epoch 34/100 Iteration 77/234: loss=0.082959 lr=0.000020 grad_norm=0.820729
Epoch 34/100 Iteration 78/234: loss=0.070629 lr=0.000020 grad_norm=0.576049
Epoch 34/100 Iteration 79/234: loss=0.091656 lr=0.000020 grad_norm=0.595113
Epoch 34/100 Iteration 80/234: loss=0.091359 lr=0.000020 grad_norm=1.277284
Epoch 34/100 Iteration 81/234: loss=0.077386 lr=0.000020 grad_norm=1.451052
Epoch 34/100 Iteration 82/234: loss=0.076616 lr=0.000020 grad_norm=0.794603
Epoch 34/100 Iteration 83/234: loss=0.095358 lr=0.000020 grad_norm=0.574929
Epoch 34/100 Iteration 84/234: loss=0.089094 lr=0.000020 grad_norm=1.153898
Epoch 34/100 Iteration 85/234: loss=0.085302 lr=0.000020 grad_norm=0.775716
Epoch 34/100 Iteration 86/234: loss=0.071075 lr=0.000020 grad_norm=0.410362
Epoch 34/100 Iteration 87/234: loss=0.075635 lr=0.000020 grad_norm=0.588758
Epoch 34/100 Iteration 88/234: loss=0.086324 lr=0.000020 grad_norm=0.529288
Epoch 34/100 Iteration 89/234: loss=0.084421 lr=0.000020 grad_norm=0.444829
Epoch 34/100 Iteration 90/234: loss=0.073266 lr=0.000020 grad_norm=0.407285
Epoch 34/100 Iteration 91/234: loss=0.082641 lr=0.000020 grad_norm=0.830122
Epoch 34/100 Iteration 92/234: loss=0.078058 lr=0.000020 grad_norm=0.651649
Epoch 34/100 Iteration 93/234: loss=0.081641 lr=0.000020 grad_norm=0.493557
Epoch 34/100 Iteration 94/234: loss=0.084964 lr=0.000020 grad_norm=0.706527
Epoch 34/100 Iteration 95/234: loss=0.079121 lr=0.000020 grad_norm=0.647329
Epoch 34/100 Iteration 96/234: loss=0.082046 lr=0.000020 grad_norm=0.597650
Epoch 34/100 Iteration 97/234: loss=0.088823 lr=0.000020 grad_norm=0.951735
Epoch 34/100 Iteration 98/234: loss=0.077403 lr=0.000020 grad_norm=0.862053
Epoch 34/100 Iteration 99/234: loss=0.083084 lr=0.000020 grad_norm=0.880652
Epoch 34/100 Iteration 100/234: loss=0.075184 lr=0.000020 grad_norm=0.500598
Epoch 34/100 Iteration 101/234: loss=0.080426 lr=0.000020 grad_norm=0.904976
Epoch 34/100 Iteration 102/234: loss=0.082596 lr=0.000020 grad_norm=1.338860
Epoch 34/100 Iteration 103/234: loss=0.084315 lr=0.000020 grad_norm=0.660960
Epoch 34/100 Iteration 104/234: loss=0.085689 lr=0.000020 grad_norm=0.625943
Epoch 34/100 Iteration 105/234: loss=0.086754 lr=0.000020 grad_norm=0.715301
Epoch 34/100 Iteration 106/234: loss=0.084724 lr=0.000020 grad_norm=0.576369
Epoch 34/100 Iteration 107/234: loss=0.075265 lr=0.000020 grad_norm=1.157120
Epoch 34/100 Iteration 108/234: loss=0.085718 lr=0.000020 grad_norm=0.900257
Epoch 34/100 Iteration 109/234: loss=0.075133 lr=0.000020 grad_norm=0.620627
Epoch 34/100 Iteration 110/234: loss=0.088468 lr=0.000020 grad_norm=1.058806
Epoch 34/100 Iteration 111/234: loss=0.085317 lr=0.000020 grad_norm=0.544605
Epoch 34/100 Iteration 112/234: loss=0.088271 lr=0.000020 grad_norm=0.842607
Epoch 34/100 Iteration 113/234: loss=0.089795 lr=0.000020 grad_norm=0.841053
Epoch 34/100 Iteration 114/234: loss=0.091454 lr=0.000020 grad_norm=0.535507
Epoch 34/100 Iteration 115/234: loss=0.088954 lr=0.000020 grad_norm=0.741508
Epoch 34/100 Iteration 116/234: loss=0.088220 lr=0.000020 grad_norm=0.671299
Epoch 34/100 Iteration 117/234: loss=0.071609 lr=0.000020 grad_norm=0.452394
Epoch 34/100 Iteration 118/234: loss=0.085023 lr=0.000020 grad_norm=0.874665
Epoch 34/100 Iteration 119/234: loss=0.071347 lr=0.000020 grad_norm=0.524034
Epoch 34/100 Iteration 120/234: loss=0.096813 lr=0.000020 grad_norm=0.832942
Epoch 34/100 Iteration 121/234: loss=0.096550 lr=0.000020 grad_norm=1.493920
Epoch 34/100 Iteration 122/234: loss=0.083208 lr=0.000020 grad_norm=1.380952
Epoch 34/100 Iteration 123/234: loss=0.086514 lr=0.000020 grad_norm=0.653667
Epoch 34/100 Iteration 124/234: loss=0.078184 lr=0.000020 grad_norm=0.707068
Epoch 34/100 Iteration 125/234: loss=0.093071 lr=0.000020 grad_norm=0.903883
Epoch 34/100 Iteration 126/234: loss=0.078848 lr=0.000020 grad_norm=0.541354
Epoch 34/100 Iteration 127/234: loss=0.076344 lr=0.000020 grad_norm=0.489348
Epoch 34/100 Iteration 128/234: loss=0.086537 lr=0.000020 grad_norm=0.443185
Epoch 34/100 Iteration 129/234: loss=0.074664 lr=0.000020 grad_norm=0.590928
Epoch 34/100 Iteration 130/234: loss=0.080623 lr=0.000020 grad_norm=0.537382
Epoch 34/100 Iteration 131/234: loss=0.080465 lr=0.000020 grad_norm=0.684567
Epoch 34/100 Iteration 132/234: loss=0.082948 lr=0.000020 grad_norm=0.907228
Epoch 34/100 Iteration 133/234: loss=0.077917 lr=0.000020 grad_norm=0.783539
Epoch 34/100 Iteration 134/234: loss=0.083922 lr=0.000020 grad_norm=0.386322
Epoch 34/100 Iteration 135/234: loss=0.079520 lr=0.000020 grad_norm=1.029936
Epoch 34/100 Iteration 136/234: loss=0.083309 lr=0.000020 grad_norm=0.999884
Epoch 34/100 Iteration 137/234: loss=0.079488 lr=0.000020 grad_norm=0.452166
Epoch 34/100 Iteration 138/234: loss=0.082775 lr=0.000020 grad_norm=0.775028
Epoch 34/100 Iteration 139/234: loss=0.064206 lr=0.000020 grad_norm=0.670080
Epoch 34/100 Iteration 140/234: loss=0.076530 lr=0.000020 grad_norm=0.396445
Epoch 34/100 Iteration 141/234: loss=0.072295 lr=0.000020 grad_norm=0.655607
Epoch 34/100 Iteration 142/234: loss=0.076837 lr=0.000020 grad_norm=0.472627
Epoch 34/100 Iteration 143/234: loss=0.099372 lr=0.000020 grad_norm=0.695146
Epoch 34/100 Iteration 144/234: loss=0.081751 lr=0.000020 grad_norm=1.051908
Epoch 34/100 Iteration 145/234: loss=0.086760 lr=0.000020 grad_norm=1.054204
Epoch 34/100 Iteration 146/234: loss=0.085183 lr=0.000020 grad_norm=0.445490
Epoch 34/100 Iteration 147/234: loss=0.078223 lr=0.000020 grad_norm=1.235543
Epoch 34/100 Iteration 148/234: loss=0.074801 lr=0.000020 grad_norm=1.546483
Epoch 34/100 Iteration 149/234: loss=0.082840 lr=0.000020 grad_norm=1.144701
Epoch 34/100 Iteration 150/234: loss=0.074919 lr=0.000020 grad_norm=1.012922
Epoch 34/100 Iteration 151/234: loss=0.078852 lr=0.000020 grad_norm=1.557816
Epoch 34/100 Iteration 152/234: loss=0.084681 lr=0.000020 grad_norm=1.697238
Epoch 34/100 Iteration 153/234: loss=0.084885 lr=0.000020 grad_norm=1.045536
Epoch 34/100 Iteration 154/234: loss=0.097253 lr=0.000020 grad_norm=0.553669
Epoch 34/100 Iteration 155/234: loss=0.078774 lr=0.000020 grad_norm=0.770447
Epoch 34/100 Iteration 156/234: loss=0.084076 lr=0.000020 grad_norm=0.803000
Epoch 34/100 Iteration 157/234: loss=0.076042 lr=0.000020 grad_norm=0.519368
Epoch 34/100 Iteration 158/234: loss=0.074153 lr=0.000020 grad_norm=0.622943
Epoch 34/100 Iteration 159/234: loss=0.078262 lr=0.000020 grad_norm=0.747550
Epoch 34/100 Iteration 160/234: loss=0.084122 lr=0.000020 grad_norm=0.592392
Epoch 34/100 Iteration 161/234: loss=0.081505 lr=0.000020 grad_norm=0.662914
Epoch 34/100 Iteration 162/234: loss=0.084113 lr=0.000020 grad_norm=1.355488
Epoch 34/100 Iteration 163/234: loss=0.076246 lr=0.000020 grad_norm=1.471456
Epoch 34/100 Iteration 164/234: loss=0.074424 lr=0.000020 grad_norm=0.756933
Epoch 34/100 Iteration 165/234: loss=0.077727 lr=0.000020 grad_norm=0.954101
Epoch 34/100 Iteration 166/234: loss=0.075529 lr=0.000020 grad_norm=1.264616
Epoch 34/100 Iteration 167/234: loss=0.079489 lr=0.000020 grad_norm=0.660481
Epoch 34/100 Iteration 168/234: loss=0.095992 lr=0.000020 grad_norm=1.347113
Epoch 34/100 Iteration 169/234: loss=0.078444 lr=0.000020 grad_norm=1.315964
Epoch 34/100 Iteration 170/234: loss=0.075487 lr=0.000020 grad_norm=0.415544
Epoch 34/100 Iteration 171/234: loss=0.075080 lr=0.000020 grad_norm=1.201928
Epoch 34/100 Iteration 172/234: loss=0.082800 lr=0.000020 grad_norm=0.600499
Epoch 34/100 Iteration 173/234: loss=0.080297 lr=0.000020 grad_norm=1.040717
Epoch 34/100 Iteration 174/234: loss=0.068887 lr=0.000020 grad_norm=1.140547
Epoch 34/100 Iteration 175/234: loss=0.090127 lr=0.000020 grad_norm=0.698329
Epoch 34/100 Iteration 176/234: loss=0.089433 lr=0.000020 grad_norm=1.659579
Epoch 34/100 Iteration 177/234: loss=0.090630 lr=0.000020 grad_norm=1.399008
Epoch 34/100 Iteration 178/234: loss=0.082921 lr=0.000020 grad_norm=0.581939
Epoch 34/100 Iteration 179/234: loss=0.085585 lr=0.000020 grad_norm=0.749157
Epoch 34/100 Iteration 180/234: loss=0.086796 lr=0.000020 grad_norm=0.842088
Epoch 34/100 Iteration 181/234: loss=0.077643 lr=0.000020 grad_norm=0.576812
Epoch 34/100 Iteration 182/234: loss=0.069894 lr=0.000020 grad_norm=0.399648
Epoch 34/100 Iteration 183/234: loss=0.070913 lr=0.000020 grad_norm=0.367716
Epoch 34/100 Iteration 184/234: loss=0.080001 lr=0.000020 grad_norm=0.375734
Epoch 34/100 Iteration 185/234: loss=0.088628 lr=0.000020 grad_norm=0.341639
Epoch 34/100 Iteration 186/234: loss=0.075218 lr=0.000020 grad_norm=0.321212
Epoch 34/100 Iteration 187/234: loss=0.082786 lr=0.000020 grad_norm=0.428543
Epoch 34/100 Iteration 188/234: loss=0.077432 lr=0.000020 grad_norm=0.351273
Epoch 34/100 Iteration 189/234: loss=0.089594 lr=0.000020 grad_norm=0.553548
Epoch 34/100 Iteration 190/234: loss=0.096753 lr=0.000020 grad_norm=0.519828
Epoch 34/100 Iteration 191/234: loss=0.072159 lr=0.000020 grad_norm=0.574422
Epoch 34/100 Iteration 192/234: loss=0.085227 lr=0.000020 grad_norm=0.591948
Epoch 34/100 Iteration 193/234: loss=0.094093 lr=0.000020 grad_norm=0.438530
Epoch 34/100 Iteration 194/234: loss=0.086565 lr=0.000020 grad_norm=0.697718
Epoch 34/100 Iteration 195/234: loss=0.081374 lr=0.000020 grad_norm=0.532129
Epoch 34/100 Iteration 196/234: loss=0.079657 lr=0.000020 grad_norm=0.606856
Epoch 34/100 Iteration 197/234: loss=0.088317 lr=0.000020 grad_norm=0.809661
Epoch 34/100 Iteration 198/234: loss=0.074583 lr=0.000020 grad_norm=0.978078
Epoch 34/100 Iteration 199/234: loss=0.079663 lr=0.000020 grad_norm=0.695047
Epoch 34/100 Iteration 200/234: loss=0.073340 lr=0.000020 grad_norm=0.407284
Epoch 34/100 Iteration 201/234: loss=0.088535 lr=0.000020 grad_norm=0.360847
Epoch 34/100 Iteration 202/234: loss=0.086691 lr=0.000020 grad_norm=0.517754
Epoch 34/100 Iteration 203/234: loss=0.074325 lr=0.000020 grad_norm=0.687546
Epoch 34/100 Iteration 204/234: loss=0.075284 lr=0.000020 grad_norm=0.640039
Epoch 34/100 Iteration 205/234: loss=0.070217 lr=0.000020 grad_norm=0.392492
Epoch 34/100 Iteration 206/234: loss=0.085322 lr=0.000020 grad_norm=0.496823
Epoch 34/100 Iteration 207/234: loss=0.084395 lr=0.000020 grad_norm=0.771891
Epoch 34/100 Iteration 208/234: loss=0.076174 lr=0.000020 grad_norm=0.500753
Epoch 34/100 Iteration 209/234: loss=0.090347 lr=0.000020 grad_norm=1.560762
Epoch 34/100 Iteration 210/234: loss=0.081531 lr=0.000020 grad_norm=1.773077
Epoch 34/100 Iteration 211/234: loss=0.077460 lr=0.000020 grad_norm=0.791002
Epoch 34/100 Iteration 212/234: loss=0.085005 lr=0.000020 grad_norm=0.772380
Epoch 34/100 Iteration 213/234: loss=0.074269 lr=0.000020 grad_norm=0.927380
Epoch 34/100 Iteration 214/234: loss=0.094362 lr=0.000020 grad_norm=0.728163
Epoch 34/100 Iteration 215/234: loss=0.080832 lr=0.000020 grad_norm=1.404168
Epoch 34/100 Iteration 216/234: loss=0.080634 lr=0.000020 grad_norm=1.551194
Epoch 34/100 Iteration 217/234: loss=0.085793 lr=0.000020 grad_norm=0.825650
Epoch 34/100 Iteration 218/234: loss=0.074283 lr=0.000020 grad_norm=0.714300
Epoch 34/100 Iteration 219/234: loss=0.076997 lr=0.000020 grad_norm=0.864560
Epoch 34/100 Iteration 220/234: loss=0.073499 lr=0.000020 grad_norm=0.841948
Epoch 34/100 Iteration 221/234: loss=0.081454 lr=0.000020 grad_norm=0.450158
Epoch 34/100 Iteration 222/234: loss=0.074215 lr=0.000020 grad_norm=0.673169
Epoch 34/100 Iteration 223/234: loss=0.078927 lr=0.000020 grad_norm=0.517854
Epoch 34/100 Iteration 224/234: loss=0.079550 lr=0.000020 grad_norm=0.906988
Epoch 34/100 Iteration 225/234: loss=0.085599 lr=0.000020 grad_norm=0.458833
Epoch 34/100 Iteration 226/234: loss=0.059654 lr=0.000020 grad_norm=0.694244
Epoch 34/100 Iteration 227/234: loss=0.073903 lr=0.000020 grad_norm=0.612830
Epoch 34/100 Iteration 228/234: loss=0.077173 lr=0.000020 grad_norm=0.459147
Epoch 34/100 Iteration 229/234: loss=0.084965 lr=0.000020 grad_norm=0.738640
Epoch 34/100 Iteration 230/234: loss=0.087425 lr=0.000020 grad_norm=0.728718
Epoch 34/100 Iteration 231/234: loss=0.079989 lr=0.000020 grad_norm=0.477598
Epoch 34/100 Iteration 232/234: loss=0.087959 lr=0.000020 grad_norm=0.879046
Epoch 34/100 Iteration 233/234: loss=0.077857 lr=0.000020 grad_norm=0.708258
Epoch 34/100 Iteration 234/234: loss=0.082872 lr=0.000020 grad_norm=0.804530
Epoch 34/100 finished. Avg Loss: 0.081651
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 35/100 Iteration 1/234: loss=0.084719 lr=0.000020 grad_norm=1.278814
Epoch 35/100 Iteration 2/234: loss=0.079503 lr=0.000020 grad_norm=0.611150
Epoch 35/100 Iteration 3/234: loss=0.084519 lr=0.000020 grad_norm=0.794954
Epoch 35/100 Iteration 4/234: loss=0.083273 lr=0.000020 grad_norm=0.822858
Epoch 35/100 Iteration 5/234: loss=0.088596 lr=0.000020 grad_norm=0.653054
Epoch 35/100 Iteration 6/234: loss=0.079868 lr=0.000020 grad_norm=1.154830
Epoch 35/100 Iteration 7/234: loss=0.084523 lr=0.000020 grad_norm=0.642437
Epoch 35/100 Iteration 8/234: loss=0.080969 lr=0.000020 grad_norm=0.890300
Epoch 35/100 Iteration 9/234: loss=0.076297 lr=0.000020 grad_norm=1.363233
Epoch 35/100 Iteration 10/234: loss=0.072711 lr=0.000020 grad_norm=0.792400
Epoch 35/100 Iteration 11/234: loss=0.078449 lr=0.000020 grad_norm=1.192387
Epoch 35/100 Iteration 12/234: loss=0.078787 lr=0.000020 grad_norm=1.598716
Epoch 35/100 Iteration 13/234: loss=0.085616 lr=0.000020 grad_norm=0.924208
Epoch 35/100 Iteration 14/234: loss=0.077094 lr=0.000020 grad_norm=0.624184
Epoch 35/100 Iteration 15/234: loss=0.065885 lr=0.000020 grad_norm=1.127212
Epoch 35/100 Iteration 16/234: loss=0.086456 lr=0.000020 grad_norm=1.043969
Epoch 35/100 Iteration 17/234: loss=0.085668 lr=0.000020 grad_norm=0.524623
Epoch 35/100 Iteration 18/234: loss=0.068996 lr=0.000020 grad_norm=0.680667
Epoch 35/100 Iteration 19/234: loss=0.083569 lr=0.000020 grad_norm=0.768572
Epoch 35/100 Iteration 20/234: loss=0.086120 lr=0.000020 grad_norm=0.553002
Epoch 35/100 Iteration 21/234: loss=0.084686 lr=0.000020 grad_norm=0.416577
Epoch 35/100 Iteration 22/234: loss=0.072874 lr=0.000020 grad_norm=0.621302
Epoch 35/100 Iteration 23/234: loss=0.061914 lr=0.000020 grad_norm=0.744369
Epoch 35/100 Iteration 24/234: loss=0.082446 lr=0.000020 grad_norm=0.636298
Epoch 35/100 Iteration 25/234: loss=0.073441 lr=0.000020 grad_norm=0.646248
Epoch 35/100 Iteration 26/234: loss=0.082812 lr=0.000020 grad_norm=0.828073
Epoch 35/100 Iteration 27/234: loss=0.078675 lr=0.000020 grad_norm=1.470592
Epoch 35/100 Iteration 28/234: loss=0.080301 lr=0.000020 grad_norm=1.642428
Epoch 35/100 Iteration 29/234: loss=0.086338 lr=0.000020 grad_norm=0.830299
Epoch 35/100 Iteration 30/234: loss=0.086125 lr=0.000020 grad_norm=1.157279
Epoch 35/100 Iteration 31/234: loss=0.083983 lr=0.000020 grad_norm=1.774694
Epoch 35/100 Iteration 32/234: loss=0.091641 lr=0.000020 grad_norm=1.287583
Epoch 35/100 Iteration 33/234: loss=0.074939 lr=0.000020 grad_norm=0.395032
Epoch 35/100 Iteration 34/234: loss=0.073287 lr=0.000020 grad_norm=1.368921
Epoch 35/100 Iteration 35/234: loss=0.081975 lr=0.000020 grad_norm=1.064779
Epoch 35/100 Iteration 36/234: loss=0.086931 lr=0.000020 grad_norm=0.810092
Epoch 35/100 Iteration 37/234: loss=0.083144 lr=0.000020 grad_norm=1.568435
Epoch 35/100 Iteration 38/234: loss=0.084433 lr=0.000020 grad_norm=0.975447
Epoch 35/100 Iteration 39/234: loss=0.090777 lr=0.000020 grad_norm=0.867653
Epoch 35/100 Iteration 40/234: loss=0.090206 lr=0.000020 grad_norm=1.615719
Epoch 35/100 Iteration 41/234: loss=0.079904 lr=0.000020 grad_norm=0.666773
Epoch 35/100 Iteration 42/234: loss=0.088583 lr=0.000020 grad_norm=1.415935
Epoch 35/100 Iteration 43/234: loss=0.089238 lr=0.000020 grad_norm=1.802296
Epoch 35/100 Iteration 44/234: loss=0.082236 lr=0.000020 grad_norm=0.860846
Epoch 35/100 Iteration 45/234: loss=0.086808 lr=0.000020 grad_norm=0.922451
Epoch 35/100 Iteration 46/234: loss=0.087033 lr=0.000020 grad_norm=1.149898
Epoch 35/100 Iteration 47/234: loss=0.082285 lr=0.000020 grad_norm=0.606425
Epoch 35/100 Iteration 48/234: loss=0.096451 lr=0.000020 grad_norm=1.285304
Epoch 35/100 Iteration 49/234: loss=0.077208 lr=0.000020 grad_norm=0.756199
Epoch 35/100 Iteration 50/234: loss=0.078969 lr=0.000020 grad_norm=0.915261
Epoch 35/100 Iteration 51/234: loss=0.077386 lr=0.000020 grad_norm=1.440950
Epoch 35/100 Iteration 52/234: loss=0.078380 lr=0.000020 grad_norm=0.987074
Epoch 35/100 Iteration 53/234: loss=0.068540 lr=0.000020 grad_norm=0.876424
Epoch 35/100 Iteration 54/234: loss=0.072440 lr=0.000020 grad_norm=1.065700
Epoch 35/100 Iteration 55/234: loss=0.080469 lr=0.000020 grad_norm=0.491690
Epoch 35/100 Iteration 56/234: loss=0.073899 lr=0.000020 grad_norm=1.063109
Epoch 35/100 Iteration 57/234: loss=0.080457 lr=0.000020 grad_norm=0.449692
Epoch 35/100 Iteration 58/234: loss=0.077915 lr=0.000020 grad_norm=1.027890
Epoch 35/100 Iteration 59/234: loss=0.069438 lr=0.000020 grad_norm=0.827374
Epoch 35/100 Iteration 60/234: loss=0.071838 lr=0.000020 grad_norm=0.427063
Epoch 35/100 Iteration 61/234: loss=0.075529 lr=0.000020 grad_norm=0.636069
Epoch 35/100 Iteration 62/234: loss=0.070021 lr=0.000020 grad_norm=0.687825
Epoch 35/100 Iteration 63/234: loss=0.074625 lr=0.000020 grad_norm=0.399662
Epoch 35/100 Iteration 64/234: loss=0.067862 lr=0.000020 grad_norm=0.737130
Epoch 35/100 Iteration 65/234: loss=0.076670 lr=0.000020 grad_norm=0.601838
Epoch 35/100 Iteration 66/234: loss=0.082313 lr=0.000020 grad_norm=0.716370
Epoch 35/100 Iteration 67/234: loss=0.080970 lr=0.000020 grad_norm=1.109166
Epoch 35/100 Iteration 68/234: loss=0.089670 lr=0.000020 grad_norm=0.838533
Epoch 35/100 Iteration 69/234: loss=0.075056 lr=0.000020 grad_norm=0.546215
Epoch 35/100 Iteration 70/234: loss=0.074223 lr=0.000020 grad_norm=0.859678
Epoch 35/100 Iteration 71/234: loss=0.075680 lr=0.000020 grad_norm=0.406717
Epoch 35/100 Iteration 72/234: loss=0.075534 lr=0.000020 grad_norm=0.964112
Epoch 35/100 Iteration 73/234: loss=0.079936 lr=0.000020 grad_norm=0.905956
Epoch 35/100 Iteration 74/234: loss=0.088001 lr=0.000020 grad_norm=0.454647
Epoch 35/100 Iteration 75/234: loss=0.086880 lr=0.000020 grad_norm=1.080770
Epoch 35/100 Iteration 76/234: loss=0.064872 lr=0.000020 grad_norm=0.753280
Epoch 35/100 Iteration 77/234: loss=0.076049 lr=0.000020 grad_norm=0.588957
Epoch 35/100 Iteration 78/234: loss=0.082281 lr=0.000020 grad_norm=0.837943
Epoch 35/100 Iteration 79/234: loss=0.078984 lr=0.000020 grad_norm=0.578502
Epoch 35/100 Iteration 80/234: loss=0.085406 lr=0.000020 grad_norm=0.635347
Epoch 35/100 Iteration 81/234: loss=0.073674 lr=0.000020 grad_norm=0.743659
Epoch 35/100 Iteration 82/234: loss=0.072878 lr=0.000020 grad_norm=0.681587
Epoch 35/100 Iteration 83/234: loss=0.071918 lr=0.000020 grad_norm=0.636731
Epoch 35/100 Iteration 84/234: loss=0.087586 lr=0.000020 grad_norm=0.528970
Epoch 35/100 Iteration 85/234: loss=0.082915 lr=0.000020 grad_norm=0.417547
Epoch 35/100 Iteration 86/234: loss=0.079758 lr=0.000020 grad_norm=0.632061
Epoch 35/100 Iteration 87/234: loss=0.089588 lr=0.000020 grad_norm=0.560727
Epoch 35/100 Iteration 88/234: loss=0.089409 lr=0.000020 grad_norm=0.416984
Epoch 35/100 Iteration 89/234: loss=0.076942 lr=0.000020 grad_norm=0.922345
Epoch 35/100 Iteration 90/234: loss=0.081153 lr=0.000020 grad_norm=0.839847
Epoch 35/100 Iteration 91/234: loss=0.085677 lr=0.000020 grad_norm=0.517457
Epoch 35/100 Iteration 92/234: loss=0.087979 lr=0.000020 grad_norm=0.848210
Epoch 35/100 Iteration 93/234: loss=0.080928 lr=0.000020 grad_norm=0.586915
Epoch 35/100 Iteration 94/234: loss=0.090402 lr=0.000020 grad_norm=0.998468
Epoch 35/100 Iteration 95/234: loss=0.085953 lr=0.000020 grad_norm=1.321843
Epoch 35/100 Iteration 96/234: loss=0.083059 lr=0.000020 grad_norm=0.828440
Epoch 35/100 Iteration 97/234: loss=0.079199 lr=0.000020 grad_norm=0.599574
Epoch 35/100 Iteration 98/234: loss=0.087268 lr=0.000020 grad_norm=0.814653
Epoch 35/100 Iteration 99/234: loss=0.075890 lr=0.000020 grad_norm=0.691503
Epoch 35/100 Iteration 100/234: loss=0.067144 lr=0.000020 grad_norm=0.540109
Epoch 35/100 Iteration 101/234: loss=0.080900 lr=0.000020 grad_norm=0.421731
Epoch 35/100 Iteration 102/234: loss=0.083659 lr=0.000020 grad_norm=0.402875
Epoch 35/100 Iteration 103/234: loss=0.092279 lr=0.000020 grad_norm=0.407459
Epoch 35/100 Iteration 104/234: loss=0.084274 lr=0.000020 grad_norm=0.818322
Epoch 35/100 Iteration 105/234: loss=0.092031 lr=0.000020 grad_norm=1.563694
Epoch 35/100 Iteration 106/234: loss=0.078651 lr=0.000020 grad_norm=1.777997
Epoch 35/100 Iteration 107/234: loss=0.075612 lr=0.000020 grad_norm=0.789035
Epoch 35/100 Iteration 108/234: loss=0.075926 lr=0.000020 grad_norm=0.861241
Epoch 35/100 Iteration 109/234: loss=0.079124 lr=0.000020 grad_norm=1.314124
Epoch 35/100 Iteration 110/234: loss=0.078242 lr=0.000020 grad_norm=1.380787
Epoch 35/100 Iteration 111/234: loss=0.082607 lr=0.000020 grad_norm=0.749278
Epoch 35/100 Iteration 112/234: loss=0.095472 lr=0.000020 grad_norm=0.922389
Epoch 35/100 Iteration 113/234: loss=0.075593 lr=0.000020 grad_norm=0.932773
Epoch 35/100 Iteration 114/234: loss=0.077078 lr=0.000020 grad_norm=0.644092
Epoch 35/100 Iteration 115/234: loss=0.083008 lr=0.000020 grad_norm=0.774690
Epoch 35/100 Iteration 116/234: loss=0.077855 lr=0.000020 grad_norm=0.661429
Epoch 35/100 Iteration 117/234: loss=0.085680 lr=0.000020 grad_norm=0.938980
Epoch 35/100 Iteration 118/234: loss=0.071013 lr=0.000020 grad_norm=0.900642
Epoch 35/100 Iteration 119/234: loss=0.074092 lr=0.000020 grad_norm=0.761911
Epoch 35/100 Iteration 120/234: loss=0.077355 lr=0.000020 grad_norm=0.419976
Epoch 35/100 Iteration 121/234: loss=0.083260 lr=0.000020 grad_norm=0.859101
Epoch 35/100 Iteration 122/234: loss=0.079984 lr=0.000020 grad_norm=0.755072
Epoch 35/100 Iteration 123/234: loss=0.080743 lr=0.000020 grad_norm=0.530758
Epoch 35/100 Iteration 124/234: loss=0.081910 lr=0.000020 grad_norm=0.453973
Epoch 35/100 Iteration 125/234: loss=0.084628 lr=0.000020 grad_norm=0.764304
Epoch 35/100 Iteration 126/234: loss=0.084655 lr=0.000020 grad_norm=0.682824
Epoch 35/100 Iteration 127/234: loss=0.086845 lr=0.000020 grad_norm=0.587302
Epoch 35/100 Iteration 128/234: loss=0.071002 lr=0.000020 grad_norm=1.044747
Epoch 35/100 Iteration 129/234: loss=0.081086 lr=0.000020 grad_norm=1.142127
Epoch 35/100 Iteration 130/234: loss=0.079807 lr=0.000020 grad_norm=0.814082
Epoch 35/100 Iteration 131/234: loss=0.084752 lr=0.000020 grad_norm=0.477711
Epoch 35/100 Iteration 132/234: loss=0.075874 lr=0.000020 grad_norm=1.032457
Epoch 35/100 Iteration 133/234: loss=0.074301 lr=0.000020 grad_norm=0.767158
Epoch 35/100 Iteration 134/234: loss=0.074159 lr=0.000020 grad_norm=0.386322
Epoch 35/100 Iteration 135/234: loss=0.068894 lr=0.000020 grad_norm=0.832613
Epoch 35/100 Iteration 136/234: loss=0.084329 lr=0.000020 grad_norm=0.563391
Epoch 35/100 Iteration 137/234: loss=0.074925 lr=0.000020 grad_norm=0.735608
Epoch 35/100 Iteration 138/234: loss=0.076183 lr=0.000020 grad_norm=1.072227
Epoch 35/100 Iteration 139/234: loss=0.080589 lr=0.000020 grad_norm=0.719293
Epoch 35/100 Iteration 140/234: loss=0.083576 lr=0.000020 grad_norm=0.403036
Epoch 35/100 Iteration 141/234: loss=0.079938 lr=0.000020 grad_norm=0.782800
Epoch 35/100 Iteration 142/234: loss=0.081840 lr=0.000020 grad_norm=0.660822
Epoch 35/100 Iteration 143/234: loss=0.074690 lr=0.000020 grad_norm=0.583686
Epoch 35/100 Iteration 144/234: loss=0.081631 lr=0.000020 grad_norm=0.478529
Epoch 35/100 Iteration 145/234: loss=0.076605 lr=0.000020 grad_norm=0.442339
Epoch 35/100 Iteration 146/234: loss=0.071426 lr=0.000020 grad_norm=0.423584
Epoch 35/100 Iteration 147/234: loss=0.088400 lr=0.000020 grad_norm=0.453432
Epoch 35/100 Iteration 148/234: loss=0.079831 lr=0.000020 grad_norm=0.405941
Epoch 35/100 Iteration 149/234: loss=0.077296 lr=0.000020 grad_norm=0.420743
Epoch 35/100 Iteration 150/234: loss=0.093174 lr=0.000020 grad_norm=0.582521
Epoch 35/100 Iteration 151/234: loss=0.077458 lr=0.000020 grad_norm=0.531230
Epoch 35/100 Iteration 152/234: loss=0.072569 lr=0.000020 grad_norm=0.546637
Epoch 35/100 Iteration 153/234: loss=0.085626 lr=0.000020 grad_norm=0.389997
Epoch 35/100 Iteration 154/234: loss=0.086821 lr=0.000020 grad_norm=0.887471
Epoch 35/100 Iteration 155/234: loss=0.071253 lr=0.000020 grad_norm=1.134861
Epoch 35/100 Iteration 156/234: loss=0.084807 lr=0.000020 grad_norm=1.028192
Epoch 35/100 Iteration 157/234: loss=0.077584 lr=0.000020 grad_norm=0.395972
Epoch 35/100 Iteration 158/234: loss=0.072846 lr=0.000020 grad_norm=1.110260
Epoch 35/100 Iteration 159/234: loss=0.082594 lr=0.000020 grad_norm=1.857413
Epoch 35/100 Iteration 160/234: loss=0.086227 lr=0.000020 grad_norm=1.914172
Epoch 35/100 Iteration 161/234: loss=0.073491 lr=0.000020 grad_norm=0.816507
Epoch 35/100 Iteration 162/234: loss=0.077119 lr=0.000020 grad_norm=0.886237
Epoch 35/100 Iteration 163/234: loss=0.075734 lr=0.000020 grad_norm=0.850027
Epoch 35/100 Iteration 164/234: loss=0.091110 lr=0.000020 grad_norm=0.950537
Epoch 35/100 Iteration 165/234: loss=0.083480 lr=0.000020 grad_norm=1.053783
Epoch 35/100 Iteration 166/234: loss=0.075354 lr=0.000020 grad_norm=0.941148
Epoch 35/100 Iteration 167/234: loss=0.075316 lr=0.000020 grad_norm=0.960232
Epoch 35/100 Iteration 168/234: loss=0.081181 lr=0.000020 grad_norm=0.816290
Epoch 35/100 Iteration 169/234: loss=0.070489 lr=0.000020 grad_norm=0.295195
Epoch 35/100 Iteration 170/234: loss=0.085931 lr=0.000020 grad_norm=0.901185
Epoch 35/100 Iteration 171/234: loss=0.069917 lr=0.000020 grad_norm=0.917935
Epoch 35/100 Iteration 172/234: loss=0.074046 lr=0.000020 grad_norm=0.655717
Epoch 35/100 Iteration 173/234: loss=0.081429 lr=0.000020 grad_norm=0.548856
Epoch 35/100 Iteration 174/234: loss=0.074542 lr=0.000020 grad_norm=0.870662
Epoch 35/100 Iteration 175/234: loss=0.079328 lr=0.000020 grad_norm=0.830318
Epoch 35/100 Iteration 176/234: loss=0.084512 lr=0.000020 grad_norm=0.608204
Epoch 35/100 Iteration 177/234: loss=0.068867 lr=0.000020 grad_norm=0.504976
Epoch 35/100 Iteration 178/234: loss=0.073703 lr=0.000020 grad_norm=0.630506
Epoch 35/100 Iteration 179/234: loss=0.074110 lr=0.000020 grad_norm=0.610741
Epoch 35/100 Iteration 180/234: loss=0.079050 lr=0.000020 grad_norm=0.831542
Epoch 35/100 Iteration 181/234: loss=0.095006 lr=0.000020 grad_norm=0.878851
Epoch 35/100 Iteration 182/234: loss=0.090415 lr=0.000020 grad_norm=1.077584
Epoch 35/100 Iteration 183/234: loss=0.074375 lr=0.000020 grad_norm=1.120616
Epoch 35/100 Iteration 184/234: loss=0.072700 lr=0.000020 grad_norm=0.400972
Epoch 35/100 Iteration 185/234: loss=0.075418 lr=0.000020 grad_norm=1.196095
Epoch 35/100 Iteration 186/234: loss=0.081086 lr=0.000020 grad_norm=0.848893
Epoch 35/100 Iteration 187/234: loss=0.085165 lr=0.000020 grad_norm=0.856837
Epoch 35/100 Iteration 188/234: loss=0.071835 lr=0.000020 grad_norm=1.522767
Epoch 35/100 Iteration 189/234: loss=0.068852 lr=0.000020 grad_norm=0.758045
Epoch 35/100 Iteration 190/234: loss=0.080064 lr=0.000020 grad_norm=1.360351
Epoch 35/100 Iteration 191/234: loss=0.075080 lr=0.000020 grad_norm=1.877947
Epoch 35/100 Iteration 192/234: loss=0.089486 lr=0.000020 grad_norm=1.325609
Epoch 35/100 Iteration 193/234: loss=0.069332 lr=0.000020 grad_norm=0.525147
Epoch 35/100 Iteration 194/234: loss=0.082314 lr=0.000020 grad_norm=1.022350
Epoch 35/100 Iteration 195/234: loss=0.085122 lr=0.000020 grad_norm=1.184883
Epoch 35/100 Iteration 196/234: loss=0.079439 lr=0.000020 grad_norm=0.701996
Epoch 35/100 Iteration 197/234: loss=0.076442 lr=0.000020 grad_norm=0.814450
Epoch 35/100 Iteration 198/234: loss=0.091526 lr=0.000020 grad_norm=1.055630
Epoch 35/100 Iteration 199/234: loss=0.088563 lr=0.000020 grad_norm=0.815203
Epoch 35/100 Iteration 200/234: loss=0.074965 lr=0.000020 grad_norm=0.399968
Epoch 35/100 Iteration 201/234: loss=0.081286 lr=0.000020 grad_norm=0.470649
Epoch 35/100 Iteration 202/234: loss=0.080558 lr=0.000020 grad_norm=0.911143
Epoch 35/100 Iteration 203/234: loss=0.076410 lr=0.000020 grad_norm=1.453221
Epoch 35/100 Iteration 204/234: loss=0.076561 lr=0.000020 grad_norm=1.670575
Epoch 35/100 Iteration 205/234: loss=0.061878 lr=0.000020 grad_norm=1.218352
Epoch 35/100 Iteration 206/234: loss=0.070970 lr=0.000020 grad_norm=0.455894
Epoch 35/100 Iteration 207/234: loss=0.077966 lr=0.000020 grad_norm=0.822769
Epoch 35/100 Iteration 208/234: loss=0.077002 lr=0.000020 grad_norm=0.575584
Epoch 35/100 Iteration 209/234: loss=0.079402 lr=0.000020 grad_norm=0.461124
Epoch 35/100 Iteration 210/234: loss=0.084438 lr=0.000020 grad_norm=0.733122
Epoch 35/100 Iteration 211/234: loss=0.075101 lr=0.000020 grad_norm=0.528385
Epoch 35/100 Iteration 212/234: loss=0.088742 lr=0.000020 grad_norm=0.752948
Epoch 35/100 Iteration 213/234: loss=0.069907 lr=0.000020 grad_norm=1.222422
Epoch 35/100 Iteration 214/234: loss=0.076380 lr=0.000020 grad_norm=0.936722
Epoch 35/100 Iteration 215/234: loss=0.072652 lr=0.000020 grad_norm=0.698676
Epoch 35/100 Iteration 216/234: loss=0.092501 lr=0.000020 grad_norm=0.899277
Epoch 35/100 Iteration 217/234: loss=0.072694 lr=0.000020 grad_norm=0.890689
Epoch 35/100 Iteration 218/234: loss=0.069923 lr=0.000020 grad_norm=0.858273
Epoch 35/100 Iteration 219/234: loss=0.084544 lr=0.000020 grad_norm=1.608544
Epoch 35/100 Iteration 220/234: loss=0.075534 lr=0.000020 grad_norm=1.216349
Epoch 35/100 Iteration 221/234: loss=0.071373 lr=0.000020 grad_norm=0.456627
Epoch 35/100 Iteration 222/234: loss=0.078357 lr=0.000020 grad_norm=1.471038
Epoch 35/100 Iteration 223/234: loss=0.082859 lr=0.000020 grad_norm=1.906526
Epoch 35/100 Iteration 224/234: loss=0.096945 lr=0.000020 grad_norm=1.397133
Epoch 35/100 Iteration 225/234: loss=0.080768 lr=0.000020 grad_norm=0.485721
Epoch 35/100 Iteration 226/234: loss=0.080241 lr=0.000020 grad_norm=0.808194
Epoch 35/100 Iteration 227/234: loss=0.081629 lr=0.000020 grad_norm=0.962518
Epoch 35/100 Iteration 228/234: loss=0.076532 lr=0.000020 grad_norm=0.350222
Epoch 35/100 Iteration 229/234: loss=0.072509 lr=0.000020 grad_norm=0.727174
Epoch 35/100 Iteration 230/234: loss=0.082519 lr=0.000020 grad_norm=0.872444
Epoch 35/100 Iteration 231/234: loss=0.071404 lr=0.000020 grad_norm=0.615965
Epoch 35/100 Iteration 232/234: loss=0.078861 lr=0.000020 grad_norm=0.397181
Epoch 35/100 Iteration 233/234: loss=0.076107 lr=0.000020 grad_norm=0.926784
Epoch 35/100 Iteration 234/234: loss=0.077265 lr=0.000020 grad_norm=1.245878
Epoch 35/100 finished. Avg Loss: 0.079719
Epoch 36/100 Iteration 1/234: loss=0.075573 lr=0.000020 grad_norm=1.118203
Epoch 36/100 Iteration 2/234: loss=0.078813 lr=0.000020 grad_norm=0.477143
Epoch 36/100 Iteration 3/234: loss=0.075237 lr=0.000020 grad_norm=0.436162
Epoch 36/100 Iteration 4/234: loss=0.074791 lr=0.000020 grad_norm=0.399556
Epoch 36/100 Iteration 5/234: loss=0.097573 lr=0.000020 grad_norm=0.512577
Epoch 36/100 Iteration 6/234: loss=0.079500 lr=0.000020 grad_norm=0.456800
Epoch 36/100 Iteration 7/234: loss=0.076446 lr=0.000020 grad_norm=0.430949
Epoch 36/100 Iteration 8/234: loss=0.079942 lr=0.000020 grad_norm=0.716648
Epoch 36/100 Iteration 9/234: loss=0.077822 lr=0.000020 grad_norm=0.768456
Epoch 36/100 Iteration 10/234: loss=0.075145 lr=0.000020 grad_norm=0.481689
Epoch 36/100 Iteration 11/234: loss=0.073354 lr=0.000020 grad_norm=0.445015
Epoch 36/100 Iteration 12/234: loss=0.080685 lr=0.000020 grad_norm=0.642308
Epoch 36/100 Iteration 13/234: loss=0.082661 lr=0.000020 grad_norm=0.764053
Epoch 36/100 Iteration 14/234: loss=0.088192 lr=0.000020 grad_norm=1.149891
Epoch 36/100 Iteration 15/234: loss=0.073499 lr=0.000020 grad_norm=1.567194
Epoch 36/100 Iteration 16/234: loss=0.076572 lr=0.000020 grad_norm=1.285491
Epoch 36/100 Iteration 17/234: loss=0.072558 lr=0.000020 grad_norm=0.465952
Epoch 36/100 Iteration 18/234: loss=0.086144 lr=0.000020 grad_norm=1.043156
Epoch 36/100 Iteration 19/234: loss=0.088237 lr=0.000020 grad_norm=1.877714
Epoch 36/100 Iteration 20/234: loss=0.064644 lr=0.000020 grad_norm=1.753713
Epoch 36/100 Iteration 21/234: loss=0.079712 lr=0.000020 grad_norm=0.464984
Epoch 36/100 Iteration 22/234: loss=0.083746 lr=0.000020 grad_norm=1.167858
Epoch 36/100 Iteration 23/234: loss=0.090553 lr=0.000020 grad_norm=0.847554
Epoch 36/100 Iteration 24/234: loss=0.079333 lr=0.000020 grad_norm=0.607933
Epoch 36/100 Iteration 25/234: loss=0.078419 lr=0.000020 grad_norm=0.919161
Epoch 36/100 Iteration 26/234: loss=0.090115 lr=0.000020 grad_norm=0.797195
Epoch 36/100 Iteration 27/234: loss=0.081085 lr=0.000020 grad_norm=0.972365
Epoch 36/100 Iteration 28/234: loss=0.075431 lr=0.000020 grad_norm=0.808973
Epoch 36/100 Iteration 29/234: loss=0.074037 lr=0.000020 grad_norm=0.740642
Epoch 36/100 Iteration 30/234: loss=0.081238 lr=0.000020 grad_norm=0.883870
Epoch 36/100 Iteration 31/234: loss=0.066767 lr=0.000020 grad_norm=0.674044
Epoch 36/100 Iteration 32/234: loss=0.074174 lr=0.000020 grad_norm=0.480589
Epoch 36/100 Iteration 33/234: loss=0.072067 lr=0.000020 grad_norm=0.587904
Epoch 36/100 Iteration 34/234: loss=0.078640 lr=0.000020 grad_norm=0.596940
Epoch 36/100 Iteration 35/234: loss=0.082489 lr=0.000020 grad_norm=0.449770
Epoch 36/100 Iteration 36/234: loss=0.072624 lr=0.000020 grad_norm=0.469995
Epoch 36/100 Iteration 37/234: loss=0.077145 lr=0.000020 grad_norm=0.676626
Epoch 36/100 Iteration 38/234: loss=0.087001 lr=0.000020 grad_norm=0.753477
Epoch 36/100 Iteration 39/234: loss=0.082012 lr=0.000020 grad_norm=0.479675
Epoch 36/100 Iteration 40/234: loss=0.078357 lr=0.000020 grad_norm=0.673902
Epoch 36/100 Iteration 41/234: loss=0.083196 lr=0.000020 grad_norm=0.968567
Epoch 36/100 Iteration 42/234: loss=0.077405 lr=0.000020 grad_norm=0.669677
Epoch 36/100 Iteration 43/234: loss=0.080499 lr=0.000020 grad_norm=0.966514
Epoch 36/100 Iteration 44/234: loss=0.076135 lr=0.000020 grad_norm=1.345985
Epoch 36/100 Iteration 45/234: loss=0.071027 lr=0.000020 grad_norm=0.737355
Epoch 36/100 Iteration 46/234: loss=0.083560 lr=0.000020 grad_norm=1.107524
Epoch 36/100 Iteration 47/234: loss=0.079052 lr=0.000020 grad_norm=1.832931
Epoch 36/100 Iteration 48/234: loss=0.072146 lr=0.000020 grad_norm=0.869900
Epoch 36/100 Iteration 49/234: loss=0.070673 lr=0.000020 grad_norm=0.966685
Epoch 36/100 Iteration 50/234: loss=0.086971 lr=0.000020 grad_norm=1.430273
Epoch 36/100 Iteration 51/234: loss=0.072783 lr=0.000020 grad_norm=0.644038
Epoch 36/100 Iteration 52/234: loss=0.080112 lr=0.000020 grad_norm=1.066132
Epoch 36/100 Iteration 53/234: loss=0.087221 lr=0.000020 grad_norm=1.253441
Epoch 36/100 Iteration 54/234: loss=0.080097 lr=0.000020 grad_norm=0.729719
Epoch 36/100 Iteration 55/234: loss=0.065069 lr=0.000020 grad_norm=0.558495
Epoch 36/100 Iteration 56/234: loss=0.070856 lr=0.000020 grad_norm=0.484442
Epoch 36/100 Iteration 57/234: loss=0.083708 lr=0.000020 grad_norm=0.806000
Epoch 36/100 Iteration 58/234: loss=0.084132 lr=0.000020 grad_norm=0.828026
Epoch 36/100 Iteration 59/234: loss=0.075739 lr=0.000020 grad_norm=0.339151
Epoch 36/100 Iteration 60/234: loss=0.066883 lr=0.000020 grad_norm=0.752180
Epoch 36/100 Iteration 61/234: loss=0.068283 lr=0.000020 grad_norm=0.607967
Epoch 36/100 Iteration 62/234: loss=0.064979 lr=0.000020 grad_norm=0.324110
Epoch 36/100 Iteration 63/234: loss=0.074657 lr=0.000020 grad_norm=0.561166
Epoch 36/100 Iteration 64/234: loss=0.088311 lr=0.000020 grad_norm=0.656146
Epoch 36/100 Iteration 65/234: loss=0.078470 lr=0.000020 grad_norm=0.687981
Epoch 36/100 Iteration 66/234: loss=0.083816 lr=0.000020 grad_norm=0.883125
Epoch 36/100 Iteration 67/234: loss=0.080732 lr=0.000020 grad_norm=0.734152
Epoch 36/100 Iteration 68/234: loss=0.074534 lr=0.000020 grad_norm=0.619858
Epoch 36/100 Iteration 69/234: loss=0.081252 lr=0.000020 grad_norm=1.489523
Epoch 36/100 Iteration 70/234: loss=0.078420 lr=0.000020 grad_norm=1.782635
Epoch 36/100 Iteration 71/234: loss=0.073784 lr=0.000020 grad_norm=0.737136
Epoch 36/100 Iteration 72/234: loss=0.088912 lr=0.000020 grad_norm=0.977161
Epoch 36/100 Iteration 73/234: loss=0.074699 lr=0.000020 grad_norm=1.550599
Epoch 36/100 Iteration 74/234: loss=0.083291 lr=0.000020 grad_norm=1.138556
Epoch 36/100 Iteration 75/234: loss=0.081086 lr=0.000020 grad_norm=0.590333
Epoch 36/100 Iteration 76/234: loss=0.077825 lr=0.000020 grad_norm=1.610737
Epoch 36/100 Iteration 77/234: loss=0.086953 lr=0.000020 grad_norm=1.041848
Epoch 36/100 Iteration 78/234: loss=0.071951 lr=0.000020 grad_norm=0.910001
Epoch 36/100 Iteration 79/234: loss=0.070432 lr=0.000020 grad_norm=1.458632
Epoch 36/100 Iteration 80/234: loss=0.071480 lr=0.000020 grad_norm=0.614066
Epoch 36/100 Iteration 81/234: loss=0.070534 lr=0.000020 grad_norm=1.391264
Epoch 36/100 Iteration 82/234: loss=0.086419 lr=0.000020 grad_norm=1.132907
Epoch 36/100 Iteration 83/234: loss=0.085222 lr=0.000020 grad_norm=0.684297
Epoch 36/100 Iteration 84/234: loss=0.075232 lr=0.000020 grad_norm=0.846015
Epoch 36/100 Iteration 85/234: loss=0.078752 lr=0.000020 grad_norm=0.597660
Epoch 36/100 Iteration 86/234: loss=0.077232 lr=0.000020 grad_norm=0.890231
Epoch 36/100 Iteration 87/234: loss=0.069681 lr=0.000020 grad_norm=0.712646
Epoch 36/100 Iteration 88/234: loss=0.073576 lr=0.000020 grad_norm=0.565361
Epoch 36/100 Iteration 89/234: loss=0.080219 lr=0.000020 grad_norm=0.835603
Epoch 36/100 Iteration 90/234: loss=0.088050 lr=0.000020 grad_norm=0.856560
Epoch 36/100 Iteration 91/234: loss=0.081555 lr=0.000020 grad_norm=0.484708
Epoch 36/100 Iteration 92/234: loss=0.070201 lr=0.000020 grad_norm=0.523568
Epoch 36/100 Iteration 93/234: loss=0.086333 lr=0.000020 grad_norm=0.645555
Epoch 36/100 Iteration 94/234: loss=0.078620 lr=0.000020 grad_norm=0.560527
Epoch 36/100 Iteration 95/234: loss=0.074234 lr=0.000020 grad_norm=0.955291
Epoch 36/100 Iteration 96/234: loss=0.071951 lr=0.000020 grad_norm=0.796387
Epoch 36/100 Iteration 97/234: loss=0.077330 lr=0.000020 grad_norm=0.367198
Epoch 36/100 Iteration 98/234: loss=0.078949 lr=0.000020 grad_norm=0.714490
Epoch 36/100 Iteration 99/234: loss=0.072195 lr=0.000020 grad_norm=0.646167
Epoch 36/100 Iteration 100/234: loss=0.072808 lr=0.000020 grad_norm=0.519245
Epoch 36/100 Iteration 101/234: loss=0.076036 lr=0.000020 grad_norm=0.633968
Epoch 36/100 Iteration 102/234: loss=0.078789 lr=0.000020 grad_norm=0.654472
Epoch 36/100 Iteration 103/234: loss=0.075480 lr=0.000020 grad_norm=0.638629
Epoch 36/100 Iteration 104/234: loss=0.075608 lr=0.000020 grad_norm=0.633657
Epoch 36/100 Iteration 105/234: loss=0.069482 lr=0.000020 grad_norm=0.501062
Epoch 36/100 Iteration 106/234: loss=0.076967 lr=0.000020 grad_norm=0.632363
Epoch 36/100 Iteration 107/234: loss=0.073407 lr=0.000020 grad_norm=0.583099
Epoch 36/100 Iteration 108/234: loss=0.074986 lr=0.000020 grad_norm=0.370032
Epoch 36/100 Iteration 109/234: loss=0.068354 lr=0.000020 grad_norm=0.785354
Epoch 36/100 Iteration 110/234: loss=0.056328 lr=0.000020 grad_norm=0.654852
Epoch 36/100 Iteration 111/234: loss=0.082979 lr=0.000020 grad_norm=0.428296
Epoch 36/100 Iteration 112/234: loss=0.086394 lr=0.000020 grad_norm=0.800235
Epoch 36/100 Iteration 113/234: loss=0.071304 lr=0.000020 grad_norm=0.580211
Epoch 36/100 Iteration 114/234: loss=0.077297 lr=0.000020 grad_norm=0.955986
Epoch 36/100 Iteration 115/234: loss=0.075066 lr=0.000020 grad_norm=1.275242
Epoch 36/100 Iteration 116/234: loss=0.064070 lr=0.000020 grad_norm=0.448971
Epoch 36/100 Iteration 117/234: loss=0.075971 lr=0.000020 grad_norm=0.971612
Epoch 36/100 Iteration 118/234: loss=0.074823 lr=0.000020 grad_norm=1.396452
Epoch 36/100 Iteration 119/234: loss=0.090064 lr=0.000020 grad_norm=1.109281
Epoch 36/100 Iteration 120/234: loss=0.076538 lr=0.000020 grad_norm=0.559436
Epoch 36/100 Iteration 121/234: loss=0.072607 lr=0.000020 grad_norm=0.995145
Epoch 36/100 Iteration 122/234: loss=0.075580 lr=0.000020 grad_norm=0.833291
Epoch 36/100 Iteration 123/234: loss=0.082394 lr=0.000020 grad_norm=0.648089
Epoch 36/100 Iteration 124/234: loss=0.076368 lr=0.000020 grad_norm=0.892334
Epoch 36/100 Iteration 125/234: loss=0.080174 lr=0.000020 grad_norm=0.620333
Epoch 36/100 Iteration 126/234: loss=0.079355 lr=0.000020 grad_norm=0.922480
Epoch 36/100 Iteration 127/234: loss=0.089122 lr=0.000020 grad_norm=1.067569
Epoch 36/100 Iteration 128/234: loss=0.079346 lr=0.000020 grad_norm=1.311464
Epoch 36/100 Iteration 129/234: loss=0.079608 lr=0.000020 grad_norm=1.260225
Epoch 36/100 Iteration 130/234: loss=0.084297 lr=0.000020 grad_norm=0.501907
Epoch 36/100 Iteration 131/234: loss=0.079464 lr=0.000020 grad_norm=1.453007
Epoch 36/100 Iteration 132/234: loss=0.072251 lr=0.000020 grad_norm=2.215370
Epoch 36/100 Iteration 133/234: loss=0.079490 lr=0.000020 grad_norm=0.913201
Epoch 36/100 Iteration 134/234: loss=0.073188 lr=0.000020 grad_norm=1.131493
Epoch 36/100 Iteration 135/234: loss=0.076399 lr=0.000020 grad_norm=1.613514
Epoch 36/100 Iteration 136/234: loss=0.064540 lr=0.000020 grad_norm=0.914077
Epoch 36/100 Iteration 137/234: loss=0.076129 lr=0.000020 grad_norm=0.585636
Epoch 36/100 Iteration 138/234: loss=0.075497 lr=0.000020 grad_norm=0.877474
Epoch 36/100 Iteration 139/234: loss=0.083542 lr=0.000020 grad_norm=0.350790
Epoch 36/100 Iteration 140/234: loss=0.080426 lr=0.000020 grad_norm=0.961120
Epoch 36/100 Iteration 141/234: loss=0.084942 lr=0.000020 grad_norm=0.926858
Epoch 36/100 Iteration 142/234: loss=0.069524 lr=0.000020 grad_norm=0.577902
Epoch 36/100 Iteration 143/234: loss=0.072815 lr=0.000020 grad_norm=0.460142
Epoch 36/100 Iteration 144/234: loss=0.079805 lr=0.000020 grad_norm=0.540954
Epoch 36/100 Iteration 145/234: loss=0.078777 lr=0.000020 grad_norm=0.748066
Epoch 36/100 Iteration 146/234: loss=0.075154 lr=0.000020 grad_norm=0.668277
Epoch 36/100 Iteration 147/234: loss=0.080077 lr=0.000020 grad_norm=0.319255
Epoch 36/100 Iteration 148/234: loss=0.088313 lr=0.000020 grad_norm=0.521305
Epoch 36/100 Iteration 149/234: loss=0.067782 lr=0.000020 grad_norm=0.835062
Epoch 36/100 Iteration 150/234: loss=0.071355 lr=0.000020 grad_norm=1.074597
Epoch 36/100 Iteration 151/234: loss=0.083146 lr=0.000020 grad_norm=0.672674
Epoch 36/100 Iteration 152/234: loss=0.073875 lr=0.000020 grad_norm=0.460705
Epoch 36/100 Iteration 153/234: loss=0.082502 lr=0.000020 grad_norm=0.963147
Epoch 36/100 Iteration 154/234: loss=0.078563 lr=0.000020 grad_norm=0.859701
Epoch 36/100 Iteration 155/234: loss=0.081112 lr=0.000020 grad_norm=0.519078
Epoch 36/100 Iteration 156/234: loss=0.081447 lr=0.000020 grad_norm=1.345815
Epoch 36/100 Iteration 157/234: loss=0.070086 lr=0.000020 grad_norm=1.552155
Epoch 36/100 Iteration 158/234: loss=0.065490 lr=0.000020 grad_norm=0.946980
Epoch 36/100 Iteration 159/234: loss=0.070245 lr=0.000020 grad_norm=0.769579
Epoch 36/100 Iteration 160/234: loss=0.069133 lr=0.000020 grad_norm=1.027512
Epoch 36/100 Iteration 161/234: loss=0.075803 lr=0.000020 grad_norm=0.564806
Epoch 36/100 Iteration 162/234: loss=0.079599 lr=0.000020 grad_norm=1.018953
Epoch 36/100 Iteration 163/234: loss=0.072257 lr=0.000020 grad_norm=1.787862
Epoch 36/100 Iteration 164/234: loss=0.076874 lr=0.000020 grad_norm=1.254896
Epoch 36/100 Iteration 165/234: loss=0.075850 lr=0.000020 grad_norm=0.886092
Epoch 36/100 Iteration 166/234: loss=0.077736 lr=0.000020 grad_norm=1.662838
Epoch 36/100 Iteration 167/234: loss=0.077223 lr=0.000020 grad_norm=1.325914
Epoch 36/100 Iteration 168/234: loss=0.089649 lr=0.000020 grad_norm=0.991755
Epoch 36/100 Iteration 169/234: loss=0.079554 lr=0.000020 grad_norm=1.172595
Epoch 36/100 Iteration 170/234: loss=0.076307 lr=0.000020 grad_norm=0.498300
Epoch 36/100 Iteration 171/234: loss=0.077023 lr=0.000020 grad_norm=1.288733
Epoch 36/100 Iteration 172/234: loss=0.075475 lr=0.000020 grad_norm=0.646360
Epoch 36/100 Iteration 173/234: loss=0.064689 lr=0.000020 grad_norm=1.543272
Epoch 36/100 Iteration 174/234: loss=0.079200 lr=0.000020 grad_norm=1.130825
Epoch 36/100 Iteration 175/234: loss=0.083314 lr=0.000020 grad_norm=0.708220
Epoch 36/100 Iteration 176/234: loss=0.078400 lr=0.000020 grad_norm=1.201945
Epoch 36/100 Iteration 177/234: loss=0.080134 lr=0.000020 grad_norm=0.509702
Epoch 36/100 Iteration 178/234: loss=0.085611 lr=0.000020 grad_norm=1.168916
Epoch 36/100 Iteration 179/234: loss=0.072939 lr=0.000020 grad_norm=0.862675
Epoch 36/100 Iteration 180/234: loss=0.071349 lr=0.000020 grad_norm=0.567308
Epoch 36/100 Iteration 181/234: loss=0.078989 lr=0.000020 grad_norm=1.164940
Epoch 36/100 Iteration 182/234: loss=0.086861 lr=0.000020 grad_norm=1.178823
Epoch 36/100 Iteration 183/234: loss=0.073274 lr=0.000020 grad_norm=0.919581
Epoch 36/100 Iteration 184/234: loss=0.078777 lr=0.000020 grad_norm=0.632197
Epoch 36/100 Iteration 185/234: loss=0.081709 lr=0.000020 grad_norm=0.899664
Epoch 36/100 Iteration 186/234: loss=0.090100 lr=0.000020 grad_norm=1.425985
Epoch 36/100 Iteration 187/234: loss=0.076887 lr=0.000020 grad_norm=1.298351
Epoch 36/100 Iteration 188/234: loss=0.085118 lr=0.000020 grad_norm=0.672725
Epoch 36/100 Iteration 189/234: loss=0.076077 lr=0.000020 grad_norm=0.926574
Epoch 36/100 Iteration 190/234: loss=0.078292 lr=0.000020 grad_norm=1.100987
Epoch 36/100 Iteration 191/234: loss=0.076799 lr=0.000020 grad_norm=0.587983
Epoch 36/100 Iteration 192/234: loss=0.082119 lr=0.000020 grad_norm=0.382478
Epoch 36/100 Iteration 193/234: loss=0.076145 lr=0.000020 grad_norm=0.454860
Epoch 36/100 Iteration 194/234: loss=0.068554 lr=0.000020 grad_norm=0.611228
Epoch 36/100 Iteration 195/234: loss=0.075357 lr=0.000020 grad_norm=0.458631
Epoch 36/100 Iteration 196/234: loss=0.063945 lr=0.000020 grad_norm=0.496300
Epoch 36/100 Iteration 197/234: loss=0.079498 lr=0.000020 grad_norm=0.550390
Epoch 36/100 Iteration 198/234: loss=0.081386 lr=0.000020 grad_norm=0.547983
Epoch 36/100 Iteration 199/234: loss=0.073416 lr=0.000020 grad_norm=0.839401
Epoch 36/100 Iteration 200/234: loss=0.077022 lr=0.000020 grad_norm=0.822197
Epoch 36/100 Iteration 201/234: loss=0.075225 lr=0.000020 grad_norm=0.371204
Epoch 36/100 Iteration 202/234: loss=0.063807 lr=0.000020 grad_norm=0.880066
Epoch 36/100 Iteration 203/234: loss=0.066919 lr=0.000020 grad_norm=1.167572
Epoch 36/100 Iteration 204/234: loss=0.064752 lr=0.000020 grad_norm=0.756263
Epoch 36/100 Iteration 205/234: loss=0.085256 lr=0.000020 grad_norm=0.503418
Epoch 36/100 Iteration 206/234: loss=0.092367 lr=0.000020 grad_norm=0.779676
Epoch 36/100 Iteration 207/234: loss=0.083484 lr=0.000020 grad_norm=0.807999
Epoch 36/100 Iteration 208/234: loss=0.073260 lr=0.000020 grad_norm=0.765534
Epoch 36/100 Iteration 209/234: loss=0.073566 lr=0.000020 grad_norm=1.236216
Epoch 36/100 Iteration 210/234: loss=0.073224 lr=0.000020 grad_norm=1.265639
Epoch 36/100 Iteration 211/234: loss=0.081065 lr=0.000020 grad_norm=1.023945
Epoch 36/100 Iteration 212/234: loss=0.063303 lr=0.000020 grad_norm=0.558682
Epoch 36/100 Iteration 213/234: loss=0.074800 lr=0.000020 grad_norm=0.477129
Epoch 36/100 Iteration 214/234: loss=0.079714 lr=0.000020 grad_norm=0.770049
Epoch 36/100 Iteration 215/234: loss=0.070098 lr=0.000020 grad_norm=0.648003
Epoch 36/100 Iteration 216/234: loss=0.074911 lr=0.000020 grad_norm=0.666439
Epoch 36/100 Iteration 217/234: loss=0.077558 lr=0.000020 grad_norm=1.114969
Epoch 36/100 Iteration 218/234: loss=0.075546 lr=0.000020 grad_norm=1.118657
Epoch 36/100 Iteration 219/234: loss=0.078163 lr=0.000020 grad_norm=0.405583
Epoch 36/100 Iteration 220/234: loss=0.084596 lr=0.000020 grad_norm=1.260047
Epoch 36/100 Iteration 221/234: loss=0.073947 lr=0.000020 grad_norm=1.550280
Epoch 36/100 Iteration 222/234: loss=0.065717 lr=0.000020 grad_norm=0.990097
Epoch 36/100 Iteration 223/234: loss=0.073948 lr=0.000020 grad_norm=0.598667
Epoch 36/100 Iteration 224/234: loss=0.088080 lr=0.000020 grad_norm=0.610584
Epoch 36/100 Iteration 225/234: loss=0.075554 lr=0.000020 grad_norm=0.828957
Epoch 36/100 Iteration 226/234: loss=0.064657 lr=0.000020 grad_norm=0.552215
Epoch 36/100 Iteration 227/234: loss=0.076864 lr=0.000020 grad_norm=0.640687
Epoch 36/100 Iteration 228/234: loss=0.069398 lr=0.000020 grad_norm=0.964791
Epoch 36/100 Iteration 229/234: loss=0.074061 lr=0.000020 grad_norm=0.689091
Epoch 36/100 Iteration 230/234: loss=0.077484 lr=0.000020 grad_norm=0.725121
Epoch 36/100 Iteration 231/234: loss=0.078229 lr=0.000020 grad_norm=1.277263
Epoch 36/100 Iteration 232/234: loss=0.078972 lr=0.000020 grad_norm=1.459185
Epoch 36/100 Iteration 233/234: loss=0.065113 lr=0.000020 grad_norm=0.700628
Epoch 36/100 Iteration 234/234: loss=0.076243 lr=0.000020 grad_norm=0.835026
Epoch 36/100 finished. Avg Loss: 0.077161
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 37/100 Iteration 1/234: loss=0.068675 lr=0.000020 grad_norm=1.432402
Epoch 37/100 Iteration 2/234: loss=0.076568 lr=0.000020 grad_norm=1.122481
Epoch 37/100 Iteration 3/234: loss=0.087110 lr=0.000020 grad_norm=0.781221
Epoch 37/100 Iteration 4/234: loss=0.068056 lr=0.000020 grad_norm=0.832137
Epoch 37/100 Iteration 5/234: loss=0.077609 lr=0.000020 grad_norm=0.795579
Epoch 37/100 Iteration 6/234: loss=0.072380 lr=0.000020 grad_norm=0.726736
Epoch 37/100 Iteration 7/234: loss=0.074422 lr=0.000020 grad_norm=0.597266
Epoch 37/100 Iteration 8/234: loss=0.078602 lr=0.000020 grad_norm=0.772531
Epoch 37/100 Iteration 9/234: loss=0.068036 lr=0.000020 grad_norm=0.676671
Epoch 37/100 Iteration 10/234: loss=0.069729 lr=0.000020 grad_norm=1.084049
Epoch 37/100 Iteration 11/234: loss=0.083258 lr=0.000020 grad_norm=1.700562
Epoch 37/100 Iteration 12/234: loss=0.077242 lr=0.000020 grad_norm=1.463479
Epoch 37/100 Iteration 13/234: loss=0.073458 lr=0.000020 grad_norm=0.486491
Epoch 37/100 Iteration 14/234: loss=0.067176 lr=0.000020 grad_norm=0.991941
Epoch 37/100 Iteration 15/234: loss=0.071125 lr=0.000020 grad_norm=0.811684
Epoch 37/100 Iteration 16/234: loss=0.078899 lr=0.000020 grad_norm=0.663668
Epoch 37/100 Iteration 17/234: loss=0.076187 lr=0.000020 grad_norm=1.378298
Epoch 37/100 Iteration 18/234: loss=0.086862 lr=0.000020 grad_norm=1.108811
Epoch 37/100 Iteration 19/234: loss=0.072843 lr=0.000020 grad_norm=0.452421
Epoch 37/100 Iteration 20/234: loss=0.071312 lr=0.000020 grad_norm=1.324071
Epoch 37/100 Iteration 21/234: loss=0.077387 lr=0.000020 grad_norm=0.934642
Epoch 37/100 Iteration 22/234: loss=0.084043 lr=0.000020 grad_norm=0.481438
Epoch 37/100 Iteration 23/234: loss=0.079247 lr=0.000020 grad_norm=1.137186
Epoch 37/100 Iteration 24/234: loss=0.067621 lr=0.000020 grad_norm=1.087229
Epoch 37/100 Iteration 25/234: loss=0.076563 lr=0.000020 grad_norm=0.428341
Epoch 37/100 Iteration 26/234: loss=0.077550 lr=0.000020 grad_norm=0.820262
Epoch 37/100 Iteration 27/234: loss=0.081262 lr=0.000020 grad_norm=0.969069
Epoch 37/100 Iteration 28/234: loss=0.085896 lr=0.000020 grad_norm=0.504150
Epoch 37/100 Iteration 29/234: loss=0.078931 lr=0.000020 grad_norm=1.297767
Epoch 37/100 Iteration 30/234: loss=0.071133 lr=0.000020 grad_norm=1.779373
Epoch 37/100 Iteration 31/234: loss=0.075049 lr=0.000020 grad_norm=1.352908
Epoch 37/100 Iteration 32/234: loss=0.075365 lr=0.000020 grad_norm=0.458633
Epoch 37/100 Iteration 33/234: loss=0.075302 lr=0.000020 grad_norm=0.709513
Epoch 37/100 Iteration 34/234: loss=0.074904 lr=0.000020 grad_norm=0.929417
Epoch 37/100 Iteration 35/234: loss=0.082776 lr=0.000020 grad_norm=0.438141
Epoch 37/100 Iteration 36/234: loss=0.078470 lr=0.000020 grad_norm=0.755338
Epoch 37/100 Iteration 37/234: loss=0.073734 lr=0.000020 grad_norm=0.891407
Epoch 37/100 Iteration 38/234: loss=0.080071 lr=0.000020 grad_norm=0.711618
Epoch 37/100 Iteration 39/234: loss=0.082134 lr=0.000020 grad_norm=0.881266
Epoch 37/100 Iteration 40/234: loss=0.067407 lr=0.000020 grad_norm=1.450159
Epoch 37/100 Iteration 41/234: loss=0.065564 lr=0.000020 grad_norm=1.097525
Epoch 37/100 Iteration 42/234: loss=0.081165 lr=0.000020 grad_norm=0.641500
Epoch 37/100 Iteration 43/234: loss=0.071630 lr=0.000020 grad_norm=0.852347
Epoch 37/100 Iteration 44/234: loss=0.076332 lr=0.000020 grad_norm=0.686510
Epoch 37/100 Iteration 45/234: loss=0.080018 lr=0.000020 grad_norm=0.758590
Epoch 37/100 Iteration 46/234: loss=0.067902 lr=0.000020 grad_norm=1.257072
Epoch 37/100 Iteration 47/234: loss=0.079090 lr=0.000020 grad_norm=1.222043
Epoch 37/100 Iteration 48/234: loss=0.083663 lr=0.000020 grad_norm=0.517779
Epoch 37/100 Iteration 49/234: loss=0.064380 lr=0.000020 grad_norm=0.868720
Epoch 37/100 Iteration 50/234: loss=0.075188 lr=0.000020 grad_norm=1.105353
Epoch 37/100 Iteration 51/234: loss=0.065811 lr=0.000020 grad_norm=0.957982
Epoch 37/100 Iteration 52/234: loss=0.069510 lr=0.000020 grad_norm=0.291218
Epoch 37/100 Iteration 53/234: loss=0.076412 lr=0.000020 grad_norm=0.953355
Epoch 37/100 Iteration 54/234: loss=0.077973 lr=0.000020 grad_norm=0.822760
Epoch 37/100 Iteration 55/234: loss=0.090073 lr=0.000020 grad_norm=0.561056
Epoch 37/100 Iteration 56/234: loss=0.062970 lr=0.000020 grad_norm=0.797283
Epoch 37/100 Iteration 57/234: loss=0.083357 lr=0.000020 grad_norm=1.162724
Epoch 37/100 Iteration 58/234: loss=0.075986 lr=0.000020 grad_norm=0.825676
Epoch 37/100 Iteration 59/234: loss=0.072509 lr=0.000020 grad_norm=0.723105
Epoch 37/100 Iteration 60/234: loss=0.066233 lr=0.000020 grad_norm=1.348424
Epoch 37/100 Iteration 61/234: loss=0.083209 lr=0.000020 grad_norm=1.463668
Epoch 37/100 Iteration 62/234: loss=0.077821 lr=0.000020 grad_norm=0.722352
Epoch 37/100 Iteration 63/234: loss=0.081628 lr=0.000020 grad_norm=1.220716
Epoch 37/100 Iteration 64/234: loss=0.073432 lr=0.000020 grad_norm=1.399881
Epoch 37/100 Iteration 65/234: loss=0.074281 lr=0.000020 grad_norm=0.494619
Epoch 37/100 Iteration 66/234: loss=0.074739 lr=0.000020 grad_norm=1.415829
Epoch 37/100 Iteration 67/234: loss=0.072269 lr=0.000020 grad_norm=1.698999
Epoch 37/100 Iteration 68/234: loss=0.077561 lr=0.000020 grad_norm=0.598481
Epoch 37/100 Iteration 69/234: loss=0.072574 lr=0.000020 grad_norm=1.497490
Epoch 37/100 Iteration 70/234: loss=0.073504 lr=0.000020 grad_norm=1.463167
Epoch 37/100 Iteration 71/234: loss=0.086425 lr=0.000020 grad_norm=0.483064
Epoch 37/100 Iteration 72/234: loss=0.081416 lr=0.000020 grad_norm=1.861738
Epoch 37/100 Iteration 73/234: loss=0.084601 lr=0.000020 grad_norm=2.474721
Epoch 37/100 Iteration 74/234: loss=0.068938 lr=0.000020 grad_norm=1.273489
Epoch 37/100 Iteration 75/234: loss=0.080272 lr=0.000020 grad_norm=1.202889
Epoch 37/100 Iteration 76/234: loss=0.072486 lr=0.000020 grad_norm=2.290960
Epoch 37/100 Iteration 77/234: loss=0.073377 lr=0.000020 grad_norm=0.901681
Epoch 37/100 Iteration 78/234: loss=0.068469 lr=0.000020 grad_norm=1.398699
Epoch 37/100 Iteration 79/234: loss=0.082524 lr=0.000020 grad_norm=1.627732
Epoch 37/100 Iteration 80/234: loss=0.077833 lr=0.000020 grad_norm=0.801392
Epoch 37/100 Iteration 81/234: loss=0.076801 lr=0.000020 grad_norm=2.411699
Epoch 37/100 Iteration 82/234: loss=0.084467 lr=0.000020 grad_norm=2.137205
Epoch 37/100 Iteration 83/234: loss=0.077072 lr=0.000020 grad_norm=0.771552
Epoch 37/100 Iteration 84/234: loss=0.064519 lr=0.000020 grad_norm=1.665417
Epoch 37/100 Iteration 85/234: loss=0.071027 lr=0.000020 grad_norm=0.861970
Epoch 37/100 Iteration 86/234: loss=0.080948 lr=0.000020 grad_norm=1.584558
Epoch 37/100 Iteration 87/234: loss=0.072216 lr=0.000020 grad_norm=1.475417
Epoch 37/100 Iteration 88/234: loss=0.072048 lr=0.000020 grad_norm=0.716544
Epoch 37/100 Iteration 89/234: loss=0.084678 lr=0.000020 grad_norm=0.995971
Epoch 37/100 Iteration 90/234: loss=0.076175 lr=0.000020 grad_norm=0.616425
Epoch 37/100 Iteration 91/234: loss=0.076753 lr=0.000020 grad_norm=1.080001
Epoch 37/100 Iteration 92/234: loss=0.081708 lr=0.000020 grad_norm=0.554255
Epoch 37/100 Iteration 93/234: loss=0.074495 lr=0.000020 grad_norm=0.815079
Epoch 37/100 Iteration 94/234: loss=0.059660 lr=0.000020 grad_norm=0.834176
Epoch 37/100 Iteration 95/234: loss=0.083280 lr=0.000020 grad_norm=0.541108
Epoch 37/100 Iteration 96/234: loss=0.073346 lr=0.000020 grad_norm=1.122954
Epoch 37/100 Iteration 97/234: loss=0.076432 lr=0.000020 grad_norm=0.631446
Epoch 37/100 Iteration 98/234: loss=0.078504 lr=0.000020 grad_norm=0.588036
Epoch 37/100 Iteration 99/234: loss=0.079442 lr=0.000020 grad_norm=0.820789
Epoch 37/100 Iteration 100/234: loss=0.077073 lr=0.000020 grad_norm=0.488102
Epoch 37/100 Iteration 101/234: loss=0.077024 lr=0.000020 grad_norm=0.675503
Epoch 37/100 Iteration 102/234: loss=0.082725 lr=0.000020 grad_norm=0.549061
Epoch 37/100 Iteration 103/234: loss=0.080233 lr=0.000020 grad_norm=0.394067
Epoch 37/100 Iteration 104/234: loss=0.068069 lr=0.000020 grad_norm=0.539538
Epoch 37/100 Iteration 105/234: loss=0.080504 lr=0.000020 grad_norm=0.501645
Epoch 37/100 Iteration 106/234: loss=0.068540 lr=0.000020 grad_norm=0.646310
Epoch 37/100 Iteration 107/234: loss=0.060884 lr=0.000020 grad_norm=0.670185
Epoch 37/100 Iteration 108/234: loss=0.079872 lr=0.000020 grad_norm=0.551976
Epoch 37/100 Iteration 109/234: loss=0.073504 lr=0.000020 grad_norm=0.596931
Epoch 37/100 Iteration 110/234: loss=0.064013 lr=0.000020 grad_norm=0.416106
Epoch 37/100 Iteration 111/234: loss=0.071844 lr=0.000020 grad_norm=1.149314
Epoch 37/100 Iteration 112/234: loss=0.079528 lr=0.000020 grad_norm=1.122425
Epoch 37/100 Iteration 113/234: loss=0.072673 lr=0.000020 grad_norm=0.456258
Epoch 37/100 Iteration 114/234: loss=0.075954 lr=0.000020 grad_norm=0.792697
Epoch 37/100 Iteration 115/234: loss=0.069663 lr=0.000020 grad_norm=0.717636
Epoch 37/100 Iteration 116/234: loss=0.071206 lr=0.000020 grad_norm=0.392953
Epoch 37/100 Iteration 117/234: loss=0.080257 lr=0.000020 grad_norm=0.671646
Epoch 37/100 Iteration 118/234: loss=0.079020 lr=0.000020 grad_norm=0.672976
Epoch 37/100 Iteration 119/234: loss=0.076741 lr=0.000020 grad_norm=0.368512
Epoch 37/100 Iteration 120/234: loss=0.076588 lr=0.000020 grad_norm=0.601570
Epoch 37/100 Iteration 121/234: loss=0.076516 lr=0.000020 grad_norm=0.568856
Epoch 37/100 Iteration 122/234: loss=0.073293 lr=0.000020 grad_norm=0.547765
Epoch 37/100 Iteration 123/234: loss=0.078840 lr=0.000020 grad_norm=0.730010
Epoch 37/100 Iteration 124/234: loss=0.074327 lr=0.000020 grad_norm=0.850690
Epoch 37/100 Iteration 125/234: loss=0.070706 lr=0.000020 grad_norm=0.458962
Epoch 37/100 Iteration 126/234: loss=0.079050 lr=0.000020 grad_norm=0.848067
Epoch 37/100 Iteration 127/234: loss=0.062030 lr=0.000020 grad_norm=1.001029
Epoch 37/100 Iteration 128/234: loss=0.085224 lr=0.000020 grad_norm=0.357330
Epoch 37/100 Iteration 129/234: loss=0.080372 lr=0.000020 grad_norm=1.388896
Epoch 37/100 Iteration 130/234: loss=0.078571 lr=0.000020 grad_norm=1.215999
Epoch 37/100 Iteration 131/234: loss=0.075292 lr=0.000020 grad_norm=0.427239
Epoch 37/100 Iteration 132/234: loss=0.068355 lr=0.000020 grad_norm=0.931229
Epoch 37/100 Iteration 133/234: loss=0.071358 lr=0.000020 grad_norm=0.909308
Epoch 37/100 Iteration 134/234: loss=0.074400 lr=0.000020 grad_norm=0.564650
Epoch 37/100 Iteration 135/234: loss=0.087845 lr=0.000020 grad_norm=1.245397
Epoch 37/100 Iteration 136/234: loss=0.075590 lr=0.000020 grad_norm=1.064068
Epoch 37/100 Iteration 137/234: loss=0.067184 lr=0.000020 grad_norm=0.532374
Epoch 37/100 Iteration 138/234: loss=0.073837 lr=0.000020 grad_norm=0.709339
Epoch 37/100 Iteration 139/234: loss=0.084059 lr=0.000020 grad_norm=0.842584
Epoch 37/100 Iteration 140/234: loss=0.068371 lr=0.000020 grad_norm=0.659131
Epoch 37/100 Iteration 141/234: loss=0.071618 lr=0.000020 grad_norm=1.172830
Epoch 37/100 Iteration 142/234: loss=0.076885 lr=0.000020 grad_norm=1.181290
Epoch 37/100 Iteration 143/234: loss=0.064376 lr=0.000020 grad_norm=0.574756
Epoch 37/100 Iteration 144/234: loss=0.080595 lr=0.000020 grad_norm=1.063553
Epoch 37/100 Iteration 145/234: loss=0.079502 lr=0.000020 grad_norm=1.189805
Epoch 37/100 Iteration 146/234: loss=0.076759 lr=0.000020 grad_norm=0.583399
Epoch 37/100 Iteration 147/234: loss=0.083456 lr=0.000020 grad_norm=1.044650
Epoch 37/100 Iteration 148/234: loss=0.067358 lr=0.000020 grad_norm=1.360101
Epoch 37/100 Iteration 149/234: loss=0.074818 lr=0.000020 grad_norm=0.615246
Epoch 37/100 Iteration 150/234: loss=0.066251 lr=0.000020 grad_norm=0.856303
Epoch 37/100 Iteration 151/234: loss=0.080572 lr=0.000020 grad_norm=0.838932
Epoch 37/100 Iteration 152/234: loss=0.082421 lr=0.000020 grad_norm=0.387291
Epoch 37/100 Iteration 153/234: loss=0.068091 lr=0.000020 grad_norm=0.558496
Epoch 37/100 Iteration 154/234: loss=0.085237 lr=0.000020 grad_norm=0.565020
Epoch 37/100 Iteration 155/234: loss=0.058128 lr=0.000020 grad_norm=0.303938
Epoch 37/100 Iteration 156/234: loss=0.060865 lr=0.000020 grad_norm=0.402067
Epoch 37/100 Iteration 157/234: loss=0.076458 lr=0.000020 grad_norm=0.612129
Epoch 37/100 Iteration 158/234: loss=0.075656 lr=0.000020 grad_norm=0.791082
Epoch 37/100 Iteration 159/234: loss=0.078119 lr=0.000020 grad_norm=0.572916
Epoch 37/100 Iteration 160/234: loss=0.085877 lr=0.000020 grad_norm=0.784048
Epoch 37/100 Iteration 161/234: loss=0.069268 lr=0.000020 grad_norm=0.633653
Epoch 37/100 Iteration 162/234: loss=0.070618 lr=0.000020 grad_norm=0.634248
Epoch 37/100 Iteration 163/234: loss=0.068203 lr=0.000020 grad_norm=0.812158
Epoch 37/100 Iteration 164/234: loss=0.078022 lr=0.000020 grad_norm=0.502219
Epoch 37/100 Iteration 165/234: loss=0.082433 lr=0.000020 grad_norm=0.808630
Epoch 37/100 Iteration 166/234: loss=0.076857 lr=0.000020 grad_norm=0.610471
Epoch 37/100 Iteration 167/234: loss=0.082081 lr=0.000020 grad_norm=0.766848
Epoch 37/100 Iteration 168/234: loss=0.068958 lr=0.000020 grad_norm=1.143666
Epoch 37/100 Iteration 169/234: loss=0.073152 lr=0.000020 grad_norm=0.888829
Epoch 37/100 Iteration 170/234: loss=0.084349 lr=0.000020 grad_norm=0.867648
Epoch 37/100 Iteration 171/234: loss=0.071123 lr=0.000020 grad_norm=1.760548
Epoch 37/100 Iteration 172/234: loss=0.075043 lr=0.000020 grad_norm=1.010749
Epoch 37/100 Iteration 173/234: loss=0.079495 lr=0.000020 grad_norm=0.955717
Epoch 37/100 Iteration 174/234: loss=0.067102 lr=0.000020 grad_norm=1.550147
Epoch 37/100 Iteration 175/234: loss=0.071330 lr=0.000020 grad_norm=1.186548
Epoch 37/100 Iteration 176/234: loss=0.080788 lr=0.000020 grad_norm=0.918636
Epoch 37/100 Iteration 177/234: loss=0.083708 lr=0.000020 grad_norm=1.824029
Epoch 37/100 Iteration 178/234: loss=0.079087 lr=0.000020 grad_norm=0.995432
Epoch 37/100 Iteration 179/234: loss=0.080744 lr=0.000020 grad_norm=1.233259
Epoch 37/100 Iteration 180/234: loss=0.078688 lr=0.000020 grad_norm=1.964639
Epoch 37/100 Iteration 181/234: loss=0.076327 lr=0.000020 grad_norm=0.941921
Epoch 37/100 Iteration 182/234: loss=0.076330 lr=0.000020 grad_norm=1.417929
Epoch 37/100 Iteration 183/234: loss=0.069467 lr=0.000020 grad_norm=1.517032
Epoch 37/100 Iteration 184/234: loss=0.068413 lr=0.000020 grad_norm=0.601897
Epoch 37/100 Iteration 185/234: loss=0.083485 lr=0.000020 grad_norm=1.721154
Epoch 37/100 Iteration 186/234: loss=0.073760 lr=0.000020 grad_norm=0.859681
Epoch 37/100 Iteration 187/234: loss=0.078621 lr=0.000020 grad_norm=0.804920
Epoch 37/100 Iteration 188/234: loss=0.063077 lr=0.000020 grad_norm=0.963026
Epoch 37/100 Iteration 189/234: loss=0.086728 lr=0.000020 grad_norm=0.507056
Epoch 37/100 Iteration 190/234: loss=0.084083 lr=0.000020 grad_norm=1.390904
Epoch 37/100 Iteration 191/234: loss=0.083962 lr=0.000020 grad_norm=1.150761
Epoch 37/100 Iteration 192/234: loss=0.079613 lr=0.000020 grad_norm=0.925734
Epoch 37/100 Iteration 193/234: loss=0.071524 lr=0.000020 grad_norm=1.443415
Epoch 37/100 Iteration 194/234: loss=0.080620 lr=0.000020 grad_norm=0.605729
Epoch 37/100 Iteration 195/234: loss=0.077015 lr=0.000020 grad_norm=1.819140
Epoch 37/100 Iteration 196/234: loss=0.077119 lr=0.000020 grad_norm=1.501871
Epoch 37/100 Iteration 197/234: loss=0.072804 lr=0.000020 grad_norm=0.711345
Epoch 37/100 Iteration 198/234: loss=0.073443 lr=0.000020 grad_norm=1.362993
Epoch 37/100 Iteration 199/234: loss=0.065126 lr=0.000020 grad_norm=1.115536
Epoch 37/100 Iteration 200/234: loss=0.080071 lr=0.000020 grad_norm=0.898601
Epoch 37/100 Iteration 201/234: loss=0.075222 lr=0.000020 grad_norm=1.292542
Epoch 37/100 Iteration 202/234: loss=0.075672 lr=0.000020 grad_norm=0.565495
Epoch 37/100 Iteration 203/234: loss=0.067489 lr=0.000020 grad_norm=0.662421
Epoch 37/100 Iteration 204/234: loss=0.077308 lr=0.000020 grad_norm=0.918556
Epoch 37/100 Iteration 205/234: loss=0.077888 lr=0.000020 grad_norm=0.641221
Epoch 37/100 Iteration 206/234: loss=0.069296 lr=0.000020 grad_norm=0.684186
Epoch 37/100 Iteration 207/234: loss=0.071035 lr=0.000020 grad_norm=1.024188
Epoch 37/100 Iteration 208/234: loss=0.081001 lr=0.000020 grad_norm=0.655574
Epoch 37/100 Iteration 209/234: loss=0.081858 lr=0.000020 grad_norm=0.946695
Epoch 37/100 Iteration 210/234: loss=0.080426 lr=0.000020 grad_norm=0.647808
Epoch 37/100 Iteration 211/234: loss=0.076431 lr=0.000020 grad_norm=0.675702
Epoch 37/100 Iteration 212/234: loss=0.074081 lr=0.000020 grad_norm=0.695546
Epoch 37/100 Iteration 213/234: loss=0.073134 lr=0.000020 grad_norm=0.607159
Epoch 37/100 Iteration 214/234: loss=0.084622 lr=0.000020 grad_norm=0.795793
Epoch 37/100 Iteration 215/234: loss=0.068265 lr=0.000020 grad_norm=0.800568
Epoch 37/100 Iteration 216/234: loss=0.071479 lr=0.000020 grad_norm=0.676262
Epoch 37/100 Iteration 217/234: loss=0.073190 lr=0.000020 grad_norm=1.311446
Epoch 37/100 Iteration 218/234: loss=0.065495 lr=0.000020 grad_norm=1.595523
Epoch 37/100 Iteration 219/234: loss=0.083563 lr=0.000020 grad_norm=0.784263
Epoch 37/100 Iteration 220/234: loss=0.077646 lr=0.000020 grad_norm=0.815329
Epoch 37/100 Iteration 221/234: loss=0.093900 lr=0.000020 grad_norm=1.447720
Epoch 37/100 Iteration 222/234: loss=0.069087 lr=0.000020 grad_norm=0.971939
Epoch 37/100 Iteration 223/234: loss=0.078112 lr=0.000020 grad_norm=0.826518
Epoch 37/100 Iteration 224/234: loss=0.075265 lr=0.000020 grad_norm=1.553335
Epoch 37/100 Iteration 225/234: loss=0.067228 lr=0.000020 grad_norm=0.467157
Epoch 37/100 Iteration 226/234: loss=0.079131 lr=0.000020 grad_norm=1.261848
Epoch 37/100 Iteration 227/234: loss=0.076553 lr=0.000020 grad_norm=1.445341
Epoch 37/100 Iteration 228/234: loss=0.074609 lr=0.000020 grad_norm=0.426836
Epoch 37/100 Iteration 229/234: loss=0.072106 lr=0.000020 grad_norm=1.199394
Epoch 37/100 Iteration 230/234: loss=0.078544 lr=0.000020 grad_norm=0.878374
Epoch 37/100 Iteration 231/234: loss=0.081634 lr=0.000020 grad_norm=1.083097
Epoch 37/100 Iteration 232/234: loss=0.075029 lr=0.000020 grad_norm=2.124502
Epoch 37/100 Iteration 233/234: loss=0.073743 lr=0.000020 grad_norm=0.963592
Epoch 37/100 Iteration 234/234: loss=0.076102 lr=0.000020 grad_norm=1.402397
Epoch 37/100 finished. Avg Loss: 0.075616
Epoch 38/100 Iteration 1/234: loss=0.074198 lr=0.000020 grad_norm=1.346416
Epoch 38/100 Iteration 2/234: loss=0.075256 lr=0.000020 grad_norm=0.601584
Epoch 38/100 Iteration 3/234: loss=0.069883 lr=0.000020 grad_norm=1.046842
Epoch 38/100 Iteration 4/234: loss=0.065902 lr=0.000020 grad_norm=0.401375
Epoch 38/100 Iteration 5/234: loss=0.076212 lr=0.000020 grad_norm=0.795365
Epoch 38/100 Iteration 6/234: loss=0.077570 lr=0.000020 grad_norm=0.620059
Epoch 38/100 Iteration 7/234: loss=0.081876 lr=0.000020 grad_norm=0.519374
Epoch 38/100 Iteration 8/234: loss=0.068290 lr=0.000020 grad_norm=0.673205
Epoch 38/100 Iteration 9/234: loss=0.084479 lr=0.000020 grad_norm=0.516074
Epoch 38/100 Iteration 10/234: loss=0.080193 lr=0.000020 grad_norm=0.418353
Epoch 38/100 Iteration 11/234: loss=0.069085 lr=0.000020 grad_norm=0.546029
Epoch 38/100 Iteration 12/234: loss=0.065223 lr=0.000020 grad_norm=0.405740
Epoch 38/100 Iteration 13/234: loss=0.066523 lr=0.000020 grad_norm=0.508695
Epoch 38/100 Iteration 14/234: loss=0.078432 lr=0.000020 grad_norm=0.579131
Epoch 38/100 Iteration 15/234: loss=0.066622 lr=0.000020 grad_norm=0.398447
Epoch 38/100 Iteration 16/234: loss=0.079342 lr=0.000020 grad_norm=0.488100
Epoch 38/100 Iteration 17/234: loss=0.069587 lr=0.000020 grad_norm=0.420678
Epoch 38/100 Iteration 18/234: loss=0.074487 lr=0.000020 grad_norm=0.533598
Epoch 38/100 Iteration 19/234: loss=0.072216 lr=0.000020 grad_norm=0.686252
Epoch 38/100 Iteration 20/234: loss=0.081944 lr=0.000020 grad_norm=0.891660
Epoch 38/100 Iteration 21/234: loss=0.079420 lr=0.000020 grad_norm=1.036037
Epoch 38/100 Iteration 22/234: loss=0.078541 lr=0.000020 grad_norm=0.918845
Epoch 38/100 Iteration 23/234: loss=0.064554 lr=0.000020 grad_norm=0.618434
Epoch 38/100 Iteration 24/234: loss=0.076054 lr=0.000020 grad_norm=0.586162
Epoch 38/100 Iteration 25/234: loss=0.072193 lr=0.000020 grad_norm=0.722876
Epoch 38/100 Iteration 26/234: loss=0.079588 lr=0.000020 grad_norm=0.458433
Epoch 38/100 Iteration 27/234: loss=0.082334 lr=0.000020 grad_norm=0.477861
Epoch 38/100 Iteration 28/234: loss=0.078188 lr=0.000020 grad_norm=0.820600
Epoch 38/100 Iteration 29/234: loss=0.075629 lr=0.000020 grad_norm=0.672668
Epoch 38/100 Iteration 30/234: loss=0.067321 lr=0.000020 grad_norm=0.362266
Epoch 38/100 Iteration 31/234: loss=0.077546 lr=0.000020 grad_norm=0.660266
Epoch 38/100 Iteration 32/234: loss=0.068104 lr=0.000020 grad_norm=0.566745
Epoch 38/100 Iteration 33/234: loss=0.074794 lr=0.000020 grad_norm=0.586569
Epoch 38/100 Iteration 34/234: loss=0.080165 lr=0.000020 grad_norm=1.027841
Epoch 38/100 Iteration 35/234: loss=0.075603 lr=0.000020 grad_norm=0.732960
Epoch 38/100 Iteration 36/234: loss=0.077040 lr=0.000020 grad_norm=0.841091
Epoch 38/100 Iteration 37/234: loss=0.079008 lr=0.000020 grad_norm=0.812250
Epoch 38/100 Iteration 38/234: loss=0.077540 lr=0.000020 grad_norm=0.523347
Epoch 38/100 Iteration 39/234: loss=0.074564 lr=0.000020 grad_norm=1.689566
Epoch 38/100 Iteration 40/234: loss=0.066797 lr=0.000020 grad_norm=1.347367
Epoch 38/100 Iteration 41/234: loss=0.068578 lr=0.000020 grad_norm=0.423043
Epoch 38/100 Iteration 42/234: loss=0.073301 lr=0.000020 grad_norm=1.088035
Epoch 38/100 Iteration 43/234: loss=0.072081 lr=0.000020 grad_norm=0.984812
Epoch 38/100 Iteration 44/234: loss=0.086753 lr=0.000020 grad_norm=0.380678
Epoch 38/100 Iteration 45/234: loss=0.059875 lr=0.000020 grad_norm=1.195454
Epoch 38/100 Iteration 46/234: loss=0.067395 lr=0.000020 grad_norm=0.987183
Epoch 38/100 Iteration 47/234: loss=0.075182 lr=0.000020 grad_norm=0.974507
Epoch 38/100 Iteration 48/234: loss=0.080360 lr=0.000020 grad_norm=2.565421
Epoch 38/100 Iteration 49/234: loss=0.074511 lr=0.000020 grad_norm=2.205082
Epoch 38/100 Iteration 50/234: loss=0.074119 lr=0.000020 grad_norm=0.640417
Epoch 38/100 Iteration 51/234: loss=0.083036 lr=0.000020 grad_norm=1.826947
Epoch 38/100 Iteration 52/234: loss=0.065194 lr=0.000020 grad_norm=0.693998
Epoch 38/100 Iteration 53/234: loss=0.080563 lr=0.000020 grad_norm=1.746226
Epoch 38/100 Iteration 54/234: loss=0.059973 lr=0.000020 grad_norm=1.168037
Epoch 38/100 Iteration 55/234: loss=0.078127 lr=0.000020 grad_norm=1.419062
Epoch 38/100 Iteration 56/234: loss=0.079967 lr=0.000020 grad_norm=1.720085
Epoch 38/100 Iteration 57/234: loss=0.066792 lr=0.000020 grad_norm=0.626699
Epoch 38/100 Iteration 58/234: loss=0.067181 lr=0.000020 grad_norm=1.584166
Epoch 38/100 Iteration 59/234: loss=0.070540 lr=0.000020 grad_norm=0.680196
Epoch 38/100 Iteration 60/234: loss=0.086941 lr=0.000020 grad_norm=1.159228
Epoch 38/100 Iteration 61/234: loss=0.068481 lr=0.000020 grad_norm=1.480851
Epoch 38/100 Iteration 62/234: loss=0.070980 lr=0.000020 grad_norm=0.558990
Epoch 38/100 Iteration 63/234: loss=0.071564 lr=0.000020 grad_norm=1.410411
Epoch 38/100 Iteration 64/234: loss=0.070987 lr=0.000020 grad_norm=0.842509
Epoch 38/100 Iteration 65/234: loss=0.089953 lr=0.000020 grad_norm=0.839772
Epoch 38/100 Iteration 66/234: loss=0.065066 lr=0.000020 grad_norm=0.944172
Epoch 38/100 Iteration 67/234: loss=0.074214 lr=0.000020 grad_norm=0.626196
Epoch 38/100 Iteration 68/234: loss=0.075456 lr=0.000020 grad_norm=1.098064
Epoch 38/100 Iteration 69/234: loss=0.072589 lr=0.000020 grad_norm=0.617914
Epoch 38/100 Iteration 70/234: loss=0.082042 lr=0.000020 grad_norm=1.084072
Epoch 38/100 Iteration 71/234: loss=0.070262 lr=0.000020 grad_norm=1.716718
Epoch 38/100 Iteration 72/234: loss=0.069223 lr=0.000020 grad_norm=0.859541
Epoch 38/100 Iteration 73/234: loss=0.076789 lr=0.000020 grad_norm=1.101640
Epoch 38/100 Iteration 74/234: loss=0.074948 lr=0.000020 grad_norm=1.085810
Epoch 38/100 Iteration 75/234: loss=0.069855 lr=0.000020 grad_norm=0.765455
Epoch 38/100 Iteration 76/234: loss=0.074773 lr=0.000020 grad_norm=1.015088
Epoch 38/100 Iteration 77/234: loss=0.067467 lr=0.000020 grad_norm=0.521413
Epoch 38/100 Iteration 78/234: loss=0.073383 lr=0.000020 grad_norm=0.888139
Epoch 38/100 Iteration 79/234: loss=0.074600 lr=0.000020 grad_norm=0.651793
Epoch 38/100 Iteration 80/234: loss=0.079508 lr=0.000020 grad_norm=0.754083
Epoch 38/100 Iteration 81/234: loss=0.070225 lr=0.000020 grad_norm=0.887337
Epoch 38/100 Iteration 82/234: loss=0.074007 lr=0.000020 grad_norm=0.598353
Epoch 38/100 Iteration 83/234: loss=0.077478 lr=0.000020 grad_norm=0.885842
Epoch 38/100 Iteration 84/234: loss=0.078857 lr=0.000020 grad_norm=0.469129
Epoch 38/100 Iteration 85/234: loss=0.070613 lr=0.000020 grad_norm=1.076782
Epoch 38/100 Iteration 86/234: loss=0.073238 lr=0.000020 grad_norm=1.012787
Epoch 38/100 Iteration 87/234: loss=0.065896 lr=0.000020 grad_norm=0.382310
Epoch 38/100 Iteration 88/234: loss=0.072979 lr=0.000020 grad_norm=0.810958
Epoch 38/100 Iteration 89/234: loss=0.075202 lr=0.000020 grad_norm=0.540427
Epoch 38/100 Iteration 90/234: loss=0.077737 lr=0.000020 grad_norm=0.971621
Epoch 38/100 Iteration 91/234: loss=0.074191 lr=0.000020 grad_norm=0.559460
Epoch 38/100 Iteration 92/234: loss=0.086173 lr=0.000020 grad_norm=1.158161
Epoch 38/100 Iteration 93/234: loss=0.075344 lr=0.000020 grad_norm=1.274795
Epoch 38/100 Iteration 94/234: loss=0.072493 lr=0.000020 grad_norm=0.432848
Epoch 38/100 Iteration 95/234: loss=0.074954 lr=0.000020 grad_norm=0.896418
Epoch 38/100 Iteration 96/234: loss=0.090602 lr=0.000020 grad_norm=1.101062
Epoch 38/100 Iteration 97/234: loss=0.077521 lr=0.000020 grad_norm=0.543673
Epoch 38/100 Iteration 98/234: loss=0.083227 lr=0.000020 grad_norm=0.766032
Epoch 38/100 Iteration 99/234: loss=0.075161 lr=0.000020 grad_norm=1.008569
Epoch 38/100 Iteration 100/234: loss=0.071969 lr=0.000020 grad_norm=0.700835
Epoch 38/100 Iteration 101/234: loss=0.076738 lr=0.000020 grad_norm=0.418430
Epoch 38/100 Iteration 102/234: loss=0.074103 lr=0.000020 grad_norm=0.543358
Epoch 38/100 Iteration 103/234: loss=0.066653 lr=0.000020 grad_norm=0.670481
Epoch 38/100 Iteration 104/234: loss=0.067254 lr=0.000020 grad_norm=0.632755
Epoch 38/100 Iteration 105/234: loss=0.069696 lr=0.000020 grad_norm=0.611165
Epoch 38/100 Iteration 106/234: loss=0.072703 lr=0.000020 grad_norm=0.952994
Epoch 38/100 Iteration 107/234: loss=0.071514 lr=0.000020 grad_norm=1.100459
Epoch 38/100 Iteration 108/234: loss=0.079229 lr=0.000020 grad_norm=0.702113
Epoch 38/100 Iteration 109/234: loss=0.073103 lr=0.000020 grad_norm=0.913965
Epoch 38/100 Iteration 110/234: loss=0.074448 lr=0.000020 grad_norm=0.802568
Epoch 38/100 Iteration 111/234: loss=0.080907 lr=0.000020 grad_norm=0.477613
Epoch 38/100 Iteration 112/234: loss=0.084343 lr=0.000020 grad_norm=0.932620
Epoch 38/100 Iteration 113/234: loss=0.066052 lr=0.000020 grad_norm=0.929123
Epoch 38/100 Iteration 114/234: loss=0.073009 lr=0.000020 grad_norm=0.630724
Epoch 38/100 Iteration 115/234: loss=0.082252 lr=0.000020 grad_norm=0.980836
Epoch 38/100 Iteration 116/234: loss=0.063649 lr=0.000020 grad_norm=1.681459
Epoch 38/100 Iteration 117/234: loss=0.067707 lr=0.000020 grad_norm=2.005133
Epoch 38/100 Iteration 118/234: loss=0.062106 lr=0.000020 grad_norm=0.869794
Epoch 38/100 Iteration 119/234: loss=0.084089 lr=0.000020 grad_norm=1.281121
Epoch 38/100 Iteration 120/234: loss=0.070728 lr=0.000020 grad_norm=1.957373
Epoch 38/100 Iteration 121/234: loss=0.074556 lr=0.000020 grad_norm=0.889706
Epoch 38/100 Iteration 122/234: loss=0.074266 lr=0.000020 grad_norm=1.221918
Epoch 38/100 Iteration 123/234: loss=0.076638 lr=0.000020 grad_norm=1.697746
Epoch 38/100 Iteration 124/234: loss=0.077111 lr=0.000020 grad_norm=0.800158
Epoch 38/100 Iteration 125/234: loss=0.061376 lr=0.000020 grad_norm=0.943760
Epoch 38/100 Iteration 126/234: loss=0.071734 lr=0.000020 grad_norm=0.603416
Epoch 38/100 Iteration 127/234: loss=0.061679 lr=0.000020 grad_norm=0.749544
Epoch 38/100 Iteration 128/234: loss=0.077804 lr=0.000020 grad_norm=0.610534
Epoch 38/100 Iteration 129/234: loss=0.072900 lr=0.000020 grad_norm=0.579291
Epoch 38/100 Iteration 130/234: loss=0.064256 lr=0.000020 grad_norm=0.597785
Epoch 38/100 Iteration 131/234: loss=0.081487 lr=0.000020 grad_norm=0.485183
Epoch 38/100 Iteration 132/234: loss=0.075605 lr=0.000020 grad_norm=0.384688
Epoch 38/100 Iteration 133/234: loss=0.074983 lr=0.000020 grad_norm=0.505987
Epoch 38/100 Iteration 134/234: loss=0.072861 lr=0.000020 grad_norm=0.957987
Epoch 38/100 Iteration 135/234: loss=0.078791 lr=0.000020 grad_norm=0.905163
Epoch 38/100 Iteration 136/234: loss=0.074775 lr=0.000020 grad_norm=0.492209
Epoch 38/100 Iteration 137/234: loss=0.076663 lr=0.000020 grad_norm=1.467830
Epoch 38/100 Iteration 138/234: loss=0.071852 lr=0.000020 grad_norm=1.256523
Epoch 38/100 Iteration 139/234: loss=0.069959 lr=0.000020 grad_norm=0.764879
Epoch 38/100 Iteration 140/234: loss=0.077469 lr=0.000020 grad_norm=1.777373
Epoch 38/100 Iteration 141/234: loss=0.072023 lr=0.000020 grad_norm=0.640380
Epoch 38/100 Iteration 142/234: loss=0.068463 lr=0.000020 grad_norm=1.310903
Epoch 38/100 Iteration 143/234: loss=0.075825 lr=0.000020 grad_norm=0.993404
Epoch 38/100 Iteration 144/234: loss=0.075695 lr=0.000020 grad_norm=1.027201
Epoch 38/100 Iteration 145/234: loss=0.064397 lr=0.000020 grad_norm=1.481973
Epoch 38/100 Iteration 146/234: loss=0.073225 lr=0.000020 grad_norm=0.569110
Epoch 38/100 Iteration 147/234: loss=0.076891 lr=0.000020 grad_norm=1.093602
Epoch 38/100 Iteration 148/234: loss=0.066919 lr=0.000020 grad_norm=0.744139
Epoch 38/100 Iteration 149/234: loss=0.077880 lr=0.000020 grad_norm=0.675161
Epoch 38/100 Iteration 150/234: loss=0.070325 lr=0.000020 grad_norm=0.761440
Epoch 38/100 Iteration 151/234: loss=0.072447 lr=0.000020 grad_norm=0.412591
Epoch 38/100 Iteration 152/234: loss=0.069378 lr=0.000020 grad_norm=0.947605
Epoch 38/100 Iteration 153/234: loss=0.073727 lr=0.000020 grad_norm=0.677932
Epoch 38/100 Iteration 154/234: loss=0.076636 lr=0.000020 grad_norm=0.520677
Epoch 38/100 Iteration 155/234: loss=0.081029 lr=0.000020 grad_norm=0.501427
Epoch 38/100 Iteration 156/234: loss=0.077231 lr=0.000020 grad_norm=0.502593
Epoch 38/100 Iteration 157/234: loss=0.077537 lr=0.000020 grad_norm=0.586031
Epoch 38/100 Iteration 158/234: loss=0.073256 lr=0.000020 grad_norm=0.553000
Epoch 38/100 Iteration 159/234: loss=0.076413 lr=0.000020 grad_norm=0.661093
Epoch 38/100 Iteration 160/234: loss=0.065841 lr=0.000020 grad_norm=0.516490
Epoch 38/100 Iteration 161/234: loss=0.068408 lr=0.000020 grad_norm=0.467560
Epoch 38/100 Iteration 162/234: loss=0.065901 lr=0.000020 grad_norm=0.469394
Epoch 38/100 Iteration 163/234: loss=0.078484 lr=0.000020 grad_norm=0.598983
Epoch 38/100 Iteration 164/234: loss=0.073700 lr=0.000020 grad_norm=0.437232
Epoch 38/100 Iteration 165/234: loss=0.077946 lr=0.000020 grad_norm=0.584564
Epoch 38/100 Iteration 166/234: loss=0.076981 lr=0.000020 grad_norm=0.690950
Epoch 38/100 Iteration 167/234: loss=0.064489 lr=0.000020 grad_norm=0.625124
Epoch 38/100 Iteration 168/234: loss=0.072624 lr=0.000020 grad_norm=0.633294
Epoch 38/100 Iteration 169/234: loss=0.071500 lr=0.000020 grad_norm=0.621342
Epoch 38/100 Iteration 170/234: loss=0.089046 lr=0.000020 grad_norm=0.792348
Epoch 38/100 Iteration 171/234: loss=0.081657 lr=0.000020 grad_norm=1.216296
Epoch 38/100 Iteration 172/234: loss=0.068714 lr=0.000020 grad_norm=1.808414
Epoch 38/100 Iteration 173/234: loss=0.067349 lr=0.000020 grad_norm=1.139388
Epoch 38/100 Iteration 174/234: loss=0.068427 lr=0.000020 grad_norm=0.765138
Epoch 38/100 Iteration 175/234: loss=0.074720 lr=0.000020 grad_norm=1.373918
Epoch 38/100 Iteration 176/234: loss=0.078201 lr=0.000020 grad_norm=0.826832
Epoch 38/100 Iteration 177/234: loss=0.079893 lr=0.000020 grad_norm=0.527064
Epoch 38/100 Iteration 178/234: loss=0.081288 lr=0.000020 grad_norm=1.148737
Epoch 38/100 Iteration 179/234: loss=0.078750 lr=0.000020 grad_norm=0.871734
Epoch 38/100 Iteration 180/234: loss=0.073287 lr=0.000020 grad_norm=0.692416
Epoch 38/100 Iteration 181/234: loss=0.073739 lr=0.000020 grad_norm=0.787737
Epoch 38/100 Iteration 182/234: loss=0.071086 lr=0.000020 grad_norm=0.764844
Epoch 38/100 Iteration 183/234: loss=0.080906 lr=0.000020 grad_norm=0.686395
Epoch 38/100 Iteration 184/234: loss=0.067297 lr=0.000020 grad_norm=0.309368
Epoch 38/100 Iteration 185/234: loss=0.076608 lr=0.000020 grad_norm=0.659600
Epoch 38/100 Iteration 186/234: loss=0.072978 lr=0.000020 grad_norm=0.873068
Epoch 38/100 Iteration 187/234: loss=0.069821 lr=0.000020 grad_norm=0.991514
Epoch 38/100 Iteration 188/234: loss=0.074501 lr=0.000020 grad_norm=1.681536
Epoch 38/100 Iteration 189/234: loss=0.076256 lr=0.000020 grad_norm=1.770887
Epoch 38/100 Iteration 190/234: loss=0.077271 lr=0.000020 grad_norm=1.321263
Epoch 38/100 Iteration 191/234: loss=0.063628 lr=0.000020 grad_norm=0.784945
Epoch 38/100 Iteration 192/234: loss=0.069990 lr=0.000020 grad_norm=0.911688
Epoch 38/100 Iteration 193/234: loss=0.071331 lr=0.000020 grad_norm=0.601973
Epoch 38/100 Iteration 194/234: loss=0.080141 lr=0.000020 grad_norm=1.211303
Epoch 38/100 Iteration 195/234: loss=0.074237 lr=0.000020 grad_norm=0.856477
Epoch 38/100 Iteration 196/234: loss=0.078027 lr=0.000020 grad_norm=0.861182
Epoch 38/100 Iteration 197/234: loss=0.074592 lr=0.000020 grad_norm=0.878879
Epoch 38/100 Iteration 198/234: loss=0.074078 lr=0.000020 grad_norm=0.489341
Epoch 38/100 Iteration 199/234: loss=0.073134 lr=0.000020 grad_norm=0.851708
Epoch 38/100 Iteration 200/234: loss=0.062718 lr=0.000020 grad_norm=1.072235
Epoch 38/100 Iteration 201/234: loss=0.078952 lr=0.000020 grad_norm=1.930854
Epoch 38/100 Iteration 202/234: loss=0.062606 lr=0.000020 grad_norm=1.177079
Epoch 38/100 Iteration 203/234: loss=0.066881 lr=0.000020 grad_norm=0.730616
Epoch 38/100 Iteration 204/234: loss=0.071625 lr=0.000020 grad_norm=1.246881
Epoch 38/100 Iteration 205/234: loss=0.076699 lr=0.000020 grad_norm=0.748405
Epoch 38/100 Iteration 206/234: loss=0.069779 lr=0.000020 grad_norm=0.735927
Epoch 38/100 Iteration 207/234: loss=0.069025 lr=0.000020 grad_norm=0.457553
Epoch 38/100 Iteration 208/234: loss=0.077971 lr=0.000020 grad_norm=0.641945
Epoch 38/100 Iteration 209/234: loss=0.070090 lr=0.000020 grad_norm=0.628323
Epoch 38/100 Iteration 210/234: loss=0.067183 lr=0.000020 grad_norm=0.668904
Epoch 38/100 Iteration 211/234: loss=0.062357 lr=0.000020 grad_norm=0.659822
Epoch 38/100 Iteration 212/234: loss=0.063469 lr=0.000020 grad_norm=0.827912
Epoch 38/100 Iteration 213/234: loss=0.072168 lr=0.000020 grad_norm=1.682425
Epoch 38/100 Iteration 214/234: loss=0.068479 lr=0.000020 grad_norm=1.557097
Epoch 38/100 Iteration 215/234: loss=0.072883 lr=0.000020 grad_norm=0.488303
Epoch 38/100 Iteration 216/234: loss=0.072440 lr=0.000020 grad_norm=1.380631
Epoch 38/100 Iteration 217/234: loss=0.072601 lr=0.000020 grad_norm=1.352414
Epoch 38/100 Iteration 218/234: loss=0.081774 lr=0.000020 grad_norm=0.570364
Epoch 38/100 Iteration 219/234: loss=0.075203 lr=0.000020 grad_norm=1.282532
Epoch 38/100 Iteration 220/234: loss=0.068328 lr=0.000020 grad_norm=1.747530
Epoch 38/100 Iteration 221/234: loss=0.079296 lr=0.000020 grad_norm=0.796800
Epoch 38/100 Iteration 222/234: loss=0.078360 lr=0.000020 grad_norm=0.994066
Epoch 38/100 Iteration 223/234: loss=0.062204 lr=0.000020 grad_norm=0.814918
Epoch 38/100 Iteration 224/234: loss=0.064800 lr=0.000020 grad_norm=0.896804
Epoch 38/100 Iteration 225/234: loss=0.066245 lr=0.000020 grad_norm=0.659762
Epoch 38/100 Iteration 226/234: loss=0.084713 lr=0.000020 grad_norm=0.930585
Epoch 38/100 Iteration 227/234: loss=0.068933 lr=0.000020 grad_norm=0.823841
Epoch 38/100 Iteration 228/234: loss=0.072065 lr=0.000020 grad_norm=0.580415
Epoch 38/100 Iteration 229/234: loss=0.070666 lr=0.000020 grad_norm=1.140354
Epoch 38/100 Iteration 230/234: loss=0.070507 lr=0.000020 grad_norm=1.068194
Epoch 38/100 Iteration 231/234: loss=0.071117 lr=0.000020 grad_norm=0.880995
Epoch 38/100 Iteration 232/234: loss=0.084817 lr=0.000020 grad_norm=0.801014
Epoch 38/100 Iteration 233/234: loss=0.070451 lr=0.000020 grad_norm=1.418887
Epoch 38/100 Iteration 234/234: loss=0.068285 lr=0.000020 grad_norm=0.944796
Epoch 38/100 finished. Avg Loss: 0.073597
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 39/100 Iteration 1/234: loss=0.071467 lr=0.000020 grad_norm=0.920630
Epoch 39/100 Iteration 2/234: loss=0.074063 lr=0.000020 grad_norm=1.982409
Epoch 39/100 Iteration 3/234: loss=0.061512 lr=0.000020 grad_norm=1.507218
Epoch 39/100 Iteration 4/234: loss=0.069178 lr=0.000020 grad_norm=0.791593
Epoch 39/100 Iteration 5/234: loss=0.073919 lr=0.000020 grad_norm=1.867026
Epoch 39/100 Iteration 6/234: loss=0.068160 lr=0.000020 grad_norm=0.790279
Epoch 39/100 Iteration 7/234: loss=0.078077 lr=0.000020 grad_norm=1.432644
Epoch 39/100 Iteration 8/234: loss=0.075265 lr=0.000020 grad_norm=1.884352
Epoch 39/100 Iteration 9/234: loss=0.073957 lr=0.000020 grad_norm=0.553455
Epoch 39/100 Iteration 10/234: loss=0.065545 lr=0.000020 grad_norm=1.284115
Epoch 39/100 Iteration 11/234: loss=0.073314 lr=0.000020 grad_norm=1.024818
Epoch 39/100 Iteration 12/234: loss=0.070904 lr=0.000020 grad_norm=0.567145
Epoch 39/100 Iteration 13/234: loss=0.087856 lr=0.000020 grad_norm=0.927486
Epoch 39/100 Iteration 14/234: loss=0.069554 lr=0.000020 grad_norm=0.408494
Epoch 39/100 Iteration 15/234: loss=0.071007 lr=0.000020 grad_norm=0.753902
Epoch 39/100 Iteration 16/234: loss=0.072844 lr=0.000020 grad_norm=0.461087
Epoch 39/100 Iteration 17/234: loss=0.065670 lr=0.000020 grad_norm=0.715465
Epoch 39/100 Iteration 18/234: loss=0.071388 lr=0.000020 grad_norm=0.771718
Epoch 39/100 Iteration 19/234: loss=0.062585 lr=0.000020 grad_norm=0.475043
Epoch 39/100 Iteration 20/234: loss=0.082081 lr=0.000020 grad_norm=0.882311
Epoch 39/100 Iteration 21/234: loss=0.072818 lr=0.000020 grad_norm=0.894333
Epoch 39/100 Iteration 22/234: loss=0.069142 lr=0.000020 grad_norm=0.708734
Epoch 39/100 Iteration 23/234: loss=0.060646 lr=0.000020 grad_norm=0.320523
Epoch 39/100 Iteration 24/234: loss=0.071737 lr=0.000020 grad_norm=0.600368
Epoch 39/100 Iteration 25/234: loss=0.069346 lr=0.000020 grad_norm=0.464795
Epoch 39/100 Iteration 26/234: loss=0.063747 lr=0.000020 grad_norm=0.590153
Epoch 39/100 Iteration 27/234: loss=0.078387 lr=0.000020 grad_norm=0.909161
Epoch 39/100 Iteration 28/234: loss=0.062924 lr=0.000020 grad_norm=0.490925
Epoch 39/100 Iteration 29/234: loss=0.074878 lr=0.000020 grad_norm=0.744104
Epoch 39/100 Iteration 30/234: loss=0.086692 lr=0.000020 grad_norm=0.776723
Epoch 39/100 Iteration 31/234: loss=0.076758 lr=0.000020 grad_norm=0.601513
Epoch 39/100 Iteration 32/234: loss=0.071929 lr=0.000020 grad_norm=1.151535
Epoch 39/100 Iteration 33/234: loss=0.075714 lr=0.000020 grad_norm=0.749563
Epoch 39/100 Iteration 34/234: loss=0.068785 lr=0.000020 grad_norm=0.749322
Epoch 39/100 Iteration 35/234: loss=0.078788 lr=0.000020 grad_norm=0.704036
Epoch 39/100 Iteration 36/234: loss=0.072215 lr=0.000020 grad_norm=0.589642
Epoch 39/100 Iteration 37/234: loss=0.069904 lr=0.000020 grad_norm=0.586679
Epoch 39/100 Iteration 38/234: loss=0.072100 lr=0.000020 grad_norm=0.497845
Epoch 39/100 Iteration 39/234: loss=0.076654 lr=0.000020 grad_norm=0.709463
Epoch 39/100 Iteration 40/234: loss=0.078199 lr=0.000020 grad_norm=0.949285
Epoch 39/100 Iteration 41/234: loss=0.064999 lr=0.000020 grad_norm=0.522734
Epoch 39/100 Iteration 42/234: loss=0.075313 lr=0.000020 grad_norm=1.036589
Epoch 39/100 Iteration 43/234: loss=0.067438 lr=0.000020 grad_norm=1.211329
Epoch 39/100 Iteration 44/234: loss=0.067439 lr=0.000020 grad_norm=0.486468
Epoch 39/100 Iteration 45/234: loss=0.077976 lr=0.000020 grad_norm=0.788757
Epoch 39/100 Iteration 46/234: loss=0.073081 lr=0.000020 grad_norm=0.631882
Epoch 39/100 Iteration 47/234: loss=0.067062 lr=0.000020 grad_norm=0.927830
Epoch 39/100 Iteration 48/234: loss=0.066152 lr=0.000020 grad_norm=1.549272
Epoch 39/100 Iteration 49/234: loss=0.079358 lr=0.000020 grad_norm=1.273895
Epoch 39/100 Iteration 50/234: loss=0.071546 lr=0.000020 grad_norm=0.793306
Epoch 39/100 Iteration 51/234: loss=0.070684 lr=0.000020 grad_norm=0.381155
Epoch 39/100 Iteration 52/234: loss=0.067277 lr=0.000020 grad_norm=0.670808
Epoch 39/100 Iteration 53/234: loss=0.061445 lr=0.000020 grad_norm=0.834927
Epoch 39/100 Iteration 54/234: loss=0.067890 lr=0.000020 grad_norm=0.446434
Epoch 39/100 Iteration 55/234: loss=0.081430 lr=0.000020 grad_norm=1.100413
Epoch 39/100 Iteration 56/234: loss=0.076546 lr=0.000020 grad_norm=0.861132
Epoch 39/100 Iteration 57/234: loss=0.066279 lr=0.000020 grad_norm=0.706812
Epoch 39/100 Iteration 58/234: loss=0.065054 lr=0.000020 grad_norm=1.089194
Epoch 39/100 Iteration 59/234: loss=0.065963 lr=0.000020 grad_norm=0.456368
Epoch 39/100 Iteration 60/234: loss=0.070385 lr=0.000020 grad_norm=0.970535
Epoch 39/100 Iteration 61/234: loss=0.068550 lr=0.000020 grad_norm=1.003225
Epoch 39/100 Iteration 62/234: loss=0.079770 lr=0.000020 grad_norm=0.476724
Epoch 39/100 Iteration 63/234: loss=0.066868 lr=0.000020 grad_norm=0.723375
Epoch 39/100 Iteration 64/234: loss=0.074306 lr=0.000020 grad_norm=0.498984
Epoch 39/100 Iteration 65/234: loss=0.070148 lr=0.000020 grad_norm=1.444609
Epoch 39/100 Iteration 66/234: loss=0.072178 lr=0.000020 grad_norm=1.362725
Epoch 39/100 Iteration 67/234: loss=0.066713 lr=0.000020 grad_norm=0.468408
Epoch 39/100 Iteration 68/234: loss=0.086921 lr=0.000020 grad_norm=1.605891
Epoch 39/100 Iteration 69/234: loss=0.073482 lr=0.000020 grad_norm=1.531491
Epoch 39/100 Iteration 70/234: loss=0.083920 lr=0.000020 grad_norm=0.762167
Epoch 39/100 Iteration 71/234: loss=0.062343 lr=0.000020 grad_norm=1.437438
Epoch 39/100 Iteration 72/234: loss=0.073422 lr=0.000020 grad_norm=0.717229
Epoch 39/100 Iteration 73/234: loss=0.069235 lr=0.000020 grad_norm=1.172819
Epoch 39/100 Iteration 74/234: loss=0.079388 lr=0.000020 grad_norm=1.279690
Epoch 39/100 Iteration 75/234: loss=0.086101 lr=0.000020 grad_norm=0.761388
Epoch 39/100 Iteration 76/234: loss=0.072830 lr=0.000020 grad_norm=0.927790
Epoch 39/100 Iteration 77/234: loss=0.069140 lr=0.000020 grad_norm=0.612695
Epoch 39/100 Iteration 78/234: loss=0.081821 lr=0.000020 grad_norm=0.643832
Epoch 39/100 Iteration 79/234: loss=0.076654 lr=0.000020 grad_norm=0.780252
Epoch 39/100 Iteration 80/234: loss=0.060049 lr=0.000020 grad_norm=0.420805
Epoch 39/100 Iteration 81/234: loss=0.072229 lr=0.000020 grad_norm=0.568228
Epoch 39/100 Iteration 82/234: loss=0.063678 lr=0.000020 grad_norm=0.596062
Epoch 39/100 Iteration 83/234: loss=0.082603 lr=0.000020 grad_norm=0.339132
Epoch 39/100 Iteration 84/234: loss=0.071506 lr=0.000020 grad_norm=1.020330
Epoch 39/100 Iteration 85/234: loss=0.073661 lr=0.000020 grad_norm=1.430261
Epoch 39/100 Iteration 86/234: loss=0.062640 lr=0.000020 grad_norm=0.845286
Epoch 39/100 Iteration 87/234: loss=0.078108 lr=0.000020 grad_norm=0.749446
Epoch 39/100 Iteration 88/234: loss=0.069024 lr=0.000020 grad_norm=1.500723
Epoch 39/100 Iteration 89/234: loss=0.079420 lr=0.000020 grad_norm=1.095119
Epoch 39/100 Iteration 90/234: loss=0.073650 lr=0.000020 grad_norm=0.685541
Epoch 39/100 Iteration 91/234: loss=0.064297 lr=0.000020 grad_norm=1.084732
Epoch 39/100 Iteration 92/234: loss=0.067020 lr=0.000020 grad_norm=0.608953
Epoch 39/100 Iteration 93/234: loss=0.065933 lr=0.000020 grad_norm=1.107344
Epoch 39/100 Iteration 94/234: loss=0.073885 lr=0.000020 grad_norm=1.220242
Epoch 39/100 Iteration 95/234: loss=0.077145 lr=0.000020 grad_norm=0.598862
Epoch 39/100 Iteration 96/234: loss=0.067408 lr=0.000020 grad_norm=1.045765
Epoch 39/100 Iteration 97/234: loss=0.068799 lr=0.000020 grad_norm=1.014418
Epoch 39/100 Iteration 98/234: loss=0.071094 lr=0.000020 grad_norm=0.748733
Epoch 39/100 Iteration 99/234: loss=0.069341 lr=0.000020 grad_norm=0.560630
Epoch 39/100 Iteration 100/234: loss=0.074796 lr=0.000020 grad_norm=0.580488
Epoch 39/100 Iteration 101/234: loss=0.080430 lr=0.000020 grad_norm=0.934983
Epoch 39/100 Iteration 102/234: loss=0.068716 lr=0.000020 grad_norm=0.814312
Epoch 39/100 Iteration 103/234: loss=0.076475 lr=0.000020 grad_norm=1.205988
Epoch 39/100 Iteration 104/234: loss=0.070824 lr=0.000020 grad_norm=0.826562
Epoch 39/100 Iteration 105/234: loss=0.077713 lr=0.000020 grad_norm=0.790679
Epoch 39/100 Iteration 106/234: loss=0.075452 lr=0.000020 grad_norm=1.045548
Epoch 39/100 Iteration 107/234: loss=0.065815 lr=0.000020 grad_norm=0.462383
Epoch 39/100 Iteration 108/234: loss=0.061662 lr=0.000020 grad_norm=0.929882
Epoch 39/100 Iteration 109/234: loss=0.056671 lr=0.000020 grad_norm=0.778515
Epoch 39/100 Iteration 110/234: loss=0.065330 lr=0.000020 grad_norm=0.609229
Epoch 39/100 Iteration 111/234: loss=0.086986 lr=0.000020 grad_norm=0.907247
Epoch 39/100 Iteration 112/234: loss=0.076506 lr=0.000020 grad_norm=1.016388
Epoch 39/100 Iteration 113/234: loss=0.066727 lr=0.000020 grad_norm=0.757832
Epoch 39/100 Iteration 114/234: loss=0.071679 lr=0.000020 grad_norm=0.528005
Epoch 39/100 Iteration 115/234: loss=0.072885 lr=0.000020 grad_norm=0.762822
Epoch 39/100 Iteration 116/234: loss=0.071003 lr=0.000020 grad_norm=1.016176
Epoch 39/100 Iteration 117/234: loss=0.073180 lr=0.000020 grad_norm=0.628087
Epoch 39/100 Iteration 118/234: loss=0.072227 lr=0.000020 grad_norm=0.622585
Epoch 39/100 Iteration 119/234: loss=0.075698 lr=0.000020 grad_norm=1.122553
Epoch 39/100 Iteration 120/234: loss=0.068046 lr=0.000020 grad_norm=0.659795
Epoch 39/100 Iteration 121/234: loss=0.075786 lr=0.000020 grad_norm=0.825517
Epoch 39/100 Iteration 122/234: loss=0.071067 lr=0.000020 grad_norm=1.345987
Epoch 39/100 Iteration 123/234: loss=0.073665 lr=0.000020 grad_norm=0.805606
Epoch 39/100 Iteration 124/234: loss=0.069159 lr=0.000020 grad_norm=0.649913
Epoch 39/100 Iteration 125/234: loss=0.064403 lr=0.000020 grad_norm=1.263444
Epoch 39/100 Iteration 126/234: loss=0.075599 lr=0.000020 grad_norm=0.991023
Epoch 39/100 Iteration 127/234: loss=0.077476 lr=0.000020 grad_norm=0.748190
Epoch 39/100 Iteration 128/234: loss=0.063919 lr=0.000020 grad_norm=0.619926
Epoch 39/100 Iteration 129/234: loss=0.072649 lr=0.000020 grad_norm=0.590044
Epoch 39/100 Iteration 130/234: loss=0.069549 lr=0.000020 grad_norm=0.705013
Epoch 39/100 Iteration 131/234: loss=0.072170 lr=0.000020 grad_norm=0.733913
Epoch 39/100 Iteration 132/234: loss=0.070517 lr=0.000020 grad_norm=0.819384
Epoch 39/100 Iteration 133/234: loss=0.070155 lr=0.000020 grad_norm=1.077504
Epoch 39/100 Iteration 134/234: loss=0.070499 lr=0.000020 grad_norm=0.679699
Epoch 39/100 Iteration 135/234: loss=0.071573 lr=0.000020 grad_norm=0.957127
Epoch 39/100 Iteration 136/234: loss=0.061693 lr=0.000020 grad_norm=1.905741
Epoch 39/100 Iteration 137/234: loss=0.070158 lr=0.000020 grad_norm=2.149190
Epoch 39/100 Iteration 138/234: loss=0.069887 lr=0.000020 grad_norm=1.163084
Epoch 39/100 Iteration 139/234: loss=0.069223 lr=0.000020 grad_norm=1.256941
Epoch 39/100 Iteration 140/234: loss=0.069068 lr=0.000020 grad_norm=1.639490
Epoch 39/100 Iteration 141/234: loss=0.072761 lr=0.000020 grad_norm=0.756126
Epoch 39/100 Iteration 142/234: loss=0.065701 lr=0.000020 grad_norm=0.682262
Epoch 39/100 Iteration 143/234: loss=0.077666 lr=0.000020 grad_norm=0.963878
Epoch 39/100 Iteration 144/234: loss=0.067673 lr=0.000020 grad_norm=0.628444
Epoch 39/100 Iteration 145/234: loss=0.073234 lr=0.000020 grad_norm=0.977917
Epoch 39/100 Iteration 146/234: loss=0.073559 lr=0.000020 grad_norm=1.423515
Epoch 39/100 Iteration 147/234: loss=0.077869 lr=0.000020 grad_norm=0.848016
Epoch 39/100 Iteration 148/234: loss=0.066493 lr=0.000020 grad_norm=0.393122
Epoch 39/100 Iteration 149/234: loss=0.069015 lr=0.000020 grad_norm=0.875203
Epoch 39/100 Iteration 150/234: loss=0.075525 lr=0.000020 grad_norm=1.131563
Epoch 39/100 Iteration 151/234: loss=0.070839 lr=0.000020 grad_norm=0.714616
Epoch 39/100 Iteration 152/234: loss=0.081541 lr=0.000020 grad_norm=0.807725
Epoch 39/100 Iteration 153/234: loss=0.070744 lr=0.000020 grad_norm=1.085976
Epoch 39/100 Iteration 154/234: loss=0.068353 lr=0.000020 grad_norm=1.304011
Epoch 39/100 Iteration 155/234: loss=0.070800 lr=0.000020 grad_norm=0.753457
Epoch 39/100 Iteration 156/234: loss=0.065713 lr=0.000020 grad_norm=0.899120
Epoch 39/100 Iteration 157/234: loss=0.074455 lr=0.000020 grad_norm=0.945987
Epoch 39/100 Iteration 158/234: loss=0.072099 lr=0.000020 grad_norm=0.613364
Epoch 39/100 Iteration 159/234: loss=0.061480 lr=0.000020 grad_norm=0.851782
Epoch 39/100 Iteration 160/234: loss=0.081516 lr=0.000020 grad_norm=0.656488
Epoch 39/100 Iteration 161/234: loss=0.067227 lr=0.000020 grad_norm=0.716276
Epoch 39/100 Iteration 162/234: loss=0.077372 lr=0.000020 grad_norm=0.982371
Epoch 39/100 Iteration 163/234: loss=0.068137 lr=0.000020 grad_norm=0.884678
Epoch 39/100 Iteration 164/234: loss=0.066462 lr=0.000020 grad_norm=0.519691
Epoch 39/100 Iteration 165/234: loss=0.070765 lr=0.000020 grad_norm=0.897463
Epoch 39/100 Iteration 166/234: loss=0.078991 lr=0.000020 grad_norm=1.200893
Epoch 39/100 Iteration 167/234: loss=0.073083 lr=0.000020 grad_norm=0.675523
Epoch 39/100 Iteration 168/234: loss=0.071459 lr=0.000020 grad_norm=0.420868
Epoch 39/100 Iteration 169/234: loss=0.059332 lr=0.000020 grad_norm=0.672787
Epoch 39/100 Iteration 170/234: loss=0.063994 lr=0.000020 grad_norm=0.519592
Epoch 39/100 Iteration 171/234: loss=0.063179 lr=0.000020 grad_norm=0.498025
Epoch 39/100 Iteration 172/234: loss=0.076869 lr=0.000020 grad_norm=0.587490
Epoch 39/100 Iteration 173/234: loss=0.071540 lr=0.000020 grad_norm=0.683599
Epoch 39/100 Iteration 174/234: loss=0.071955 lr=0.000020 grad_norm=0.575632
Epoch 39/100 Iteration 175/234: loss=0.081350 lr=0.000020 grad_norm=0.422579
Epoch 39/100 Iteration 176/234: loss=0.062896 lr=0.000020 grad_norm=0.533974
Epoch 39/100 Iteration 177/234: loss=0.079724 lr=0.000020 grad_norm=0.614956
Epoch 39/100 Iteration 178/234: loss=0.062132 lr=0.000020 grad_norm=0.385945
Epoch 39/100 Iteration 179/234: loss=0.070865 lr=0.000020 grad_norm=0.616166
Epoch 39/100 Iteration 180/234: loss=0.071439 lr=0.000020 grad_norm=0.820131
Epoch 39/100 Iteration 181/234: loss=0.081575 lr=0.000020 grad_norm=0.682654
Epoch 39/100 Iteration 182/234: loss=0.070929 lr=0.000020 grad_norm=0.502415
Epoch 39/100 Iteration 183/234: loss=0.072269 lr=0.000020 grad_norm=0.970731
Epoch 39/100 Iteration 184/234: loss=0.076629 lr=0.000020 grad_norm=0.754413
Epoch 39/100 Iteration 185/234: loss=0.080415 lr=0.000020 grad_norm=0.549859
Epoch 39/100 Iteration 186/234: loss=0.072870 lr=0.000020 grad_norm=0.597013
Epoch 39/100 Iteration 187/234: loss=0.071538 lr=0.000020 grad_norm=0.631953
Epoch 39/100 Iteration 188/234: loss=0.077164 lr=0.000020 grad_norm=1.140550
Epoch 39/100 Iteration 189/234: loss=0.066814 lr=0.000020 grad_norm=1.143298
Epoch 39/100 Iteration 190/234: loss=0.072808 lr=0.000020 grad_norm=0.671796
Epoch 39/100 Iteration 191/234: loss=0.055002 lr=0.000020 grad_norm=0.639261
Epoch 39/100 Iteration 192/234: loss=0.062732 lr=0.000020 grad_norm=0.799642
Epoch 39/100 Iteration 193/234: loss=0.073316 lr=0.000020 grad_norm=0.499059
Epoch 39/100 Iteration 194/234: loss=0.077266 lr=0.000020 grad_norm=0.476290
Epoch 39/100 Iteration 195/234: loss=0.065552 lr=0.000020 grad_norm=0.707054
Epoch 39/100 Iteration 196/234: loss=0.069255 lr=0.000020 grad_norm=0.979071
Epoch 39/100 Iteration 197/234: loss=0.065359 lr=0.000020 grad_norm=0.594632
Epoch 39/100 Iteration 198/234: loss=0.066897 lr=0.000020 grad_norm=0.806547
Epoch 39/100 Iteration 199/234: loss=0.065736 lr=0.000020 grad_norm=1.523529
Epoch 39/100 Iteration 200/234: loss=0.066220 lr=0.000020 grad_norm=1.500872
Epoch 39/100 Iteration 201/234: loss=0.075419 lr=0.000020 grad_norm=0.674620
Epoch 39/100 Iteration 202/234: loss=0.071754 lr=0.000020 grad_norm=0.789733
Epoch 39/100 Iteration 203/234: loss=0.067332 lr=0.000020 grad_norm=0.808623
Epoch 39/100 Iteration 204/234: loss=0.062501 lr=0.000020 grad_norm=0.612839
Epoch 39/100 Iteration 205/234: loss=0.077299 lr=0.000020 grad_norm=0.494703
Epoch 39/100 Iteration 206/234: loss=0.079952 lr=0.000020 grad_norm=0.597599
Epoch 39/100 Iteration 207/234: loss=0.087675 lr=0.000020 grad_norm=0.675764
Epoch 39/100 Iteration 208/234: loss=0.081557 lr=0.000020 grad_norm=0.707998
Epoch 39/100 Iteration 209/234: loss=0.077426 lr=0.000020 grad_norm=0.528150
Epoch 39/100 Iteration 210/234: loss=0.076225 lr=0.000020 grad_norm=0.639630
Epoch 39/100 Iteration 211/234: loss=0.075058 lr=0.000020 grad_norm=0.534164
Epoch 39/100 Iteration 212/234: loss=0.076122 lr=0.000020 grad_norm=0.484230
Epoch 39/100 Iteration 213/234: loss=0.068598 lr=0.000020 grad_norm=0.618726
Epoch 39/100 Iteration 214/234: loss=0.074136 lr=0.000020 grad_norm=0.746408
Epoch 39/100 Iteration 215/234: loss=0.073188 lr=0.000020 grad_norm=0.565808
Epoch 39/100 Iteration 216/234: loss=0.073786 lr=0.000020 grad_norm=0.701350
Epoch 39/100 Iteration 217/234: loss=0.066977 lr=0.000020 grad_norm=0.730896
Epoch 39/100 Iteration 218/234: loss=0.067639 lr=0.000020 grad_norm=0.666780
Epoch 39/100 Iteration 219/234: loss=0.064933 lr=0.000020 grad_norm=0.878485
Epoch 39/100 Iteration 220/234: loss=0.071632 lr=0.000020 grad_norm=0.746574
Epoch 39/100 Iteration 221/234: loss=0.071060 lr=0.000020 grad_norm=0.471490
Epoch 39/100 Iteration 222/234: loss=0.069331 lr=0.000020 grad_norm=0.477553
Epoch 39/100 Iteration 223/234: loss=0.077156 lr=0.000020 grad_norm=0.835645
Epoch 39/100 Iteration 224/234: loss=0.073205 lr=0.000020 grad_norm=0.801948
Epoch 39/100 Iteration 225/234: loss=0.071567 lr=0.000020 grad_norm=1.042752
Epoch 39/100 Iteration 226/234: loss=0.074259 lr=0.000020 grad_norm=1.150893
Epoch 39/100 Iteration 227/234: loss=0.072734 lr=0.000020 grad_norm=0.682306
Epoch 39/100 Iteration 228/234: loss=0.075293 lr=0.000020 grad_norm=0.524876
Epoch 39/100 Iteration 229/234: loss=0.067013 lr=0.000020 grad_norm=0.821353
Epoch 39/100 Iteration 230/234: loss=0.064892 lr=0.000020 grad_norm=0.813910
Epoch 39/100 Iteration 231/234: loss=0.067524 lr=0.000020 grad_norm=0.423487
Epoch 39/100 Iteration 232/234: loss=0.066615 lr=0.000020 grad_norm=0.691964
Epoch 39/100 Iteration 233/234: loss=0.072490 lr=0.000020 grad_norm=0.749988
Epoch 39/100 Iteration 234/234: loss=0.071555 lr=0.000020 grad_norm=0.741860
Epoch 39/100 finished. Avg Loss: 0.071604
Epoch 40/100 Iteration 1/234: loss=0.066143 lr=0.000020 grad_norm=0.418338
Epoch 40/100 Iteration 2/234: loss=0.067703 lr=0.000020 grad_norm=0.692405
Epoch 40/100 Iteration 3/234: loss=0.070420 lr=0.000020 grad_norm=1.301735
Epoch 40/100 Iteration 4/234: loss=0.080771 lr=0.000020 grad_norm=1.463079
Epoch 40/100 Iteration 5/234: loss=0.075490 lr=0.000020 grad_norm=1.031751
Epoch 40/100 Iteration 6/234: loss=0.076145 lr=0.000020 grad_norm=0.378316
Epoch 40/100 Iteration 7/234: loss=0.065027 lr=0.000020 grad_norm=0.885739
Epoch 40/100 Iteration 8/234: loss=0.062769 lr=0.000020 grad_norm=0.530727
Epoch 40/100 Iteration 9/234: loss=0.069350 lr=0.000020 grad_norm=0.764531
Epoch 40/100 Iteration 10/234: loss=0.082494 lr=0.000020 grad_norm=1.347451
Epoch 40/100 Iteration 11/234: loss=0.074789 lr=0.000020 grad_norm=1.535373
Epoch 40/100 Iteration 12/234: loss=0.071465 lr=0.000020 grad_norm=0.831131
Epoch 40/100 Iteration 13/234: loss=0.072092 lr=0.000020 grad_norm=0.441731
Epoch 40/100 Iteration 14/234: loss=0.065369 lr=0.000020 grad_norm=0.749499
Epoch 40/100 Iteration 15/234: loss=0.080642 lr=0.000020 grad_norm=0.876187
Epoch 40/100 Iteration 16/234: loss=0.074547 lr=0.000020 grad_norm=0.573762
Epoch 40/100 Iteration 17/234: loss=0.064832 lr=0.000020 grad_norm=0.364033
Epoch 40/100 Iteration 18/234: loss=0.066250 lr=0.000020 grad_norm=0.375650
Epoch 40/100 Iteration 19/234: loss=0.071704 lr=0.000020 grad_norm=0.700893
Epoch 40/100 Iteration 20/234: loss=0.070223 lr=0.000020 grad_norm=1.106099
Epoch 40/100 Iteration 21/234: loss=0.074901 lr=0.000020 grad_norm=1.006398
Epoch 40/100 Iteration 22/234: loss=0.072038 lr=0.000020 grad_norm=0.468475
Epoch 40/100 Iteration 23/234: loss=0.059002 lr=0.000020 grad_norm=1.187009
Epoch 40/100 Iteration 24/234: loss=0.075226 lr=0.000020 grad_norm=0.973981
Epoch 40/100 Iteration 25/234: loss=0.077735 lr=0.000020 grad_norm=0.566078
Epoch 40/100 Iteration 26/234: loss=0.071268 lr=0.000020 grad_norm=0.687235
Epoch 40/100 Iteration 27/234: loss=0.063387 lr=0.000020 grad_norm=0.708690
Epoch 40/100 Iteration 28/234: loss=0.077214 lr=0.000020 grad_norm=0.474020
Epoch 40/100 Iteration 29/234: loss=0.071028 lr=0.000020 grad_norm=0.707687
Epoch 40/100 Iteration 30/234: loss=0.070906 lr=0.000020 grad_norm=0.361726
Epoch 40/100 Iteration 31/234: loss=0.069069 lr=0.000020 grad_norm=0.493170
Epoch 40/100 Iteration 32/234: loss=0.061505 lr=0.000020 grad_norm=1.078555
Epoch 40/100 Iteration 33/234: loss=0.071338 lr=0.000020 grad_norm=1.785093
Epoch 40/100 Iteration 34/234: loss=0.056452 lr=0.000020 grad_norm=1.308672
Epoch 40/100 Iteration 35/234: loss=0.071758 lr=0.000020 grad_norm=0.848208
Epoch 40/100 Iteration 36/234: loss=0.073366 lr=0.000020 grad_norm=0.912981
Epoch 40/100 Iteration 37/234: loss=0.068480 lr=0.000020 grad_norm=0.494896
Epoch 40/100 Iteration 38/234: loss=0.062311 lr=0.000020 grad_norm=0.898015
Epoch 40/100 Iteration 39/234: loss=0.077433 lr=0.000020 grad_norm=0.923262
Epoch 40/100 Iteration 40/234: loss=0.068395 lr=0.000020 grad_norm=0.439389
Epoch 40/100 Iteration 41/234: loss=0.063263 lr=0.000020 grad_norm=0.695106
Epoch 40/100 Iteration 42/234: loss=0.072038 lr=0.000020 grad_norm=1.091161
Epoch 40/100 Iteration 43/234: loss=0.067772 lr=0.000020 grad_norm=0.910490
Epoch 40/100 Iteration 44/234: loss=0.055493 lr=0.000020 grad_norm=0.605445
Epoch 40/100 Iteration 45/234: loss=0.076921 lr=0.000020 grad_norm=0.479568
Epoch 40/100 Iteration 46/234: loss=0.073851 lr=0.000020 grad_norm=0.762898
Epoch 40/100 Iteration 47/234: loss=0.075298 lr=0.000020 grad_norm=0.952270
Epoch 40/100 Iteration 48/234: loss=0.058146 lr=0.000020 grad_norm=0.556141
Epoch 40/100 Iteration 49/234: loss=0.068915 lr=0.000020 grad_norm=0.728949
Epoch 40/100 Iteration 50/234: loss=0.069579 lr=0.000020 grad_norm=0.759630
Epoch 40/100 Iteration 51/234: loss=0.068957 lr=0.000020 grad_norm=0.644004
Epoch 40/100 Iteration 52/234: loss=0.066085 lr=0.000020 grad_norm=0.585571
Epoch 40/100 Iteration 53/234: loss=0.065595 lr=0.000020 grad_norm=0.939011
Epoch 40/100 Iteration 54/234: loss=0.076429 lr=0.000020 grad_norm=0.903300
Epoch 40/100 Iteration 55/234: loss=0.073173 lr=0.000020 grad_norm=1.016575
Epoch 40/100 Iteration 56/234: loss=0.074646 lr=0.000020 grad_norm=1.252378
Epoch 40/100 Iteration 57/234: loss=0.078902 lr=0.000020 grad_norm=1.618156
Epoch 40/100 Iteration 58/234: loss=0.069980 lr=0.000020 grad_norm=1.613191
Epoch 40/100 Iteration 59/234: loss=0.066578 lr=0.000020 grad_norm=0.555966
Epoch 40/100 Iteration 60/234: loss=0.066949 lr=0.000020 grad_norm=1.365612
Epoch 40/100 Iteration 61/234: loss=0.063486 lr=0.000020 grad_norm=1.230477
Epoch 40/100 Iteration 62/234: loss=0.076556 lr=0.000020 grad_norm=0.640080
Epoch 40/100 Iteration 63/234: loss=0.066498 lr=0.000020 grad_norm=1.237473
Epoch 40/100 Iteration 64/234: loss=0.062938 lr=0.000020 grad_norm=0.910012
Epoch 40/100 Iteration 65/234: loss=0.074581 lr=0.000020 grad_norm=0.652286
Epoch 40/100 Iteration 66/234: loss=0.066014 lr=0.000020 grad_norm=1.066711
Epoch 40/100 Iteration 67/234: loss=0.078204 lr=0.000020 grad_norm=0.438296
Epoch 40/100 Iteration 68/234: loss=0.073646 lr=0.000020 grad_norm=1.423452
Epoch 40/100 Iteration 69/234: loss=0.069734 lr=0.000020 grad_norm=1.052268
Epoch 40/100 Iteration 70/234: loss=0.073169 lr=0.000020 grad_norm=0.720276
Epoch 40/100 Iteration 71/234: loss=0.074497 lr=0.000020 grad_norm=1.423733
Epoch 40/100 Iteration 72/234: loss=0.070487 lr=0.000020 grad_norm=0.481273
Epoch 40/100 Iteration 73/234: loss=0.067549 lr=0.000020 grad_norm=1.330931
Epoch 40/100 Iteration 74/234: loss=0.073973 lr=0.000020 grad_norm=1.409369
Epoch 40/100 Iteration 75/234: loss=0.069675 lr=0.000020 grad_norm=0.439715
Epoch 40/100 Iteration 76/234: loss=0.070645 lr=0.000020 grad_norm=2.066505
Epoch 40/100 Iteration 77/234: loss=0.064426 lr=0.000020 grad_norm=1.789262
Epoch 40/100 Iteration 78/234: loss=0.070138 lr=0.000020 grad_norm=0.875982
Epoch 40/100 Iteration 79/234: loss=0.070929 lr=0.000020 grad_norm=1.644136
Epoch 40/100 Iteration 80/234: loss=0.071076 lr=0.000020 grad_norm=1.202206
Epoch 40/100 Iteration 81/234: loss=0.068909 lr=0.000020 grad_norm=1.298674
Epoch 40/100 Iteration 82/234: loss=0.065190 lr=0.000020 grad_norm=0.984816
Epoch 40/100 Iteration 83/234: loss=0.071142 lr=0.000020 grad_norm=0.704868
Epoch 40/100 Iteration 84/234: loss=0.075830 lr=0.000020 grad_norm=1.102056
Epoch 40/100 Iteration 85/234: loss=0.075443 lr=0.000020 grad_norm=0.657100
Epoch 40/100 Iteration 86/234: loss=0.078190 lr=0.000020 grad_norm=1.000199
Epoch 40/100 Iteration 87/234: loss=0.067364 lr=0.000020 grad_norm=1.406650
Epoch 40/100 Iteration 88/234: loss=0.065368 lr=0.000020 grad_norm=1.033096
Epoch 40/100 Iteration 89/234: loss=0.061903 lr=0.000020 grad_norm=0.974296
Epoch 40/100 Iteration 90/234: loss=0.068436 lr=0.000020 grad_norm=1.052614
Epoch 40/100 Iteration 91/234: loss=0.067776 lr=0.000020 grad_norm=0.638678
Epoch 40/100 Iteration 92/234: loss=0.072415 lr=0.000020 grad_norm=1.333166
Epoch 40/100 Iteration 93/234: loss=0.068405 lr=0.000020 grad_norm=1.292417
Epoch 40/100 Iteration 94/234: loss=0.064063 lr=0.000020 grad_norm=0.656833
Epoch 40/100 Iteration 95/234: loss=0.076556 lr=0.000020 grad_norm=0.689864
Epoch 40/100 Iteration 96/234: loss=0.079688 lr=0.000020 grad_norm=0.925006
Epoch 40/100 Iteration 97/234: loss=0.076147 lr=0.000020 grad_norm=1.309493
Epoch 40/100 Iteration 98/234: loss=0.075202 lr=0.000020 grad_norm=1.061596
Epoch 40/100 Iteration 99/234: loss=0.080076 lr=0.000020 grad_norm=0.358504
Epoch 40/100 Iteration 100/234: loss=0.066800 lr=0.000020 grad_norm=1.482836
Epoch 40/100 Iteration 101/234: loss=0.065339 lr=0.000020 grad_norm=1.673806
Epoch 40/100 Iteration 102/234: loss=0.077622 lr=0.000020 grad_norm=0.516910
Epoch 40/100 Iteration 103/234: loss=0.064559 lr=0.000020 grad_norm=1.547213
Epoch 40/100 Iteration 104/234: loss=0.073187 lr=0.000020 grad_norm=1.493916
Epoch 40/100 Iteration 105/234: loss=0.076176 lr=0.000020 grad_norm=0.684943
Epoch 40/100 Iteration 106/234: loss=0.074112 lr=0.000020 grad_norm=1.716770
Epoch 40/100 Iteration 107/234: loss=0.075607 lr=0.000020 grad_norm=2.019018
Epoch 40/100 Iteration 108/234: loss=0.058502 lr=0.000020 grad_norm=0.911080
Epoch 40/100 Iteration 109/234: loss=0.068547 lr=0.000020 grad_norm=0.939652
Epoch 40/100 Iteration 110/234: loss=0.071911 lr=0.000020 grad_norm=1.235222
Epoch 40/100 Iteration 111/234: loss=0.069435 lr=0.000020 grad_norm=0.748182
Epoch 40/100 Iteration 112/234: loss=0.066792 lr=0.000020 grad_norm=1.029201
Epoch 40/100 Iteration 113/234: loss=0.070592 lr=0.000020 grad_norm=1.057976
Epoch 40/100 Iteration 114/234: loss=0.064146 lr=0.000020 grad_norm=0.504102
Epoch 40/100 Iteration 115/234: loss=0.070956 lr=0.000020 grad_norm=0.979668
Epoch 40/100 Iteration 116/234: loss=0.066628 lr=0.000020 grad_norm=0.812208
Epoch 40/100 Iteration 117/234: loss=0.061283 lr=0.000020 grad_norm=0.686462
Epoch 40/100 Iteration 118/234: loss=0.068226 lr=0.000020 grad_norm=0.911165
Epoch 40/100 Iteration 119/234: loss=0.075591 lr=0.000020 grad_norm=0.796708
Epoch 40/100 Iteration 120/234: loss=0.082035 lr=0.000020 grad_norm=0.515533
Epoch 40/100 Iteration 121/234: loss=0.072049 lr=0.000020 grad_norm=0.968423
Epoch 40/100 Iteration 122/234: loss=0.067076 lr=0.000020 grad_norm=0.752865
Epoch 40/100 Iteration 123/234: loss=0.073771 lr=0.000020 grad_norm=0.650125
Epoch 40/100 Iteration 124/234: loss=0.059079 lr=0.000020 grad_norm=0.778797
Epoch 40/100 Iteration 125/234: loss=0.072981 lr=0.000020 grad_norm=1.154808
Epoch 40/100 Iteration 126/234: loss=0.070620 lr=0.000020 grad_norm=0.833806
Epoch 40/100 Iteration 127/234: loss=0.059952 lr=0.000020 grad_norm=0.495718
Epoch 40/100 Iteration 128/234: loss=0.076808 lr=0.000020 grad_norm=0.836066
Epoch 40/100 Iteration 129/234: loss=0.073302 lr=0.000020 grad_norm=0.669239
Epoch 40/100 Iteration 130/234: loss=0.067842 lr=0.000020 grad_norm=0.404566
Epoch 40/100 Iteration 131/234: loss=0.076674 lr=0.000020 grad_norm=0.987782
Epoch 40/100 Iteration 132/234: loss=0.075546 lr=0.000020 grad_norm=1.108235
Epoch 40/100 Iteration 133/234: loss=0.072072 lr=0.000020 grad_norm=0.741034
Epoch 40/100 Iteration 134/234: loss=0.080506 lr=0.000020 grad_norm=0.397156
Epoch 40/100 Iteration 135/234: loss=0.073931 lr=0.000020 grad_norm=0.923161
Epoch 40/100 Iteration 136/234: loss=0.064511 lr=0.000020 grad_norm=0.990090
Epoch 40/100 Iteration 137/234: loss=0.077625 lr=0.000020 grad_norm=0.575072
Epoch 40/100 Iteration 138/234: loss=0.065119 lr=0.000020 grad_norm=0.961152
Epoch 40/100 Iteration 139/234: loss=0.056172 lr=0.000020 grad_norm=1.309780
Epoch 40/100 Iteration 140/234: loss=0.073646 lr=0.000020 grad_norm=0.595636
Epoch 40/100 Iteration 141/234: loss=0.072292 lr=0.000020 grad_norm=1.760910
Epoch 40/100 Iteration 142/234: loss=0.075941 lr=0.000020 grad_norm=2.535018
Epoch 40/100 Iteration 143/234: loss=0.061559 lr=0.000020 grad_norm=0.856056
Epoch 40/100 Iteration 144/234: loss=0.070320 lr=0.000020 grad_norm=1.461929
Epoch 40/100 Iteration 145/234: loss=0.076240 lr=0.000020 grad_norm=1.493465
Epoch 40/100 Iteration 146/234: loss=0.078610 lr=0.000020 grad_norm=0.469684
Epoch 40/100 Iteration 147/234: loss=0.075198 lr=0.000020 grad_norm=1.311448
Epoch 40/100 Iteration 148/234: loss=0.063292 lr=0.000020 grad_norm=0.938506
Epoch 40/100 Iteration 149/234: loss=0.073774 lr=0.000020 grad_norm=0.765696
Epoch 40/100 Iteration 150/234: loss=0.065320 lr=0.000020 grad_norm=1.743050
Epoch 40/100 Iteration 151/234: loss=0.071800 lr=0.000020 grad_norm=1.002002
Epoch 40/100 Iteration 152/234: loss=0.072753 lr=0.000020 grad_norm=1.037363
Epoch 40/100 Iteration 153/234: loss=0.072568 lr=0.000020 grad_norm=1.418824
Epoch 40/100 Iteration 154/234: loss=0.074913 lr=0.000020 grad_norm=0.626025
Epoch 40/100 Iteration 155/234: loss=0.065025 lr=0.000020 grad_norm=0.835301
Epoch 40/100 Iteration 156/234: loss=0.073872 lr=0.000020 grad_norm=0.754210
Epoch 40/100 Iteration 157/234: loss=0.072587 lr=0.000020 grad_norm=0.527095
Epoch 40/100 Iteration 158/234: loss=0.073030 lr=0.000020 grad_norm=0.884251
Epoch 40/100 Iteration 159/234: loss=0.076307 lr=0.000020 grad_norm=0.608630
Epoch 40/100 Iteration 160/234: loss=0.085617 lr=0.000020 grad_norm=0.613925
Epoch 40/100 Iteration 161/234: loss=0.066333 lr=0.000020 grad_norm=0.988881
Epoch 40/100 Iteration 162/234: loss=0.066554 lr=0.000020 grad_norm=1.184102
Epoch 40/100 Iteration 163/234: loss=0.066395 lr=0.000020 grad_norm=0.853018
Epoch 40/100 Iteration 164/234: loss=0.066036 lr=0.000020 grad_norm=0.483401
Epoch 40/100 Iteration 165/234: loss=0.073504 lr=0.000020 grad_norm=1.020361
Epoch 40/100 Iteration 166/234: loss=0.072270 lr=0.000020 grad_norm=0.848589
Epoch 40/100 Iteration 167/234: loss=0.076065 lr=0.000020 grad_norm=0.486833
Epoch 40/100 Iteration 168/234: loss=0.065032 lr=0.000020 grad_norm=0.506602
Epoch 40/100 Iteration 169/234: loss=0.074679 lr=0.000020 grad_norm=0.606721
Epoch 40/100 Iteration 170/234: loss=0.067384 lr=0.000020 grad_norm=0.592043
Epoch 40/100 Iteration 171/234: loss=0.057581 lr=0.000020 grad_norm=0.506737
Epoch 40/100 Iteration 172/234: loss=0.072169 lr=0.000020 grad_norm=0.546429
Epoch 40/100 Iteration 173/234: loss=0.072721 lr=0.000020 grad_norm=0.896610
Epoch 40/100 Iteration 174/234: loss=0.058265 lr=0.000020 grad_norm=0.566529
Epoch 40/100 Iteration 175/234: loss=0.076328 lr=0.000020 grad_norm=0.648255
Epoch 40/100 Iteration 176/234: loss=0.072944 lr=0.000020 grad_norm=0.819039
Epoch 40/100 Iteration 177/234: loss=0.070281 lr=0.000020 grad_norm=0.454910
Epoch 40/100 Iteration 178/234: loss=0.070695 lr=0.000020 grad_norm=0.583709
Epoch 40/100 Iteration 179/234: loss=0.076740 lr=0.000020 grad_norm=0.920049
Epoch 40/100 Iteration 180/234: loss=0.079354 lr=0.000020 grad_norm=0.801507
Epoch 40/100 Iteration 181/234: loss=0.068094 lr=0.000020 grad_norm=0.513924
Epoch 40/100 Iteration 182/234: loss=0.065052 lr=0.000020 grad_norm=0.411347
Epoch 40/100 Iteration 183/234: loss=0.060485 lr=0.000020 grad_norm=0.495537
Epoch 40/100 Iteration 184/234: loss=0.055923 lr=0.000020 grad_norm=0.652260
Epoch 40/100 Iteration 185/234: loss=0.072515 lr=0.000020 grad_norm=0.625048
Epoch 40/100 Iteration 186/234: loss=0.073264 lr=0.000020 grad_norm=0.652915
Epoch 40/100 Iteration 187/234: loss=0.059989 lr=0.000020 grad_norm=0.591644
Epoch 40/100 Iteration 188/234: loss=0.077511 lr=0.000020 grad_norm=0.599246
Epoch 40/100 Iteration 189/234: loss=0.071004 lr=0.000020 grad_norm=0.666094
Epoch 40/100 Iteration 190/234: loss=0.073781 lr=0.000020 grad_norm=0.611535
Epoch 40/100 Iteration 191/234: loss=0.073139 lr=0.000020 grad_norm=0.480788
Epoch 40/100 Iteration 192/234: loss=0.072027 lr=0.000020 grad_norm=0.576196
Epoch 40/100 Iteration 193/234: loss=0.063982 lr=0.000020 grad_norm=0.426722
Epoch 40/100 Iteration 194/234: loss=0.078828 lr=0.000020 grad_norm=0.618901
Epoch 40/100 Iteration 195/234: loss=0.070182 lr=0.000020 grad_norm=0.718887
Epoch 40/100 Iteration 196/234: loss=0.072111 lr=0.000020 grad_norm=0.620816
Epoch 40/100 Iteration 197/234: loss=0.076866 lr=0.000020 grad_norm=0.492606
Epoch 40/100 Iteration 198/234: loss=0.064133 lr=0.000020 grad_norm=0.972630
Epoch 40/100 Iteration 199/234: loss=0.057333 lr=0.000020 grad_norm=1.178434
Epoch 40/100 Iteration 200/234: loss=0.065273 lr=0.000020 grad_norm=0.964761
Epoch 40/100 Iteration 201/234: loss=0.074617 lr=0.000020 grad_norm=0.523217
Epoch 40/100 Iteration 202/234: loss=0.065180 lr=0.000020 grad_norm=0.570926
Epoch 40/100 Iteration 203/234: loss=0.070052 lr=0.000020 grad_norm=0.877840
Epoch 40/100 Iteration 204/234: loss=0.092276 lr=0.000020 grad_norm=0.539387
Epoch 40/100 Iteration 205/234: loss=0.081303 lr=0.000020 grad_norm=0.644583
Epoch 40/100 Iteration 206/234: loss=0.070786 lr=0.000020 grad_norm=0.880975
Epoch 40/100 Iteration 207/234: loss=0.070779 lr=0.000020 grad_norm=0.582236
Epoch 40/100 Iteration 208/234: loss=0.079743 lr=0.000020 grad_norm=0.945641
Epoch 40/100 Iteration 209/234: loss=0.068748 lr=0.000020 grad_norm=1.618573
Epoch 40/100 Iteration 210/234: loss=0.065881 lr=0.000020 grad_norm=1.242944
Epoch 40/100 Iteration 211/234: loss=0.072742 lr=0.000020 grad_norm=0.559602
Epoch 40/100 Iteration 212/234: loss=0.065903 lr=0.000020 grad_norm=1.385825
Epoch 40/100 Iteration 213/234: loss=0.070838 lr=0.000020 grad_norm=0.814504
Epoch 40/100 Iteration 214/234: loss=0.069415 lr=0.000020 grad_norm=0.720570
Epoch 40/100 Iteration 215/234: loss=0.070726 lr=0.000020 grad_norm=1.107289
Epoch 40/100 Iteration 216/234: loss=0.071195 lr=0.000020 grad_norm=0.770572
Epoch 40/100 Iteration 217/234: loss=0.060705 lr=0.000020 grad_norm=0.731448
Epoch 40/100 Iteration 218/234: loss=0.078431 lr=0.000020 grad_norm=0.695564
Epoch 40/100 Iteration 219/234: loss=0.071888 lr=0.000020 grad_norm=0.629885
Epoch 40/100 Iteration 220/234: loss=0.066584 lr=0.000020 grad_norm=0.757277
Epoch 40/100 Iteration 221/234: loss=0.060242 lr=0.000020 grad_norm=0.649344
Epoch 40/100 Iteration 222/234: loss=0.061826 lr=0.000020 grad_norm=0.416582
Epoch 40/100 Iteration 223/234: loss=0.065893 lr=0.000020 grad_norm=0.960369
Epoch 40/100 Iteration 224/234: loss=0.072473 lr=0.000020 grad_norm=1.040034
Epoch 40/100 Iteration 225/234: loss=0.080009 lr=0.000020 grad_norm=0.960098
Epoch 40/100 Iteration 226/234: loss=0.083787 lr=0.000020 grad_norm=0.958560
Epoch 40/100 Iteration 227/234: loss=0.079464 lr=0.000020 grad_norm=0.887021
Epoch 40/100 Iteration 228/234: loss=0.059100 lr=0.000020 grad_norm=0.599148
Epoch 40/100 Iteration 229/234: loss=0.074113 lr=0.000020 grad_norm=0.821849
Epoch 40/100 Iteration 230/234: loss=0.070299 lr=0.000020 grad_norm=0.571438
Epoch 40/100 Iteration 231/234: loss=0.066839 lr=0.000020 grad_norm=0.805889
Epoch 40/100 Iteration 232/234: loss=0.061394 lr=0.000020 grad_norm=0.976491
Epoch 40/100 Iteration 233/234: loss=0.062878 lr=0.000020 grad_norm=0.666434
Epoch 40/100 Iteration 234/234: loss=0.064226 lr=0.000020 grad_norm=0.940001
Epoch 40/100 finished. Avg Loss: 0.070416
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 41/100 Iteration 1/234: loss=0.072557 lr=0.000020 grad_norm=1.049884
Epoch 41/100 Iteration 2/234: loss=0.067093 lr=0.000020 grad_norm=0.732386
Epoch 41/100 Iteration 3/234: loss=0.065002 lr=0.000020 grad_norm=0.543128
Epoch 41/100 Iteration 4/234: loss=0.077956 lr=0.000020 grad_norm=1.226879
Epoch 41/100 Iteration 5/234: loss=0.068417 lr=0.000020 grad_norm=1.194396
Epoch 41/100 Iteration 6/234: loss=0.076288 lr=0.000020 grad_norm=0.626430
Epoch 41/100 Iteration 7/234: loss=0.073294 lr=0.000020 grad_norm=1.670051
Epoch 41/100 Iteration 8/234: loss=0.070405 lr=0.000020 grad_norm=2.581779
Epoch 41/100 Iteration 9/234: loss=0.073488 lr=0.000020 grad_norm=1.555788
Epoch 41/100 Iteration 10/234: loss=0.075982 lr=0.000020 grad_norm=1.389926
Epoch 41/100 Iteration 11/234: loss=0.065322 lr=0.000020 grad_norm=1.633569
Epoch 41/100 Iteration 12/234: loss=0.067942 lr=0.000020 grad_norm=0.913422
Epoch 41/100 Iteration 13/234: loss=0.071724 lr=0.000020 grad_norm=2.317369
Epoch 41/100 Iteration 14/234: loss=0.063832 lr=0.000020 grad_norm=1.640448
Epoch 41/100 Iteration 15/234: loss=0.069166 lr=0.000020 grad_norm=1.008232
Epoch 41/100 Iteration 16/234: loss=0.063421 lr=0.000020 grad_norm=1.701305
Epoch 41/100 Iteration 17/234: loss=0.051014 lr=0.000020 grad_norm=1.116286
Epoch 41/100 Iteration 18/234: loss=0.066452 lr=0.000020 grad_norm=0.905823
Epoch 41/100 Iteration 19/234: loss=0.079568 lr=0.000020 grad_norm=1.575156
Epoch 41/100 Iteration 20/234: loss=0.074041 lr=0.000020 grad_norm=0.745194
Epoch 41/100 Iteration 21/234: loss=0.063052 lr=0.000020 grad_norm=1.151684
Epoch 41/100 Iteration 22/234: loss=0.073697 lr=0.000020 grad_norm=1.090018
Epoch 41/100 Iteration 23/234: loss=0.073358 lr=0.000020 grad_norm=0.485886
Epoch 41/100 Iteration 24/234: loss=0.075736 lr=0.000020 grad_norm=1.077854
Epoch 41/100 Iteration 25/234: loss=0.076429 lr=0.000020 grad_norm=0.619763
Epoch 41/100 Iteration 26/234: loss=0.073396 lr=0.000020 grad_norm=0.809271
Epoch 41/100 Iteration 27/234: loss=0.063656 lr=0.000020 grad_norm=0.848529
Epoch 41/100 Iteration 28/234: loss=0.074263 lr=0.000020 grad_norm=0.589141
Epoch 41/100 Iteration 29/234: loss=0.079138 lr=0.000020 grad_norm=1.276022
Epoch 41/100 Iteration 30/234: loss=0.072434 lr=0.000020 grad_norm=0.827013
Epoch 41/100 Iteration 31/234: loss=0.071817 lr=0.000020 grad_norm=0.684598
Epoch 41/100 Iteration 32/234: loss=0.075015 lr=0.000020 grad_norm=1.174748
Epoch 41/100 Iteration 33/234: loss=0.068598 lr=0.000020 grad_norm=0.734127
Epoch 41/100 Iteration 34/234: loss=0.077659 lr=0.000020 grad_norm=0.957336
Epoch 41/100 Iteration 35/234: loss=0.066553 lr=0.000020 grad_norm=1.198909
Epoch 41/100 Iteration 36/234: loss=0.063567 lr=0.000020 grad_norm=0.464091
Epoch 41/100 Iteration 37/234: loss=0.070513 lr=0.000020 grad_norm=0.791304
Epoch 41/100 Iteration 38/234: loss=0.072577 lr=0.000020 grad_norm=0.509993
Epoch 41/100 Iteration 39/234: loss=0.069578 lr=0.000020 grad_norm=0.740225
Epoch 41/100 Iteration 40/234: loss=0.070316 lr=0.000020 grad_norm=0.858215
Epoch 41/100 Iteration 41/234: loss=0.074155 lr=0.000020 grad_norm=0.601068
Epoch 41/100 Iteration 42/234: loss=0.071960 lr=0.000020 grad_norm=0.916379
Epoch 41/100 Iteration 43/234: loss=0.067520 lr=0.000020 grad_norm=0.452498
Epoch 41/100 Iteration 44/234: loss=0.069197 lr=0.000020 grad_norm=0.519679
Epoch 41/100 Iteration 45/234: loss=0.064983 lr=0.000020 grad_norm=0.639682
Epoch 41/100 Iteration 46/234: loss=0.064429 lr=0.000020 grad_norm=0.560956
Epoch 41/100 Iteration 47/234: loss=0.072691 lr=0.000020 grad_norm=1.103733
Epoch 41/100 Iteration 48/234: loss=0.074363 lr=0.000020 grad_norm=0.970388
Epoch 41/100 Iteration 49/234: loss=0.067627 lr=0.000020 grad_norm=0.362674
Epoch 41/100 Iteration 50/234: loss=0.061310 lr=0.000020 grad_norm=0.767165
Epoch 41/100 Iteration 51/234: loss=0.062070 lr=0.000020 grad_norm=0.378684
Epoch 41/100 Iteration 52/234: loss=0.072584 lr=0.000020 grad_norm=0.875046
Epoch 41/100 Iteration 53/234: loss=0.064845 lr=0.000020 grad_norm=1.261595
Epoch 41/100 Iteration 54/234: loss=0.070532 lr=0.000020 grad_norm=1.145941
Epoch 41/100 Iteration 55/234: loss=0.072510 lr=0.000020 grad_norm=0.721830
Epoch 41/100 Iteration 56/234: loss=0.062949 lr=0.000020 grad_norm=0.540896
Epoch 41/100 Iteration 57/234: loss=0.067762 lr=0.000020 grad_norm=0.351798
Epoch 41/100 Iteration 58/234: loss=0.069320 lr=0.000020 grad_norm=0.540719
Epoch 41/100 Iteration 59/234: loss=0.074700 lr=0.000020 grad_norm=0.557902
Epoch 41/100 Iteration 60/234: loss=0.070400 lr=0.000020 grad_norm=0.453350
Epoch 41/100 Iteration 61/234: loss=0.053135 lr=0.000020 grad_norm=0.292631
Epoch 41/100 Iteration 62/234: loss=0.075987 lr=0.000020 grad_norm=0.379730
Epoch 41/100 Iteration 63/234: loss=0.064791 lr=0.000020 grad_norm=0.350509
Epoch 41/100 Iteration 64/234: loss=0.065596 lr=0.000020 grad_norm=0.402033
Epoch 41/100 Iteration 65/234: loss=0.062947 lr=0.000020 grad_norm=0.439341
Epoch 41/100 Iteration 66/234: loss=0.080385 lr=0.000020 grad_norm=0.898757
Epoch 41/100 Iteration 67/234: loss=0.063338 lr=0.000020 grad_norm=0.801993
Epoch 41/100 Iteration 68/234: loss=0.060759 lr=0.000020 grad_norm=0.520773
Epoch 41/100 Iteration 69/234: loss=0.062016 lr=0.000020 grad_norm=0.515655
Epoch 41/100 Iteration 70/234: loss=0.069305 lr=0.000020 grad_norm=0.998606
Epoch 41/100 Iteration 71/234: loss=0.069398 lr=0.000020 grad_norm=1.167105
Epoch 41/100 Iteration 72/234: loss=0.068442 lr=0.000020 grad_norm=0.881265
Epoch 41/100 Iteration 73/234: loss=0.065327 lr=0.000020 grad_norm=0.535512
Epoch 41/100 Iteration 74/234: loss=0.076573 lr=0.000020 grad_norm=0.498160
Epoch 41/100 Iteration 75/234: loss=0.061076 lr=0.000020 grad_norm=0.652013
Epoch 41/100 Iteration 76/234: loss=0.060953 lr=0.000020 grad_norm=0.610958
Epoch 41/100 Iteration 77/234: loss=0.066579 lr=0.000020 grad_norm=0.522556
Epoch 41/100 Iteration 78/234: loss=0.074479 lr=0.000020 grad_norm=0.957289
Epoch 41/100 Iteration 79/234: loss=0.074112 lr=0.000020 grad_norm=1.861936
Epoch 41/100 Iteration 80/234: loss=0.073926 lr=0.000020 grad_norm=2.018945
Epoch 41/100 Iteration 81/234: loss=0.073049 lr=0.000020 grad_norm=0.506451
Epoch 41/100 Iteration 82/234: loss=0.073348 lr=0.000020 grad_norm=1.675198
Epoch 41/100 Iteration 83/234: loss=0.066359 lr=0.000020 grad_norm=1.932466
Epoch 41/100 Iteration 84/234: loss=0.073346 lr=0.000020 grad_norm=0.648942
Epoch 41/100 Iteration 85/234: loss=0.067643 lr=0.000020 grad_norm=1.624952
Epoch 41/100 Iteration 86/234: loss=0.064323 lr=0.000020 grad_norm=1.243419
Epoch 41/100 Iteration 87/234: loss=0.070468 lr=0.000020 grad_norm=0.982210
Epoch 41/100 Iteration 88/234: loss=0.069083 lr=0.000020 grad_norm=1.678509
Epoch 41/100 Iteration 89/234: loss=0.071807 lr=0.000020 grad_norm=0.839786
Epoch 41/100 Iteration 90/234: loss=0.067396 lr=0.000020 grad_norm=1.104078
Epoch 41/100 Iteration 91/234: loss=0.063445 lr=0.000020 grad_norm=1.416708
Epoch 41/100 Iteration 92/234: loss=0.073062 lr=0.000020 grad_norm=0.459524
Epoch 41/100 Iteration 93/234: loss=0.064929 lr=0.000020 grad_norm=1.069570
Epoch 41/100 Iteration 94/234: loss=0.069144 lr=0.000020 grad_norm=0.544331
Epoch 41/100 Iteration 95/234: loss=0.068791 lr=0.000020 grad_norm=0.894224
Epoch 41/100 Iteration 96/234: loss=0.070305 lr=0.000020 grad_norm=1.221876
Epoch 41/100 Iteration 97/234: loss=0.064495 lr=0.000020 grad_norm=0.444688
Epoch 41/100 Iteration 98/234: loss=0.072287 lr=0.000020 grad_norm=1.241792
Epoch 41/100 Iteration 99/234: loss=0.061394 lr=0.000020 grad_norm=0.965773
Epoch 41/100 Iteration 100/234: loss=0.072078 lr=0.000020 grad_norm=0.515197
Epoch 41/100 Iteration 101/234: loss=0.066335 lr=0.000020 grad_norm=1.299607
Epoch 41/100 Iteration 102/234: loss=0.063160 lr=0.000020 grad_norm=0.924397
Epoch 41/100 Iteration 103/234: loss=0.071723 lr=0.000020 grad_norm=0.627128
Epoch 41/100 Iteration 104/234: loss=0.057115 lr=0.000020 grad_norm=1.120105
Epoch 41/100 Iteration 105/234: loss=0.076031 lr=0.000020 grad_norm=0.891897
Epoch 41/100 Iteration 106/234: loss=0.065824 lr=0.000020 grad_norm=0.825361
Epoch 41/100 Iteration 107/234: loss=0.070457 lr=0.000020 grad_norm=0.752148
Epoch 41/100 Iteration 108/234: loss=0.060416 lr=0.000020 grad_norm=0.779446
Epoch 41/100 Iteration 109/234: loss=0.070863 lr=0.000020 grad_norm=0.435785
Epoch 41/100 Iteration 110/234: loss=0.073923 lr=0.000020 grad_norm=1.080758
Epoch 41/100 Iteration 111/234: loss=0.068398 lr=0.000020 grad_norm=1.528697
Epoch 41/100 Iteration 112/234: loss=0.059065 lr=0.000020 grad_norm=1.204941
Epoch 41/100 Iteration 113/234: loss=0.076232 lr=0.000020 grad_norm=0.462784
Epoch 41/100 Iteration 114/234: loss=0.074721 lr=0.000020 grad_norm=1.801616
Epoch 41/100 Iteration 115/234: loss=0.074230 lr=0.000020 grad_norm=1.846066
Epoch 41/100 Iteration 116/234: loss=0.068176 lr=0.000020 grad_norm=0.702898
Epoch 41/100 Iteration 117/234: loss=0.075108 lr=0.000020 grad_norm=0.824565
Epoch 41/100 Iteration 118/234: loss=0.065233 lr=0.000020 grad_norm=1.187985
Epoch 41/100 Iteration 119/234: loss=0.064397 lr=0.000020 grad_norm=0.549036
Epoch 41/100 Iteration 120/234: loss=0.062575 lr=0.000020 grad_norm=1.247554
Epoch 41/100 Iteration 121/234: loss=0.064637 lr=0.000020 grad_norm=0.914521
Epoch 41/100 Iteration 122/234: loss=0.075695 lr=0.000020 grad_norm=0.812434
Epoch 41/100 Iteration 123/234: loss=0.064388 lr=0.000020 grad_norm=0.924250
Epoch 41/100 Iteration 124/234: loss=0.062746 lr=0.000020 grad_norm=0.636438
Epoch 41/100 Iteration 125/234: loss=0.061408 lr=0.000020 grad_norm=0.898373
Epoch 41/100 Iteration 126/234: loss=0.064821 lr=0.000020 grad_norm=0.767758
Epoch 41/100 Iteration 127/234: loss=0.069574 lr=0.000020 grad_norm=0.792154
Epoch 41/100 Iteration 128/234: loss=0.074957 lr=0.000020 grad_norm=0.707050
Epoch 41/100 Iteration 129/234: loss=0.065060 lr=0.000020 grad_norm=0.362716
Epoch 41/100 Iteration 130/234: loss=0.064516 lr=0.000020 grad_norm=0.644096
Epoch 41/100 Iteration 131/234: loss=0.063461 lr=0.000020 grad_norm=0.871781
Epoch 41/100 Iteration 132/234: loss=0.063333 lr=0.000020 grad_norm=0.628630
Epoch 41/100 Iteration 133/234: loss=0.070645 lr=0.000020 grad_norm=0.726175
Epoch 41/100 Iteration 134/234: loss=0.055852 lr=0.000020 grad_norm=0.839530
Epoch 41/100 Iteration 135/234: loss=0.062890 lr=0.000020 grad_norm=0.519626
Epoch 41/100 Iteration 136/234: loss=0.075296 lr=0.000020 grad_norm=0.917141
Epoch 41/100 Iteration 137/234: loss=0.065829 lr=0.000020 grad_norm=1.655435
Epoch 41/100 Iteration 138/234: loss=0.063859 lr=0.000020 grad_norm=1.746055
Epoch 41/100 Iteration 139/234: loss=0.076533 lr=0.000020 grad_norm=0.826993
Epoch 41/100 Iteration 140/234: loss=0.049828 lr=0.000020 grad_norm=0.938690
Epoch 41/100 Iteration 141/234: loss=0.067434 lr=0.000020 grad_norm=1.218843
Epoch 41/100 Iteration 142/234: loss=0.066086 lr=0.000020 grad_norm=0.414897
Epoch 41/100 Iteration 143/234: loss=0.066164 lr=0.000020 grad_norm=1.667044
Epoch 41/100 Iteration 144/234: loss=0.071435 lr=0.000020 grad_norm=2.081956
Epoch 41/100 Iteration 145/234: loss=0.070019 lr=0.000020 grad_norm=1.143609
Epoch 41/100 Iteration 146/234: loss=0.056211 lr=0.000020 grad_norm=0.563464
Epoch 41/100 Iteration 147/234: loss=0.061445 lr=0.000020 grad_norm=0.602532
Epoch 41/100 Iteration 148/234: loss=0.065650 lr=0.000020 grad_norm=0.553749
Epoch 41/100 Iteration 149/234: loss=0.062545 lr=0.000020 grad_norm=0.804054
Epoch 41/100 Iteration 150/234: loss=0.072640 lr=0.000020 grad_norm=0.740133
Epoch 41/100 Iteration 151/234: loss=0.073832 lr=0.000020 grad_norm=0.522891
Epoch 41/100 Iteration 152/234: loss=0.061481 lr=0.000020 grad_norm=0.541897
Epoch 41/100 Iteration 153/234: loss=0.066764 lr=0.000020 grad_norm=0.693938
Epoch 41/100 Iteration 154/234: loss=0.079791 lr=0.000020 grad_norm=0.566992
Epoch 41/100 Iteration 155/234: loss=0.061739 lr=0.000020 grad_norm=0.817884
Epoch 41/100 Iteration 156/234: loss=0.077190 lr=0.000020 grad_norm=0.916221
Epoch 41/100 Iteration 157/234: loss=0.066456 lr=0.000020 grad_norm=0.893124
Epoch 41/100 Iteration 158/234: loss=0.068098 lr=0.000020 grad_norm=0.630628
Epoch 41/100 Iteration 159/234: loss=0.067392 lr=0.000020 grad_norm=0.420236
Epoch 41/100 Iteration 160/234: loss=0.064812 lr=0.000020 grad_norm=0.716537
Epoch 41/100 Iteration 161/234: loss=0.071802 lr=0.000020 grad_norm=0.616357
Epoch 41/100 Iteration 162/234: loss=0.068162 lr=0.000020 grad_norm=0.461971
Epoch 41/100 Iteration 163/234: loss=0.080976 lr=0.000020 grad_norm=0.808398
Epoch 41/100 Iteration 164/234: loss=0.069663 lr=0.000020 grad_norm=0.675289
Epoch 41/100 Iteration 165/234: loss=0.071608 lr=0.000020 grad_norm=0.387545
Epoch 41/100 Iteration 166/234: loss=0.068360 lr=0.000020 grad_norm=0.643908
Epoch 41/100 Iteration 167/234: loss=0.072110 lr=0.000020 grad_norm=0.761380
Epoch 41/100 Iteration 168/234: loss=0.067001 lr=0.000020 grad_norm=0.821604
Epoch 41/100 Iteration 169/234: loss=0.072477 lr=0.000020 grad_norm=0.611779
Epoch 41/100 Iteration 170/234: loss=0.075554 lr=0.000020 grad_norm=0.970769
Epoch 41/100 Iteration 171/234: loss=0.079495 lr=0.000020 grad_norm=2.105956
Epoch 41/100 Iteration 172/234: loss=0.068127 lr=0.000020 grad_norm=1.997615
Epoch 41/100 Iteration 173/234: loss=0.072287 lr=0.000020 grad_norm=0.401896
Epoch 41/100 Iteration 174/234: loss=0.060737 lr=0.000020 grad_norm=1.653921
Epoch 41/100 Iteration 175/234: loss=0.055506 lr=0.000020 grad_norm=1.276237
Epoch 41/100 Iteration 176/234: loss=0.061889 lr=0.000020 grad_norm=0.833226
Epoch 41/100 Iteration 177/234: loss=0.066024 lr=0.000020 grad_norm=1.530162
Epoch 41/100 Iteration 178/234: loss=0.072386 lr=0.000020 grad_norm=0.594236
Epoch 41/100 Iteration 179/234: loss=0.062642 lr=0.000020 grad_norm=0.848038
Epoch 41/100 Iteration 180/234: loss=0.072127 lr=0.000020 grad_norm=0.910469
Epoch 41/100 Iteration 181/234: loss=0.083898 lr=0.000020 grad_norm=0.432455
Epoch 41/100 Iteration 182/234: loss=0.073361 lr=0.000020 grad_norm=0.934469
Epoch 41/100 Iteration 183/234: loss=0.075842 lr=0.000020 grad_norm=0.704361
Epoch 41/100 Iteration 184/234: loss=0.064182 lr=0.000020 grad_norm=0.512200
Epoch 41/100 Iteration 185/234: loss=0.071324 lr=0.000020 grad_norm=0.401337
Epoch 41/100 Iteration 186/234: loss=0.072913 lr=0.000020 grad_norm=0.535728
Epoch 41/100 Iteration 187/234: loss=0.075795 lr=0.000020 grad_norm=0.382458
Epoch 41/100 Iteration 188/234: loss=0.066402 lr=0.000020 grad_norm=1.106806
Epoch 41/100 Iteration 189/234: loss=0.060497 lr=0.000020 grad_norm=1.160339
Epoch 41/100 Iteration 190/234: loss=0.065634 lr=0.000020 grad_norm=0.434594
Epoch 41/100 Iteration 191/234: loss=0.063044 lr=0.000020 grad_norm=0.847062
Epoch 41/100 Iteration 192/234: loss=0.063021 lr=0.000020 grad_norm=0.953818
Epoch 41/100 Iteration 193/234: loss=0.069054 lr=0.000020 grad_norm=0.680360
Epoch 41/100 Iteration 194/234: loss=0.070930 lr=0.000020 grad_norm=0.705479
Epoch 41/100 Iteration 195/234: loss=0.062142 lr=0.000020 grad_norm=0.476382
Epoch 41/100 Iteration 196/234: loss=0.068761 lr=0.000020 grad_norm=0.530444
Epoch 41/100 Iteration 197/234: loss=0.064731 lr=0.000020 grad_norm=0.483186
Epoch 41/100 Iteration 198/234: loss=0.061626 lr=0.000020 grad_norm=0.936605
Epoch 41/100 Iteration 199/234: loss=0.060716 lr=0.000020 grad_norm=0.796210
Epoch 41/100 Iteration 200/234: loss=0.070687 lr=0.000020 grad_norm=0.512850
Epoch 41/100 Iteration 201/234: loss=0.065276 lr=0.000020 grad_norm=1.095247
Epoch 41/100 Iteration 202/234: loss=0.068799 lr=0.000020 grad_norm=1.083448
Epoch 41/100 Iteration 203/234: loss=0.060644 lr=0.000020 grad_norm=0.552407
Epoch 41/100 Iteration 204/234: loss=0.068509 lr=0.000020 grad_norm=1.473276
Epoch 41/100 Iteration 205/234: loss=0.073372 lr=0.000020 grad_norm=2.197037
Epoch 41/100 Iteration 206/234: loss=0.065024 lr=0.000020 grad_norm=1.880759
Epoch 41/100 Iteration 207/234: loss=0.069093 lr=0.000020 grad_norm=0.465382
Epoch 41/100 Iteration 208/234: loss=0.063077 lr=0.000020 grad_norm=1.312679
Epoch 41/100 Iteration 209/234: loss=0.062436 lr=0.000020 grad_norm=0.795499
Epoch 41/100 Iteration 210/234: loss=0.071155 lr=0.000020 grad_norm=1.459376
Epoch 41/100 Iteration 211/234: loss=0.070041 lr=0.000020 grad_norm=1.755332
Epoch 41/100 Iteration 212/234: loss=0.073327 lr=0.000020 grad_norm=0.810098
Epoch 41/100 Iteration 213/234: loss=0.063647 lr=0.000020 grad_norm=1.318856
Epoch 41/100 Iteration 214/234: loss=0.069959 lr=0.000020 grad_norm=0.989463
Epoch 41/100 Iteration 215/234: loss=0.063327 lr=0.000020 grad_norm=0.821693
Epoch 41/100 Iteration 216/234: loss=0.076722 lr=0.000020 grad_norm=1.518947
Epoch 41/100 Iteration 217/234: loss=0.073595 lr=0.000020 grad_norm=1.466089
Epoch 41/100 Iteration 218/234: loss=0.075817 lr=0.000020 grad_norm=0.664259
Epoch 41/100 Iteration 219/234: loss=0.069652 lr=0.000020 grad_norm=1.580422
Epoch 41/100 Iteration 220/234: loss=0.062906 lr=0.000020 grad_norm=1.537186
Epoch 41/100 Iteration 221/234: loss=0.062561 lr=0.000020 grad_norm=0.684395
Epoch 41/100 Iteration 222/234: loss=0.061579 lr=0.000020 grad_norm=0.883957
Epoch 41/100 Iteration 223/234: loss=0.067915 lr=0.000020 grad_norm=1.096951
Epoch 41/100 Iteration 224/234: loss=0.073952 lr=0.000020 grad_norm=0.752609
Epoch 41/100 Iteration 225/234: loss=0.064678 lr=0.000020 grad_norm=0.591112
Epoch 41/100 Iteration 226/234: loss=0.075652 lr=0.000020 grad_norm=0.755503
Epoch 41/100 Iteration 227/234: loss=0.062307 lr=0.000020 grad_norm=0.871779
Epoch 41/100 Iteration 228/234: loss=0.072569 lr=0.000020 grad_norm=0.512765
Epoch 41/100 Iteration 229/234: loss=0.067469 lr=0.000020 grad_norm=1.325393
Epoch 41/100 Iteration 230/234: loss=0.067704 lr=0.000020 grad_norm=1.254777
Epoch 41/100 Iteration 231/234: loss=0.074625 lr=0.000020 grad_norm=0.791093
Epoch 41/100 Iteration 232/234: loss=0.060013 lr=0.000020 grad_norm=1.486231
Epoch 41/100 Iteration 233/234: loss=0.066579 lr=0.000020 grad_norm=0.620300
Epoch 41/100 Iteration 234/234: loss=0.065540 lr=0.000020 grad_norm=1.287692
Epoch 41/100 finished. Avg Loss: 0.068443
Epoch 42/100 Iteration 1/234: loss=0.069546 lr=0.000020 grad_norm=1.123467
Epoch 42/100 Iteration 2/234: loss=0.056186 lr=0.000020 grad_norm=0.442547
Epoch 42/100 Iteration 3/234: loss=0.064222 lr=0.000020 grad_norm=1.206267
Epoch 42/100 Iteration 4/234: loss=0.066423 lr=0.000020 grad_norm=1.246777
Epoch 42/100 Iteration 5/234: loss=0.062018 lr=0.000020 grad_norm=0.637072
Epoch 42/100 Iteration 6/234: loss=0.070595 lr=0.000020 grad_norm=1.098610
Epoch 42/100 Iteration 7/234: loss=0.076412 lr=0.000020 grad_norm=1.322562
Epoch 42/100 Iteration 8/234: loss=0.070763 lr=0.000020 grad_norm=1.148399
Epoch 42/100 Iteration 9/234: loss=0.064908 lr=0.000020 grad_norm=0.573737
Epoch 42/100 Iteration 10/234: loss=0.067221 lr=0.000020 grad_norm=0.694402
Epoch 42/100 Iteration 11/234: loss=0.068442 lr=0.000020 grad_norm=1.036186
Epoch 42/100 Iteration 12/234: loss=0.061967 lr=0.000020 grad_norm=0.766547
Epoch 42/100 Iteration 13/234: loss=0.071832 lr=0.000020 grad_norm=0.496518
Epoch 42/100 Iteration 14/234: loss=0.069667 lr=0.000020 grad_norm=1.084822
Epoch 42/100 Iteration 15/234: loss=0.055133 lr=0.000020 grad_norm=1.777135
Epoch 42/100 Iteration 16/234: loss=0.069474 lr=0.000020 grad_norm=1.425700
Epoch 42/100 Iteration 17/234: loss=0.061919 lr=0.000020 grad_norm=0.618585
Epoch 42/100 Iteration 18/234: loss=0.066222 lr=0.000020 grad_norm=2.009072
Epoch 42/100 Iteration 19/234: loss=0.072412 lr=0.000020 grad_norm=1.734031
Epoch 42/100 Iteration 20/234: loss=0.062426 lr=0.000020 grad_norm=0.541215
Epoch 42/100 Iteration 21/234: loss=0.072624 lr=0.000020 grad_norm=1.248271
Epoch 42/100 Iteration 22/234: loss=0.060991 lr=0.000020 grad_norm=0.875826
Epoch 42/100 Iteration 23/234: loss=0.069345 lr=0.000020 grad_norm=0.704998
Epoch 42/100 Iteration 24/234: loss=0.068543 lr=0.000020 grad_norm=0.990786
Epoch 42/100 Iteration 25/234: loss=0.073544 lr=0.000020 grad_norm=0.528144
Epoch 42/100 Iteration 26/234: loss=0.062582 lr=0.000020 grad_norm=0.992765
Epoch 42/100 Iteration 27/234: loss=0.056183 lr=0.000020 grad_norm=0.823787
Epoch 42/100 Iteration 28/234: loss=0.066389 lr=0.000020 grad_norm=0.793374
Epoch 42/100 Iteration 29/234: loss=0.058134 lr=0.000020 grad_norm=0.691434
Epoch 42/100 Iteration 30/234: loss=0.067023 lr=0.000020 grad_norm=0.935661
Epoch 42/100 Iteration 31/234: loss=0.065693 lr=0.000020 grad_norm=1.533110
Epoch 42/100 Iteration 32/234: loss=0.073930 lr=0.000020 grad_norm=1.039475
Epoch 42/100 Iteration 33/234: loss=0.073408 lr=0.000020 grad_norm=1.053932
Epoch 42/100 Iteration 34/234: loss=0.063020 lr=0.000020 grad_norm=0.753497
Epoch 42/100 Iteration 35/234: loss=0.065637 lr=0.000020 grad_norm=0.541930
Epoch 42/100 Iteration 36/234: loss=0.060488 lr=0.000020 grad_norm=0.766304
Epoch 42/100 Iteration 37/234: loss=0.067744 lr=0.000020 grad_norm=0.665952
Epoch 42/100 Iteration 38/234: loss=0.059151 lr=0.000020 grad_norm=0.518558
Epoch 42/100 Iteration 39/234: loss=0.069764 lr=0.000020 grad_norm=0.818779
Epoch 42/100 Iteration 40/234: loss=0.064772 lr=0.000020 grad_norm=0.662540
Epoch 42/100 Iteration 41/234: loss=0.064763 lr=0.000020 grad_norm=0.539185
Epoch 42/100 Iteration 42/234: loss=0.064225 lr=0.000020 grad_norm=0.960413
Epoch 42/100 Iteration 43/234: loss=0.063447 lr=0.000020 grad_norm=0.520205
Epoch 42/100 Iteration 44/234: loss=0.062771 lr=0.000020 grad_norm=0.764239
Epoch 42/100 Iteration 45/234: loss=0.078128 lr=0.000020 grad_norm=0.550417
Epoch 42/100 Iteration 46/234: loss=0.066691 lr=0.000020 grad_norm=0.451828
Epoch 42/100 Iteration 47/234: loss=0.057505 lr=0.000020 grad_norm=0.540462
Epoch 42/100 Iteration 48/234: loss=0.066185 lr=0.000020 grad_norm=0.866053
Epoch 42/100 Iteration 49/234: loss=0.072978 lr=0.000020 grad_norm=1.043024
Epoch 42/100 Iteration 50/234: loss=0.064847 lr=0.000020 grad_norm=0.557161
Epoch 42/100 Iteration 51/234: loss=0.065395 lr=0.000020 grad_norm=0.487451
Epoch 42/100 Iteration 52/234: loss=0.066330 lr=0.000020 grad_norm=0.519734
Epoch 42/100 Iteration 53/234: loss=0.063781 lr=0.000020 grad_norm=0.735694
Epoch 42/100 Iteration 54/234: loss=0.065695 lr=0.000020 grad_norm=0.589665
Epoch 42/100 Iteration 55/234: loss=0.066501 lr=0.000020 grad_norm=0.432936
Epoch 42/100 Iteration 56/234: loss=0.065154 lr=0.000020 grad_norm=0.503720
Epoch 42/100 Iteration 57/234: loss=0.062092 lr=0.000020 grad_norm=0.842208
Epoch 42/100 Iteration 58/234: loss=0.066658 lr=0.000020 grad_norm=0.619872
Epoch 42/100 Iteration 59/234: loss=0.072201 lr=0.000020 grad_norm=0.515089
Epoch 42/100 Iteration 60/234: loss=0.058108 lr=0.000020 grad_norm=0.644593
Epoch 42/100 Iteration 61/234: loss=0.062076 lr=0.000020 grad_norm=0.484357
Epoch 42/100 Iteration 62/234: loss=0.069155 lr=0.000020 grad_norm=0.391070
Epoch 42/100 Iteration 63/234: loss=0.067801 lr=0.000020 grad_norm=0.483938
Epoch 42/100 Iteration 64/234: loss=0.067543 lr=0.000020 grad_norm=0.495427
Epoch 42/100 Iteration 65/234: loss=0.059755 lr=0.000020 grad_norm=0.430458
Epoch 42/100 Iteration 66/234: loss=0.066767 lr=0.000020 grad_norm=0.582600
Epoch 42/100 Iteration 67/234: loss=0.065771 lr=0.000020 grad_norm=0.948609
Epoch 42/100 Iteration 68/234: loss=0.063526 lr=0.000020 grad_norm=0.661798
Epoch 42/100 Iteration 69/234: loss=0.051683 lr=0.000020 grad_norm=0.406858
Epoch 42/100 Iteration 70/234: loss=0.074967 lr=0.000020 grad_norm=0.694842
Epoch 42/100 Iteration 71/234: loss=0.053325 lr=0.000020 grad_norm=0.868063
Epoch 42/100 Iteration 72/234: loss=0.066617 lr=0.000020 grad_norm=0.493968
Epoch 42/100 Iteration 73/234: loss=0.062416 lr=0.000020 grad_norm=0.799992
Epoch 42/100 Iteration 74/234: loss=0.053706 lr=0.000020 grad_norm=1.218217
Epoch 42/100 Iteration 75/234: loss=0.075356 lr=0.000020 grad_norm=1.452828
Epoch 42/100 Iteration 76/234: loss=0.065418 lr=0.000020 grad_norm=1.255726
Epoch 42/100 Iteration 77/234: loss=0.066364 lr=0.000020 grad_norm=0.365465
Epoch 42/100 Iteration 78/234: loss=0.064439 lr=0.000020 grad_norm=1.026115
Epoch 42/100 Iteration 79/234: loss=0.061401 lr=0.000020 grad_norm=1.042892
Epoch 42/100 Iteration 80/234: loss=0.074060 lr=0.000020 grad_norm=0.683081
Epoch 42/100 Iteration 81/234: loss=0.076948 lr=0.000020 grad_norm=0.806962
Epoch 42/100 Iteration 82/234: loss=0.059185 lr=0.000020 grad_norm=0.962574
Epoch 42/100 Iteration 83/234: loss=0.066779 lr=0.000020 grad_norm=1.143336
Epoch 42/100 Iteration 84/234: loss=0.068924 lr=0.000020 grad_norm=1.042915
Epoch 42/100 Iteration 85/234: loss=0.059003 lr=0.000020 grad_norm=0.359073
Epoch 42/100 Iteration 86/234: loss=0.069971 lr=0.000020 grad_norm=1.156630
Epoch 42/100 Iteration 87/234: loss=0.066831 lr=0.000020 grad_norm=1.697269
Epoch 42/100 Iteration 88/234: loss=0.056946 lr=0.000020 grad_norm=1.125331
Epoch 42/100 Iteration 89/234: loss=0.064186 lr=0.000020 grad_norm=0.562385
Epoch 42/100 Iteration 90/234: loss=0.070011 lr=0.000020 grad_norm=1.440637
Epoch 42/100 Iteration 91/234: loss=0.064157 lr=0.000020 grad_norm=1.323769
Epoch 42/100 Iteration 92/234: loss=0.065776 lr=0.000020 grad_norm=0.810641
Epoch 42/100 Iteration 93/234: loss=0.064559 lr=0.000020 grad_norm=2.102901
Epoch 42/100 Iteration 94/234: loss=0.063781 lr=0.000020 grad_norm=1.320181
Epoch 42/100 Iteration 95/234: loss=0.073392 lr=0.000020 grad_norm=0.847124
Epoch 42/100 Iteration 96/234: loss=0.060064 lr=0.000020 grad_norm=1.832020
Epoch 42/100 Iteration 97/234: loss=0.071206 lr=0.000020 grad_norm=1.141929
Epoch 42/100 Iteration 98/234: loss=0.072497 lr=0.000020 grad_norm=0.898271
Epoch 42/100 Iteration 99/234: loss=0.057439 lr=0.000020 grad_norm=0.810831
Epoch 42/100 Iteration 100/234: loss=0.073633 lr=0.000020 grad_norm=0.753443
Epoch 42/100 Iteration 101/234: loss=0.065119 lr=0.000020 grad_norm=1.000332
Epoch 42/100 Iteration 102/234: loss=0.062486 lr=0.000020 grad_norm=0.401063
Epoch 42/100 Iteration 103/234: loss=0.064259 lr=0.000020 grad_norm=0.816398
Epoch 42/100 Iteration 104/234: loss=0.064118 lr=0.000020 grad_norm=0.509232
Epoch 42/100 Iteration 105/234: loss=0.065199 lr=0.000020 grad_norm=0.618642
Epoch 42/100 Iteration 106/234: loss=0.068267 lr=0.000020 grad_norm=0.744812
Epoch 42/100 Iteration 107/234: loss=0.068275 lr=0.000020 grad_norm=0.326168
Epoch 42/100 Iteration 108/234: loss=0.077801 lr=0.000020 grad_norm=0.603608
Epoch 42/100 Iteration 109/234: loss=0.067787 lr=0.000020 grad_norm=0.582435
Epoch 42/100 Iteration 110/234: loss=0.076277 lr=0.000020 grad_norm=0.411857
Epoch 42/100 Iteration 111/234: loss=0.062262 lr=0.000020 grad_norm=0.421073
Epoch 42/100 Iteration 112/234: loss=0.075035 lr=0.000020 grad_norm=0.542349
Epoch 42/100 Iteration 113/234: loss=0.071901 lr=0.000020 grad_norm=0.616052
Epoch 42/100 Iteration 114/234: loss=0.078031 lr=0.000020 grad_norm=0.740137
Epoch 42/100 Iteration 115/234: loss=0.065648 lr=0.000020 grad_norm=0.519370
Epoch 42/100 Iteration 116/234: loss=0.070283 lr=0.000020 grad_norm=0.795324
Epoch 42/100 Iteration 117/234: loss=0.067165 lr=0.000020 grad_norm=1.035349
Epoch 42/100 Iteration 118/234: loss=0.072292 lr=0.000020 grad_norm=1.016723
Epoch 42/100 Iteration 119/234: loss=0.065307 lr=0.000020 grad_norm=0.799615
Epoch 42/100 Iteration 120/234: loss=0.071900 lr=0.000020 grad_norm=0.504517
Epoch 42/100 Iteration 121/234: loss=0.056696 lr=0.000020 grad_norm=0.536861
Epoch 42/100 Iteration 122/234: loss=0.070624 lr=0.000020 grad_norm=0.651753
Epoch 42/100 Iteration 123/234: loss=0.077840 lr=0.000020 grad_norm=0.473192
Epoch 42/100 Iteration 124/234: loss=0.070329 lr=0.000020 grad_norm=0.575111
Epoch 42/100 Iteration 125/234: loss=0.064294 lr=0.000020 grad_norm=0.727968
Epoch 42/100 Iteration 126/234: loss=0.068310 lr=0.000020 grad_norm=1.235536
Epoch 42/100 Iteration 127/234: loss=0.070350 lr=0.000020 grad_norm=0.798086
Epoch 42/100 Iteration 128/234: loss=0.064905 lr=0.000020 grad_norm=0.686338
Epoch 42/100 Iteration 129/234: loss=0.066984 lr=0.000020 grad_norm=1.173825
Epoch 42/100 Iteration 130/234: loss=0.074343 lr=0.000020 grad_norm=0.728486
Epoch 42/100 Iteration 131/234: loss=0.066692 lr=0.000020 grad_norm=0.527849
Epoch 42/100 Iteration 132/234: loss=0.074944 lr=0.000020 grad_norm=0.999325
Epoch 42/100 Iteration 133/234: loss=0.082857 lr=0.000020 grad_norm=1.102489
Epoch 42/100 Iteration 134/234: loss=0.065927 lr=0.000020 grad_norm=0.549199
Epoch 42/100 Iteration 135/234: loss=0.071523 lr=0.000020 grad_norm=0.957216
Epoch 42/100 Iteration 136/234: loss=0.056433 lr=0.000020 grad_norm=1.336895
Epoch 42/100 Iteration 137/234: loss=0.065836 lr=0.000020 grad_norm=0.723204
Epoch 42/100 Iteration 138/234: loss=0.074729 lr=0.000020 grad_norm=0.541425
Epoch 42/100 Iteration 139/234: loss=0.065101 lr=0.000020 grad_norm=1.273011
Epoch 42/100 Iteration 140/234: loss=0.071869 lr=0.000020 grad_norm=1.898749
Epoch 42/100 Iteration 141/234: loss=0.062308 lr=0.000020 grad_norm=1.847038
Epoch 42/100 Iteration 142/234: loss=0.078374 lr=0.000020 grad_norm=1.179260
Epoch 42/100 Iteration 143/234: loss=0.067534 lr=0.000020 grad_norm=0.560419
Epoch 42/100 Iteration 144/234: loss=0.062815 lr=0.000020 grad_norm=1.722657
Epoch 42/100 Iteration 145/234: loss=0.063719 lr=0.000020 grad_norm=1.954112
Epoch 42/100 Iteration 146/234: loss=0.069767 lr=0.000020 grad_norm=0.808757
Epoch 42/100 Iteration 147/234: loss=0.062187 lr=0.000020 grad_norm=1.140411
Epoch 42/100 Iteration 148/234: loss=0.065369 lr=0.000020 grad_norm=1.509200
Epoch 42/100 Iteration 149/234: loss=0.065848 lr=0.000020 grad_norm=0.699687
Epoch 42/100 Iteration 150/234: loss=0.063708 lr=0.000020 grad_norm=1.271581
Epoch 42/100 Iteration 151/234: loss=0.061883 lr=0.000020 grad_norm=1.055366
Epoch 42/100 Iteration 152/234: loss=0.056974 lr=0.000020 grad_norm=0.999452
Epoch 42/100 Iteration 153/234: loss=0.067068 lr=0.000020 grad_norm=1.459096
Epoch 42/100 Iteration 154/234: loss=0.072766 lr=0.000020 grad_norm=0.730540
Epoch 42/100 Iteration 155/234: loss=0.062181 lr=0.000020 grad_norm=1.231290
Epoch 42/100 Iteration 156/234: loss=0.067050 lr=0.000020 grad_norm=0.715022
Epoch 42/100 Iteration 157/234: loss=0.064494 lr=0.000020 grad_norm=1.022977
Epoch 42/100 Iteration 158/234: loss=0.064419 lr=0.000020 grad_norm=1.216019
Epoch 42/100 Iteration 159/234: loss=0.063659 lr=0.000020 grad_norm=0.891012
Epoch 42/100 Iteration 160/234: loss=0.071112 lr=0.000020 grad_norm=1.232160
Epoch 42/100 Iteration 161/234: loss=0.061118 lr=0.000020 grad_norm=0.617869
Epoch 42/100 Iteration 162/234: loss=0.052358 lr=0.000020 grad_norm=0.642204
Epoch 42/100 Iteration 163/234: loss=0.063982 lr=0.000020 grad_norm=0.794911
Epoch 42/100 Iteration 164/234: loss=0.064198 lr=0.000020 grad_norm=0.534026
Epoch 42/100 Iteration 165/234: loss=0.069081 lr=0.000020 grad_norm=0.706163
Epoch 42/100 Iteration 166/234: loss=0.067205 lr=0.000020 grad_norm=1.095732
Epoch 42/100 Iteration 167/234: loss=0.067859 lr=0.000020 grad_norm=0.772843
Epoch 42/100 Iteration 168/234: loss=0.070381 lr=0.000020 grad_norm=0.888800
Epoch 42/100 Iteration 169/234: loss=0.063811 lr=0.000020 grad_norm=1.220159
Epoch 42/100 Iteration 170/234: loss=0.061691 lr=0.000020 grad_norm=0.951667
Epoch 42/100 Iteration 171/234: loss=0.062245 lr=0.000020 grad_norm=0.590143
Epoch 42/100 Iteration 172/234: loss=0.069663 lr=0.000020 grad_norm=0.863222
Epoch 42/100 Iteration 173/234: loss=0.063574 lr=0.000020 grad_norm=0.933536
Epoch 42/100 Iteration 174/234: loss=0.060239 lr=0.000020 grad_norm=0.935074
Epoch 42/100 Iteration 175/234: loss=0.061494 lr=0.000020 grad_norm=0.679890
Epoch 42/100 Iteration 176/234: loss=0.075211 lr=0.000020 grad_norm=0.743322
Epoch 42/100 Iteration 177/234: loss=0.064787 lr=0.000020 grad_norm=0.667156
Epoch 42/100 Iteration 178/234: loss=0.073608 lr=0.000020 grad_norm=0.534325
Epoch 42/100 Iteration 179/234: loss=0.066891 lr=0.000020 grad_norm=0.621115
Epoch 42/100 Iteration 180/234: loss=0.068454 lr=0.000020 grad_norm=0.550882
Epoch 42/100 Iteration 181/234: loss=0.066277 lr=0.000020 grad_norm=0.544856
Epoch 42/100 Iteration 182/234: loss=0.061296 lr=0.000020 grad_norm=0.787570
Epoch 42/100 Iteration 183/234: loss=0.072553 lr=0.000020 grad_norm=1.096026
Epoch 42/100 Iteration 184/234: loss=0.059962 lr=0.000020 grad_norm=0.877869
Epoch 42/100 Iteration 185/234: loss=0.064836 lr=0.000020 grad_norm=0.695335
Epoch 42/100 Iteration 186/234: loss=0.064349 lr=0.000020 grad_norm=0.556348
Epoch 42/100 Iteration 187/234: loss=0.068355 lr=0.000020 grad_norm=0.899927
Epoch 42/100 Iteration 188/234: loss=0.064533 lr=0.000020 grad_norm=1.216721
Epoch 42/100 Iteration 189/234: loss=0.062419 lr=0.000020 grad_norm=0.738179
Epoch 42/100 Iteration 190/234: loss=0.072163 lr=0.000020 grad_norm=0.458141
Epoch 42/100 Iteration 191/234: loss=0.066787 lr=0.000020 grad_norm=0.864346
Epoch 42/100 Iteration 192/234: loss=0.079738 lr=0.000020 grad_norm=1.161636
Epoch 42/100 Iteration 193/234: loss=0.067268 lr=0.000020 grad_norm=0.792907
Epoch 42/100 Iteration 194/234: loss=0.061348 lr=0.000020 grad_norm=0.685195
Epoch 42/100 Iteration 195/234: loss=0.066032 lr=0.000020 grad_norm=1.395221
Epoch 42/100 Iteration 196/234: loss=0.071669 lr=0.000020 grad_norm=1.964982
Epoch 42/100 Iteration 197/234: loss=0.064890 lr=0.000020 grad_norm=1.168574
Epoch 42/100 Iteration 198/234: loss=0.065388 lr=0.000020 grad_norm=0.919725
Epoch 42/100 Iteration 199/234: loss=0.065739 lr=0.000020 grad_norm=1.765839
Epoch 42/100 Iteration 200/234: loss=0.071655 lr=0.000020 grad_norm=1.188176
Epoch 42/100 Iteration 201/234: loss=0.070593 lr=0.000020 grad_norm=0.757785
Epoch 42/100 Iteration 202/234: loss=0.068991 lr=0.000020 grad_norm=1.487715
Epoch 42/100 Iteration 203/234: loss=0.068704 lr=0.000020 grad_norm=0.830954
Epoch 42/100 Iteration 204/234: loss=0.073567 lr=0.000020 grad_norm=1.007618
Epoch 42/100 Iteration 205/234: loss=0.064727 lr=0.000020 grad_norm=1.699124
Epoch 42/100 Iteration 206/234: loss=0.059883 lr=0.000020 grad_norm=0.850147
Epoch 42/100 Iteration 207/234: loss=0.066636 lr=0.000020 grad_norm=0.997067
Epoch 42/100 Iteration 208/234: loss=0.056864 lr=0.000020 grad_norm=1.285330
Epoch 42/100 Iteration 209/234: loss=0.057112 lr=0.000020 grad_norm=0.631856
Epoch 42/100 Iteration 210/234: loss=0.067696 lr=0.000020 grad_norm=0.672482
Epoch 42/100 Iteration 211/234: loss=0.068896 lr=0.000020 grad_norm=1.012680
Epoch 42/100 Iteration 212/234: loss=0.074161 lr=0.000020 grad_norm=0.725547
Epoch 42/100 Iteration 213/234: loss=0.057871 lr=0.000020 grad_norm=0.737923
Epoch 42/100 Iteration 214/234: loss=0.062214 lr=0.000020 grad_norm=0.959333
Epoch 42/100 Iteration 215/234: loss=0.065874 lr=0.000020 grad_norm=0.420588
Epoch 42/100 Iteration 216/234: loss=0.070858 lr=0.000020 grad_norm=1.015573
Epoch 42/100 Iteration 217/234: loss=0.064055 lr=0.000020 grad_norm=0.548573
Epoch 42/100 Iteration 218/234: loss=0.069697 lr=0.000020 grad_norm=1.059383
Epoch 42/100 Iteration 219/234: loss=0.057598 lr=0.000020 grad_norm=1.878997
Epoch 42/100 Iteration 220/234: loss=0.065886 lr=0.000020 grad_norm=1.478712
Epoch 42/100 Iteration 221/234: loss=0.070478 lr=0.000020 grad_norm=0.567896
Epoch 42/100 Iteration 222/234: loss=0.068151 lr=0.000020 grad_norm=0.792957
Epoch 42/100 Iteration 223/234: loss=0.071285 lr=0.000020 grad_norm=0.845174
Epoch 42/100 Iteration 224/234: loss=0.067589 lr=0.000020 grad_norm=0.538343
Epoch 42/100 Iteration 225/234: loss=0.052860 lr=0.000020 grad_norm=0.525053
Epoch 42/100 Iteration 226/234: loss=0.060653 lr=0.000020 grad_norm=0.562625
Epoch 42/100 Iteration 227/234: loss=0.067851 lr=0.000020 grad_norm=0.391826
Epoch 42/100 Iteration 228/234: loss=0.073561 lr=0.000020 grad_norm=0.359662
Epoch 42/100 Iteration 229/234: loss=0.062719 lr=0.000020 grad_norm=0.451545
Epoch 42/100 Iteration 230/234: loss=0.064869 lr=0.000020 grad_norm=0.555227
Epoch 42/100 Iteration 231/234: loss=0.059481 lr=0.000020 grad_norm=0.388409
Epoch 42/100 Iteration 232/234: loss=0.062135 lr=0.000020 grad_norm=0.509311
Epoch 42/100 Iteration 233/234: loss=0.067414 lr=0.000020 grad_norm=0.578092
Epoch 42/100 Iteration 234/234: loss=0.057387 lr=0.000020 grad_norm=0.667309
Epoch 42/100 finished. Avg Loss: 0.066300
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 43/100 Iteration 1/234: loss=0.067469 lr=0.000020 grad_norm=0.519966
Epoch 43/100 Iteration 2/234: loss=0.071955 lr=0.000020 grad_norm=0.553742
Epoch 43/100 Iteration 3/234: loss=0.066675 lr=0.000020 grad_norm=0.637292
Epoch 43/100 Iteration 4/234: loss=0.071030 lr=0.000020 grad_norm=0.488185
Epoch 43/100 Iteration 5/234: loss=0.063811 lr=0.000020 grad_norm=0.591199
Epoch 43/100 Iteration 6/234: loss=0.064395 lr=0.000020 grad_norm=0.629606
Epoch 43/100 Iteration 7/234: loss=0.060973 lr=0.000020 grad_norm=0.334059
Epoch 43/100 Iteration 8/234: loss=0.065630 lr=0.000020 grad_norm=0.646191
Epoch 43/100 Iteration 9/234: loss=0.065545 lr=0.000020 grad_norm=0.631544
Epoch 43/100 Iteration 10/234: loss=0.063598 lr=0.000020 grad_norm=0.527935
Epoch 43/100 Iteration 11/234: loss=0.071458 lr=0.000020 grad_norm=0.808308
Epoch 43/100 Iteration 12/234: loss=0.066651 lr=0.000020 grad_norm=1.056315
Epoch 43/100 Iteration 13/234: loss=0.066080 lr=0.000020 grad_norm=1.042773
Epoch 43/100 Iteration 14/234: loss=0.061844 lr=0.000020 grad_norm=0.440570
Epoch 43/100 Iteration 15/234: loss=0.058859 lr=0.000020 grad_norm=0.754630
Epoch 43/100 Iteration 16/234: loss=0.065002 lr=0.000020 grad_norm=0.869691
Epoch 43/100 Iteration 17/234: loss=0.058951 lr=0.000020 grad_norm=0.454096
Epoch 43/100 Iteration 18/234: loss=0.067816 lr=0.000020 grad_norm=0.771390
Epoch 43/100 Iteration 19/234: loss=0.074928 lr=0.000020 grad_norm=0.997681
Epoch 43/100 Iteration 20/234: loss=0.063629 lr=0.000020 grad_norm=0.415149
Epoch 43/100 Iteration 21/234: loss=0.067672 lr=0.000020 grad_norm=1.191358
Epoch 43/100 Iteration 22/234: loss=0.063889 lr=0.000020 grad_norm=0.880581
Epoch 43/100 Iteration 23/234: loss=0.075462 lr=0.000020 grad_norm=0.676850
Epoch 43/100 Iteration 24/234: loss=0.066882 lr=0.000020 grad_norm=1.261599
Epoch 43/100 Iteration 25/234: loss=0.058880 lr=0.000020 grad_norm=0.540401
Epoch 43/100 Iteration 26/234: loss=0.065645 lr=0.000020 grad_norm=1.059518
Epoch 43/100 Iteration 27/234: loss=0.076001 lr=0.000020 grad_norm=1.457331
Epoch 43/100 Iteration 28/234: loss=0.066751 lr=0.000020 grad_norm=0.969553
Epoch 43/100 Iteration 29/234: loss=0.065520 lr=0.000020 grad_norm=0.660185
Epoch 43/100 Iteration 30/234: loss=0.072270 lr=0.000020 grad_norm=0.653834
Epoch 43/100 Iteration 31/234: loss=0.070901 lr=0.000020 grad_norm=0.657960
Epoch 43/100 Iteration 32/234: loss=0.062499 lr=0.000020 grad_norm=0.732184
Epoch 43/100 Iteration 33/234: loss=0.061196 lr=0.000020 grad_norm=0.554846
Epoch 43/100 Iteration 34/234: loss=0.057228 lr=0.000020 grad_norm=0.586896
Epoch 43/100 Iteration 35/234: loss=0.071157 lr=0.000020 grad_norm=1.238719
Epoch 43/100 Iteration 36/234: loss=0.065022 lr=0.000020 grad_norm=0.999943
Epoch 43/100 Iteration 37/234: loss=0.061348 lr=0.000020 grad_norm=0.567707
Epoch 43/100 Iteration 38/234: loss=0.062921 lr=0.000020 grad_norm=0.823161
Epoch 43/100 Iteration 39/234: loss=0.064066 lr=0.000020 grad_norm=0.677202
Epoch 43/100 Iteration 40/234: loss=0.059690 lr=0.000020 grad_norm=0.563161
Epoch 43/100 Iteration 41/234: loss=0.058099 lr=0.000020 grad_norm=0.914362
Epoch 43/100 Iteration 42/234: loss=0.064447 lr=0.000020 grad_norm=1.104593
Epoch 43/100 Iteration 43/234: loss=0.066288 lr=0.000020 grad_norm=0.796705
Epoch 43/100 Iteration 44/234: loss=0.057465 lr=0.000020 grad_norm=0.429099
Epoch 43/100 Iteration 45/234: loss=0.059408 lr=0.000020 grad_norm=0.915095
Epoch 43/100 Iteration 46/234: loss=0.064282 lr=0.000020 grad_norm=1.022191
Epoch 43/100 Iteration 47/234: loss=0.056392 lr=0.000020 grad_norm=0.661357
Epoch 43/100 Iteration 48/234: loss=0.071910 lr=0.000020 grad_norm=0.481712
Epoch 43/100 Iteration 49/234: loss=0.067789 lr=0.000020 grad_norm=0.632529
Epoch 43/100 Iteration 50/234: loss=0.066123 lr=0.000020 grad_norm=1.126312
Epoch 43/100 Iteration 51/234: loss=0.061420 lr=0.000020 grad_norm=0.932160
Epoch 43/100 Iteration 52/234: loss=0.069231 lr=0.000020 grad_norm=0.487996
Epoch 43/100 Iteration 53/234: loss=0.071194 lr=0.000020 grad_norm=0.872741
Epoch 43/100 Iteration 54/234: loss=0.065824 lr=0.000020 grad_norm=1.285674
Epoch 43/100 Iteration 55/234: loss=0.069859 lr=0.000020 grad_norm=1.225383
Epoch 43/100 Iteration 56/234: loss=0.067843 lr=0.000020 grad_norm=0.715010
Epoch 43/100 Iteration 57/234: loss=0.056902 lr=0.000020 grad_norm=0.634843
Epoch 43/100 Iteration 58/234: loss=0.060811 lr=0.000020 grad_norm=0.981131
Epoch 43/100 Iteration 59/234: loss=0.068864 lr=0.000020 grad_norm=0.821230
Epoch 43/100 Iteration 60/234: loss=0.063907 lr=0.000020 grad_norm=0.584782
Epoch 43/100 Iteration 61/234: loss=0.062480 lr=0.000020 grad_norm=0.804350
Epoch 43/100 Iteration 62/234: loss=0.065222 lr=0.000020 grad_norm=1.204359
Epoch 43/100 Iteration 63/234: loss=0.063414 lr=0.000020 grad_norm=1.172451
Epoch 43/100 Iteration 64/234: loss=0.062066 lr=0.000020 grad_norm=1.090485
Epoch 43/100 Iteration 65/234: loss=0.068572 lr=0.000020 grad_norm=0.956038
Epoch 43/100 Iteration 66/234: loss=0.064660 lr=0.000020 grad_norm=0.519055
Epoch 43/100 Iteration 67/234: loss=0.059030 lr=0.000020 grad_norm=0.965075
Epoch 43/100 Iteration 68/234: loss=0.072023 lr=0.000020 grad_norm=0.763471
Epoch 43/100 Iteration 69/234: loss=0.065944 lr=0.000020 grad_norm=0.892285
Epoch 43/100 Iteration 70/234: loss=0.057909 lr=0.000020 grad_norm=0.638521
Epoch 43/100 Iteration 71/234: loss=0.058440 lr=0.000020 grad_norm=0.838892
Epoch 43/100 Iteration 72/234: loss=0.068439 lr=0.000020 grad_norm=0.970323
Epoch 43/100 Iteration 73/234: loss=0.068485 lr=0.000020 grad_norm=0.477600
Epoch 43/100 Iteration 74/234: loss=0.056600 lr=0.000020 grad_norm=1.219408
Epoch 43/100 Iteration 75/234: loss=0.068150 lr=0.000020 grad_norm=1.623428
Epoch 43/100 Iteration 76/234: loss=0.076919 lr=0.000020 grad_norm=1.237946
Epoch 43/100 Iteration 77/234: loss=0.076864 lr=0.000020 grad_norm=0.597213
Epoch 43/100 Iteration 78/234: loss=0.072891 lr=0.000020 grad_norm=0.617442
Epoch 43/100 Iteration 79/234: loss=0.059092 lr=0.000020 grad_norm=1.180943
Epoch 43/100 Iteration 80/234: loss=0.059497 lr=0.000020 grad_norm=1.474503
Epoch 43/100 Iteration 81/234: loss=0.059780 lr=0.000020 grad_norm=1.071015
Epoch 43/100 Iteration 82/234: loss=0.060548 lr=0.000020 grad_norm=0.480366
Epoch 43/100 Iteration 83/234: loss=0.061322 lr=0.000020 grad_norm=0.856051
Epoch 43/100 Iteration 84/234: loss=0.073372 lr=0.000020 grad_norm=0.835196
Epoch 43/100 Iteration 85/234: loss=0.067090 lr=0.000020 grad_norm=0.526366
Epoch 43/100 Iteration 86/234: loss=0.069002 lr=0.000020 grad_norm=0.667160
Epoch 43/100 Iteration 87/234: loss=0.066214 lr=0.000020 grad_norm=0.932453
Epoch 43/100 Iteration 88/234: loss=0.072507 lr=0.000020 grad_norm=0.652944
Epoch 43/100 Iteration 89/234: loss=0.063414 lr=0.000020 grad_norm=0.501760
Epoch 43/100 Iteration 90/234: loss=0.058749 lr=0.000020 grad_norm=0.960506
Epoch 43/100 Iteration 91/234: loss=0.057840 lr=0.000020 grad_norm=0.731902
Epoch 43/100 Iteration 92/234: loss=0.061471 lr=0.000020 grad_norm=0.429604
Epoch 43/100 Iteration 93/234: loss=0.068959 lr=0.000020 grad_norm=0.707221
Epoch 43/100 Iteration 94/234: loss=0.075592 lr=0.000020 grad_norm=0.837644
Epoch 43/100 Iteration 95/234: loss=0.063878 lr=0.000020 grad_norm=0.466846
Epoch 43/100 Iteration 96/234: loss=0.074161 lr=0.000020 grad_norm=0.564464
Epoch 43/100 Iteration 97/234: loss=0.062116 lr=0.000020 grad_norm=0.579242
Epoch 43/100 Iteration 98/234: loss=0.063689 lr=0.000020 grad_norm=0.537751
Epoch 43/100 Iteration 99/234: loss=0.062425 lr=0.000020 grad_norm=0.377577
Epoch 43/100 Iteration 100/234: loss=0.062138 lr=0.000020 grad_norm=0.719911
Epoch 43/100 Iteration 101/234: loss=0.062691 lr=0.000020 grad_norm=0.950699
Epoch 43/100 Iteration 102/234: loss=0.061612 lr=0.000020 grad_norm=0.454515
Epoch 43/100 Iteration 103/234: loss=0.067797 lr=0.000020 grad_norm=0.737638
Epoch 43/100 Iteration 104/234: loss=0.064918 lr=0.000020 grad_norm=1.475385
Epoch 43/100 Iteration 105/234: loss=0.077991 lr=0.000020 grad_norm=1.796193
Epoch 43/100 Iteration 106/234: loss=0.066464 lr=0.000020 grad_norm=1.388862
Epoch 43/100 Iteration 107/234: loss=0.062963 lr=0.000020 grad_norm=0.670818
Epoch 43/100 Iteration 108/234: loss=0.069609 lr=0.000020 grad_norm=0.726154
Epoch 43/100 Iteration 109/234: loss=0.075903 lr=0.000020 grad_norm=1.416876
Epoch 43/100 Iteration 110/234: loss=0.066057 lr=0.000020 grad_norm=1.810321
Epoch 43/100 Iteration 111/234: loss=0.064119 lr=0.000020 grad_norm=1.273529
Epoch 43/100 Iteration 112/234: loss=0.061208 lr=0.000020 grad_norm=0.537611
Epoch 43/100 Iteration 113/234: loss=0.058590 lr=0.000020 grad_norm=1.096869
Epoch 43/100 Iteration 114/234: loss=0.057552 lr=0.000020 grad_norm=0.821255
Epoch 43/100 Iteration 115/234: loss=0.064246 lr=0.000020 grad_norm=0.574609
Epoch 43/100 Iteration 116/234: loss=0.061609 lr=0.000020 grad_norm=0.963092
Epoch 43/100 Iteration 117/234: loss=0.063155 lr=0.000020 grad_norm=0.983001
Epoch 43/100 Iteration 118/234: loss=0.070258 lr=0.000020 grad_norm=0.646245
Epoch 43/100 Iteration 119/234: loss=0.060656 lr=0.000020 grad_norm=0.696251
Epoch 43/100 Iteration 120/234: loss=0.066211 lr=0.000020 grad_norm=1.255363
Epoch 43/100 Iteration 121/234: loss=0.064909 lr=0.000020 grad_norm=1.208469
Epoch 43/100 Iteration 122/234: loss=0.063785 lr=0.000020 grad_norm=0.548222
Epoch 43/100 Iteration 123/234: loss=0.062996 lr=0.000020 grad_norm=0.588551
Epoch 43/100 Iteration 124/234: loss=0.061843 lr=0.000020 grad_norm=0.517004
Epoch 43/100 Iteration 125/234: loss=0.060319 lr=0.000020 grad_norm=0.579965
Epoch 43/100 Iteration 126/234: loss=0.058416 lr=0.000020 grad_norm=0.681798
Epoch 43/100 Iteration 127/234: loss=0.065552 lr=0.000020 grad_norm=0.853547
Epoch 43/100 Iteration 128/234: loss=0.067630 lr=0.000020 grad_norm=0.780528
Epoch 43/100 Iteration 129/234: loss=0.066520 lr=0.000020 grad_norm=0.758281
Epoch 43/100 Iteration 130/234: loss=0.065980 lr=0.000020 grad_norm=0.852385
Epoch 43/100 Iteration 131/234: loss=0.059635 lr=0.000020 grad_norm=0.587071
Epoch 43/100 Iteration 132/234: loss=0.067239 lr=0.000020 grad_norm=0.540577
Epoch 43/100 Iteration 133/234: loss=0.061149 lr=0.000020 grad_norm=0.960666
Epoch 43/100 Iteration 134/234: loss=0.071239 lr=0.000020 grad_norm=0.608503
Epoch 43/100 Iteration 135/234: loss=0.068969 lr=0.000020 grad_norm=0.489330
Epoch 43/100 Iteration 136/234: loss=0.061696 lr=0.000020 grad_norm=0.587857
Epoch 43/100 Iteration 137/234: loss=0.071190 lr=0.000020 grad_norm=0.641706
Epoch 43/100 Iteration 138/234: loss=0.066487 lr=0.000020 grad_norm=0.791034
Epoch 43/100 Iteration 139/234: loss=0.063712 lr=0.000020 grad_norm=0.575404
Epoch 43/100 Iteration 140/234: loss=0.064469 lr=0.000020 grad_norm=0.963348
Epoch 43/100 Iteration 141/234: loss=0.062435 lr=0.000020 grad_norm=1.105983
Epoch 43/100 Iteration 142/234: loss=0.068411 lr=0.000020 grad_norm=0.536586
Epoch 43/100 Iteration 143/234: loss=0.055385 lr=0.000020 grad_norm=0.851880
Epoch 43/100 Iteration 144/234: loss=0.075074 lr=0.000020 grad_norm=2.129269
Epoch 43/100 Iteration 145/234: loss=0.067394 lr=0.000020 grad_norm=2.367613
Epoch 43/100 Iteration 146/234: loss=0.060712 lr=0.000020 grad_norm=0.694817
Epoch 43/100 Iteration 147/234: loss=0.071390 lr=0.000020 grad_norm=2.040120
Epoch 43/100 Iteration 148/234: loss=0.065799 lr=0.000020 grad_norm=2.016453
Epoch 43/100 Iteration 149/234: loss=0.061380 lr=0.000020 grad_norm=0.813215
Epoch 43/100 Iteration 150/234: loss=0.064413 lr=0.000020 grad_norm=1.883789
Epoch 43/100 Iteration 151/234: loss=0.054091 lr=0.000020 grad_norm=0.758888
Epoch 43/100 Iteration 152/234: loss=0.063982 lr=0.000020 grad_norm=1.516873
Epoch 43/100 Iteration 153/234: loss=0.056273 lr=0.000020 grad_norm=1.001521
Epoch 43/100 Iteration 154/234: loss=0.066544 lr=0.000020 grad_norm=1.205626
Epoch 43/100 Iteration 155/234: loss=0.066075 lr=0.000020 grad_norm=2.064508
Epoch 43/100 Iteration 156/234: loss=0.066837 lr=0.000020 grad_norm=0.727872
Epoch 43/100 Iteration 157/234: loss=0.061457 lr=0.000020 grad_norm=1.326825
Epoch 43/100 Iteration 158/234: loss=0.061739 lr=0.000020 grad_norm=1.138945
Epoch 43/100 Iteration 159/234: loss=0.064083 lr=0.000020 grad_norm=0.664121
Epoch 43/100 Iteration 160/234: loss=0.073425 lr=0.000020 grad_norm=1.250277
Epoch 43/100 Iteration 161/234: loss=0.060794 lr=0.000020 grad_norm=0.650491
Epoch 43/100 Iteration 162/234: loss=0.057828 lr=0.000020 grad_norm=0.971605
Epoch 43/100 Iteration 163/234: loss=0.063847 lr=0.000020 grad_norm=0.929515
Epoch 43/100 Iteration 164/234: loss=0.061315 lr=0.000020 grad_norm=1.027966
Epoch 43/100 Iteration 165/234: loss=0.073761 lr=0.000020 grad_norm=1.725700
Epoch 43/100 Iteration 166/234: loss=0.063729 lr=0.000020 grad_norm=0.584854
Epoch 43/100 Iteration 167/234: loss=0.061684 lr=0.000020 grad_norm=1.148326
Epoch 43/100 Iteration 168/234: loss=0.058356 lr=0.000020 grad_norm=0.924117
Epoch 43/100 Iteration 169/234: loss=0.068289 lr=0.000020 grad_norm=0.645363
Epoch 43/100 Iteration 170/234: loss=0.062568 lr=0.000020 grad_norm=1.122066
Epoch 43/100 Iteration 171/234: loss=0.071486 lr=0.000020 grad_norm=0.470384
Epoch 43/100 Iteration 172/234: loss=0.062488 lr=0.000020 grad_norm=1.092532
Epoch 43/100 Iteration 173/234: loss=0.068580 lr=0.000020 grad_norm=1.119299
Epoch 43/100 Iteration 174/234: loss=0.069498 lr=0.000020 grad_norm=0.393762
Epoch 43/100 Iteration 175/234: loss=0.051856 lr=0.000020 grad_norm=0.626899
Epoch 43/100 Iteration 176/234: loss=0.078295 lr=0.000020 grad_norm=0.585988
Epoch 43/100 Iteration 177/234: loss=0.066297 lr=0.000020 grad_norm=0.653272
Epoch 43/100 Iteration 178/234: loss=0.063589 lr=0.000020 grad_norm=0.488542
Epoch 43/100 Iteration 179/234: loss=0.049873 lr=0.000020 grad_norm=0.454018
Epoch 43/100 Iteration 180/234: loss=0.067125 lr=0.000020 grad_norm=0.569024
Epoch 43/100 Iteration 181/234: loss=0.067998 lr=0.000020 grad_norm=0.699905
Epoch 43/100 Iteration 182/234: loss=0.064910 lr=0.000020 grad_norm=0.886647
Epoch 43/100 Iteration 183/234: loss=0.063335 lr=0.000020 grad_norm=1.379347
Epoch 43/100 Iteration 184/234: loss=0.059462 lr=0.000020 grad_norm=1.095774
Epoch 43/100 Iteration 185/234: loss=0.066384 lr=0.000020 grad_norm=0.479284
Epoch 43/100 Iteration 186/234: loss=0.061689 lr=0.000020 grad_norm=1.007734
Epoch 43/100 Iteration 187/234: loss=0.057006 lr=0.000020 grad_norm=0.613952
Epoch 43/100 Iteration 188/234: loss=0.062176 lr=0.000020 grad_norm=0.615019
Epoch 43/100 Iteration 189/234: loss=0.068649 lr=0.000020 grad_norm=0.776612
Epoch 43/100 Iteration 190/234: loss=0.061193 lr=0.000020 grad_norm=0.363468
Epoch 43/100 Iteration 191/234: loss=0.058136 lr=0.000020 grad_norm=1.023946
Epoch 43/100 Iteration 192/234: loss=0.065841 lr=0.000020 grad_norm=1.001056
Epoch 43/100 Iteration 193/234: loss=0.075615 lr=0.000020 grad_norm=0.976003
Epoch 43/100 Iteration 194/234: loss=0.064458 lr=0.000020 grad_norm=1.043275
Epoch 43/100 Iteration 195/234: loss=0.063319 lr=0.000020 grad_norm=0.742983
Epoch 43/100 Iteration 196/234: loss=0.062134 lr=0.000020 grad_norm=0.713962
Epoch 43/100 Iteration 197/234: loss=0.059554 lr=0.000020 grad_norm=0.853766
Epoch 43/100 Iteration 198/234: loss=0.064757 lr=0.000020 grad_norm=0.657968
Epoch 43/100 Iteration 199/234: loss=0.064962 lr=0.000020 grad_norm=0.775223
Epoch 43/100 Iteration 200/234: loss=0.063627 lr=0.000020 grad_norm=1.089059
Epoch 43/100 Iteration 201/234: loss=0.071452 lr=0.000020 grad_norm=0.731919
Epoch 43/100 Iteration 202/234: loss=0.059324 lr=0.000020 grad_norm=0.331304
Epoch 43/100 Iteration 203/234: loss=0.057113 lr=0.000020 grad_norm=0.462859
Epoch 43/100 Iteration 204/234: loss=0.059030 lr=0.000020 grad_norm=0.455530
Epoch 43/100 Iteration 205/234: loss=0.052788 lr=0.000020 grad_norm=0.446617
Epoch 43/100 Iteration 206/234: loss=0.065052 lr=0.000020 grad_norm=0.611359
Epoch 43/100 Iteration 207/234: loss=0.070158 lr=0.000020 grad_norm=0.479844
Epoch 43/100 Iteration 208/234: loss=0.068368 lr=0.000020 grad_norm=0.932166
Epoch 43/100 Iteration 209/234: loss=0.049860 lr=0.000020 grad_norm=0.938449
Epoch 43/100 Iteration 210/234: loss=0.070357 lr=0.000020 grad_norm=0.442163
Epoch 43/100 Iteration 211/234: loss=0.057762 lr=0.000020 grad_norm=0.588154
Epoch 43/100 Iteration 212/234: loss=0.064069 lr=0.000020 grad_norm=0.539829
Epoch 43/100 Iteration 213/234: loss=0.065145 lr=0.000020 grad_norm=0.365077
Epoch 43/100 Iteration 214/234: loss=0.069779 lr=0.000020 grad_norm=1.013616
Epoch 43/100 Iteration 215/234: loss=0.065772 lr=0.000020 grad_norm=2.086827
Epoch 43/100 Iteration 216/234: loss=0.065122 lr=0.000020 grad_norm=1.876954
Epoch 43/100 Iteration 217/234: loss=0.061054 lr=0.000020 grad_norm=0.428861
Epoch 43/100 Iteration 218/234: loss=0.058146 lr=0.000020 grad_norm=1.787437
Epoch 43/100 Iteration 219/234: loss=0.060975 lr=0.000020 grad_norm=1.517053
Epoch 43/100 Iteration 220/234: loss=0.062652 lr=0.000020 grad_norm=0.490040
Epoch 43/100 Iteration 221/234: loss=0.067177 lr=0.000020 grad_norm=2.313220
Epoch 43/100 Iteration 222/234: loss=0.058493 lr=0.000020 grad_norm=1.365308
Epoch 43/100 Iteration 223/234: loss=0.064882 lr=0.000020 grad_norm=1.687687
Epoch 43/100 Iteration 224/234: loss=0.059873 lr=0.000020 grad_norm=2.726140
Epoch 43/100 Iteration 225/234: loss=0.065152 lr=0.000020 grad_norm=1.130830
Epoch 43/100 Iteration 226/234: loss=0.065170 lr=0.000020 grad_norm=1.658048
Epoch 43/100 Iteration 227/234: loss=0.064324 lr=0.000020 grad_norm=1.610761
Epoch 43/100 Iteration 228/234: loss=0.067046 lr=0.000020 grad_norm=1.148046
Epoch 43/100 Iteration 229/234: loss=0.061451 lr=0.000020 grad_norm=1.503011
Epoch 43/100 Iteration 230/234: loss=0.069286 lr=0.000020 grad_norm=1.152313
Epoch 43/100 Iteration 231/234: loss=0.065358 lr=0.000020 grad_norm=2.515387
Epoch 43/100 Iteration 232/234: loss=0.068836 lr=0.000020 grad_norm=1.147360
Epoch 43/100 Iteration 233/234: loss=0.070298 lr=0.000020 grad_norm=1.390682
Epoch 43/100 Iteration 234/234: loss=0.061428 lr=0.000020 grad_norm=1.453441
Epoch 43/100 finished. Avg Loss: 0.064674
Epoch 44/100 Iteration 1/234: loss=0.064291 lr=0.000020 grad_norm=0.779384
Epoch 44/100 Iteration 2/234: loss=0.066242 lr=0.000020 grad_norm=1.390329
Epoch 44/100 Iteration 3/234: loss=0.060553 lr=0.000020 grad_norm=0.497296
Epoch 44/100 Iteration 4/234: loss=0.063242 lr=0.000020 grad_norm=1.353882
Epoch 44/100 Iteration 5/234: loss=0.067219 lr=0.000020 grad_norm=0.813089
Epoch 44/100 Iteration 6/234: loss=0.064710 lr=0.000020 grad_norm=0.799102
Epoch 44/100 Iteration 7/234: loss=0.066699 lr=0.000020 grad_norm=1.130770
Epoch 44/100 Iteration 8/234: loss=0.066281 lr=0.000020 grad_norm=0.536008
Epoch 44/100 Iteration 9/234: loss=0.074802 lr=0.000020 grad_norm=0.880354
Epoch 44/100 Iteration 10/234: loss=0.064079 lr=0.000020 grad_norm=0.518457
Epoch 44/100 Iteration 11/234: loss=0.067102 lr=0.000020 grad_norm=0.631960
Epoch 44/100 Iteration 12/234: loss=0.069342 lr=0.000020 grad_norm=0.716881
Epoch 44/100 Iteration 13/234: loss=0.055567 lr=0.000020 grad_norm=0.586011
Epoch 44/100 Iteration 14/234: loss=0.072995 lr=0.000020 grad_norm=0.507281
Epoch 44/100 Iteration 15/234: loss=0.065808 lr=0.000020 grad_norm=0.593257
Epoch 44/100 Iteration 16/234: loss=0.066017 lr=0.000020 grad_norm=0.556307
Epoch 44/100 Iteration 17/234: loss=0.056359 lr=0.000020 grad_norm=0.995410
Epoch 44/100 Iteration 18/234: loss=0.071509 lr=0.000020 grad_norm=0.880664
Epoch 44/100 Iteration 19/234: loss=0.058789 lr=0.000020 grad_norm=0.689965
Epoch 44/100 Iteration 20/234: loss=0.059312 lr=0.000020 grad_norm=1.089761
Epoch 44/100 Iteration 21/234: loss=0.061787 lr=0.000020 grad_norm=0.449379
Epoch 44/100 Iteration 22/234: loss=0.059520 lr=0.000020 grad_norm=0.503724
Epoch 44/100 Iteration 23/234: loss=0.067485 lr=0.000020 grad_norm=0.463297
Epoch 44/100 Iteration 24/234: loss=0.078909 lr=0.000020 grad_norm=0.600229
Epoch 44/100 Iteration 25/234: loss=0.069083 lr=0.000020 grad_norm=0.413247
Epoch 44/100 Iteration 26/234: loss=0.062735 lr=0.000020 grad_norm=0.422105
Epoch 44/100 Iteration 27/234: loss=0.058153 lr=0.000020 grad_norm=0.399492
Epoch 44/100 Iteration 28/234: loss=0.062652 lr=0.000020 grad_norm=0.602446
Epoch 44/100 Iteration 29/234: loss=0.061404 lr=0.000020 grad_norm=0.854107
Epoch 44/100 Iteration 30/234: loss=0.052306 lr=0.000020 grad_norm=0.367224
Epoch 44/100 Iteration 31/234: loss=0.057435 lr=0.000020 grad_norm=0.633208
Epoch 44/100 Iteration 32/234: loss=0.060453 lr=0.000020 grad_norm=0.926246
Epoch 44/100 Iteration 33/234: loss=0.068032 lr=0.000020 grad_norm=0.657552
Epoch 44/100 Iteration 34/234: loss=0.065082 lr=0.000020 grad_norm=0.479550
Epoch 44/100 Iteration 35/234: loss=0.060440 lr=0.000020 grad_norm=0.846553
Epoch 44/100 Iteration 36/234: loss=0.069192 lr=0.000020 grad_norm=0.688475
Epoch 44/100 Iteration 37/234: loss=0.059598 lr=0.000020 grad_norm=0.465581
Epoch 44/100 Iteration 38/234: loss=0.060782 lr=0.000020 grad_norm=0.479710
Epoch 44/100 Iteration 39/234: loss=0.059784 lr=0.000020 grad_norm=0.624420
Epoch 44/100 Iteration 40/234: loss=0.060618 lr=0.000020 grad_norm=0.495635
Epoch 44/100 Iteration 41/234: loss=0.062927 lr=0.000020 grad_norm=0.710391
Epoch 44/100 Iteration 42/234: loss=0.066961 lr=0.000020 grad_norm=1.514311
Epoch 44/100 Iteration 43/234: loss=0.059091 lr=0.000020 grad_norm=1.771918
Epoch 44/100 Iteration 44/234: loss=0.056382 lr=0.000020 grad_norm=0.666341
Epoch 44/100 Iteration 45/234: loss=0.066941 lr=0.000020 grad_norm=1.584517
Epoch 44/100 Iteration 46/234: loss=0.056793 lr=0.000020 grad_norm=2.150056
Epoch 44/100 Iteration 47/234: loss=0.068228 lr=0.000020 grad_norm=0.920112
Epoch 44/100 Iteration 48/234: loss=0.062975 lr=0.000020 grad_norm=1.679180
Epoch 44/100 Iteration 49/234: loss=0.059271 lr=0.000020 grad_norm=2.058663
Epoch 44/100 Iteration 50/234: loss=0.067612 lr=0.000020 grad_norm=0.971851
Epoch 44/100 Iteration 51/234: loss=0.065274 lr=0.000020 grad_norm=2.037311
Epoch 44/100 Iteration 52/234: loss=0.074652 lr=0.000020 grad_norm=1.574805
Epoch 44/100 Iteration 53/234: loss=0.066127 lr=0.000020 grad_norm=1.178018
Epoch 44/100 Iteration 54/234: loss=0.060988 lr=0.000020 grad_norm=1.339787
Epoch 44/100 Iteration 55/234: loss=0.059852 lr=0.000020 grad_norm=1.148957
Epoch 44/100 Iteration 56/234: loss=0.062324 lr=0.000020 grad_norm=1.091130
Epoch 44/100 Iteration 57/234: loss=0.065491 lr=0.000020 grad_norm=0.983513
Epoch 44/100 Iteration 58/234: loss=0.062731 lr=0.000020 grad_norm=0.669139
Epoch 44/100 Iteration 59/234: loss=0.063418 lr=0.000020 grad_norm=0.592379
Epoch 44/100 Iteration 60/234: loss=0.062503 lr=0.000020 grad_norm=0.595719
Epoch 44/100 Iteration 61/234: loss=0.059431 lr=0.000020 grad_norm=0.535276
Epoch 44/100 Iteration 62/234: loss=0.063849 lr=0.000020 grad_norm=0.634066
Epoch 44/100 Iteration 63/234: loss=0.067342 lr=0.000020 grad_norm=0.558911
Epoch 44/100 Iteration 64/234: loss=0.064154 lr=0.000020 grad_norm=0.500513
Epoch 44/100 Iteration 65/234: loss=0.072358 lr=0.000020 grad_norm=0.443068
Epoch 44/100 Iteration 66/234: loss=0.065056 lr=0.000020 grad_norm=0.520571
Epoch 44/100 Iteration 67/234: loss=0.062227 lr=0.000020 grad_norm=0.594497
Epoch 44/100 Iteration 68/234: loss=0.061862 lr=0.000020 grad_norm=0.649951
Epoch 44/100 Iteration 69/234: loss=0.061461 lr=0.000020 grad_norm=0.551400
Epoch 44/100 Iteration 70/234: loss=0.066197 lr=0.000020 grad_norm=0.884729
Epoch 44/100 Iteration 71/234: loss=0.059488 lr=0.000020 grad_norm=1.198466
Epoch 44/100 Iteration 72/234: loss=0.072273 lr=0.000020 grad_norm=0.984871
Epoch 44/100 Iteration 73/234: loss=0.054731 lr=0.000020 grad_norm=0.514719
Epoch 44/100 Iteration 74/234: loss=0.056441 lr=0.000020 grad_norm=0.466733
Epoch 44/100 Iteration 75/234: loss=0.058161 lr=0.000020 grad_norm=0.704952
Epoch 44/100 Iteration 76/234: loss=0.059461 lr=0.000020 grad_norm=0.520890
Epoch 44/100 Iteration 77/234: loss=0.055423 lr=0.000020 grad_norm=0.502864
Epoch 44/100 Iteration 78/234: loss=0.064756 lr=0.000020 grad_norm=0.413603
Epoch 44/100 Iteration 79/234: loss=0.059969 lr=0.000020 grad_norm=0.386190
Epoch 44/100 Iteration 80/234: loss=0.062053 lr=0.000020 grad_norm=0.371723
Epoch 44/100 Iteration 81/234: loss=0.067545 lr=0.000020 grad_norm=0.450987
Epoch 44/100 Iteration 82/234: loss=0.063443 lr=0.000020 grad_norm=0.412256
Epoch 44/100 Iteration 83/234: loss=0.060398 lr=0.000020 grad_norm=0.714680
Epoch 44/100 Iteration 84/234: loss=0.072867 lr=0.000020 grad_norm=0.928972
Epoch 44/100 Iteration 85/234: loss=0.064439 lr=0.000020 grad_norm=0.544418
Epoch 44/100 Iteration 86/234: loss=0.062682 lr=0.000020 grad_norm=0.799946
Epoch 44/100 Iteration 87/234: loss=0.065858 lr=0.000020 grad_norm=1.405213
Epoch 44/100 Iteration 88/234: loss=0.057383 lr=0.000020 grad_norm=1.023567
Epoch 44/100 Iteration 89/234: loss=0.060133 lr=0.000020 grad_norm=0.553981
Epoch 44/100 Iteration 90/234: loss=0.067340 lr=0.000020 grad_norm=1.086280
Epoch 44/100 Iteration 91/234: loss=0.052663 lr=0.000020 grad_norm=0.820114
Epoch 44/100 Iteration 92/234: loss=0.067082 lr=0.000020 grad_norm=0.531865
Epoch 44/100 Iteration 93/234: loss=0.063339 lr=0.000020 grad_norm=1.337586
Epoch 44/100 Iteration 94/234: loss=0.049882 lr=0.000020 grad_norm=1.001791
Epoch 44/100 Iteration 95/234: loss=0.069791 lr=0.000020 grad_norm=0.755966
Epoch 44/100 Iteration 96/234: loss=0.065396 lr=0.000020 grad_norm=2.583415
Epoch 44/100 Iteration 97/234: loss=0.071668 lr=0.000020 grad_norm=2.901387
Epoch 44/100 Iteration 98/234: loss=0.066216 lr=0.000020 grad_norm=0.954272
Epoch 44/100 Iteration 99/234: loss=0.061460 lr=0.000020 grad_norm=1.812489
Epoch 44/100 Iteration 100/234: loss=0.073137 lr=0.000020 grad_norm=1.755662
Epoch 44/100 Iteration 101/234: loss=0.052323 lr=0.000020 grad_norm=0.854881
Epoch 44/100 Iteration 102/234: loss=0.061616 lr=0.000020 grad_norm=1.854892
Epoch 44/100 Iteration 103/234: loss=0.056811 lr=0.000020 grad_norm=0.891910
Epoch 44/100 Iteration 104/234: loss=0.056732 lr=0.000020 grad_norm=1.126301
Epoch 44/100 Iteration 105/234: loss=0.078709 lr=0.000020 grad_norm=0.669519
Epoch 44/100 Iteration 106/234: loss=0.058194 lr=0.000020 grad_norm=0.764919
Epoch 44/100 Iteration 107/234: loss=0.058747 lr=0.000020 grad_norm=0.723915
Epoch 44/100 Iteration 108/234: loss=0.065786 lr=0.000020 grad_norm=0.496770
Epoch 44/100 Iteration 109/234: loss=0.057973 lr=0.000020 grad_norm=0.829850
Epoch 44/100 Iteration 110/234: loss=0.065987 lr=0.000020 grad_norm=0.814653
Epoch 44/100 Iteration 111/234: loss=0.073557 lr=0.000020 grad_norm=0.469764
Epoch 44/100 Iteration 112/234: loss=0.066433 lr=0.000020 grad_norm=0.714791
Epoch 44/100 Iteration 113/234: loss=0.058365 lr=0.000020 grad_norm=0.642559
Epoch 44/100 Iteration 114/234: loss=0.058706 lr=0.000020 grad_norm=0.442535
Epoch 44/100 Iteration 115/234: loss=0.059287 lr=0.000020 grad_norm=0.554151
Epoch 44/100 Iteration 116/234: loss=0.063088 lr=0.000020 grad_norm=0.423703
Epoch 44/100 Iteration 117/234: loss=0.061588 lr=0.000020 grad_norm=0.465851
Epoch 44/100 Iteration 118/234: loss=0.060469 lr=0.000020 grad_norm=0.560848
Epoch 44/100 Iteration 119/234: loss=0.066156 lr=0.000020 grad_norm=0.854034
Epoch 44/100 Iteration 120/234: loss=0.062539 lr=0.000020 grad_norm=0.451076
Epoch 44/100 Iteration 121/234: loss=0.063389 lr=0.000020 grad_norm=0.726294
Epoch 44/100 Iteration 122/234: loss=0.066267 lr=0.000020 grad_norm=0.928386
Epoch 44/100 Iteration 123/234: loss=0.060103 lr=0.000020 grad_norm=0.551138
Epoch 44/100 Iteration 124/234: loss=0.057567 lr=0.000020 grad_norm=0.535000
Epoch 44/100 Iteration 125/234: loss=0.060208 lr=0.000020 grad_norm=0.793559
Epoch 44/100 Iteration 126/234: loss=0.068392 lr=0.000020 grad_norm=0.709743
Epoch 44/100 Iteration 127/234: loss=0.068704 lr=0.000020 grad_norm=0.582449
Epoch 44/100 Iteration 128/234: loss=0.060212 lr=0.000020 grad_norm=0.791823
Epoch 44/100 Iteration 129/234: loss=0.062469 lr=0.000020 grad_norm=0.740130
Epoch 44/100 Iteration 130/234: loss=0.058215 lr=0.000020 grad_norm=0.623408
Epoch 44/100 Iteration 131/234: loss=0.067132 lr=0.000020 grad_norm=0.537585
Epoch 44/100 Iteration 132/234: loss=0.054372 lr=0.000020 grad_norm=0.559913
Epoch 44/100 Iteration 133/234: loss=0.059068 lr=0.000020 grad_norm=0.535828
Epoch 44/100 Iteration 134/234: loss=0.062115 lr=0.000020 grad_norm=0.325772
Epoch 44/100 Iteration 135/234: loss=0.069516 lr=0.000020 grad_norm=0.525603
Epoch 44/100 Iteration 136/234: loss=0.057332 lr=0.000020 grad_norm=0.796172
Epoch 44/100 Iteration 137/234: loss=0.053074 lr=0.000020 grad_norm=0.563684
Epoch 44/100 Iteration 138/234: loss=0.067194 lr=0.000020 grad_norm=0.436236
Epoch 44/100 Iteration 139/234: loss=0.056889 lr=0.000020 grad_norm=0.572318
Epoch 44/100 Iteration 140/234: loss=0.064451 lr=0.000020 grad_norm=0.999574
Epoch 44/100 Iteration 141/234: loss=0.060334 lr=0.000020 grad_norm=0.676746
Epoch 44/100 Iteration 142/234: loss=0.056770 lr=0.000020 grad_norm=0.592472
Epoch 44/100 Iteration 143/234: loss=0.052640 lr=0.000020 grad_norm=0.562472
Epoch 44/100 Iteration 144/234: loss=0.052489 lr=0.000020 grad_norm=0.427984
Epoch 44/100 Iteration 145/234: loss=0.062960 lr=0.000020 grad_norm=0.475866
Epoch 44/100 Iteration 146/234: loss=0.056671 lr=0.000020 grad_norm=0.432286
Epoch 44/100 Iteration 147/234: loss=0.059510 lr=0.000020 grad_norm=0.758830
Epoch 44/100 Iteration 148/234: loss=0.075654 lr=0.000020 grad_norm=1.116409
Epoch 44/100 Iteration 149/234: loss=0.060499 lr=0.000020 grad_norm=0.837957
Epoch 44/100 Iteration 150/234: loss=0.052807 lr=0.000020 grad_norm=0.582002
Epoch 44/100 Iteration 151/234: loss=0.063023 lr=0.000020 grad_norm=0.583442
Epoch 44/100 Iteration 152/234: loss=0.068605 lr=0.000020 grad_norm=0.850551
Epoch 44/100 Iteration 153/234: loss=0.056509 lr=0.000020 grad_norm=0.789894
Epoch 44/100 Iteration 154/234: loss=0.054344 lr=0.000020 grad_norm=0.438481
Epoch 44/100 Iteration 155/234: loss=0.065548 lr=0.000020 grad_norm=0.560281
Epoch 44/100 Iteration 156/234: loss=0.052194 lr=0.000020 grad_norm=0.626536
Epoch 44/100 Iteration 157/234: loss=0.064067 lr=0.000020 grad_norm=0.592962
Epoch 44/100 Iteration 158/234: loss=0.059874 lr=0.000020 grad_norm=0.531405
Epoch 44/100 Iteration 159/234: loss=0.060127 lr=0.000020 grad_norm=0.682593
Epoch 44/100 Iteration 160/234: loss=0.079483 lr=0.000020 grad_norm=0.549516
Epoch 44/100 Iteration 161/234: loss=0.055885 lr=0.000020 grad_norm=0.757903
Epoch 44/100 Iteration 162/234: loss=0.061672 lr=0.000020 grad_norm=0.559749
Epoch 44/100 Iteration 163/234: loss=0.068427 lr=0.000020 grad_norm=0.514091
Epoch 44/100 Iteration 164/234: loss=0.056239 lr=0.000020 grad_norm=0.785704
Epoch 44/100 Iteration 165/234: loss=0.059003 lr=0.000020 grad_norm=1.045928
Epoch 44/100 Iteration 166/234: loss=0.064324 lr=0.000020 grad_norm=1.303911
Epoch 44/100 Iteration 167/234: loss=0.065951 lr=0.000020 grad_norm=1.258069
Epoch 44/100 Iteration 168/234: loss=0.060461 lr=0.000020 grad_norm=0.918552
Epoch 44/100 Iteration 169/234: loss=0.064682 lr=0.000020 grad_norm=0.629917
Epoch 44/100 Iteration 170/234: loss=0.058030 lr=0.000020 grad_norm=0.484847
Epoch 44/100 Iteration 171/234: loss=0.057075 lr=0.000020 grad_norm=0.807115
Epoch 44/100 Iteration 172/234: loss=0.067820 lr=0.000020 grad_norm=0.711639
Epoch 44/100 Iteration 173/234: loss=0.066113 lr=0.000020 grad_norm=0.475519
Epoch 44/100 Iteration 174/234: loss=0.060336 lr=0.000020 grad_norm=0.367027
Epoch 44/100 Iteration 175/234: loss=0.060244 lr=0.000020 grad_norm=0.334133
Epoch 44/100 Iteration 176/234: loss=0.059971 lr=0.000020 grad_norm=0.727230
Epoch 44/100 Iteration 177/234: loss=0.058128 lr=0.000020 grad_norm=0.837359
Epoch 44/100 Iteration 178/234: loss=0.058067 lr=0.000020 grad_norm=0.399926
Epoch 44/100 Iteration 179/234: loss=0.060848 lr=0.000020 grad_norm=1.059245
Epoch 44/100 Iteration 180/234: loss=0.065201 lr=0.000020 grad_norm=1.594267
Epoch 44/100 Iteration 181/234: loss=0.062258 lr=0.000020 grad_norm=1.166211
Epoch 44/100 Iteration 182/234: loss=0.063370 lr=0.000020 grad_norm=0.421928
Epoch 44/100 Iteration 183/234: loss=0.061209 lr=0.000020 grad_norm=1.087747
Epoch 44/100 Iteration 184/234: loss=0.068471 lr=0.000020 grad_norm=1.744926
Epoch 44/100 Iteration 185/234: loss=0.068296 lr=0.000020 grad_norm=1.903229
Epoch 44/100 Iteration 186/234: loss=0.067237 lr=0.000020 grad_norm=1.220288
Epoch 44/100 Iteration 187/234: loss=0.068250 lr=0.000020 grad_norm=0.841514
Epoch 44/100 Iteration 188/234: loss=0.059342 lr=0.000020 grad_norm=0.848242
Epoch 44/100 Iteration 189/234: loss=0.064903 lr=0.000020 grad_norm=0.654738
Epoch 44/100 Iteration 190/234: loss=0.067413 lr=0.000020 grad_norm=0.658613
Epoch 44/100 Iteration 191/234: loss=0.056235 lr=0.000020 grad_norm=0.824754
Epoch 44/100 Iteration 192/234: loss=0.058239 lr=0.000020 grad_norm=0.793794
Epoch 44/100 Iteration 193/234: loss=0.067825 lr=0.000020 grad_norm=0.596896
Epoch 44/100 Iteration 194/234: loss=0.065633 lr=0.000020 grad_norm=1.238101
Epoch 44/100 Iteration 195/234: loss=0.061647 lr=0.000020 grad_norm=1.552177
Epoch 44/100 Iteration 196/234: loss=0.064438 lr=0.000020 grad_norm=0.726549
Epoch 44/100 Iteration 197/234: loss=0.059292 lr=0.000020 grad_norm=1.577946
Epoch 44/100 Iteration 198/234: loss=0.069722 lr=0.000020 grad_norm=2.095749
Epoch 44/100 Iteration 199/234: loss=0.065152 lr=0.000020 grad_norm=1.077160
Epoch 44/100 Iteration 200/234: loss=0.062348 lr=0.000020 grad_norm=0.995371
Epoch 44/100 Iteration 201/234: loss=0.061100 lr=0.000020 grad_norm=1.438789
Epoch 44/100 Iteration 202/234: loss=0.056903 lr=0.000020 grad_norm=0.657445
Epoch 44/100 Iteration 203/234: loss=0.061998 lr=0.000020 grad_norm=1.226792
Epoch 44/100 Iteration 204/234: loss=0.063374 lr=0.000020 grad_norm=1.267860
Epoch 44/100 Iteration 205/234: loss=0.068065 lr=0.000020 grad_norm=0.537442
Epoch 44/100 Iteration 206/234: loss=0.062184 lr=0.000020 grad_norm=1.763745
Epoch 44/100 Iteration 207/234: loss=0.058171 lr=0.000020 grad_norm=1.574680
Epoch 44/100 Iteration 208/234: loss=0.064526 lr=0.000020 grad_norm=0.797142
Epoch 44/100 Iteration 209/234: loss=0.060722 lr=0.000020 grad_norm=1.410943
Epoch 44/100 Iteration 210/234: loss=0.063172 lr=0.000020 grad_norm=1.171891
Epoch 44/100 Iteration 211/234: loss=0.065556 lr=0.000020 grad_norm=0.746805
Epoch 44/100 Iteration 212/234: loss=0.066208 lr=0.000020 grad_norm=1.271106
Epoch 44/100 Iteration 213/234: loss=0.061370 lr=0.000020 grad_norm=1.230095
Epoch 44/100 Iteration 214/234: loss=0.062584 lr=0.000020 grad_norm=0.666888
Epoch 44/100 Iteration 215/234: loss=0.058411 lr=0.000020 grad_norm=0.927632
Epoch 44/100 Iteration 216/234: loss=0.072969 lr=0.000020 grad_norm=0.763458
Epoch 44/100 Iteration 217/234: loss=0.062229 lr=0.000020 grad_norm=0.665426
Epoch 44/100 Iteration 218/234: loss=0.060987 lr=0.000020 grad_norm=0.542676
Epoch 44/100 Iteration 219/234: loss=0.061400 lr=0.000020 grad_norm=0.470737
Epoch 44/100 Iteration 220/234: loss=0.060244 lr=0.000020 grad_norm=0.613281
Epoch 44/100 Iteration 221/234: loss=0.057918 lr=0.000020 grad_norm=0.622287
Epoch 44/100 Iteration 222/234: loss=0.064034 lr=0.000020 grad_norm=0.505125
Epoch 44/100 Iteration 223/234: loss=0.069203 lr=0.000020 grad_norm=1.109924
Epoch 44/100 Iteration 224/234: loss=0.055872 lr=0.000020 grad_norm=1.239573
Epoch 44/100 Iteration 225/234: loss=0.068439 lr=0.000020 grad_norm=0.939419
Epoch 44/100 Iteration 226/234: loss=0.064591 lr=0.000020 grad_norm=0.811076
Epoch 44/100 Iteration 227/234: loss=0.069924 lr=0.000020 grad_norm=0.995834
Epoch 44/100 Iteration 228/234: loss=0.067335 lr=0.000020 grad_norm=1.602638
Epoch 44/100 Iteration 229/234: loss=0.067794 lr=0.000020 grad_norm=1.073029
Epoch 44/100 Iteration 230/234: loss=0.053076 lr=0.000020 grad_norm=0.737645
Epoch 44/100 Iteration 231/234: loss=0.067088 lr=0.000020 grad_norm=1.185843
Epoch 44/100 Iteration 232/234: loss=0.064607 lr=0.000020 grad_norm=0.425636
Epoch 44/100 Iteration 233/234: loss=0.059866 lr=0.000020 grad_norm=1.110586
Epoch 44/100 Iteration 234/234: loss=0.072287 lr=0.000020 grad_norm=1.628730
Epoch 44/100 finished. Avg Loss: 0.062881
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 45/100 Iteration 1/234: loss=0.066476 lr=0.000020 grad_norm=1.262947
Epoch 45/100 Iteration 2/234: loss=0.059405 lr=0.000020 grad_norm=0.430345
Epoch 45/100 Iteration 3/234: loss=0.053243 lr=0.000020 grad_norm=1.057438
Epoch 45/100 Iteration 4/234: loss=0.065866 lr=0.000020 grad_norm=0.852158
Epoch 45/100 Iteration 5/234: loss=0.065498 lr=0.000020 grad_norm=0.739957
Epoch 45/100 Iteration 6/234: loss=0.067996 lr=0.000020 grad_norm=1.332446
Epoch 45/100 Iteration 7/234: loss=0.065606 lr=0.000020 grad_norm=1.136800
Epoch 45/100 Iteration 8/234: loss=0.061939 lr=0.000020 grad_norm=0.556381
Epoch 45/100 Iteration 9/234: loss=0.058175 lr=0.000020 grad_norm=0.613557
Epoch 45/100 Iteration 10/234: loss=0.053780 lr=0.000020 grad_norm=0.643145
Epoch 45/100 Iteration 11/234: loss=0.054357 lr=0.000020 grad_norm=0.486511
Epoch 45/100 Iteration 12/234: loss=0.059093 lr=0.000020 grad_norm=0.798640
Epoch 45/100 Iteration 13/234: loss=0.071402 lr=0.000020 grad_norm=0.422898
Epoch 45/100 Iteration 14/234: loss=0.054968 lr=0.000020 grad_norm=0.910740
Epoch 45/100 Iteration 15/234: loss=0.057359 lr=0.000020 grad_norm=0.550966
Epoch 45/100 Iteration 16/234: loss=0.061450 lr=0.000020 grad_norm=0.607293
Epoch 45/100 Iteration 17/234: loss=0.067045 lr=0.000020 grad_norm=0.967937
Epoch 45/100 Iteration 18/234: loss=0.064677 lr=0.000020 grad_norm=1.123422
Epoch 45/100 Iteration 19/234: loss=0.053608 lr=0.000020 grad_norm=1.078194
Epoch 45/100 Iteration 20/234: loss=0.060376 lr=0.000020 grad_norm=0.682772
Epoch 45/100 Iteration 21/234: loss=0.058777 lr=0.000020 grad_norm=0.331929
Epoch 45/100 Iteration 22/234: loss=0.065767 lr=0.000020 grad_norm=0.605883
Epoch 45/100 Iteration 23/234: loss=0.065720 lr=0.000020 grad_norm=0.513801
Epoch 45/100 Iteration 24/234: loss=0.062986 lr=0.000020 grad_norm=0.350684
Epoch 45/100 Iteration 25/234: loss=0.062505 lr=0.000020 grad_norm=0.456925
Epoch 45/100 Iteration 26/234: loss=0.062310 lr=0.000020 grad_norm=0.626796
Epoch 45/100 Iteration 27/234: loss=0.065474 lr=0.000020 grad_norm=0.514793
Epoch 45/100 Iteration 28/234: loss=0.065305 lr=0.000020 grad_norm=0.636943
Epoch 45/100 Iteration 29/234: loss=0.065380 lr=0.000020 grad_norm=0.573905
Epoch 45/100 Iteration 30/234: loss=0.056730 lr=0.000020 grad_norm=0.416715
Epoch 45/100 Iteration 31/234: loss=0.054286 lr=0.000020 grad_norm=0.776462
Epoch 45/100 Iteration 32/234: loss=0.069140 lr=0.000020 grad_norm=0.934448
Epoch 45/100 Iteration 33/234: loss=0.058514 lr=0.000020 grad_norm=0.605664
Epoch 45/100 Iteration 34/234: loss=0.058389 lr=0.000020 grad_norm=0.471715
Epoch 45/100 Iteration 35/234: loss=0.050251 lr=0.000020 grad_norm=0.605875
Epoch 45/100 Iteration 36/234: loss=0.062842 lr=0.000020 grad_norm=0.799276
Epoch 45/100 Iteration 37/234: loss=0.063129 lr=0.000020 grad_norm=0.737780
Epoch 45/100 Iteration 38/234: loss=0.054448 lr=0.000020 grad_norm=0.650380
Epoch 45/100 Iteration 39/234: loss=0.060751 lr=0.000020 grad_norm=0.532944
Epoch 45/100 Iteration 40/234: loss=0.062772 lr=0.000020 grad_norm=0.627776
Epoch 45/100 Iteration 41/234: loss=0.064237 lr=0.000020 grad_norm=0.951253
Epoch 45/100 Iteration 42/234: loss=0.056091 lr=0.000020 grad_norm=0.816082
Epoch 45/100 Iteration 43/234: loss=0.059973 lr=0.000020 grad_norm=0.596017
Epoch 45/100 Iteration 44/234: loss=0.057534 lr=0.000020 grad_norm=0.498476
Epoch 45/100 Iteration 45/234: loss=0.056302 lr=0.000020 grad_norm=0.483534
Epoch 45/100 Iteration 46/234: loss=0.056770 lr=0.000020 grad_norm=0.413550
Epoch 45/100 Iteration 47/234: loss=0.061838 lr=0.000020 grad_norm=0.532282
Epoch 45/100 Iteration 48/234: loss=0.058982 lr=0.000020 grad_norm=0.526136
Epoch 45/100 Iteration 49/234: loss=0.066970 lr=0.000020 grad_norm=0.661467
Epoch 45/100 Iteration 50/234: loss=0.054720 lr=0.000020 grad_norm=0.879029
Epoch 45/100 Iteration 51/234: loss=0.055067 lr=0.000020 grad_norm=0.886488
Epoch 45/100 Iteration 52/234: loss=0.058808 lr=0.000020 grad_norm=1.040810
Epoch 45/100 Iteration 53/234: loss=0.067555 lr=0.000020 grad_norm=1.200922
Epoch 45/100 Iteration 54/234: loss=0.060200 lr=0.000020 grad_norm=0.810098
Epoch 45/100 Iteration 55/234: loss=0.052191 lr=0.000020 grad_norm=0.557597
Epoch 45/100 Iteration 56/234: loss=0.064942 lr=0.000020 grad_norm=0.851580
Epoch 45/100 Iteration 57/234: loss=0.060357 lr=0.000020 grad_norm=0.970396
Epoch 45/100 Iteration 58/234: loss=0.056542 lr=0.000020 grad_norm=0.743327
Epoch 45/100 Iteration 59/234: loss=0.061946 lr=0.000020 grad_norm=0.502370
Epoch 45/100 Iteration 60/234: loss=0.067757 lr=0.000020 grad_norm=0.875564
Epoch 45/100 Iteration 61/234: loss=0.054729 lr=0.000020 grad_norm=0.934276
Epoch 45/100 Iteration 62/234: loss=0.059817 lr=0.000020 grad_norm=0.472304
Epoch 45/100 Iteration 63/234: loss=0.068947 lr=0.000020 grad_norm=0.830236
Epoch 45/100 Iteration 64/234: loss=0.070629 lr=0.000020 grad_norm=0.940460
Epoch 45/100 Iteration 65/234: loss=0.065203 lr=0.000020 grad_norm=0.547717
Epoch 45/100 Iteration 66/234: loss=0.070821 lr=0.000020 grad_norm=0.891483
Epoch 45/100 Iteration 67/234: loss=0.064543 lr=0.000020 grad_norm=1.371221
Epoch 45/100 Iteration 68/234: loss=0.057384 lr=0.000020 grad_norm=1.601715
Epoch 45/100 Iteration 69/234: loss=0.054192 lr=0.000020 grad_norm=1.528918
Epoch 45/100 Iteration 70/234: loss=0.058738 lr=0.000020 grad_norm=0.639218
Epoch 45/100 Iteration 71/234: loss=0.058482 lr=0.000020 grad_norm=1.124207
Epoch 45/100 Iteration 72/234: loss=0.067653 lr=0.000020 grad_norm=1.216760
Epoch 45/100 Iteration 73/234: loss=0.070805 lr=0.000020 grad_norm=0.661153
Epoch 45/100 Iteration 74/234: loss=0.070890 lr=0.000020 grad_norm=0.637242
Epoch 45/100 Iteration 75/234: loss=0.068167 lr=0.000020 grad_norm=0.875921
Epoch 45/100 Iteration 76/234: loss=0.061406 lr=0.000020 grad_norm=0.922102
Epoch 45/100 Iteration 77/234: loss=0.065985 lr=0.000020 grad_norm=0.669859
Epoch 45/100 Iteration 78/234: loss=0.057165 lr=0.000020 grad_norm=0.567970
Epoch 45/100 Iteration 79/234: loss=0.055299 lr=0.000020 grad_norm=0.604511
Epoch 45/100 Iteration 80/234: loss=0.062809 lr=0.000020 grad_norm=0.491265
Epoch 45/100 Iteration 81/234: loss=0.060866 lr=0.000020 grad_norm=0.555223
Epoch 45/100 Iteration 82/234: loss=0.060015 lr=0.000020 grad_norm=0.554739
Epoch 45/100 Iteration 83/234: loss=0.062639 lr=0.000020 grad_norm=1.351038
Epoch 45/100 Iteration 84/234: loss=0.064674 lr=0.000020 grad_norm=1.621723
Epoch 45/100 Iteration 85/234: loss=0.063809 lr=0.000020 grad_norm=0.954983
Epoch 45/100 Iteration 86/234: loss=0.064918 lr=0.000020 grad_norm=0.557376
Epoch 45/100 Iteration 87/234: loss=0.052950 lr=0.000020 grad_norm=0.794110
Epoch 45/100 Iteration 88/234: loss=0.057502 lr=0.000020 grad_norm=0.470296
Epoch 45/100 Iteration 89/234: loss=0.057045 lr=0.000020 grad_norm=0.480221
Epoch 45/100 Iteration 90/234: loss=0.057203 lr=0.000020 grad_norm=0.485650
Epoch 45/100 Iteration 91/234: loss=0.063766 lr=0.000020 grad_norm=0.753209
Epoch 45/100 Iteration 92/234: loss=0.061364 lr=0.000020 grad_norm=0.926571
Epoch 45/100 Iteration 93/234: loss=0.057855 lr=0.000020 grad_norm=0.497038
Epoch 45/100 Iteration 94/234: loss=0.067641 lr=0.000020 grad_norm=1.013252
Epoch 45/100 Iteration 95/234: loss=0.061387 lr=0.000020 grad_norm=1.463729
Epoch 45/100 Iteration 96/234: loss=0.059602 lr=0.000020 grad_norm=0.968391
Epoch 45/100 Iteration 97/234: loss=0.067550 lr=0.000020 grad_norm=0.538826
Epoch 45/100 Iteration 98/234: loss=0.058376 lr=0.000020 grad_norm=1.099454
Epoch 45/100 Iteration 99/234: loss=0.056921 lr=0.000020 grad_norm=0.793178
Epoch 45/100 Iteration 100/234: loss=0.063535 lr=0.000020 grad_norm=0.447441
Epoch 45/100 Iteration 101/234: loss=0.063183 lr=0.000020 grad_norm=0.736687
Epoch 45/100 Iteration 102/234: loss=0.064801 lr=0.000020 grad_norm=0.802289
Epoch 45/100 Iteration 103/234: loss=0.069041 lr=0.000020 grad_norm=0.603028
Epoch 45/100 Iteration 104/234: loss=0.062648 lr=0.000020 grad_norm=0.584379
Epoch 45/100 Iteration 105/234: loss=0.067067 lr=0.000020 grad_norm=1.259691
Epoch 45/100 Iteration 106/234: loss=0.055840 lr=0.000020 grad_norm=1.470121
Epoch 45/100 Iteration 107/234: loss=0.061341 lr=0.000020 grad_norm=1.095536
Epoch 45/100 Iteration 108/234: loss=0.057807 lr=0.000020 grad_norm=0.367520
Epoch 45/100 Iteration 109/234: loss=0.057368 lr=0.000020 grad_norm=1.016750
Epoch 45/100 Iteration 110/234: loss=0.060201 lr=0.000020 grad_norm=1.189131
Epoch 45/100 Iteration 111/234: loss=0.065674 lr=0.000020 grad_norm=0.725645
Epoch 45/100 Iteration 112/234: loss=0.059395 lr=0.000020 grad_norm=0.419870
Epoch 45/100 Iteration 113/234: loss=0.062715 lr=0.000020 grad_norm=1.038275
Epoch 45/100 Iteration 114/234: loss=0.063329 lr=0.000020 grad_norm=1.098785
Epoch 45/100 Iteration 115/234: loss=0.053851 lr=0.000020 grad_norm=0.459947
Epoch 45/100 Iteration 116/234: loss=0.056048 lr=0.000020 grad_norm=0.802239
Epoch 45/100 Iteration 117/234: loss=0.058210 lr=0.000020 grad_norm=1.054267
Epoch 45/100 Iteration 118/234: loss=0.066393 lr=0.000020 grad_norm=1.193862
Epoch 45/100 Iteration 119/234: loss=0.066556 lr=0.000020 grad_norm=0.992286
Epoch 45/100 Iteration 120/234: loss=0.061615 lr=0.000020 grad_norm=0.416078
Epoch 45/100 Iteration 121/234: loss=0.065861 lr=0.000020 grad_norm=0.947450
Epoch 45/100 Iteration 122/234: loss=0.058207 lr=0.000020 grad_norm=1.165489
Epoch 45/100 Iteration 123/234: loss=0.063534 lr=0.000020 grad_norm=1.297027
Epoch 45/100 Iteration 124/234: loss=0.059598 lr=0.000020 grad_norm=0.687460
Epoch 45/100 Iteration 125/234: loss=0.058892 lr=0.000020 grad_norm=0.579800
Epoch 45/100 Iteration 126/234: loss=0.058587 lr=0.000020 grad_norm=0.749317
Epoch 45/100 Iteration 127/234: loss=0.051300 lr=0.000020 grad_norm=0.551535
Epoch 45/100 Iteration 128/234: loss=0.058403 lr=0.000020 grad_norm=0.608785
Epoch 45/100 Iteration 129/234: loss=0.054282 lr=0.000020 grad_norm=1.054142
Epoch 45/100 Iteration 130/234: loss=0.069584 lr=0.000020 grad_norm=0.923744
Epoch 45/100 Iteration 131/234: loss=0.062462 lr=0.000020 grad_norm=0.710920
Epoch 45/100 Iteration 132/234: loss=0.066572 lr=0.000020 grad_norm=0.901497
Epoch 45/100 Iteration 133/234: loss=0.068304 lr=0.000020 grad_norm=0.550640
Epoch 45/100 Iteration 134/234: loss=0.071336 lr=0.000020 grad_norm=0.620036
Epoch 45/100 Iteration 135/234: loss=0.058392 lr=0.000020 grad_norm=1.089130
Epoch 45/100 Iteration 136/234: loss=0.057260 lr=0.000020 grad_norm=0.893414
Epoch 45/100 Iteration 137/234: loss=0.059727 lr=0.000020 grad_norm=0.372166
Epoch 45/100 Iteration 138/234: loss=0.060476 lr=0.000020 grad_norm=0.690160
Epoch 45/100 Iteration 139/234: loss=0.060526 lr=0.000020 grad_norm=0.908709
Epoch 45/100 Iteration 140/234: loss=0.066325 lr=0.000020 grad_norm=0.836991
Epoch 45/100 Iteration 141/234: loss=0.064085 lr=0.000020 grad_norm=1.148193
Epoch 45/100 Iteration 142/234: loss=0.062429 lr=0.000020 grad_norm=1.212060
Epoch 45/100 Iteration 143/234: loss=0.066290 lr=0.000020 grad_norm=0.725474
Epoch 45/100 Iteration 144/234: loss=0.061452 lr=0.000020 grad_norm=0.682027
Epoch 45/100 Iteration 145/234: loss=0.064219 lr=0.000020 grad_norm=1.239059
Epoch 45/100 Iteration 146/234: loss=0.067857 lr=0.000020 grad_norm=1.145943
Epoch 45/100 Iteration 147/234: loss=0.062244 lr=0.000020 grad_norm=0.800079
Epoch 45/100 Iteration 148/234: loss=0.063271 lr=0.000020 grad_norm=0.481815
Epoch 45/100 Iteration 149/234: loss=0.062593 lr=0.000020 grad_norm=0.688118
Epoch 45/100 Iteration 150/234: loss=0.053126 lr=0.000020 grad_norm=1.127745
Epoch 45/100 Iteration 151/234: loss=0.066546 lr=0.000020 grad_norm=0.846831
Epoch 45/100 Iteration 152/234: loss=0.065471 lr=0.000020 grad_norm=0.483194
Epoch 45/100 Iteration 153/234: loss=0.069248 lr=0.000020 grad_norm=0.792325
Epoch 45/100 Iteration 154/234: loss=0.057153 lr=0.000020 grad_norm=1.380550
Epoch 45/100 Iteration 155/234: loss=0.062011 lr=0.000020 grad_norm=1.202892
Epoch 45/100 Iteration 156/234: loss=0.055504 lr=0.000020 grad_norm=0.551880
Epoch 45/100 Iteration 157/234: loss=0.070513 lr=0.000020 grad_norm=0.642744
Epoch 45/100 Iteration 158/234: loss=0.070963 lr=0.000020 grad_norm=0.940708
Epoch 45/100 Iteration 159/234: loss=0.063071 lr=0.000020 grad_norm=1.064664
Epoch 45/100 Iteration 160/234: loss=0.063553 lr=0.000020 grad_norm=1.071707
Epoch 45/100 Iteration 161/234: loss=0.060482 lr=0.000020 grad_norm=0.750835
Epoch 45/100 Iteration 162/234: loss=0.064055 lr=0.000020 grad_norm=0.803765
Epoch 45/100 Iteration 163/234: loss=0.061223 lr=0.000020 grad_norm=1.638419
Epoch 45/100 Iteration 164/234: loss=0.066304 lr=0.000020 grad_norm=1.289024
Epoch 45/100 Iteration 165/234: loss=0.066862 lr=0.000020 grad_norm=0.385875
Epoch 45/100 Iteration 166/234: loss=0.067773 lr=0.000020 grad_norm=1.068318
Epoch 45/100 Iteration 167/234: loss=0.059381 lr=0.000020 grad_norm=0.891980
Epoch 45/100 Iteration 168/234: loss=0.068715 lr=0.000020 grad_norm=0.721867
Epoch 45/100 Iteration 169/234: loss=0.070859 lr=0.000020 grad_norm=1.941073
Epoch 45/100 Iteration 170/234: loss=0.064879 lr=0.000020 grad_norm=2.570077
Epoch 45/100 Iteration 171/234: loss=0.061592 lr=0.000020 grad_norm=1.344975
Epoch 45/100 Iteration 172/234: loss=0.057727 lr=0.000020 grad_norm=1.104469
Epoch 45/100 Iteration 173/234: loss=0.058526 lr=0.000020 grad_norm=1.222813
Epoch 45/100 Iteration 174/234: loss=0.057117 lr=0.000020 grad_norm=0.961230
Epoch 45/100 Iteration 175/234: loss=0.062353 lr=0.000020 grad_norm=0.917386
Epoch 45/100 Iteration 176/234: loss=0.056150 lr=0.000020 grad_norm=0.884347
Epoch 45/100 Iteration 177/234: loss=0.063570 lr=0.000020 grad_norm=0.646947
Epoch 45/100 Iteration 178/234: loss=0.060366 lr=0.000020 grad_norm=0.512495
Epoch 45/100 Iteration 179/234: loss=0.066627 lr=0.000020 grad_norm=0.479498
Epoch 45/100 Iteration 180/234: loss=0.065167 lr=0.000020 grad_norm=0.583259
Epoch 45/100 Iteration 181/234: loss=0.061532 lr=0.000020 grad_norm=0.649435
Epoch 45/100 Iteration 182/234: loss=0.056238 lr=0.000020 grad_norm=0.458264
Epoch 45/100 Iteration 183/234: loss=0.053336 lr=0.000020 grad_norm=0.418013
Epoch 45/100 Iteration 184/234: loss=0.067017 lr=0.000020 grad_norm=0.601809
Epoch 45/100 Iteration 185/234: loss=0.061424 lr=0.000020 grad_norm=0.442607
Epoch 45/100 Iteration 186/234: loss=0.065592 lr=0.000020 grad_norm=0.826141
Epoch 45/100 Iteration 187/234: loss=0.056488 lr=0.000020 grad_norm=0.755858
Epoch 45/100 Iteration 188/234: loss=0.063089 lr=0.000020 grad_norm=0.563440
Epoch 45/100 Iteration 189/234: loss=0.053746 lr=0.000020 grad_norm=0.540137
Epoch 45/100 Iteration 190/234: loss=0.061383 lr=0.000020 grad_norm=0.503958
Epoch 45/100 Iteration 191/234: loss=0.065036 lr=0.000020 grad_norm=0.742536
Epoch 45/100 Iteration 192/234: loss=0.057411 lr=0.000020 grad_norm=0.858219
Epoch 45/100 Iteration 193/234: loss=0.061856 lr=0.000020 grad_norm=0.729091
Epoch 45/100 Iteration 194/234: loss=0.063555 lr=0.000020 grad_norm=1.088780
Epoch 45/100 Iteration 195/234: loss=0.057580 lr=0.000020 grad_norm=0.734174
Epoch 45/100 Iteration 196/234: loss=0.059706 lr=0.000020 grad_norm=0.755590
Epoch 45/100 Iteration 197/234: loss=0.059331 lr=0.000020 grad_norm=0.612379
Epoch 45/100 Iteration 198/234: loss=0.061848 lr=0.000020 grad_norm=0.959992
Epoch 45/100 Iteration 199/234: loss=0.049268 lr=0.000020 grad_norm=0.769899
Epoch 45/100 Iteration 200/234: loss=0.064269 lr=0.000020 grad_norm=0.542710
Epoch 45/100 Iteration 201/234: loss=0.051271 lr=0.000020 grad_norm=0.736003
Epoch 45/100 Iteration 202/234: loss=0.062370 lr=0.000020 grad_norm=0.859269
Epoch 45/100 Iteration 203/234: loss=0.065202 lr=0.000020 grad_norm=0.504034
Epoch 45/100 Iteration 204/234: loss=0.067066 lr=0.000020 grad_norm=0.428995
Epoch 45/100 Iteration 205/234: loss=0.069263 lr=0.000020 grad_norm=0.496000
Epoch 45/100 Iteration 206/234: loss=0.069226 lr=0.000020 grad_norm=0.542568
Epoch 45/100 Iteration 207/234: loss=0.069168 lr=0.000020 grad_norm=0.573293
Epoch 45/100 Iteration 208/234: loss=0.055530 lr=0.000020 grad_norm=0.380817
Epoch 45/100 Iteration 209/234: loss=0.055395 lr=0.000020 grad_norm=0.467613
Epoch 45/100 Iteration 210/234: loss=0.061149 lr=0.000020 grad_norm=0.699491
Epoch 45/100 Iteration 211/234: loss=0.061847 lr=0.000020 grad_norm=0.903402
Epoch 45/100 Iteration 212/234: loss=0.063225 lr=0.000020 grad_norm=0.783179
Epoch 45/100 Iteration 213/234: loss=0.057077 lr=0.000020 grad_norm=0.814269
Epoch 45/100 Iteration 214/234: loss=0.061312 lr=0.000020 grad_norm=0.525758
Epoch 45/100 Iteration 215/234: loss=0.060342 lr=0.000020 grad_norm=0.555081
Epoch 45/100 Iteration 216/234: loss=0.062734 lr=0.000020 grad_norm=0.915230
Epoch 45/100 Iteration 217/234: loss=0.056259 lr=0.000020 grad_norm=0.610814
Epoch 45/100 Iteration 218/234: loss=0.066251 lr=0.000020 grad_norm=0.636632
Epoch 45/100 Iteration 219/234: loss=0.061763 lr=0.000020 grad_norm=1.285161
Epoch 45/100 Iteration 220/234: loss=0.064542 lr=0.000020 grad_norm=1.982557
Epoch 45/100 Iteration 221/234: loss=0.048928 lr=0.000020 grad_norm=1.563020
Epoch 45/100 Iteration 222/234: loss=0.063687 lr=0.000020 grad_norm=0.551244
Epoch 45/100 Iteration 223/234: loss=0.062479 lr=0.000020 grad_norm=2.019511
Epoch 45/100 Iteration 224/234: loss=0.053298 lr=0.000020 grad_norm=1.708980
Epoch 45/100 Iteration 225/234: loss=0.059888 lr=0.000020 grad_norm=0.585379
Epoch 45/100 Iteration 226/234: loss=0.058795 lr=0.000020 grad_norm=1.589101
Epoch 45/100 Iteration 227/234: loss=0.076089 lr=0.000020 grad_norm=1.280358
Epoch 45/100 Iteration 228/234: loss=0.054143 lr=0.000020 grad_norm=0.422433
Epoch 45/100 Iteration 229/234: loss=0.060300 lr=0.000020 grad_norm=0.645593
Epoch 45/100 Iteration 230/234: loss=0.061719 lr=0.000020 grad_norm=0.822475
Epoch 45/100 Iteration 231/234: loss=0.066953 lr=0.000020 grad_norm=0.815849
Epoch 45/100 Iteration 232/234: loss=0.063799 lr=0.000020 grad_norm=0.665448
Epoch 45/100 Iteration 233/234: loss=0.063185 lr=0.000020 grad_norm=0.654223
Epoch 45/100 Iteration 234/234: loss=0.068981 lr=0.000020 grad_norm=0.971259
Epoch 45/100 finished. Avg Loss: 0.061653
Epoch 46/100 Iteration 1/234: loss=0.061267 lr=0.000020 grad_norm=1.029687
Epoch 46/100 Iteration 2/234: loss=0.050544 lr=0.000020 grad_norm=0.894563
Epoch 46/100 Iteration 3/234: loss=0.061739 lr=0.000020 grad_norm=0.639670
Epoch 46/100 Iteration 4/234: loss=0.056254 lr=0.000020 grad_norm=0.483903
Epoch 46/100 Iteration 5/234: loss=0.063421 lr=0.000020 grad_norm=0.715032
Epoch 46/100 Iteration 6/234: loss=0.064104 lr=0.000020 grad_norm=1.159408
Epoch 46/100 Iteration 7/234: loss=0.055023 lr=0.000020 grad_norm=0.971474
Epoch 46/100 Iteration 8/234: loss=0.062619 lr=0.000020 grad_norm=0.545635
Epoch 46/100 Iteration 9/234: loss=0.055390 lr=0.000020 grad_norm=0.907636
Epoch 46/100 Iteration 10/234: loss=0.059703 lr=0.000020 grad_norm=1.449040
Epoch 46/100 Iteration 11/234: loss=0.059106 lr=0.000020 grad_norm=1.184068
Epoch 46/100 Iteration 12/234: loss=0.058736 lr=0.000020 grad_norm=0.436862
Epoch 46/100 Iteration 13/234: loss=0.065858 lr=0.000020 grad_norm=0.984772
Epoch 46/100 Iteration 14/234: loss=0.053397 lr=0.000020 grad_norm=1.322203
Epoch 46/100 Iteration 15/234: loss=0.053335 lr=0.000020 grad_norm=0.865994
Epoch 46/100 Iteration 16/234: loss=0.066699 lr=0.000020 grad_norm=0.656924
Epoch 46/100 Iteration 17/234: loss=0.058045 lr=0.000020 grad_norm=1.181302
Epoch 46/100 Iteration 18/234: loss=0.056522 lr=0.000020 grad_norm=0.833201
Epoch 46/100 Iteration 19/234: loss=0.056174 lr=0.000020 grad_norm=0.397329
Epoch 46/100 Iteration 20/234: loss=0.056459 lr=0.000020 grad_norm=1.092150
Epoch 46/100 Iteration 21/234: loss=0.057822 lr=0.000020 grad_norm=1.263597
Epoch 46/100 Iteration 22/234: loss=0.061191 lr=0.000020 grad_norm=0.963694
Epoch 46/100 Iteration 23/234: loss=0.060598 lr=0.000020 grad_norm=0.400201
Epoch 46/100 Iteration 24/234: loss=0.061753 lr=0.000020 grad_norm=1.270761
Epoch 46/100 Iteration 25/234: loss=0.063736 lr=0.000020 grad_norm=1.593608
Epoch 46/100 Iteration 26/234: loss=0.064010 lr=0.000020 grad_norm=0.824570
Epoch 46/100 Iteration 27/234: loss=0.050856 lr=0.000020 grad_norm=0.534176
Epoch 46/100 Iteration 28/234: loss=0.062510 lr=0.000020 grad_norm=1.135509
Epoch 46/100 Iteration 29/234: loss=0.058461 lr=0.000020 grad_norm=0.813690
Epoch 46/100 Iteration 30/234: loss=0.060496 lr=0.000020 grad_norm=0.958724
Epoch 46/100 Iteration 31/234: loss=0.058255 lr=0.000020 grad_norm=1.398743
Epoch 46/100 Iteration 32/234: loss=0.063725 lr=0.000020 grad_norm=0.606496
Epoch 46/100 Iteration 33/234: loss=0.063143 lr=0.000020 grad_norm=0.823513
Epoch 46/100 Iteration 34/234: loss=0.060353 lr=0.000020 grad_norm=1.293299
Epoch 46/100 Iteration 35/234: loss=0.066005 lr=0.000020 grad_norm=0.905166
Epoch 46/100 Iteration 36/234: loss=0.069457 lr=0.000020 grad_norm=0.513782
Epoch 46/100 Iteration 37/234: loss=0.053220 lr=0.000020 grad_norm=0.738312
Epoch 46/100 Iteration 38/234: loss=0.059108 lr=0.000020 grad_norm=0.916537
Epoch 46/100 Iteration 39/234: loss=0.063140 lr=0.000020 grad_norm=0.520368
Epoch 46/100 Iteration 40/234: loss=0.053172 lr=0.000020 grad_norm=0.705832
Epoch 46/100 Iteration 41/234: loss=0.051488 lr=0.000020 grad_norm=0.792185
Epoch 46/100 Iteration 42/234: loss=0.049634 lr=0.000020 grad_norm=0.846481
Epoch 46/100 Iteration 43/234: loss=0.057056 lr=0.000020 grad_norm=0.927507
Epoch 46/100 Iteration 44/234: loss=0.060502 lr=0.000020 grad_norm=0.526064
Epoch 46/100 Iteration 45/234: loss=0.066065 lr=0.000020 grad_norm=0.431106
Epoch 46/100 Iteration 46/234: loss=0.062265 lr=0.000020 grad_norm=0.720381
Epoch 46/100 Iteration 47/234: loss=0.064842 lr=0.000020 grad_norm=0.749516
Epoch 46/100 Iteration 48/234: loss=0.061019 lr=0.000020 grad_norm=0.351548
Epoch 46/100 Iteration 49/234: loss=0.066252 lr=0.000020 grad_norm=0.961540
Epoch 46/100 Iteration 50/234: loss=0.059204 lr=0.000020 grad_norm=1.865647
Epoch 46/100 Iteration 51/234: loss=0.057075 lr=0.000020 grad_norm=1.896106
Epoch 46/100 Iteration 52/234: loss=0.060039 lr=0.000020 grad_norm=0.332989
Epoch 46/100 Iteration 53/234: loss=0.067085 lr=0.000020 grad_norm=1.747849
Epoch 46/100 Iteration 54/234: loss=0.064040 lr=0.000020 grad_norm=1.121202
Epoch 46/100 Iteration 55/234: loss=0.058200 lr=0.000020 grad_norm=0.859601
Epoch 46/100 Iteration 56/234: loss=0.057716 lr=0.000020 grad_norm=2.239920
Epoch 46/100 Iteration 57/234: loss=0.058565 lr=0.000020 grad_norm=2.260764
Epoch 46/100 Iteration 58/234: loss=0.060378 lr=0.000020 grad_norm=1.013252
Epoch 46/100 Iteration 59/234: loss=0.060562 lr=0.000020 grad_norm=2.521594
Epoch 46/100 Iteration 60/234: loss=0.061277 lr=0.000020 grad_norm=1.691495
Epoch 46/100 Iteration 61/234: loss=0.065652 lr=0.000020 grad_norm=1.379726
Epoch 46/100 Iteration 62/234: loss=0.066941 lr=0.000020 grad_norm=2.257883
Epoch 46/100 Iteration 63/234: loss=0.061477 lr=0.000020 grad_norm=0.956960
Epoch 46/100 Iteration 64/234: loss=0.062163 lr=0.000020 grad_norm=1.362825
Epoch 46/100 Iteration 65/234: loss=0.061813 lr=0.000020 grad_norm=0.761038
Epoch 46/100 Iteration 66/234: loss=0.057276 lr=0.000020 grad_norm=0.857397
Epoch 46/100 Iteration 67/234: loss=0.056049 lr=0.000020 grad_norm=1.008188
Epoch 46/100 Iteration 68/234: loss=0.070368 lr=0.000020 grad_norm=0.783613
Epoch 46/100 Iteration 69/234: loss=0.061009 lr=0.000020 grad_norm=1.260309
Epoch 46/100 Iteration 70/234: loss=0.056635 lr=0.000020 grad_norm=0.606949
Epoch 46/100 Iteration 71/234: loss=0.057971 lr=0.000020 grad_norm=0.952321
Epoch 46/100 Iteration 72/234: loss=0.060621 lr=0.000020 grad_norm=0.532921
Epoch 46/100 Iteration 73/234: loss=0.061942 lr=0.000020 grad_norm=0.738911
Epoch 46/100 Iteration 74/234: loss=0.056439 lr=0.000020 grad_norm=0.893598
Epoch 46/100 Iteration 75/234: loss=0.060575 lr=0.000020 grad_norm=0.688816
Epoch 46/100 Iteration 76/234: loss=0.061342 lr=0.000020 grad_norm=0.899249
Epoch 46/100 Iteration 77/234: loss=0.061611 lr=0.000020 grad_norm=0.984138
Epoch 46/100 Iteration 78/234: loss=0.063415 lr=0.000020 grad_norm=0.394608
Epoch 46/100 Iteration 79/234: loss=0.067369 lr=0.000020 grad_norm=0.678207
Epoch 46/100 Iteration 80/234: loss=0.065819 lr=0.000020 grad_norm=0.515391
Epoch 46/100 Iteration 81/234: loss=0.062698 lr=0.000020 grad_norm=0.649674
Epoch 46/100 Iteration 82/234: loss=0.061432 lr=0.000020 grad_norm=0.543169
Epoch 46/100 Iteration 83/234: loss=0.054543 lr=0.000020 grad_norm=0.408671
Epoch 46/100 Iteration 84/234: loss=0.055754 lr=0.000020 grad_norm=0.537069
Epoch 46/100 Iteration 85/234: loss=0.060349 lr=0.000020 grad_norm=0.573462
Epoch 46/100 Iteration 86/234: loss=0.072850 lr=0.000020 grad_norm=0.936090
Epoch 46/100 Iteration 87/234: loss=0.064600 lr=0.000020 grad_norm=1.365200
Epoch 46/100 Iteration 88/234: loss=0.057071 lr=0.000020 grad_norm=1.271148
Epoch 46/100 Iteration 89/234: loss=0.059512 lr=0.000020 grad_norm=0.546029
Epoch 46/100 Iteration 90/234: loss=0.062864 lr=0.000020 grad_norm=0.511001
Epoch 46/100 Iteration 91/234: loss=0.067171 lr=0.000020 grad_norm=0.941579
Epoch 46/100 Iteration 92/234: loss=0.058069 lr=0.000020 grad_norm=0.798273
Epoch 46/100 Iteration 93/234: loss=0.054912 lr=0.000020 grad_norm=0.453472
Epoch 46/100 Iteration 94/234: loss=0.065018 lr=0.000020 grad_norm=0.459477
Epoch 46/100 Iteration 95/234: loss=0.053414 lr=0.000020 grad_norm=0.440921
Epoch 46/100 Iteration 96/234: loss=0.055121 lr=0.000020 grad_norm=0.422102
Epoch 46/100 Iteration 97/234: loss=0.062965 lr=0.000020 grad_norm=0.446804
Epoch 46/100 Iteration 98/234: loss=0.071509 lr=0.000020 grad_norm=0.549553
Epoch 46/100 Iteration 99/234: loss=0.063626 lr=0.000020 grad_norm=0.456879
Epoch 46/100 Iteration 100/234: loss=0.057673 lr=0.000020 grad_norm=0.512489
Epoch 46/100 Iteration 101/234: loss=0.065857 lr=0.000020 grad_norm=0.701684
Epoch 46/100 Iteration 102/234: loss=0.065934 lr=0.000020 grad_norm=0.605883
Epoch 46/100 Iteration 103/234: loss=0.050479 lr=0.000020 grad_norm=0.358043
Epoch 46/100 Iteration 104/234: loss=0.065989 lr=0.000020 grad_norm=0.657680
Epoch 46/100 Iteration 105/234: loss=0.054059 lr=0.000020 grad_norm=1.080403
Epoch 46/100 Iteration 106/234: loss=0.067943 lr=0.000020 grad_norm=1.699735
Epoch 46/100 Iteration 107/234: loss=0.055630 lr=0.000020 grad_norm=1.705883
Epoch 46/100 Iteration 108/234: loss=0.057298 lr=0.000020 grad_norm=0.780387
Epoch 46/100 Iteration 109/234: loss=0.057967 lr=0.000020 grad_norm=1.063420
Epoch 46/100 Iteration 110/234: loss=0.061029 lr=0.000020 grad_norm=1.440165
Epoch 46/100 Iteration 111/234: loss=0.059267 lr=0.000020 grad_norm=0.929603
Epoch 46/100 Iteration 112/234: loss=0.069689 lr=0.000020 grad_norm=0.985965
Epoch 46/100 Iteration 113/234: loss=0.061410 lr=0.000020 grad_norm=1.369656
Epoch 46/100 Iteration 114/234: loss=0.062378 lr=0.000020 grad_norm=1.247424
Epoch 46/100 Iteration 115/234: loss=0.060263 lr=0.000020 grad_norm=1.010461
Epoch 46/100 Iteration 116/234: loss=0.054876 lr=0.000020 grad_norm=1.213544
Epoch 46/100 Iteration 117/234: loss=0.051177 lr=0.000020 grad_norm=1.328625
Epoch 46/100 Iteration 118/234: loss=0.061171 lr=0.000020 grad_norm=0.808187
Epoch 46/100 Iteration 119/234: loss=0.056313 lr=0.000020 grad_norm=1.666137
Epoch 46/100 Iteration 120/234: loss=0.058692 lr=0.000020 grad_norm=1.347155
Epoch 46/100 Iteration 121/234: loss=0.058711 lr=0.000020 grad_norm=0.946924
Epoch 46/100 Iteration 122/234: loss=0.061969 lr=0.000020 grad_norm=0.830878
Epoch 46/100 Iteration 123/234: loss=0.053617 lr=0.000020 grad_norm=0.617529
Epoch 46/100 Iteration 124/234: loss=0.059348 lr=0.000020 grad_norm=1.212244
Epoch 46/100 Iteration 125/234: loss=0.054900 lr=0.000020 grad_norm=0.688487
Epoch 46/100 Iteration 126/234: loss=0.057157 lr=0.000020 grad_norm=0.647477
Epoch 46/100 Iteration 127/234: loss=0.067460 lr=0.000020 grad_norm=1.159236
Epoch 46/100 Iteration 128/234: loss=0.065667 lr=0.000020 grad_norm=1.141504
Epoch 46/100 Iteration 129/234: loss=0.063612 lr=0.000020 grad_norm=0.698293
Epoch 46/100 Iteration 130/234: loss=0.056450 lr=0.000020 grad_norm=1.045105
Epoch 46/100 Iteration 131/234: loss=0.065164 lr=0.000020 grad_norm=0.941642
Epoch 46/100 Iteration 132/234: loss=0.055457 lr=0.000020 grad_norm=0.607535
Epoch 46/100 Iteration 133/234: loss=0.055052 lr=0.000020 grad_norm=0.841928
Epoch 46/100 Iteration 134/234: loss=0.066045 lr=0.000020 grad_norm=0.568860
Epoch 46/100 Iteration 135/234: loss=0.051163 lr=0.000020 grad_norm=0.699173
Epoch 46/100 Iteration 136/234: loss=0.067621 lr=0.000020 grad_norm=0.547780
Epoch 46/100 Iteration 137/234: loss=0.059645 lr=0.000020 grad_norm=0.518256
Epoch 46/100 Iteration 138/234: loss=0.069242 lr=0.000020 grad_norm=0.687243
Epoch 46/100 Iteration 139/234: loss=0.060487 lr=0.000020 grad_norm=0.751352
Epoch 46/100 Iteration 140/234: loss=0.062874 lr=0.000020 grad_norm=1.226441
Epoch 46/100 Iteration 141/234: loss=0.058053 lr=0.000020 grad_norm=1.289321
Epoch 46/100 Iteration 142/234: loss=0.059517 lr=0.000020 grad_norm=0.756007
Epoch 46/100 Iteration 143/234: loss=0.061348 lr=0.000020 grad_norm=0.560030
Epoch 46/100 Iteration 144/234: loss=0.053712 lr=0.000020 grad_norm=0.522861
Epoch 46/100 Iteration 145/234: loss=0.057742 lr=0.000020 grad_norm=0.443265
Epoch 46/100 Iteration 146/234: loss=0.050882 lr=0.000020 grad_norm=0.434981
Epoch 46/100 Iteration 147/234: loss=0.063623 lr=0.000020 grad_norm=0.616401
Epoch 46/100 Iteration 148/234: loss=0.054573 lr=0.000020 grad_norm=0.416892
Epoch 46/100 Iteration 149/234: loss=0.065626 lr=0.000020 grad_norm=0.610219
Epoch 46/100 Iteration 150/234: loss=0.056571 lr=0.000020 grad_norm=0.760776
Epoch 46/100 Iteration 151/234: loss=0.058152 lr=0.000020 grad_norm=0.590493
Epoch 46/100 Iteration 152/234: loss=0.062160 lr=0.000020 grad_norm=1.065178
Epoch 46/100 Iteration 153/234: loss=0.056996 lr=0.000020 grad_norm=0.930628
Epoch 46/100 Iteration 154/234: loss=0.059057 lr=0.000020 grad_norm=0.710777
Epoch 46/100 Iteration 155/234: loss=0.054987 lr=0.000020 grad_norm=0.866737
Epoch 46/100 Iteration 156/234: loss=0.062322 lr=0.000020 grad_norm=0.587322
Epoch 46/100 Iteration 157/234: loss=0.066363 lr=0.000020 grad_norm=0.747989
Epoch 46/100 Iteration 158/234: loss=0.062399 lr=0.000020 grad_norm=0.553638
Epoch 46/100 Iteration 159/234: loss=0.057782 lr=0.000020 grad_norm=0.370003
Epoch 46/100 Iteration 160/234: loss=0.060528 lr=0.000020 grad_norm=0.517273
Epoch 46/100 Iteration 161/234: loss=0.066198 lr=0.000020 grad_norm=0.572697
Epoch 46/100 Iteration 162/234: loss=0.056109 lr=0.000020 grad_norm=0.690716
Epoch 46/100 Iteration 163/234: loss=0.070224 lr=0.000020 grad_norm=0.563999
Epoch 46/100 Iteration 164/234: loss=0.060916 lr=0.000020 grad_norm=0.354737
Epoch 46/100 Iteration 165/234: loss=0.058555 lr=0.000020 grad_norm=0.677497
Epoch 46/100 Iteration 166/234: loss=0.053939 lr=0.000020 grad_norm=1.536823
Epoch 46/100 Iteration 167/234: loss=0.059911 lr=0.000020 grad_norm=1.944125
Epoch 46/100 Iteration 168/234: loss=0.065576 lr=0.000020 grad_norm=1.267204
Epoch 46/100 Iteration 169/234: loss=0.062886 lr=0.000020 grad_norm=0.529711
Epoch 46/100 Iteration 170/234: loss=0.068337 lr=0.000020 grad_norm=1.066859
Epoch 46/100 Iteration 171/234: loss=0.058078 lr=0.000020 grad_norm=0.850105
Epoch 46/100 Iteration 172/234: loss=0.050112 lr=0.000020 grad_norm=0.690864
Epoch 46/100 Iteration 173/234: loss=0.058362 lr=0.000020 grad_norm=1.160716
Epoch 46/100 Iteration 174/234: loss=0.059786 lr=0.000020 grad_norm=0.716692
Epoch 46/100 Iteration 175/234: loss=0.057385 lr=0.000020 grad_norm=0.548999
Epoch 46/100 Iteration 176/234: loss=0.054233 lr=0.000020 grad_norm=0.466440
Epoch 46/100 Iteration 177/234: loss=0.056201 lr=0.000020 grad_norm=1.113733
Epoch 46/100 Iteration 178/234: loss=0.064307 lr=0.000020 grad_norm=1.234693
Epoch 46/100 Iteration 179/234: loss=0.068269 lr=0.000020 grad_norm=0.681449
Epoch 46/100 Iteration 180/234: loss=0.063175 lr=0.000020 grad_norm=0.676288
Epoch 46/100 Iteration 181/234: loss=0.060330 lr=0.000020 grad_norm=0.860049
Epoch 46/100 Iteration 182/234: loss=0.051655 lr=0.000020 grad_norm=0.505495
Epoch 46/100 Iteration 183/234: loss=0.061148 lr=0.000020 grad_norm=0.808205
Epoch 46/100 Iteration 184/234: loss=0.055685 lr=0.000020 grad_norm=0.626766
Epoch 46/100 Iteration 185/234: loss=0.063515 lr=0.000020 grad_norm=0.719968
Epoch 46/100 Iteration 186/234: loss=0.057473 lr=0.000020 grad_norm=1.058065
Epoch 46/100 Iteration 187/234: loss=0.053732 lr=0.000020 grad_norm=0.596956
Epoch 46/100 Iteration 188/234: loss=0.059529 lr=0.000020 grad_norm=0.862841
Epoch 46/100 Iteration 189/234: loss=0.054159 lr=0.000020 grad_norm=0.887501
Epoch 46/100 Iteration 190/234: loss=0.063245 lr=0.000020 grad_norm=1.341125
Epoch 46/100 Iteration 191/234: loss=0.066769 lr=0.000020 grad_norm=2.726653
Epoch 46/100 Iteration 192/234: loss=0.045196 lr=0.000020 grad_norm=1.557684
Epoch 46/100 Iteration 193/234: loss=0.057694 lr=0.000020 grad_norm=2.019472
Epoch 46/100 Iteration 194/234: loss=0.057726 lr=0.000020 grad_norm=2.798104
Epoch 46/100 Iteration 195/234: loss=0.062391 lr=0.000020 grad_norm=0.653494
Epoch 46/100 Iteration 196/234: loss=0.061272 lr=0.000020 grad_norm=1.995362
Epoch 46/100 Iteration 197/234: loss=0.058556 lr=0.000020 grad_norm=1.369646
Epoch 46/100 Iteration 198/234: loss=0.067219 lr=0.000020 grad_norm=1.416551
Epoch 46/100 Iteration 199/234: loss=0.075823 lr=0.000020 grad_norm=2.260733
Epoch 46/100 Iteration 200/234: loss=0.061244 lr=0.000020 grad_norm=0.944213
Epoch 46/100 Iteration 201/234: loss=0.054016 lr=0.000020 grad_norm=1.486622
Epoch 46/100 Iteration 202/234: loss=0.058534 lr=0.000020 grad_norm=0.563538
Epoch 46/100 Iteration 203/234: loss=0.047543 lr=0.000020 grad_norm=1.518766
Epoch 46/100 Iteration 204/234: loss=0.059494 lr=0.000020 grad_norm=1.064322
Epoch 46/100 Iteration 205/234: loss=0.063856 lr=0.000020 grad_norm=1.097709
Epoch 46/100 Iteration 206/234: loss=0.050898 lr=0.000020 grad_norm=1.344013
Epoch 46/100 Iteration 207/234: loss=0.050570 lr=0.000020 grad_norm=0.496975
Epoch 46/100 Iteration 208/234: loss=0.062433 lr=0.000020 grad_norm=1.313805
Epoch 46/100 Iteration 209/234: loss=0.059240 lr=0.000020 grad_norm=0.820874
Epoch 46/100 Iteration 210/234: loss=0.060346 lr=0.000020 grad_norm=0.576888
Epoch 46/100 Iteration 211/234: loss=0.063744 lr=0.000020 grad_norm=0.965797
Epoch 46/100 Iteration 212/234: loss=0.048954 lr=0.000020 grad_norm=0.592016
Epoch 46/100 Iteration 213/234: loss=0.065906 lr=0.000020 grad_norm=0.960549
Epoch 46/100 Iteration 214/234: loss=0.059687 lr=0.000020 grad_norm=0.692813
Epoch 46/100 Iteration 215/234: loss=0.057099 lr=0.000020 grad_norm=0.725889
Epoch 46/100 Iteration 216/234: loss=0.064252 lr=0.000020 grad_norm=1.224681
Epoch 46/100 Iteration 217/234: loss=0.061493 lr=0.000020 grad_norm=0.914192
Epoch 46/100 Iteration 218/234: loss=0.058601 lr=0.000020 grad_norm=0.502587
Epoch 46/100 Iteration 219/234: loss=0.064561 lr=0.000020 grad_norm=1.498559
Epoch 46/100 Iteration 220/234: loss=0.061402 lr=0.000020 grad_norm=1.807092
Epoch 46/100 Iteration 221/234: loss=0.055003 lr=0.000020 grad_norm=0.749405
Epoch 46/100 Iteration 222/234: loss=0.067507 lr=0.000020 grad_norm=1.246949
Epoch 46/100 Iteration 223/234: loss=0.061474 lr=0.000020 grad_norm=1.955561
Epoch 46/100 Iteration 224/234: loss=0.065138 lr=0.000020 grad_norm=1.180309
Epoch 46/100 Iteration 225/234: loss=0.052856 lr=0.000020 grad_norm=0.636475
Epoch 46/100 Iteration 226/234: loss=0.056750 lr=0.000020 grad_norm=1.074822
Epoch 46/100 Iteration 227/234: loss=0.057945 lr=0.000020 grad_norm=0.725854
Epoch 46/100 Iteration 228/234: loss=0.052911 lr=0.000020 grad_norm=0.879911
Epoch 46/100 Iteration 229/234: loss=0.055785 lr=0.000020 grad_norm=1.100700
Epoch 46/100 Iteration 230/234: loss=0.061619 lr=0.000020 grad_norm=0.630016
Epoch 46/100 Iteration 231/234: loss=0.062172 lr=0.000020 grad_norm=0.486064
Epoch 46/100 Iteration 232/234: loss=0.053692 lr=0.000020 grad_norm=0.828356
Epoch 46/100 Iteration 233/234: loss=0.063636 lr=0.000020 grad_norm=0.474603
Epoch 46/100 Iteration 234/234: loss=0.064331 lr=0.000020 grad_norm=0.603917
Epoch 46/100 finished. Avg Loss: 0.060035
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 47/100 Iteration 1/234: loss=0.063235 lr=0.000020 grad_norm=0.789460
Epoch 47/100 Iteration 2/234: loss=0.062726 lr=0.000020 grad_norm=0.479211
Epoch 47/100 Iteration 3/234: loss=0.056733 lr=0.000020 grad_norm=0.513515
Epoch 47/100 Iteration 4/234: loss=0.060756 lr=0.000020 grad_norm=0.551472
Epoch 47/100 Iteration 5/234: loss=0.057617 lr=0.000020 grad_norm=0.581912
Epoch 47/100 Iteration 6/234: loss=0.057843 lr=0.000020 grad_norm=0.546721
Epoch 47/100 Iteration 7/234: loss=0.056681 lr=0.000020 grad_norm=0.845133
Epoch 47/100 Iteration 8/234: loss=0.061117 lr=0.000020 grad_norm=1.055261
Epoch 47/100 Iteration 9/234: loss=0.063092 lr=0.000020 grad_norm=1.076868
Epoch 47/100 Iteration 10/234: loss=0.056934 lr=0.000020 grad_norm=0.505701
Epoch 47/100 Iteration 11/234: loss=0.052212 lr=0.000020 grad_norm=0.672481
Epoch 47/100 Iteration 12/234: loss=0.057040 lr=0.000020 grad_norm=0.770405
Epoch 47/100 Iteration 13/234: loss=0.063010 lr=0.000020 grad_norm=0.487583
Epoch 47/100 Iteration 14/234: loss=0.055661 lr=0.000020 grad_norm=0.474756
Epoch 47/100 Iteration 15/234: loss=0.067068 lr=0.000020 grad_norm=0.765766
Epoch 47/100 Iteration 16/234: loss=0.064318 lr=0.000020 grad_norm=0.991112
Epoch 47/100 Iteration 17/234: loss=0.059690 lr=0.000020 grad_norm=0.662261
Epoch 47/100 Iteration 18/234: loss=0.065064 lr=0.000020 grad_norm=1.024972
Epoch 47/100 Iteration 19/234: loss=0.062086 lr=0.000020 grad_norm=1.784980
Epoch 47/100 Iteration 20/234: loss=0.064529 lr=0.000020 grad_norm=1.182324
Epoch 47/100 Iteration 21/234: loss=0.058573 lr=0.000020 grad_norm=0.472900
Epoch 47/100 Iteration 22/234: loss=0.057661 lr=0.000020 grad_norm=1.203938
Epoch 47/100 Iteration 23/234: loss=0.058584 lr=0.000020 grad_norm=0.615870
Epoch 47/100 Iteration 24/234: loss=0.057145 lr=0.000020 grad_norm=1.394845
Epoch 47/100 Iteration 25/234: loss=0.066095 lr=0.000020 grad_norm=2.067884
Epoch 47/100 Iteration 26/234: loss=0.059279 lr=0.000020 grad_norm=1.440180
Epoch 47/100 Iteration 27/234: loss=0.075020 lr=0.000020 grad_norm=0.514606
Epoch 47/100 Iteration 28/234: loss=0.068955 lr=0.000020 grad_norm=1.967058
Epoch 47/100 Iteration 29/234: loss=0.063810 lr=0.000020 grad_norm=2.210475
Epoch 47/100 Iteration 30/234: loss=0.064234 lr=0.000020 grad_norm=0.817709
Epoch 47/100 Iteration 31/234: loss=0.055092 lr=0.000020 grad_norm=1.135635
Epoch 47/100 Iteration 32/234: loss=0.059025 lr=0.000020 grad_norm=1.315839
Epoch 47/100 Iteration 33/234: loss=0.059925 lr=0.000020 grad_norm=0.399729
Epoch 47/100 Iteration 34/234: loss=0.057940 lr=0.000020 grad_norm=1.066668
Epoch 47/100 Iteration 35/234: loss=0.068021 lr=0.000020 grad_norm=0.621799
Epoch 47/100 Iteration 36/234: loss=0.065619 lr=0.000020 grad_norm=1.238253
Epoch 47/100 Iteration 37/234: loss=0.055810 lr=0.000020 grad_norm=1.741065
Epoch 47/100 Iteration 38/234: loss=0.058380 lr=0.000020 grad_norm=0.675889
Epoch 47/100 Iteration 39/234: loss=0.067778 lr=0.000020 grad_norm=1.248811
Epoch 47/100 Iteration 40/234: loss=0.057014 lr=0.000020 grad_norm=1.798904
Epoch 47/100 Iteration 41/234: loss=0.061922 lr=0.000020 grad_norm=1.009839
Epoch 47/100 Iteration 42/234: loss=0.053658 lr=0.000020 grad_norm=0.824267
Epoch 47/100 Iteration 43/234: loss=0.065337 lr=0.000020 grad_norm=1.443878
Epoch 47/100 Iteration 44/234: loss=0.061737 lr=0.000020 grad_norm=0.777229
Epoch 47/100 Iteration 45/234: loss=0.054971 lr=0.000020 grad_norm=0.757859
Epoch 47/100 Iteration 46/234: loss=0.065253 lr=0.000020 grad_norm=1.205443
Epoch 47/100 Iteration 47/234: loss=0.060132 lr=0.000020 grad_norm=0.606881
Epoch 47/100 Iteration 48/234: loss=0.061012 lr=0.000020 grad_norm=0.707867
Epoch 47/100 Iteration 49/234: loss=0.045254 lr=0.000020 grad_norm=0.794345
Epoch 47/100 Iteration 50/234: loss=0.059470 lr=0.000020 grad_norm=0.474327
Epoch 47/100 Iteration 51/234: loss=0.062711 lr=0.000020 grad_norm=0.918423
Epoch 47/100 Iteration 52/234: loss=0.060701 lr=0.000020 grad_norm=0.654684
Epoch 47/100 Iteration 53/234: loss=0.064806 lr=0.000020 grad_norm=0.509275
Epoch 47/100 Iteration 54/234: loss=0.053336 lr=0.000020 grad_norm=0.671773
Epoch 47/100 Iteration 55/234: loss=0.054500 lr=0.000020 grad_norm=0.352298
Epoch 47/100 Iteration 56/234: loss=0.059499 lr=0.000020 grad_norm=0.772219
Epoch 47/100 Iteration 57/234: loss=0.054583 lr=0.000020 grad_norm=1.042751
Epoch 47/100 Iteration 58/234: loss=0.060284 lr=0.000020 grad_norm=0.367560
Epoch 47/100 Iteration 59/234: loss=0.055531 lr=0.000020 grad_norm=1.021826
Epoch 47/100 Iteration 60/234: loss=0.055726 lr=0.000020 grad_norm=1.048806
Epoch 47/100 Iteration 61/234: loss=0.061157 lr=0.000020 grad_norm=0.887515
Epoch 47/100 Iteration 62/234: loss=0.058550 lr=0.000020 grad_norm=0.693624
Epoch 47/100 Iteration 63/234: loss=0.063255 lr=0.000020 grad_norm=0.497354
Epoch 47/100 Iteration 64/234: loss=0.051848 lr=0.000020 grad_norm=0.483735
Epoch 47/100 Iteration 65/234: loss=0.059085 lr=0.000020 grad_norm=0.766458
Epoch 47/100 Iteration 66/234: loss=0.057242 lr=0.000020 grad_norm=0.905245
Epoch 47/100 Iteration 67/234: loss=0.052215 lr=0.000020 grad_norm=0.595959
Epoch 47/100 Iteration 68/234: loss=0.054877 lr=0.000020 grad_norm=0.651876
Epoch 47/100 Iteration 69/234: loss=0.059960 lr=0.000020 grad_norm=1.354963
Epoch 47/100 Iteration 70/234: loss=0.064678 lr=0.000020 grad_norm=1.115060
Epoch 47/100 Iteration 71/234: loss=0.058657 lr=0.000020 grad_norm=0.412902
Epoch 47/100 Iteration 72/234: loss=0.067817 lr=0.000020 grad_norm=1.384570
Epoch 47/100 Iteration 73/234: loss=0.052460 lr=0.000020 grad_norm=1.358992
Epoch 47/100 Iteration 74/234: loss=0.059362 lr=0.000020 grad_norm=0.744631
Epoch 47/100 Iteration 75/234: loss=0.061999 lr=0.000020 grad_norm=0.709158
Epoch 47/100 Iteration 76/234: loss=0.054985 lr=0.000020 grad_norm=0.442357
Epoch 47/100 Iteration 77/234: loss=0.057260 lr=0.000020 grad_norm=0.542172
Epoch 47/100 Iteration 78/234: loss=0.058936 lr=0.000020 grad_norm=0.642076
Epoch 47/100 Iteration 79/234: loss=0.057335 lr=0.000020 grad_norm=0.583294
Epoch 47/100 Iteration 80/234: loss=0.057490 lr=0.000020 grad_norm=0.671677
Epoch 47/100 Iteration 81/234: loss=0.057662 lr=0.000020 grad_norm=0.910136
Epoch 47/100 Iteration 82/234: loss=0.060724 lr=0.000020 grad_norm=0.444122
Epoch 47/100 Iteration 83/234: loss=0.056891 lr=0.000020 grad_norm=0.690175
Epoch 47/100 Iteration 84/234: loss=0.053168 lr=0.000020 grad_norm=1.143521
Epoch 47/100 Iteration 85/234: loss=0.064918 lr=0.000020 grad_norm=1.315016
Epoch 47/100 Iteration 86/234: loss=0.052479 lr=0.000020 grad_norm=0.858670
Epoch 47/100 Iteration 87/234: loss=0.053978 lr=0.000020 grad_norm=0.661278
Epoch 47/100 Iteration 88/234: loss=0.058083 lr=0.000020 grad_norm=0.901754
Epoch 47/100 Iteration 89/234: loss=0.062505 lr=0.000020 grad_norm=1.063632
Epoch 47/100 Iteration 90/234: loss=0.059030 lr=0.000020 grad_norm=0.684301
Epoch 47/100 Iteration 91/234: loss=0.059198 lr=0.000020 grad_norm=0.714027
Epoch 47/100 Iteration 92/234: loss=0.057638 lr=0.000020 grad_norm=1.335352
Epoch 47/100 Iteration 93/234: loss=0.050415 lr=0.000020 grad_norm=0.892507
Epoch 47/100 Iteration 94/234: loss=0.056354 lr=0.000020 grad_norm=0.947998
Epoch 47/100 Iteration 95/234: loss=0.050072 lr=0.000020 grad_norm=1.468386
Epoch 47/100 Iteration 96/234: loss=0.060334 lr=0.000020 grad_norm=1.026841
Epoch 47/100 Iteration 97/234: loss=0.062341 lr=0.000020 grad_norm=0.534419
Epoch 47/100 Iteration 98/234: loss=0.057150 lr=0.000020 grad_norm=1.158314
Epoch 47/100 Iteration 99/234: loss=0.065161 lr=0.000020 grad_norm=1.015410
Epoch 47/100 Iteration 100/234: loss=0.050672 lr=0.000020 grad_norm=0.418689
Epoch 47/100 Iteration 101/234: loss=0.059810 lr=0.000020 grad_norm=0.768581
Epoch 47/100 Iteration 102/234: loss=0.055927 lr=0.000020 grad_norm=0.642315
Epoch 47/100 Iteration 103/234: loss=0.054395 lr=0.000020 grad_norm=0.499911
Epoch 47/100 Iteration 104/234: loss=0.057413 lr=0.000020 grad_norm=0.922349
Epoch 47/100 Iteration 105/234: loss=0.057555 lr=0.000020 grad_norm=0.831194
Epoch 47/100 Iteration 106/234: loss=0.057381 lr=0.000020 grad_norm=0.535066
Epoch 47/100 Iteration 107/234: loss=0.055851 lr=0.000020 grad_norm=0.527498
Epoch 47/100 Iteration 108/234: loss=0.064865 lr=0.000020 grad_norm=0.564612
Epoch 47/100 Iteration 109/234: loss=0.057966 lr=0.000020 grad_norm=0.526231
Epoch 47/100 Iteration 110/234: loss=0.060970 lr=0.000020 grad_norm=0.446189
Epoch 47/100 Iteration 111/234: loss=0.057899 lr=0.000020 grad_norm=0.594839
Epoch 47/100 Iteration 112/234: loss=0.059694 lr=0.000020 grad_norm=0.454970
Epoch 47/100 Iteration 113/234: loss=0.056758 lr=0.000020 grad_norm=0.420802
Epoch 47/100 Iteration 114/234: loss=0.063840 lr=0.000020 grad_norm=0.381816
Epoch 47/100 Iteration 115/234: loss=0.058770 lr=0.000020 grad_norm=0.521070
Epoch 47/100 Iteration 116/234: loss=0.066465 lr=0.000020 grad_norm=0.651585
Epoch 47/100 Iteration 117/234: loss=0.049733 lr=0.000020 grad_norm=0.480959
Epoch 47/100 Iteration 118/234: loss=0.060893 lr=0.000020 grad_norm=0.617518
Epoch 47/100 Iteration 119/234: loss=0.060789 lr=0.000020 grad_norm=0.632685
Epoch 47/100 Iteration 120/234: loss=0.057169 lr=0.000020 grad_norm=0.448913
Epoch 47/100 Iteration 121/234: loss=0.061714 lr=0.000020 grad_norm=0.541741
Epoch 47/100 Iteration 122/234: loss=0.053813 lr=0.000020 grad_norm=0.435900
Epoch 47/100 Iteration 123/234: loss=0.059854 lr=0.000020 grad_norm=0.659996
Epoch 47/100 Iteration 124/234: loss=0.057108 lr=0.000020 grad_norm=0.878561
Epoch 47/100 Iteration 125/234: loss=0.065504 lr=0.000020 grad_norm=0.911679
Epoch 47/100 Iteration 126/234: loss=0.067156 lr=0.000020 grad_norm=0.986012
Epoch 47/100 Iteration 127/234: loss=0.063027 lr=0.000020 grad_norm=1.350723
Epoch 47/100 Iteration 128/234: loss=0.057103 lr=0.000020 grad_norm=0.865321
Epoch 47/100 Iteration 129/234: loss=0.054198 lr=0.000020 grad_norm=0.583492
Epoch 47/100 Iteration 130/234: loss=0.064053 lr=0.000020 grad_norm=1.242614
Epoch 47/100 Iteration 131/234: loss=0.064021 lr=0.000020 grad_norm=1.045993
Epoch 47/100 Iteration 132/234: loss=0.061178 lr=0.000020 grad_norm=0.486842
Epoch 47/100 Iteration 133/234: loss=0.061753 lr=0.000020 grad_norm=0.760610
Epoch 47/100 Iteration 134/234: loss=0.054662 lr=0.000020 grad_norm=1.111124
Epoch 47/100 Iteration 135/234: loss=0.058397 lr=0.000020 grad_norm=1.212892
Epoch 47/100 Iteration 136/234: loss=0.060852 lr=0.000020 grad_norm=0.890211
Epoch 47/100 Iteration 137/234: loss=0.057897 lr=0.000020 grad_norm=0.505871
Epoch 47/100 Iteration 138/234: loss=0.062398 lr=0.000020 grad_norm=1.275552
Epoch 47/100 Iteration 139/234: loss=0.055056 lr=0.000020 grad_norm=0.705362
Epoch 47/100 Iteration 140/234: loss=0.057803 lr=0.000020 grad_norm=0.944453
Epoch 47/100 Iteration 141/234: loss=0.061994 lr=0.000020 grad_norm=1.395371
Epoch 47/100 Iteration 142/234: loss=0.059851 lr=0.000020 grad_norm=0.597779
Epoch 47/100 Iteration 143/234: loss=0.054348 lr=0.000020 grad_norm=0.999726
Epoch 47/100 Iteration 144/234: loss=0.055947 lr=0.000020 grad_norm=1.417821
Epoch 47/100 Iteration 145/234: loss=0.057970 lr=0.000020 grad_norm=0.614325
Epoch 47/100 Iteration 146/234: loss=0.059744 lr=0.000020 grad_norm=1.245058
Epoch 47/100 Iteration 147/234: loss=0.054391 lr=0.000020 grad_norm=1.261616
Epoch 47/100 Iteration 148/234: loss=0.060094 lr=0.000020 grad_norm=0.850305
Epoch 47/100 Iteration 149/234: loss=0.056127 lr=0.000020 grad_norm=0.829660
Epoch 47/100 Iteration 150/234: loss=0.055814 lr=0.000020 grad_norm=0.593575
Epoch 47/100 Iteration 151/234: loss=0.054940 lr=0.000020 grad_norm=0.790162
Epoch 47/100 Iteration 152/234: loss=0.060992 lr=0.000020 grad_norm=0.624168
Epoch 47/100 Iteration 153/234: loss=0.070679 lr=0.000020 grad_norm=0.466892
Epoch 47/100 Iteration 154/234: loss=0.054289 lr=0.000020 grad_norm=0.746697
Epoch 47/100 Iteration 155/234: loss=0.049336 lr=0.000020 grad_norm=0.551144
Epoch 47/100 Iteration 156/234: loss=0.064362 lr=0.000020 grad_norm=0.617952
Epoch 47/100 Iteration 157/234: loss=0.069795 lr=0.000020 grad_norm=1.295404
Epoch 47/100 Iteration 158/234: loss=0.061867 lr=0.000020 grad_norm=2.326737
Epoch 47/100 Iteration 159/234: loss=0.062893 lr=0.000020 grad_norm=2.083435
Epoch 47/100 Iteration 160/234: loss=0.060392 lr=0.000020 grad_norm=0.948145
Epoch 47/100 Iteration 161/234: loss=0.055609 lr=0.000020 grad_norm=1.754455
Epoch 47/100 Iteration 162/234: loss=0.060427 lr=0.000020 grad_norm=1.216172
Epoch 47/100 Iteration 163/234: loss=0.057588 lr=0.000020 grad_norm=1.221284
Epoch 47/100 Iteration 164/234: loss=0.062960 lr=0.000020 grad_norm=1.462929
Epoch 47/100 Iteration 165/234: loss=0.059382 lr=0.000020 grad_norm=0.965538
Epoch 47/100 Iteration 166/234: loss=0.064946 lr=0.000020 grad_norm=1.224320
Epoch 47/100 Iteration 167/234: loss=0.047424 lr=0.000020 grad_norm=0.696309
Epoch 47/100 Iteration 168/234: loss=0.056349 lr=0.000020 grad_norm=0.921773
Epoch 47/100 Iteration 169/234: loss=0.062195 lr=0.000020 grad_norm=1.128476
Epoch 47/100 Iteration 170/234: loss=0.055702 lr=0.000020 grad_norm=0.355899
Epoch 47/100 Iteration 171/234: loss=0.061026 lr=0.000020 grad_norm=0.999112
Epoch 47/100 Iteration 172/234: loss=0.051839 lr=0.000020 grad_norm=0.728977
Epoch 47/100 Iteration 173/234: loss=0.067978 lr=0.000020 grad_norm=0.547681
Epoch 47/100 Iteration 174/234: loss=0.057141 lr=0.000020 grad_norm=1.052952
Epoch 47/100 Iteration 175/234: loss=0.065519 lr=0.000020 grad_norm=0.955956
Epoch 47/100 Iteration 176/234: loss=0.055669 lr=0.000020 grad_norm=0.522191
Epoch 47/100 Iteration 177/234: loss=0.062061 lr=0.000020 grad_norm=1.037048
Epoch 47/100 Iteration 178/234: loss=0.063323 lr=0.000020 grad_norm=1.023904
Epoch 47/100 Iteration 179/234: loss=0.065390 lr=0.000020 grad_norm=0.774209
Epoch 47/100 Iteration 180/234: loss=0.070092 lr=0.000020 grad_norm=0.874826
Epoch 47/100 Iteration 181/234: loss=0.064151 lr=0.000020 grad_norm=1.005423
Epoch 47/100 Iteration 182/234: loss=0.065235 lr=0.000020 grad_norm=1.429950
Epoch 47/100 Iteration 183/234: loss=0.062436 lr=0.000020 grad_norm=1.174709
Epoch 47/100 Iteration 184/234: loss=0.049743 lr=0.000020 grad_norm=0.357835
Epoch 47/100 Iteration 185/234: loss=0.055570 lr=0.000020 grad_norm=0.919160
Epoch 47/100 Iteration 186/234: loss=0.054491 lr=0.000020 grad_norm=0.612922
Epoch 47/100 Iteration 187/234: loss=0.066809 lr=0.000020 grad_norm=0.571946
Epoch 47/100 Iteration 188/234: loss=0.058246 lr=0.000020 grad_norm=0.630590
Epoch 47/100 Iteration 189/234: loss=0.056993 lr=0.000020 grad_norm=0.806295
Epoch 47/100 Iteration 190/234: loss=0.059979 lr=0.000020 grad_norm=0.921572
Epoch 47/100 Iteration 191/234: loss=0.056036 lr=0.000020 grad_norm=0.652033
Epoch 47/100 Iteration 192/234: loss=0.059485 lr=0.000020 grad_norm=0.419065
Epoch 47/100 Iteration 193/234: loss=0.056063 lr=0.000020 grad_norm=0.713668
Epoch 47/100 Iteration 194/234: loss=0.066882 lr=0.000020 grad_norm=0.533686
Epoch 47/100 Iteration 195/234: loss=0.057387 lr=0.000020 grad_norm=0.949986
Epoch 47/100 Iteration 196/234: loss=0.063951 lr=0.000020 grad_norm=1.227158
Epoch 47/100 Iteration 197/234: loss=0.055501 lr=0.000020 grad_norm=0.559566
Epoch 47/100 Iteration 198/234: loss=0.059953 lr=0.000020 grad_norm=0.801599
Epoch 47/100 Iteration 199/234: loss=0.056233 lr=0.000020 grad_norm=1.074713
Epoch 47/100 Iteration 200/234: loss=0.054931 lr=0.000020 grad_norm=0.782243
Epoch 47/100 Iteration 201/234: loss=0.064648 lr=0.000020 grad_norm=0.504078
Epoch 47/100 Iteration 202/234: loss=0.051245 lr=0.000020 grad_norm=0.782202
Epoch 47/100 Iteration 203/234: loss=0.051489 lr=0.000020 grad_norm=0.590522
Epoch 47/100 Iteration 204/234: loss=0.058131 lr=0.000020 grad_norm=0.587791
Epoch 47/100 Iteration 205/234: loss=0.048886 lr=0.000020 grad_norm=0.397462
Epoch 47/100 Iteration 206/234: loss=0.059441 lr=0.000020 grad_norm=0.662825
Epoch 47/100 Iteration 207/234: loss=0.060708 lr=0.000020 grad_norm=0.929700
Epoch 47/100 Iteration 208/234: loss=0.060553 lr=0.000020 grad_norm=0.865739
Epoch 47/100 Iteration 209/234: loss=0.055997 lr=0.000020 grad_norm=0.519844
Epoch 47/100 Iteration 210/234: loss=0.058964 lr=0.000020 grad_norm=0.622876
Epoch 47/100 Iteration 211/234: loss=0.052189 lr=0.000020 grad_norm=0.840487
Epoch 47/100 Iteration 212/234: loss=0.065797 lr=0.000020 grad_norm=0.403687
Epoch 47/100 Iteration 213/234: loss=0.062974 lr=0.000020 grad_norm=0.606842
Epoch 47/100 Iteration 214/234: loss=0.062233 lr=0.000020 grad_norm=0.349860
Epoch 47/100 Iteration 215/234: loss=0.054678 lr=0.000020 grad_norm=0.846676
Epoch 47/100 Iteration 216/234: loss=0.060231 lr=0.000020 grad_norm=1.420391
Epoch 47/100 Iteration 217/234: loss=0.061153 lr=0.000020 grad_norm=1.677793
Epoch 47/100 Iteration 218/234: loss=0.060014 lr=0.000020 grad_norm=0.806555
Epoch 47/100 Iteration 219/234: loss=0.055431 lr=0.000020 grad_norm=0.909120
Epoch 47/100 Iteration 220/234: loss=0.058662 lr=0.000020 grad_norm=1.106211
Epoch 47/100 Iteration 221/234: loss=0.064006 lr=0.000020 grad_norm=1.054981
Epoch 47/100 Iteration 222/234: loss=0.050993 lr=0.000020 grad_norm=0.607583
Epoch 47/100 Iteration 223/234: loss=0.064876 lr=0.000020 grad_norm=1.103727
Epoch 47/100 Iteration 224/234: loss=0.046248 lr=0.000020 grad_norm=1.142153
Epoch 47/100 Iteration 225/234: loss=0.056510 lr=0.000020 grad_norm=1.215776
Epoch 47/100 Iteration 226/234: loss=0.051553 lr=0.000020 grad_norm=0.599467
Epoch 47/100 Iteration 227/234: loss=0.061687 lr=0.000020 grad_norm=0.871807
Epoch 47/100 Iteration 228/234: loss=0.055421 lr=0.000020 grad_norm=1.293835
Epoch 47/100 Iteration 229/234: loss=0.051549 lr=0.000020 grad_norm=1.738682
Epoch 47/100 Iteration 230/234: loss=0.059779 lr=0.000020 grad_norm=1.502636
Epoch 47/100 Iteration 231/234: loss=0.055752 lr=0.000020 grad_norm=0.525973
Epoch 47/100 Iteration 232/234: loss=0.055070 lr=0.000020 grad_norm=0.699251
Epoch 47/100 Iteration 233/234: loss=0.058020 lr=0.000020 grad_norm=0.719769
Epoch 47/100 Iteration 234/234: loss=0.057698 lr=0.000020 grad_norm=0.454353
Epoch 47/100 finished. Avg Loss: 0.059071
Epoch 48/100 Iteration 1/234: loss=0.056008 lr=0.000020 grad_norm=0.616129
Epoch 48/100 Iteration 2/234: loss=0.054155 lr=0.000020 grad_norm=0.669641
Epoch 48/100 Iteration 3/234: loss=0.055448 lr=0.000020 grad_norm=0.803685
Epoch 48/100 Iteration 4/234: loss=0.060367 lr=0.000020 grad_norm=0.720698
Epoch 48/100 Iteration 5/234: loss=0.060203 lr=0.000020 grad_norm=0.668597
Epoch 48/100 Iteration 6/234: loss=0.059537 lr=0.000020 grad_norm=0.932747
Epoch 48/100 Iteration 7/234: loss=0.062904 lr=0.000020 grad_norm=0.491762
Epoch 48/100 Iteration 8/234: loss=0.065279 lr=0.000020 grad_norm=0.729244
Epoch 48/100 Iteration 9/234: loss=0.057399 lr=0.000020 grad_norm=1.201135
Epoch 48/100 Iteration 10/234: loss=0.054213 lr=0.000020 grad_norm=0.778748
Epoch 48/100 Iteration 11/234: loss=0.056794 lr=0.000020 grad_norm=0.460425
Epoch 48/100 Iteration 12/234: loss=0.050758 lr=0.000020 grad_norm=0.842273
Epoch 48/100 Iteration 13/234: loss=0.063006 lr=0.000020 grad_norm=0.648431
Epoch 48/100 Iteration 14/234: loss=0.052162 lr=0.000020 grad_norm=0.478531
Epoch 48/100 Iteration 15/234: loss=0.068487 lr=0.000020 grad_norm=1.102880
Epoch 48/100 Iteration 16/234: loss=0.055741 lr=0.000020 grad_norm=1.097305
Epoch 48/100 Iteration 17/234: loss=0.057366 lr=0.000020 grad_norm=0.742782
Epoch 48/100 Iteration 18/234: loss=0.052412 lr=0.000020 grad_norm=0.550979
Epoch 48/100 Iteration 19/234: loss=0.058604 lr=0.000020 grad_norm=1.467480
Epoch 48/100 Iteration 20/234: loss=0.059865 lr=0.000020 grad_norm=1.881432
Epoch 48/100 Iteration 21/234: loss=0.058931 lr=0.000020 grad_norm=0.897569
Epoch 48/100 Iteration 22/234: loss=0.055544 lr=0.000020 grad_norm=1.222911
Epoch 48/100 Iteration 23/234: loss=0.055940 lr=0.000020 grad_norm=1.994257
Epoch 48/100 Iteration 24/234: loss=0.053118 lr=0.000020 grad_norm=0.585368
Epoch 48/100 Iteration 25/234: loss=0.055537 lr=0.000020 grad_norm=1.438429
Epoch 48/100 Iteration 26/234: loss=0.049848 lr=0.000020 grad_norm=1.387102
Epoch 48/100 Iteration 27/234: loss=0.060322 lr=0.000020 grad_norm=0.642318
Epoch 48/100 Iteration 28/234: loss=0.061616 lr=0.000020 grad_norm=1.844865
Epoch 48/100 Iteration 29/234: loss=0.057260 lr=0.000020 grad_norm=1.044303
Epoch 48/100 Iteration 30/234: loss=0.066020 lr=0.000020 grad_norm=1.291344
Epoch 48/100 Iteration 31/234: loss=0.060934 lr=0.000020 grad_norm=2.295029
Epoch 48/100 Iteration 32/234: loss=0.061038 lr=0.000020 grad_norm=1.569444
Epoch 48/100 Iteration 33/234: loss=0.058071 lr=0.000020 grad_norm=1.141555
Epoch 48/100 Iteration 34/234: loss=0.058235 lr=0.000020 grad_norm=2.189478
Epoch 48/100 Iteration 35/234: loss=0.057392 lr=0.000020 grad_norm=1.252790
Epoch 48/100 Iteration 36/234: loss=0.055825 lr=0.000020 grad_norm=0.722842
Epoch 48/100 Iteration 37/234: loss=0.054272 lr=0.000020 grad_norm=1.260586
Epoch 48/100 Iteration 38/234: loss=0.056828 lr=0.000020 grad_norm=0.699838
Epoch 48/100 Iteration 39/234: loss=0.062303 lr=0.000020 grad_norm=0.634349
Epoch 48/100 Iteration 40/234: loss=0.067720 lr=0.000020 grad_norm=0.674626
Epoch 48/100 Iteration 41/234: loss=0.058923 lr=0.000020 grad_norm=0.821746
Epoch 48/100 Iteration 42/234: loss=0.055518 lr=0.000020 grad_norm=0.646581
Epoch 48/100 Iteration 43/234: loss=0.053942 lr=0.000020 grad_norm=0.902085
Epoch 48/100 Iteration 44/234: loss=0.059321 lr=0.000020 grad_norm=1.060870
Epoch 48/100 Iteration 45/234: loss=0.058913 lr=0.000020 grad_norm=0.696842
Epoch 48/100 Iteration 46/234: loss=0.057815 lr=0.000020 grad_norm=1.727181
Epoch 48/100 Iteration 47/234: loss=0.056690 lr=0.000020 grad_norm=1.224340
Epoch 48/100 Iteration 48/234: loss=0.052911 lr=0.000020 grad_norm=0.587062
Epoch 48/100 Iteration 49/234: loss=0.054914 lr=0.000020 grad_norm=1.376687
Epoch 48/100 Iteration 50/234: loss=0.055571 lr=0.000020 grad_norm=0.720138
Epoch 48/100 Iteration 51/234: loss=0.059314 lr=0.000020 grad_norm=0.701990
Epoch 48/100 Iteration 52/234: loss=0.058460 lr=0.000020 grad_norm=0.816250
Epoch 48/100 Iteration 53/234: loss=0.050024 lr=0.000020 grad_norm=0.374560
Epoch 48/100 Iteration 54/234: loss=0.055704 lr=0.000020 grad_norm=0.672028
Epoch 48/100 Iteration 55/234: loss=0.059155 lr=0.000020 grad_norm=0.686010
Epoch 48/100 Iteration 56/234: loss=0.059928 lr=0.000020 grad_norm=0.418345
Epoch 48/100 Iteration 57/234: loss=0.064773 lr=0.000020 grad_norm=0.511815
Epoch 48/100 Iteration 58/234: loss=0.051813 lr=0.000020 grad_norm=0.420052
Epoch 48/100 Iteration 59/234: loss=0.061834 lr=0.000020 grad_norm=0.528833
Epoch 48/100 Iteration 60/234: loss=0.063516 lr=0.000020 grad_norm=0.662636
Epoch 48/100 Iteration 61/234: loss=0.056385 lr=0.000020 grad_norm=0.595610
Epoch 48/100 Iteration 62/234: loss=0.055625 lr=0.000020 grad_norm=0.550298
Epoch 48/100 Iteration 63/234: loss=0.059424 lr=0.000020 grad_norm=0.790908
Epoch 48/100 Iteration 64/234: loss=0.055785 lr=0.000020 grad_norm=0.765454
Epoch 48/100 Iteration 65/234: loss=0.058859 lr=0.000020 grad_norm=0.781477
Epoch 48/100 Iteration 66/234: loss=0.064005 lr=0.000020 grad_norm=1.063458
Epoch 48/100 Iteration 67/234: loss=0.049348 lr=0.000020 grad_norm=0.901445
Epoch 48/100 Iteration 68/234: loss=0.052095 lr=0.000020 grad_norm=0.865520
Epoch 48/100 Iteration 69/234: loss=0.062139 lr=0.000020 grad_norm=0.978262
Epoch 48/100 Iteration 70/234: loss=0.060996 lr=0.000020 grad_norm=0.879666
Epoch 48/100 Iteration 71/234: loss=0.054161 lr=0.000020 grad_norm=0.976290
Epoch 48/100 Iteration 72/234: loss=0.063922 lr=0.000020 grad_norm=0.779088
Epoch 48/100 Iteration 73/234: loss=0.055592 lr=0.000020 grad_norm=0.831912
Epoch 48/100 Iteration 74/234: loss=0.064565 lr=0.000020 grad_norm=0.851445
Epoch 48/100 Iteration 75/234: loss=0.064851 lr=0.000020 grad_norm=1.311062
Epoch 48/100 Iteration 76/234: loss=0.050748 lr=0.000020 grad_norm=1.460985
Epoch 48/100 Iteration 77/234: loss=0.064233 lr=0.000020 grad_norm=0.499731
Epoch 48/100 Iteration 78/234: loss=0.065626 lr=0.000020 grad_norm=1.517539
Epoch 48/100 Iteration 79/234: loss=0.066921 lr=0.000020 grad_norm=1.498202
Epoch 48/100 Iteration 80/234: loss=0.050578 lr=0.000020 grad_norm=0.427666
Epoch 48/100 Iteration 81/234: loss=0.060473 lr=0.000020 grad_norm=1.574962
Epoch 48/100 Iteration 82/234: loss=0.061492 lr=0.000020 grad_norm=1.308892
Epoch 48/100 Iteration 83/234: loss=0.056251 lr=0.000020 grad_norm=0.358268
Epoch 48/100 Iteration 84/234: loss=0.051876 lr=0.000020 grad_norm=0.784406
Epoch 48/100 Iteration 85/234: loss=0.053190 lr=0.000020 grad_norm=0.679133
Epoch 48/100 Iteration 86/234: loss=0.058421 lr=0.000020 grad_norm=0.814319
Epoch 48/100 Iteration 87/234: loss=0.054375 lr=0.000020 grad_norm=1.063951
Epoch 48/100 Iteration 88/234: loss=0.052356 lr=0.000020 grad_norm=0.739138
Epoch 48/100 Iteration 89/234: loss=0.056678 lr=0.000020 grad_norm=0.382646
Epoch 48/100 Iteration 90/234: loss=0.058717 lr=0.000020 grad_norm=0.585205
Epoch 48/100 Iteration 91/234: loss=0.057595 lr=0.000020 grad_norm=0.972554
Epoch 48/100 Iteration 92/234: loss=0.060078 lr=0.000020 grad_norm=1.135152
Epoch 48/100 Iteration 93/234: loss=0.055197 lr=0.000020 grad_norm=0.593777
Epoch 48/100 Iteration 94/234: loss=0.052544 lr=0.000020 grad_norm=1.030542
Epoch 48/100 Iteration 95/234: loss=0.061711 lr=0.000020 grad_norm=1.518608
Epoch 48/100 Iteration 96/234: loss=0.056597 lr=0.000020 grad_norm=1.145229
Epoch 48/100 Iteration 97/234: loss=0.060370 lr=0.000020 grad_norm=0.634338
Epoch 48/100 Iteration 98/234: loss=0.055497 lr=0.000020 grad_norm=0.919516
Epoch 48/100 Iteration 99/234: loss=0.058501 lr=0.000020 grad_norm=1.078115
Epoch 48/100 Iteration 100/234: loss=0.057216 lr=0.000020 grad_norm=0.435743
Epoch 48/100 Iteration 101/234: loss=0.053086 lr=0.000020 grad_norm=0.996699
Epoch 48/100 Iteration 102/234: loss=0.049625 lr=0.000020 grad_norm=1.239941
Epoch 48/100 Iteration 103/234: loss=0.057003 lr=0.000020 grad_norm=0.554645
Epoch 48/100 Iteration 104/234: loss=0.056827 lr=0.000020 grad_norm=1.038861
Epoch 48/100 Iteration 105/234: loss=0.054863 lr=0.000020 grad_norm=1.287222
Epoch 48/100 Iteration 106/234: loss=0.056219 lr=0.000020 grad_norm=0.437863
Epoch 48/100 Iteration 107/234: loss=0.059915 lr=0.000020 grad_norm=1.138116
Epoch 48/100 Iteration 108/234: loss=0.060048 lr=0.000020 grad_norm=1.083375
Epoch 48/100 Iteration 109/234: loss=0.055141 lr=0.000020 grad_norm=0.560890
Epoch 48/100 Iteration 110/234: loss=0.057136 lr=0.000020 grad_norm=1.162487
Epoch 48/100 Iteration 111/234: loss=0.058100 lr=0.000020 grad_norm=1.096299
Epoch 48/100 Iteration 112/234: loss=0.050241 lr=0.000020 grad_norm=0.619300
Epoch 48/100 Iteration 113/234: loss=0.054911 lr=0.000020 grad_norm=0.693058
Epoch 48/100 Iteration 114/234: loss=0.058668 lr=0.000020 grad_norm=0.858016
Epoch 48/100 Iteration 115/234: loss=0.055597 lr=0.000020 grad_norm=0.896448
Epoch 48/100 Iteration 116/234: loss=0.052074 lr=0.000020 grad_norm=0.756456
Epoch 48/100 Iteration 117/234: loss=0.046267 lr=0.000020 grad_norm=0.306033
Epoch 48/100 Iteration 118/234: loss=0.062262 lr=0.000020 grad_norm=0.768466
Epoch 48/100 Iteration 119/234: loss=0.064876 lr=0.000020 grad_norm=0.994759
Epoch 48/100 Iteration 120/234: loss=0.063020 lr=0.000020 grad_norm=1.658627
Epoch 48/100 Iteration 121/234: loss=0.054811 lr=0.000020 grad_norm=1.401864
Epoch 48/100 Iteration 122/234: loss=0.061369 lr=0.000020 grad_norm=0.656290
Epoch 48/100 Iteration 123/234: loss=0.060469 lr=0.000020 grad_norm=1.029358
Epoch 48/100 Iteration 124/234: loss=0.050086 lr=0.000020 grad_norm=0.778604
Epoch 48/100 Iteration 125/234: loss=0.060234 lr=0.000020 grad_norm=1.296003
Epoch 48/100 Iteration 126/234: loss=0.060531 lr=0.000020 grad_norm=1.610466
Epoch 48/100 Iteration 127/234: loss=0.047408 lr=0.000020 grad_norm=1.003254
Epoch 48/100 Iteration 128/234: loss=0.062687 lr=0.000020 grad_norm=0.884584
Epoch 48/100 Iteration 129/234: loss=0.070633 lr=0.000020 grad_norm=1.255972
Epoch 48/100 Iteration 130/234: loss=0.055699 lr=0.000020 grad_norm=1.388485
Epoch 48/100 Iteration 131/234: loss=0.049497 lr=0.000020 grad_norm=0.844952
Epoch 48/100 Iteration 132/234: loss=0.057683 lr=0.000020 grad_norm=0.674455
Epoch 48/100 Iteration 133/234: loss=0.054719 lr=0.000020 grad_norm=1.552550
Epoch 48/100 Iteration 134/234: loss=0.056857 lr=0.000020 grad_norm=1.095466
Epoch 48/100 Iteration 135/234: loss=0.057822 lr=0.000020 grad_norm=0.770969
Epoch 48/100 Iteration 136/234: loss=0.055701 lr=0.000020 grad_norm=1.743707
Epoch 48/100 Iteration 137/234: loss=0.054440 lr=0.000020 grad_norm=1.474216
Epoch 48/100 Iteration 138/234: loss=0.058943 lr=0.000020 grad_norm=0.699370
Epoch 48/100 Iteration 139/234: loss=0.054022 lr=0.000020 grad_norm=1.774274
Epoch 48/100 Iteration 140/234: loss=0.051150 lr=0.000020 grad_norm=1.924339
Epoch 48/100 Iteration 141/234: loss=0.055958 lr=0.000020 grad_norm=0.677391
Epoch 48/100 Iteration 142/234: loss=0.059995 lr=0.000020 grad_norm=1.890454
Epoch 48/100 Iteration 143/234: loss=0.066189 lr=0.000020 grad_norm=1.530451
Epoch 48/100 Iteration 144/234: loss=0.054806 lr=0.000020 grad_norm=0.587290
Epoch 48/100 Iteration 145/234: loss=0.061170 lr=0.000020 grad_norm=1.754281
Epoch 48/100 Iteration 146/234: loss=0.052766 lr=0.000020 grad_norm=1.161217
Epoch 48/100 Iteration 147/234: loss=0.064051 lr=0.000020 grad_norm=1.180747
Epoch 48/100 Iteration 148/234: loss=0.060181 lr=0.000020 grad_norm=2.180127
Epoch 48/100 Iteration 149/234: loss=0.054633 lr=0.000020 grad_norm=0.914437
Epoch 48/100 Iteration 150/234: loss=0.061015 lr=0.000020 grad_norm=1.487268
Epoch 48/100 Iteration 151/234: loss=0.046841 lr=0.000020 grad_norm=1.182630
Epoch 48/100 Iteration 152/234: loss=0.061035 lr=0.000020 grad_norm=1.470341
Epoch 48/100 Iteration 153/234: loss=0.063903 lr=0.000020 grad_norm=1.797190
Epoch 48/100 Iteration 154/234: loss=0.059925 lr=0.000020 grad_norm=1.087939
Epoch 48/100 Iteration 155/234: loss=0.060746 lr=0.000020 grad_norm=1.949885
Epoch 48/100 Iteration 156/234: loss=0.063058 lr=0.000020 grad_norm=0.744330
Epoch 48/100 Iteration 157/234: loss=0.051516 lr=0.000020 grad_norm=1.791042
Epoch 48/100 Iteration 158/234: loss=0.050280 lr=0.000020 grad_norm=1.590131
Epoch 48/100 Iteration 159/234: loss=0.049986 lr=0.000020 grad_norm=0.970167
Epoch 48/100 Iteration 160/234: loss=0.057959 lr=0.000020 grad_norm=1.712337
Epoch 48/100 Iteration 161/234: loss=0.046774 lr=0.000020 grad_norm=0.871995
Epoch 48/100 Iteration 162/234: loss=0.051952 lr=0.000020 grad_norm=1.005996
Epoch 48/100 Iteration 163/234: loss=0.063333 lr=0.000020 grad_norm=1.449940
Epoch 48/100 Iteration 164/234: loss=0.058834 lr=0.000020 grad_norm=0.642260
Epoch 48/100 Iteration 165/234: loss=0.055321 lr=0.000020 grad_norm=0.971056
Epoch 48/100 Iteration 166/234: loss=0.061449 lr=0.000020 grad_norm=0.633957
Epoch 48/100 Iteration 167/234: loss=0.058143 lr=0.000020 grad_norm=0.536688
Epoch 48/100 Iteration 168/234: loss=0.062375 lr=0.000020 grad_norm=1.087111
Epoch 48/100 Iteration 169/234: loss=0.059160 lr=0.000020 grad_norm=0.905953
Epoch 48/100 Iteration 170/234: loss=0.057110 lr=0.000020 grad_norm=0.470103
Epoch 48/100 Iteration 171/234: loss=0.057628 lr=0.000020 grad_norm=0.743197
Epoch 48/100 Iteration 172/234: loss=0.064682 lr=0.000020 grad_norm=0.578111
Epoch 48/100 Iteration 173/234: loss=0.052101 lr=0.000020 grad_norm=0.356756
Epoch 48/100 Iteration 174/234: loss=0.063152 lr=0.000020 grad_norm=0.434715
Epoch 48/100 Iteration 175/234: loss=0.050513 lr=0.000020 grad_norm=0.509710
Epoch 48/100 Iteration 176/234: loss=0.054015 lr=0.000020 grad_norm=0.426157
Epoch 48/100 Iteration 177/234: loss=0.067298 lr=0.000020 grad_norm=0.460811
Epoch 48/100 Iteration 178/234: loss=0.054926 lr=0.000020 grad_norm=0.535316
Epoch 48/100 Iteration 179/234: loss=0.063480 lr=0.000020 grad_norm=0.613519
Epoch 48/100 Iteration 180/234: loss=0.057125 lr=0.000020 grad_norm=0.414411
Epoch 48/100 Iteration 181/234: loss=0.055706 lr=0.000020 grad_norm=0.851363
Epoch 48/100 Iteration 182/234: loss=0.053675 lr=0.000020 grad_norm=0.558788
Epoch 48/100 Iteration 183/234: loss=0.058727 lr=0.000020 grad_norm=0.527412
Epoch 48/100 Iteration 184/234: loss=0.050017 lr=0.000020 grad_norm=0.692210
Epoch 48/100 Iteration 185/234: loss=0.060612 lr=0.000020 grad_norm=0.991270
Epoch 48/100 Iteration 186/234: loss=0.063008 lr=0.000020 grad_norm=0.877764
Epoch 48/100 Iteration 187/234: loss=0.054950 lr=0.000020 grad_norm=0.422904
Epoch 48/100 Iteration 188/234: loss=0.060415 lr=0.000020 grad_norm=0.876220
Epoch 48/100 Iteration 189/234: loss=0.061122 lr=0.000020 grad_norm=0.964203
Epoch 48/100 Iteration 190/234: loss=0.054918 lr=0.000020 grad_norm=0.496739
Epoch 48/100 Iteration 191/234: loss=0.065829 lr=0.000020 grad_norm=0.549916
Epoch 48/100 Iteration 192/234: loss=0.059572 lr=0.000020 grad_norm=0.745185
Epoch 48/100 Iteration 193/234: loss=0.063361 lr=0.000020 grad_norm=0.487630
Epoch 48/100 Iteration 194/234: loss=0.055974 lr=0.000020 grad_norm=0.484433
Epoch 48/100 Iteration 195/234: loss=0.053883 lr=0.000020 grad_norm=0.598321
Epoch 48/100 Iteration 196/234: loss=0.061760 lr=0.000020 grad_norm=0.659673
Epoch 48/100 Iteration 197/234: loss=0.055471 lr=0.000020 grad_norm=0.753194
Epoch 48/100 Iteration 198/234: loss=0.050881 lr=0.000020 grad_norm=0.362632
Epoch 48/100 Iteration 199/234: loss=0.048428 lr=0.000020 grad_norm=0.612278
Epoch 48/100 Iteration 200/234: loss=0.061714 lr=0.000020 grad_norm=0.758032
Epoch 48/100 Iteration 201/234: loss=0.065155 lr=0.000020 grad_norm=0.474146
Epoch 48/100 Iteration 202/234: loss=0.060506 lr=0.000020 grad_norm=0.591055
Epoch 48/100 Iteration 203/234: loss=0.061645 lr=0.000020 grad_norm=1.018685
Epoch 48/100 Iteration 204/234: loss=0.053543 lr=0.000020 grad_norm=0.724977
Epoch 48/100 Iteration 205/234: loss=0.057275 lr=0.000020 grad_norm=0.601249
Epoch 48/100 Iteration 206/234: loss=0.057138 lr=0.000020 grad_norm=1.456251
Epoch 48/100 Iteration 207/234: loss=0.058786 lr=0.000020 grad_norm=1.420841
Epoch 48/100 Iteration 208/234: loss=0.054079 lr=0.000020 grad_norm=0.655013
Epoch 48/100 Iteration 209/234: loss=0.054445 lr=0.000020 grad_norm=0.672263
Epoch 48/100 Iteration 210/234: loss=0.061303 lr=0.000020 grad_norm=1.036179
Epoch 48/100 Iteration 211/234: loss=0.054242 lr=0.000020 grad_norm=0.682313
Epoch 48/100 Iteration 212/234: loss=0.059148 lr=0.000020 grad_norm=0.432082
Epoch 48/100 Iteration 213/234: loss=0.060273 lr=0.000020 grad_norm=0.697731
Epoch 48/100 Iteration 214/234: loss=0.060272 lr=0.000020 grad_norm=1.037698
Epoch 48/100 Iteration 215/234: loss=0.062225 lr=0.000020 grad_norm=0.763741
Epoch 48/100 Iteration 216/234: loss=0.059080 lr=0.000020 grad_norm=0.403514
Epoch 48/100 Iteration 217/234: loss=0.052911 lr=0.000020 grad_norm=0.399196
Epoch 48/100 Iteration 218/234: loss=0.063813 lr=0.000020 grad_norm=0.686395
Epoch 48/100 Iteration 219/234: loss=0.055595 lr=0.000020 grad_norm=0.529831
Epoch 48/100 Iteration 220/234: loss=0.056187 lr=0.000020 grad_norm=0.393281
Epoch 48/100 Iteration 221/234: loss=0.063969 lr=0.000020 grad_norm=1.108946
Epoch 48/100 Iteration 222/234: loss=0.059848 lr=0.000020 grad_norm=1.456622
Epoch 48/100 Iteration 223/234: loss=0.057929 lr=0.000020 grad_norm=0.733588
Epoch 48/100 Iteration 224/234: loss=0.060500 lr=0.000020 grad_norm=0.999655
Epoch 48/100 Iteration 225/234: loss=0.050241 lr=0.000020 grad_norm=1.303023
Epoch 48/100 Iteration 226/234: loss=0.050533 lr=0.000020 grad_norm=0.628393
Epoch 48/100 Iteration 227/234: loss=0.064333 lr=0.000020 grad_norm=1.404751
Epoch 48/100 Iteration 228/234: loss=0.064423 lr=0.000020 grad_norm=1.658345
Epoch 48/100 Iteration 229/234: loss=0.059929 lr=0.000020 grad_norm=1.136011
Epoch 48/100 Iteration 230/234: loss=0.056305 lr=0.000020 grad_norm=1.196712
Epoch 48/100 Iteration 231/234: loss=0.054149 lr=0.000020 grad_norm=0.704802
Epoch 48/100 Iteration 232/234: loss=0.054657 lr=0.000020 grad_norm=0.872241
Epoch 48/100 Iteration 233/234: loss=0.063681 lr=0.000020 grad_norm=1.371574
Epoch 48/100 Iteration 234/234: loss=0.052872 lr=0.000020 grad_norm=1.124567
Epoch 48/100 finished. Avg Loss: 0.057686
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 49/100 Iteration 1/234: loss=0.059263 lr=0.000020 grad_norm=0.834624
Epoch 49/100 Iteration 2/234: loss=0.057818 lr=0.000020 grad_norm=1.025317
Epoch 49/100 Iteration 3/234: loss=0.058593 lr=0.000020 grad_norm=0.540576
Epoch 49/100 Iteration 4/234: loss=0.056495 lr=0.000020 grad_norm=0.740434
Epoch 49/100 Iteration 5/234: loss=0.057459 lr=0.000020 grad_norm=0.968329
Epoch 49/100 Iteration 6/234: loss=0.060572 lr=0.000020 grad_norm=0.624763
Epoch 49/100 Iteration 7/234: loss=0.055724 lr=0.000020 grad_norm=0.664272
Epoch 49/100 Iteration 8/234: loss=0.049734 lr=0.000020 grad_norm=1.401539
Epoch 49/100 Iteration 9/234: loss=0.065250 lr=0.000020 grad_norm=1.666466
Epoch 49/100 Iteration 10/234: loss=0.059098 lr=0.000020 grad_norm=1.349744
Epoch 49/100 Iteration 11/234: loss=0.062545 lr=0.000020 grad_norm=0.809530
Epoch 49/100 Iteration 12/234: loss=0.060712 lr=0.000020 grad_norm=0.603077
Epoch 49/100 Iteration 13/234: loss=0.059621 lr=0.000020 grad_norm=0.670869
Epoch 49/100 Iteration 14/234: loss=0.056312 lr=0.000020 grad_norm=0.659157
Epoch 49/100 Iteration 15/234: loss=0.053902 lr=0.000020 grad_norm=1.371076
Epoch 49/100 Iteration 16/234: loss=0.061664 lr=0.000020 grad_norm=1.189023
Epoch 49/100 Iteration 17/234: loss=0.059451 lr=0.000020 grad_norm=0.526216
Epoch 49/100 Iteration 18/234: loss=0.057350 lr=0.000020 grad_norm=1.040055
Epoch 49/100 Iteration 19/234: loss=0.062638 lr=0.000020 grad_norm=1.460918
Epoch 49/100 Iteration 20/234: loss=0.053569 lr=0.000020 grad_norm=0.707420
Epoch 49/100 Iteration 21/234: loss=0.055705 lr=0.000020 grad_norm=0.732721
Epoch 49/100 Iteration 22/234: loss=0.055448 lr=0.000020 grad_norm=0.795578
Epoch 49/100 Iteration 23/234: loss=0.052145 lr=0.000020 grad_norm=1.032823
Epoch 49/100 Iteration 24/234: loss=0.050422 lr=0.000020 grad_norm=1.205415
Epoch 49/100 Iteration 25/234: loss=0.061023 lr=0.000020 grad_norm=0.858768
Epoch 49/100 Iteration 26/234: loss=0.062924 lr=0.000020 grad_norm=0.593155
Epoch 49/100 Iteration 27/234: loss=0.067354 lr=0.000020 grad_norm=1.389350
Epoch 49/100 Iteration 28/234: loss=0.057057 lr=0.000020 grad_norm=1.267725
Epoch 49/100 Iteration 29/234: loss=0.060601 lr=0.000020 grad_norm=0.702868
Epoch 49/100 Iteration 30/234: loss=0.065076 lr=0.000020 grad_norm=1.033947
Epoch 49/100 Iteration 31/234: loss=0.065514 lr=0.000020 grad_norm=1.471942
Epoch 49/100 Iteration 32/234: loss=0.059611 lr=0.000020 grad_norm=1.218510
Epoch 49/100 Iteration 33/234: loss=0.059928 lr=0.000020 grad_norm=0.507407
Epoch 49/100 Iteration 34/234: loss=0.058401 lr=0.000020 grad_norm=0.516540
Epoch 49/100 Iteration 35/234: loss=0.058569 lr=0.000020 grad_norm=0.559947
Epoch 49/100 Iteration 36/234: loss=0.057417 lr=0.000020 grad_norm=0.839763
Epoch 49/100 Iteration 37/234: loss=0.056685 lr=0.000020 grad_norm=0.808202
Epoch 49/100 Iteration 38/234: loss=0.060412 lr=0.000020 grad_norm=0.626750
Epoch 49/100 Iteration 39/234: loss=0.059936 lr=0.000020 grad_norm=0.418896
Epoch 49/100 Iteration 40/234: loss=0.058683 lr=0.000020 grad_norm=0.477831
Epoch 49/100 Iteration 41/234: loss=0.056718 lr=0.000020 grad_norm=0.553954
Epoch 49/100 Iteration 42/234: loss=0.056393 lr=0.000020 grad_norm=1.001717
Epoch 49/100 Iteration 43/234: loss=0.051317 lr=0.000020 grad_norm=1.444237
Epoch 49/100 Iteration 44/234: loss=0.057787 lr=0.000020 grad_norm=1.395645
Epoch 49/100 Iteration 45/234: loss=0.052186 lr=0.000020 grad_norm=0.756147
Epoch 49/100 Iteration 46/234: loss=0.054598 lr=0.000020 grad_norm=0.531400
Epoch 49/100 Iteration 47/234: loss=0.051903 lr=0.000020 grad_norm=1.106852
Epoch 49/100 Iteration 48/234: loss=0.050154 lr=0.000020 grad_norm=1.028822
Epoch 49/100 Iteration 49/234: loss=0.055713 lr=0.000020 grad_norm=0.539570
Epoch 49/100 Iteration 50/234: loss=0.052284 lr=0.000020 grad_norm=0.590955
Epoch 49/100 Iteration 51/234: loss=0.049637 lr=0.000020 grad_norm=0.688031
Epoch 49/100 Iteration 52/234: loss=0.060032 lr=0.000020 grad_norm=0.501565
Epoch 49/100 Iteration 53/234: loss=0.051298 lr=0.000020 grad_norm=0.563382
Epoch 49/100 Iteration 54/234: loss=0.050835 lr=0.000020 grad_norm=0.728279
Epoch 49/100 Iteration 55/234: loss=0.062424 lr=0.000020 grad_norm=0.475412
Epoch 49/100 Iteration 56/234: loss=0.053505 lr=0.000020 grad_norm=0.651852
Epoch 49/100 Iteration 57/234: loss=0.058189 lr=0.000020 grad_norm=1.028925
Epoch 49/100 Iteration 58/234: loss=0.061656 lr=0.000020 grad_norm=0.645450
Epoch 49/100 Iteration 59/234: loss=0.049592 lr=0.000020 grad_norm=0.363561
Epoch 49/100 Iteration 60/234: loss=0.058782 lr=0.000020 grad_norm=1.184019
Epoch 49/100 Iteration 61/234: loss=0.059547 lr=0.000020 grad_norm=1.901309
Epoch 49/100 Iteration 62/234: loss=0.063875 lr=0.000020 grad_norm=1.286470
Epoch 49/100 Iteration 63/234: loss=0.054986 lr=0.000020 grad_norm=0.591667
Epoch 49/100 Iteration 64/234: loss=0.049773 lr=0.000020 grad_norm=1.255332
Epoch 49/100 Iteration 65/234: loss=0.048686 lr=0.000020 grad_norm=0.713734
Epoch 49/100 Iteration 66/234: loss=0.066840 lr=0.000020 grad_norm=0.897441
Epoch 49/100 Iteration 67/234: loss=0.062438 lr=0.000020 grad_norm=0.722652
Epoch 49/100 Iteration 68/234: loss=0.064819 lr=0.000020 grad_norm=0.698068
Epoch 49/100 Iteration 69/234: loss=0.055972 lr=0.000020 grad_norm=0.718669
Epoch 49/100 Iteration 70/234: loss=0.057330 lr=0.000020 grad_norm=0.575401
Epoch 49/100 Iteration 71/234: loss=0.057712 lr=0.000020 grad_norm=0.763588
Epoch 49/100 Iteration 72/234: loss=0.053646 lr=0.000020 grad_norm=0.606954
Epoch 49/100 Iteration 73/234: loss=0.055673 lr=0.000020 grad_norm=0.490484
Epoch 49/100 Iteration 74/234: loss=0.062249 lr=0.000020 grad_norm=0.817357
Epoch 49/100 Iteration 75/234: loss=0.061587 lr=0.000020 grad_norm=1.036782
Epoch 49/100 Iteration 76/234: loss=0.050561 lr=0.000020 grad_norm=0.650521
Epoch 49/100 Iteration 77/234: loss=0.058239 lr=0.000020 grad_norm=0.667160
Epoch 49/100 Iteration 78/234: loss=0.069549 lr=0.000020 grad_norm=0.780529
Epoch 49/100 Iteration 79/234: loss=0.055950 lr=0.000020 grad_norm=0.616782
Epoch 49/100 Iteration 80/234: loss=0.062915 lr=0.000020 grad_norm=0.556584
Epoch 49/100 Iteration 81/234: loss=0.058918 lr=0.000020 grad_norm=0.857672
Epoch 49/100 Iteration 82/234: loss=0.051911 lr=0.000020 grad_norm=0.792685
Epoch 49/100 Iteration 83/234: loss=0.059284 lr=0.000020 grad_norm=0.525420
Epoch 49/100 Iteration 84/234: loss=0.051599 lr=0.000020 grad_norm=1.079044
Epoch 49/100 Iteration 85/234: loss=0.057024 lr=0.000020 grad_norm=0.859249
Epoch 49/100 Iteration 86/234: loss=0.061321 lr=0.000020 grad_norm=0.728760
Epoch 49/100 Iteration 87/234: loss=0.051663 lr=0.000020 grad_norm=0.911202
Epoch 49/100 Iteration 88/234: loss=0.060136 lr=0.000020 grad_norm=0.916391
Epoch 49/100 Iteration 89/234: loss=0.051426 lr=0.000020 grad_norm=0.854074
Epoch 49/100 Iteration 90/234: loss=0.052042 lr=0.000020 grad_norm=0.593883
Epoch 49/100 Iteration 91/234: loss=0.056109 lr=0.000020 grad_norm=0.656475
Epoch 49/100 Iteration 92/234: loss=0.052762 lr=0.000020 grad_norm=0.837765
Epoch 49/100 Iteration 93/234: loss=0.056598 lr=0.000020 grad_norm=0.844462
Epoch 49/100 Iteration 94/234: loss=0.054266 lr=0.000020 grad_norm=0.754543
Epoch 49/100 Iteration 95/234: loss=0.055661 lr=0.000020 grad_norm=1.082145
Epoch 49/100 Iteration 96/234: loss=0.051506 lr=0.000020 grad_norm=1.165365
Epoch 49/100 Iteration 97/234: loss=0.056946 lr=0.000020 grad_norm=0.734026
Epoch 49/100 Iteration 98/234: loss=0.054758 lr=0.000020 grad_norm=0.544550
Epoch 49/100 Iteration 99/234: loss=0.061192 lr=0.000020 grad_norm=0.784782
Epoch 49/100 Iteration 100/234: loss=0.054736 lr=0.000020 grad_norm=1.398336
Epoch 49/100 Iteration 101/234: loss=0.065555 lr=0.000020 grad_norm=1.663650
Epoch 49/100 Iteration 102/234: loss=0.059585 lr=0.000020 grad_norm=1.210774
Epoch 49/100 Iteration 103/234: loss=0.056324 lr=0.000020 grad_norm=0.406952
Epoch 49/100 Iteration 104/234: loss=0.061004 lr=0.000020 grad_norm=1.634548
Epoch 49/100 Iteration 105/234: loss=0.055609 lr=0.000020 grad_norm=2.055356
Epoch 49/100 Iteration 106/234: loss=0.059547 lr=0.000020 grad_norm=1.070908
Epoch 49/100 Iteration 107/234: loss=0.057513 lr=0.000020 grad_norm=0.998263
Epoch 49/100 Iteration 108/234: loss=0.059169 lr=0.000020 grad_norm=0.638812
Epoch 49/100 Iteration 109/234: loss=0.062554 lr=0.000020 grad_norm=0.938663
Epoch 49/100 Iteration 110/234: loss=0.059084 lr=0.000020 grad_norm=1.247279
Epoch 49/100 Iteration 111/234: loss=0.055053 lr=0.000020 grad_norm=0.372691
Epoch 49/100 Iteration 112/234: loss=0.055213 lr=0.000020 grad_norm=1.254223
Epoch 49/100 Iteration 113/234: loss=0.051674 lr=0.000020 grad_norm=0.974543
Epoch 49/100 Iteration 114/234: loss=0.063619 lr=0.000020 grad_norm=0.451634
Epoch 49/100 Iteration 115/234: loss=0.059683 lr=0.000020 grad_norm=1.128381
Epoch 49/100 Iteration 116/234: loss=0.060957 lr=0.000020 grad_norm=0.962961
Epoch 49/100 Iteration 117/234: loss=0.057809 lr=0.000020 grad_norm=0.506682
Epoch 49/100 Iteration 118/234: loss=0.054064 lr=0.000020 grad_norm=0.768330
Epoch 49/100 Iteration 119/234: loss=0.056591 lr=0.000020 grad_norm=0.404095
Epoch 49/100 Iteration 120/234: loss=0.054598 lr=0.000020 grad_norm=0.769017
Epoch 49/100 Iteration 121/234: loss=0.053979 lr=0.000020 grad_norm=1.312175
Epoch 49/100 Iteration 122/234: loss=0.055270 lr=0.000020 grad_norm=1.199894
Epoch 49/100 Iteration 123/234: loss=0.061395 lr=0.000020 grad_norm=0.460076
Epoch 49/100 Iteration 124/234: loss=0.053925 lr=0.000020 grad_norm=0.606857
Epoch 49/100 Iteration 125/234: loss=0.055326 lr=0.000020 grad_norm=0.697341
Epoch 49/100 Iteration 126/234: loss=0.054796 lr=0.000020 grad_norm=0.560138
Epoch 49/100 Iteration 127/234: loss=0.053770 lr=0.000020 grad_norm=0.375825
Epoch 49/100 Iteration 128/234: loss=0.055501 lr=0.000020 grad_norm=0.653666
Epoch 49/100 Iteration 129/234: loss=0.061231 lr=0.000020 grad_norm=0.865387
Epoch 49/100 Iteration 130/234: loss=0.051833 lr=0.000020 grad_norm=0.721726
Epoch 49/100 Iteration 131/234: loss=0.051114 lr=0.000020 grad_norm=0.438818
Epoch 49/100 Iteration 132/234: loss=0.058456 lr=0.000020 grad_norm=0.683850
Epoch 49/100 Iteration 133/234: loss=0.055659 lr=0.000020 grad_norm=0.962366
Epoch 49/100 Iteration 134/234: loss=0.057428 lr=0.000020 grad_norm=0.784736
Epoch 49/100 Iteration 135/234: loss=0.059334 lr=0.000020 grad_norm=0.644787
Epoch 49/100 Iteration 136/234: loss=0.056824 lr=0.000020 grad_norm=0.452972
Epoch 49/100 Iteration 137/234: loss=0.056778 lr=0.000020 grad_norm=0.615190
Epoch 49/100 Iteration 138/234: loss=0.058841 lr=0.000020 grad_norm=0.653513
Epoch 49/100 Iteration 139/234: loss=0.059942 lr=0.000020 grad_norm=0.517611
Epoch 49/100 Iteration 140/234: loss=0.053406 lr=0.000020 grad_norm=0.533599
Epoch 49/100 Iteration 141/234: loss=0.054118 lr=0.000020 grad_norm=1.120190
Epoch 49/100 Iteration 142/234: loss=0.059465 lr=0.000020 grad_norm=1.213156
Epoch 49/100 Iteration 143/234: loss=0.062686 lr=0.000020 grad_norm=0.784214
Epoch 49/100 Iteration 144/234: loss=0.048537 lr=0.000020 grad_norm=0.368659
Epoch 49/100 Iteration 145/234: loss=0.055587 lr=0.000020 grad_norm=0.387108
Epoch 49/100 Iteration 146/234: loss=0.048448 lr=0.000020 grad_norm=0.419378
Epoch 49/100 Iteration 147/234: loss=0.065852 lr=0.000020 grad_norm=0.511700
Epoch 49/100 Iteration 148/234: loss=0.057801 lr=0.000020 grad_norm=0.622857
Epoch 49/100 Iteration 149/234: loss=0.045916 lr=0.000020 grad_norm=0.650119
Epoch 49/100 Iteration 150/234: loss=0.067966 lr=0.000020 grad_norm=0.518608
Epoch 49/100 Iteration 151/234: loss=0.052860 lr=0.000020 grad_norm=0.548854
Epoch 49/100 Iteration 152/234: loss=0.058044 lr=0.000020 grad_norm=0.826135
Epoch 49/100 Iteration 153/234: loss=0.058250 lr=0.000020 grad_norm=0.615617
Epoch 49/100 Iteration 154/234: loss=0.057240 lr=0.000020 grad_norm=0.533830
Epoch 49/100 Iteration 155/234: loss=0.044188 lr=0.000020 grad_norm=0.461981
Epoch 49/100 Iteration 156/234: loss=0.046470 lr=0.000020 grad_norm=0.534049
Epoch 49/100 Iteration 157/234: loss=0.054661 lr=0.000020 grad_norm=0.485426
Epoch 49/100 Iteration 158/234: loss=0.062075 lr=0.000020 grad_norm=0.443643
Epoch 49/100 Iteration 159/234: loss=0.062103 lr=0.000020 grad_norm=0.559504
Epoch 49/100 Iteration 160/234: loss=0.061166 lr=0.000020 grad_norm=0.664164
Epoch 49/100 Iteration 161/234: loss=0.054564 lr=0.000020 grad_norm=0.861932
Epoch 49/100 Iteration 162/234: loss=0.054718 lr=0.000020 grad_norm=1.092815
Epoch 49/100 Iteration 163/234: loss=0.056083 lr=0.000020 grad_norm=1.119012
Epoch 49/100 Iteration 164/234: loss=0.060513 lr=0.000020 grad_norm=1.102045
Epoch 49/100 Iteration 165/234: loss=0.055038 lr=0.000020 grad_norm=0.927912
Epoch 49/100 Iteration 166/234: loss=0.051040 lr=0.000020 grad_norm=0.743227
Epoch 49/100 Iteration 167/234: loss=0.057427 lr=0.000020 grad_norm=0.833550
Epoch 49/100 Iteration 168/234: loss=0.061983 lr=0.000020 grad_norm=0.808341
Epoch 49/100 Iteration 169/234: loss=0.058957 lr=0.000020 grad_norm=0.903039
Epoch 49/100 Iteration 170/234: loss=0.053000 lr=0.000020 grad_norm=1.197711
Epoch 49/100 Iteration 171/234: loss=0.058139 lr=0.000020 grad_norm=0.768677
Epoch 49/100 Iteration 172/234: loss=0.050285 lr=0.000020 grad_norm=0.369792
Epoch 49/100 Iteration 173/234: loss=0.059760 lr=0.000020 grad_norm=0.676689
Epoch 49/100 Iteration 174/234: loss=0.060585 lr=0.000020 grad_norm=0.468918
Epoch 49/100 Iteration 175/234: loss=0.055310 lr=0.000020 grad_norm=0.664098
Epoch 49/100 Iteration 176/234: loss=0.050921 lr=0.000020 grad_norm=0.807026
Epoch 49/100 Iteration 177/234: loss=0.063823 lr=0.000020 grad_norm=0.965860
Epoch 49/100 Iteration 178/234: loss=0.045280 lr=0.000020 grad_norm=0.899924
Epoch 49/100 Iteration 179/234: loss=0.062146 lr=0.000020 grad_norm=0.705063
Epoch 49/100 Iteration 180/234: loss=0.059059 lr=0.000020 grad_norm=0.764241
Epoch 49/100 Iteration 181/234: loss=0.054943 lr=0.000020 grad_norm=0.903462
Epoch 49/100 Iteration 182/234: loss=0.054315 lr=0.000020 grad_norm=1.473857
Epoch 49/100 Iteration 183/234: loss=0.056029 lr=0.000020 grad_norm=1.721915
Epoch 49/100 Iteration 184/234: loss=0.055049 lr=0.000020 grad_norm=1.062014
Epoch 49/100 Iteration 185/234: loss=0.065170 lr=0.000020 grad_norm=0.454031
Epoch 49/100 Iteration 186/234: loss=0.056398 lr=0.000020 grad_norm=1.407748
Epoch 49/100 Iteration 187/234: loss=0.063594 lr=0.000020 grad_norm=1.408795
Epoch 49/100 Iteration 188/234: loss=0.058118 lr=0.000020 grad_norm=0.678475
Epoch 49/100 Iteration 189/234: loss=0.057562 lr=0.000020 grad_norm=0.571065
Epoch 49/100 Iteration 190/234: loss=0.056084 lr=0.000020 grad_norm=1.084464
Epoch 49/100 Iteration 191/234: loss=0.056103 lr=0.000020 grad_norm=1.050267
Epoch 49/100 Iteration 192/234: loss=0.055322 lr=0.000020 grad_norm=0.947348
Epoch 49/100 Iteration 193/234: loss=0.054920 lr=0.000020 grad_norm=0.839803
Epoch 49/100 Iteration 194/234: loss=0.060939 lr=0.000020 grad_norm=0.485948
Epoch 49/100 Iteration 195/234: loss=0.059745 lr=0.000020 grad_norm=0.629760
Epoch 49/100 Iteration 196/234: loss=0.053776 lr=0.000020 grad_norm=0.994442
Epoch 49/100 Iteration 197/234: loss=0.054300 lr=0.000020 grad_norm=1.043087
Epoch 49/100 Iteration 198/234: loss=0.059146 lr=0.000020 grad_norm=0.524028
Epoch 49/100 Iteration 199/234: loss=0.056602 lr=0.000020 grad_norm=0.550600
Epoch 49/100 Iteration 200/234: loss=0.053297 lr=0.000020 grad_norm=0.568153
Epoch 49/100 Iteration 201/234: loss=0.053840 lr=0.000020 grad_norm=0.614487
Epoch 49/100 Iteration 202/234: loss=0.056798 lr=0.000020 grad_norm=0.771508
Epoch 49/100 Iteration 203/234: loss=0.064098 lr=0.000020 grad_norm=0.992001
Epoch 49/100 Iteration 204/234: loss=0.051135 lr=0.000020 grad_norm=1.136194
Epoch 49/100 Iteration 205/234: loss=0.058250 lr=0.000020 grad_norm=1.294233
Epoch 49/100 Iteration 206/234: loss=0.055885 lr=0.000020 grad_norm=1.547599
Epoch 49/100 Iteration 207/234: loss=0.055980 lr=0.000020 grad_norm=1.845707
Epoch 49/100 Iteration 208/234: loss=0.065255 lr=0.000020 grad_norm=1.985857
Epoch 49/100 Iteration 209/234: loss=0.058938 lr=0.000020 grad_norm=1.339652
Epoch 49/100 Iteration 210/234: loss=0.051958 lr=0.000020 grad_norm=0.473225
Epoch 49/100 Iteration 211/234: loss=0.054568 lr=0.000020 grad_norm=0.979563
Epoch 49/100 Iteration 212/234: loss=0.054172 lr=0.000020 grad_norm=0.749333
Epoch 49/100 Iteration 213/234: loss=0.046731 lr=0.000020 grad_norm=0.358899
Epoch 49/100 Iteration 214/234: loss=0.051005 lr=0.000020 grad_norm=0.585550
Epoch 49/100 Iteration 215/234: loss=0.050294 lr=0.000020 grad_norm=0.692235
Epoch 49/100 Iteration 216/234: loss=0.061570 lr=0.000020 grad_norm=1.280217
Epoch 49/100 Iteration 217/234: loss=0.052231 lr=0.000020 grad_norm=1.249323
Epoch 49/100 Iteration 218/234: loss=0.057727 lr=0.000020 grad_norm=0.414038
Epoch 49/100 Iteration 219/234: loss=0.057585 lr=0.000020 grad_norm=0.740412
Epoch 49/100 Iteration 220/234: loss=0.059661 lr=0.000020 grad_norm=0.534707
Epoch 49/100 Iteration 221/234: loss=0.062654 lr=0.000020 grad_norm=0.824321
Epoch 49/100 Iteration 222/234: loss=0.054788 lr=0.000020 grad_norm=1.311618
Epoch 49/100 Iteration 223/234: loss=0.061486 lr=0.000020 grad_norm=0.399523
Epoch 49/100 Iteration 224/234: loss=0.059927 lr=0.000020 grad_norm=0.878669
Epoch 49/100 Iteration 225/234: loss=0.062546 lr=0.000020 grad_norm=0.809538
Epoch 49/100 Iteration 226/234: loss=0.057699 lr=0.000020 grad_norm=0.550459
Epoch 49/100 Iteration 227/234: loss=0.051536 lr=0.000020 grad_norm=1.886359
Epoch 49/100 Iteration 228/234: loss=0.063585 lr=0.000020 grad_norm=2.524119
Epoch 49/100 Iteration 229/234: loss=0.057412 lr=0.000020 grad_norm=2.035257
Epoch 49/100 Iteration 230/234: loss=0.057861 lr=0.000020 grad_norm=0.754777
Epoch 49/100 Iteration 231/234: loss=0.064439 lr=0.000020 grad_norm=1.330633
Epoch 49/100 Iteration 232/234: loss=0.051297 lr=0.000020 grad_norm=1.376965
Epoch 49/100 Iteration 233/234: loss=0.056414 lr=0.000020 grad_norm=0.905233
Epoch 49/100 Iteration 234/234: loss=0.054441 lr=0.000020 grad_norm=0.847008
Epoch 49/100 finished. Avg Loss: 0.057031
Epoch 50/100 Iteration 1/234: loss=0.048289 lr=0.000020 grad_norm=0.636963
Epoch 50/100 Iteration 2/234: loss=0.055154 lr=0.000020 grad_norm=0.703384
Epoch 50/100 Iteration 3/234: loss=0.055448 lr=0.000020 grad_norm=1.366123
Epoch 50/100 Iteration 4/234: loss=0.044149 lr=0.000020 grad_norm=0.957620
Epoch 50/100 Iteration 5/234: loss=0.056916 lr=0.000020 grad_norm=0.493744
Epoch 50/100 Iteration 6/234: loss=0.055881 lr=0.000020 grad_norm=0.487151
Epoch 50/100 Iteration 7/234: loss=0.056484 lr=0.000020 grad_norm=0.477247
Epoch 50/100 Iteration 8/234: loss=0.060049 lr=0.000020 grad_norm=0.590906
Epoch 50/100 Iteration 9/234: loss=0.057920 lr=0.000020 grad_norm=0.688814
Epoch 50/100 Iteration 10/234: loss=0.050720 lr=0.000020 grad_norm=0.719218
Epoch 50/100 Iteration 11/234: loss=0.060361 lr=0.000020 grad_norm=0.593767
Epoch 50/100 Iteration 12/234: loss=0.051161 lr=0.000020 grad_norm=0.861728
Epoch 50/100 Iteration 13/234: loss=0.056165 lr=0.000020 grad_norm=0.914831
Epoch 50/100 Iteration 14/234: loss=0.056100 lr=0.000020 grad_norm=0.742978
Epoch 50/100 Iteration 15/234: loss=0.052050 lr=0.000020 grad_norm=0.706153
Epoch 50/100 Iteration 16/234: loss=0.052930 lr=0.000020 grad_norm=0.677808
Epoch 50/100 Iteration 17/234: loss=0.058943 lr=0.000020 grad_norm=0.688658
Epoch 50/100 Iteration 18/234: loss=0.055166 lr=0.000020 grad_norm=0.515025
Epoch 50/100 Iteration 19/234: loss=0.060493 lr=0.000020 grad_norm=0.638408
Epoch 50/100 Iteration 20/234: loss=0.049959 lr=0.000020 grad_norm=0.612056
Epoch 50/100 Iteration 21/234: loss=0.056473 lr=0.000020 grad_norm=0.570234
Epoch 50/100 Iteration 22/234: loss=0.047289 lr=0.000020 grad_norm=0.979313
Epoch 50/100 Iteration 23/234: loss=0.055998 lr=0.000020 grad_norm=1.271455
Epoch 50/100 Iteration 24/234: loss=0.051799 lr=0.000020 grad_norm=0.746062
Epoch 50/100 Iteration 25/234: loss=0.057260 lr=0.000020 grad_norm=0.898159
Epoch 50/100 Iteration 26/234: loss=0.054121 lr=0.000020 grad_norm=1.440237
Epoch 50/100 Iteration 27/234: loss=0.053957 lr=0.000020 grad_norm=1.256346
Epoch 50/100 Iteration 28/234: loss=0.056778 lr=0.000020 grad_norm=1.216361
Epoch 50/100 Iteration 29/234: loss=0.053520 lr=0.000020 grad_norm=0.815908
Epoch 50/100 Iteration 30/234: loss=0.058571 lr=0.000020 grad_norm=0.880971
Epoch 50/100 Iteration 31/234: loss=0.056319 lr=0.000020 grad_norm=1.779975
Epoch 50/100 Iteration 32/234: loss=0.053443 lr=0.000020 grad_norm=1.499007
Epoch 50/100 Iteration 33/234: loss=0.049081 lr=0.000020 grad_norm=0.673543
Epoch 50/100 Iteration 34/234: loss=0.054237 lr=0.000020 grad_norm=1.172352
Epoch 50/100 Iteration 35/234: loss=0.059346 lr=0.000020 grad_norm=0.923500
Epoch 50/100 Iteration 36/234: loss=0.056651 lr=0.000020 grad_norm=0.476503
Epoch 50/100 Iteration 37/234: loss=0.058375 lr=0.000020 grad_norm=1.047090
Epoch 50/100 Iteration 38/234: loss=0.051097 lr=0.000020 grad_norm=0.965390
Epoch 50/100 Iteration 39/234: loss=0.052121 lr=0.000020 grad_norm=0.413334
Epoch 50/100 Iteration 40/234: loss=0.054416 lr=0.000020 grad_norm=1.003364
Epoch 50/100 Iteration 41/234: loss=0.056082 lr=0.000020 grad_norm=0.919232
Epoch 50/100 Iteration 42/234: loss=0.049899 lr=0.000020 grad_norm=0.351844
Epoch 50/100 Iteration 43/234: loss=0.058515 lr=0.000020 grad_norm=0.830056
Epoch 50/100 Iteration 44/234: loss=0.054920 lr=0.000020 grad_norm=1.033131
Epoch 50/100 Iteration 45/234: loss=0.056108 lr=0.000020 grad_norm=0.502034
Epoch 50/100 Iteration 46/234: loss=0.056418 lr=0.000020 grad_norm=1.038871
Epoch 50/100 Iteration 47/234: loss=0.057603 lr=0.000020 grad_norm=1.539490
Epoch 50/100 Iteration 48/234: loss=0.046381 lr=0.000020 grad_norm=0.935481
Epoch 50/100 Iteration 49/234: loss=0.061460 lr=0.000020 grad_norm=0.724653
Epoch 50/100 Iteration 50/234: loss=0.059216 lr=0.000020 grad_norm=1.613725
Epoch 50/100 Iteration 51/234: loss=0.051745 lr=0.000020 grad_norm=1.142393
Epoch 50/100 Iteration 52/234: loss=0.067802 lr=0.000020 grad_norm=1.035009
Epoch 50/100 Iteration 53/234: loss=0.053526 lr=0.000020 grad_norm=1.197464
Epoch 50/100 Iteration 54/234: loss=0.053774 lr=0.000020 grad_norm=0.665842
Epoch 50/100 Iteration 55/234: loss=0.047618 lr=0.000020 grad_norm=0.815763
Epoch 50/100 Iteration 56/234: loss=0.058406 lr=0.000020 grad_norm=0.958410
Epoch 50/100 Iteration 57/234: loss=0.051536 lr=0.000020 grad_norm=0.637154
Epoch 50/100 Iteration 58/234: loss=0.053027 lr=0.000020 grad_norm=0.884167
Epoch 50/100 Iteration 59/234: loss=0.052627 lr=0.000020 grad_norm=0.935076
Epoch 50/100 Iteration 60/234: loss=0.062785 lr=0.000020 grad_norm=0.691645
Epoch 50/100 Iteration 61/234: loss=0.049818 lr=0.000020 grad_norm=1.022639
Epoch 50/100 Iteration 62/234: loss=0.055300 lr=0.000020 grad_norm=1.099702
Epoch 50/100 Iteration 63/234: loss=0.061672 lr=0.000020 grad_norm=1.019308
Epoch 50/100 Iteration 64/234: loss=0.062789 lr=0.000020 grad_norm=1.289280
Epoch 50/100 Iteration 65/234: loss=0.052675 lr=0.000020 grad_norm=0.887589
Epoch 50/100 Iteration 66/234: loss=0.051173 lr=0.000020 grad_norm=0.517804
Epoch 50/100 Iteration 67/234: loss=0.049580 lr=0.000020 grad_norm=0.912266
Epoch 50/100 Iteration 68/234: loss=0.052739 lr=0.000020 grad_norm=0.907009
Epoch 50/100 Iteration 69/234: loss=0.050359 lr=0.000020 grad_norm=0.503776
Epoch 50/100 Iteration 70/234: loss=0.058050 lr=0.000020 grad_norm=0.678668
Epoch 50/100 Iteration 71/234: loss=0.053142 lr=0.000020 grad_norm=1.185567
Epoch 50/100 Iteration 72/234: loss=0.053867 lr=0.000020 grad_norm=1.462707
Epoch 50/100 Iteration 73/234: loss=0.052281 lr=0.000020 grad_norm=0.878889
Epoch 50/100 Iteration 74/234: loss=0.056492 lr=0.000020 grad_norm=1.028868
Epoch 50/100 Iteration 75/234: loss=0.053383 lr=0.000020 grad_norm=2.025860
Epoch 50/100 Iteration 76/234: loss=0.062823 lr=0.000020 grad_norm=1.740404
Epoch 50/100 Iteration 77/234: loss=0.055275 lr=0.000020 grad_norm=0.653857
Epoch 50/100 Iteration 78/234: loss=0.056398 lr=0.000020 grad_norm=0.726396
Epoch 50/100 Iteration 79/234: loss=0.057919 lr=0.000020 grad_norm=0.850603
Epoch 50/100 Iteration 80/234: loss=0.056747 lr=0.000020 grad_norm=0.535373
Epoch 50/100 Iteration 81/234: loss=0.059477 lr=0.000020 grad_norm=0.860042
Epoch 50/100 Iteration 82/234: loss=0.056016 lr=0.000020 grad_norm=1.525329
Epoch 50/100 Iteration 83/234: loss=0.054190 lr=0.000020 grad_norm=1.400511
Epoch 50/100 Iteration 84/234: loss=0.044325 lr=0.000020 grad_norm=0.532133
Epoch 50/100 Iteration 85/234: loss=0.053567 lr=0.000020 grad_norm=1.651043
Epoch 50/100 Iteration 86/234: loss=0.052243 lr=0.000020 grad_norm=1.893391
Epoch 50/100 Iteration 87/234: loss=0.058833 lr=0.000020 grad_norm=0.992552
Epoch 50/100 Iteration 88/234: loss=0.056165 lr=0.000020 grad_norm=0.764422
Epoch 50/100 Iteration 89/234: loss=0.055413 lr=0.000020 grad_norm=0.682861
Epoch 50/100 Iteration 90/234: loss=0.055549 lr=0.000020 grad_norm=0.934131
Epoch 50/100 Iteration 91/234: loss=0.051310 lr=0.000020 grad_norm=0.896825
Epoch 50/100 Iteration 92/234: loss=0.055597 lr=0.000020 grad_norm=0.948134
Epoch 50/100 Iteration 93/234: loss=0.054062 lr=0.000020 grad_norm=0.597015
Epoch 50/100 Iteration 94/234: loss=0.062084 lr=0.000020 grad_norm=0.932269
Epoch 50/100 Iteration 95/234: loss=0.050277 lr=0.000020 grad_norm=1.604902
Epoch 50/100 Iteration 96/234: loss=0.067392 lr=0.000020 grad_norm=1.464499
Epoch 50/100 Iteration 97/234: loss=0.054317 lr=0.000020 grad_norm=1.104152
Epoch 50/100 Iteration 98/234: loss=0.061650 lr=0.000020 grad_norm=0.707977
Epoch 50/100 Iteration 99/234: loss=0.053613 lr=0.000020 grad_norm=1.189925
Epoch 50/100 Iteration 100/234: loss=0.051621 lr=0.000020 grad_norm=0.986402
Epoch 50/100 Iteration 101/234: loss=0.050709 lr=0.000020 grad_norm=0.470335
Epoch 50/100 Iteration 102/234: loss=0.060326 lr=0.000020 grad_norm=0.826522
Epoch 50/100 Iteration 103/234: loss=0.050575 lr=0.000020 grad_norm=0.696734
Epoch 50/100 Iteration 104/234: loss=0.061707 lr=0.000020 grad_norm=0.529039
Epoch 50/100 Iteration 105/234: loss=0.051727 lr=0.000020 grad_norm=0.638726
Epoch 50/100 Iteration 106/234: loss=0.053916 lr=0.000020 grad_norm=0.600406
Epoch 50/100 Iteration 107/234: loss=0.058672 lr=0.000020 grad_norm=0.584088
Epoch 50/100 Iteration 108/234: loss=0.056860 lr=0.000020 grad_norm=0.652193
Epoch 50/100 Iteration 109/234: loss=0.049144 lr=0.000020 grad_norm=0.653175
Epoch 50/100 Iteration 110/234: loss=0.053460 lr=0.000020 grad_norm=0.800712
Epoch 50/100 Iteration 111/234: loss=0.061679 lr=0.000020 grad_norm=0.697302
Epoch 50/100 Iteration 112/234: loss=0.058014 lr=0.000020 grad_norm=0.509098
Epoch 50/100 Iteration 113/234: loss=0.053061 lr=0.000020 grad_norm=0.946093
Epoch 50/100 Iteration 114/234: loss=0.050789 lr=0.000020 grad_norm=0.865900
Epoch 50/100 Iteration 115/234: loss=0.056489 lr=0.000020 grad_norm=0.421413
Epoch 50/100 Iteration 116/234: loss=0.053175 lr=0.000020 grad_norm=0.870401
Epoch 50/100 Iteration 117/234: loss=0.060397 lr=0.000020 grad_norm=0.686400
Epoch 50/100 Iteration 118/234: loss=0.051902 lr=0.000020 grad_norm=0.837848
Epoch 50/100 Iteration 119/234: loss=0.053966 lr=0.000020 grad_norm=1.689654
Epoch 50/100 Iteration 120/234: loss=0.054674 lr=0.000020 grad_norm=1.885177
Epoch 50/100 Iteration 121/234: loss=0.049203 lr=0.000020 grad_norm=0.740378
Epoch 50/100 Iteration 122/234: loss=0.060277 lr=0.000020 grad_norm=1.340557
Epoch 50/100 Iteration 123/234: loss=0.049010 lr=0.000020 grad_norm=1.805683
Epoch 50/100 Iteration 124/234: loss=0.058536 lr=0.000020 grad_norm=0.971854
Epoch 50/100 Iteration 125/234: loss=0.053101 lr=0.000020 grad_norm=0.855689
Epoch 50/100 Iteration 126/234: loss=0.058660 lr=0.000020 grad_norm=1.136043
Epoch 50/100 Iteration 127/234: loss=0.054383 lr=0.000020 grad_norm=1.274137
Epoch 50/100 Iteration 128/234: loss=0.057012 lr=0.000020 grad_norm=0.751537
Epoch 50/100 Iteration 129/234: loss=0.054435 lr=0.000020 grad_norm=0.569608
Epoch 50/100 Iteration 130/234: loss=0.052277 lr=0.000020 grad_norm=0.649177
Epoch 50/100 Iteration 131/234: loss=0.052054 lr=0.000020 grad_norm=0.686419
Epoch 50/100 Iteration 132/234: loss=0.051849 lr=0.000020 grad_norm=0.623077
Epoch 50/100 Iteration 133/234: loss=0.060223 lr=0.000020 grad_norm=0.716693
Epoch 50/100 Iteration 134/234: loss=0.057673 lr=0.000020 grad_norm=1.160447
Epoch 50/100 Iteration 135/234: loss=0.052274 lr=0.000020 grad_norm=1.265999
Epoch 50/100 Iteration 136/234: loss=0.055313 lr=0.000020 grad_norm=0.728544
Epoch 50/100 Iteration 137/234: loss=0.054804 lr=0.000020 grad_norm=1.057053
Epoch 50/100 Iteration 138/234: loss=0.058444 lr=0.000020 grad_norm=1.313262
Epoch 50/100 Iteration 139/234: loss=0.055892 lr=0.000020 grad_norm=0.595296
Epoch 50/100 Iteration 140/234: loss=0.051923 lr=0.000020 grad_norm=1.193401
Epoch 50/100 Iteration 141/234: loss=0.055880 lr=0.000020 grad_norm=0.965040
Epoch 50/100 Iteration 142/234: loss=0.056287 lr=0.000020 grad_norm=0.784071
Epoch 50/100 Iteration 143/234: loss=0.052347 lr=0.000020 grad_norm=1.496207
Epoch 50/100 Iteration 144/234: loss=0.056433 lr=0.000020 grad_norm=0.941928
Epoch 50/100 Iteration 145/234: loss=0.055566 lr=0.000020 grad_norm=0.871885
Epoch 50/100 Iteration 146/234: loss=0.056972 lr=0.000020 grad_norm=1.636119
Epoch 50/100 Iteration 147/234: loss=0.059612 lr=0.000020 grad_norm=0.951505
Epoch 50/100 Iteration 148/234: loss=0.052929 lr=0.000020 grad_norm=0.766409
Epoch 50/100 Iteration 149/234: loss=0.053566 lr=0.000020 grad_norm=1.130000
Epoch 50/100 Iteration 150/234: loss=0.055282 lr=0.000020 grad_norm=0.873458
Epoch 50/100 Iteration 151/234: loss=0.054704 lr=0.000020 grad_norm=0.514771
Epoch 50/100 Iteration 152/234: loss=0.057342 lr=0.000020 grad_norm=0.682483
Epoch 50/100 Iteration 153/234: loss=0.058638 lr=0.000020 grad_norm=0.886052
Epoch 50/100 Iteration 154/234: loss=0.052551 lr=0.000020 grad_norm=0.505603
Epoch 50/100 Iteration 155/234: loss=0.062476 lr=0.000020 grad_norm=0.751161
Epoch 50/100 Iteration 156/234: loss=0.055591 lr=0.000020 grad_norm=0.654424
Epoch 50/100 Iteration 157/234: loss=0.058999 lr=0.000020 grad_norm=0.590273
Epoch 50/100 Iteration 158/234: loss=0.049195 lr=0.000020 grad_norm=0.363554
Epoch 50/100 Iteration 159/234: loss=0.060662 lr=0.000020 grad_norm=0.600472
Epoch 50/100 Iteration 160/234: loss=0.058250 lr=0.000020 grad_norm=0.699211
Epoch 50/100 Iteration 161/234: loss=0.055154 lr=0.000020 grad_norm=0.552560
Epoch 50/100 Iteration 162/234: loss=0.058836 lr=0.000020 grad_norm=0.446714
Epoch 50/100 Iteration 163/234: loss=0.055904 lr=0.000020 grad_norm=0.502577
Epoch 50/100 Iteration 164/234: loss=0.066520 lr=0.000020 grad_norm=0.796628
Epoch 50/100 Iteration 165/234: loss=0.057577 lr=0.000020 grad_norm=0.666199
Epoch 50/100 Iteration 166/234: loss=0.049460 lr=0.000020 grad_norm=0.537354
Epoch 50/100 Iteration 167/234: loss=0.052008 lr=0.000020 grad_norm=0.368524
Epoch 50/100 Iteration 168/234: loss=0.059049 lr=0.000020 grad_norm=0.486192
Epoch 50/100 Iteration 169/234: loss=0.051894 lr=0.000020 grad_norm=0.826048
Epoch 50/100 Iteration 170/234: loss=0.054548 lr=0.000020 grad_norm=1.512939
Epoch 50/100 Iteration 171/234: loss=0.050503 lr=0.000020 grad_norm=1.220908
Epoch 50/100 Iteration 172/234: loss=0.052027 lr=0.000020 grad_norm=0.632929
Epoch 50/100 Iteration 173/234: loss=0.051796 lr=0.000020 grad_norm=1.166218
Epoch 50/100 Iteration 174/234: loss=0.064494 lr=0.000020 grad_norm=2.057394
Epoch 50/100 Iteration 175/234: loss=0.059212 lr=0.000020 grad_norm=2.157017
Epoch 50/100 Iteration 176/234: loss=0.045232 lr=0.000020 grad_norm=0.946891
Epoch 50/100 Iteration 177/234: loss=0.053605 lr=0.000020 grad_norm=1.141042
Epoch 50/100 Iteration 178/234: loss=0.054556 lr=0.000020 grad_norm=1.701173
Epoch 50/100 Iteration 179/234: loss=0.063845 lr=0.000020 grad_norm=0.576289
Epoch 50/100 Iteration 180/234: loss=0.058634 lr=0.000020 grad_norm=1.482299
Epoch 50/100 Iteration 181/234: loss=0.062328 lr=0.000020 grad_norm=1.269253
Epoch 50/100 Iteration 182/234: loss=0.052140 lr=0.000020 grad_norm=0.672647
Epoch 50/100 Iteration 183/234: loss=0.050859 lr=0.000020 grad_norm=1.756066
Epoch 50/100 Iteration 184/234: loss=0.056331 lr=0.000020 grad_norm=1.494945
Epoch 50/100 Iteration 185/234: loss=0.057744 lr=0.000020 grad_norm=0.578652
Epoch 50/100 Iteration 186/234: loss=0.053194 lr=0.000020 grad_norm=0.635332
Epoch 50/100 Iteration 187/234: loss=0.059157 lr=0.000020 grad_norm=1.100628
Epoch 50/100 Iteration 188/234: loss=0.052806 lr=0.000020 grad_norm=0.898440
Epoch 50/100 Iteration 189/234: loss=0.057238 lr=0.000020 grad_norm=0.726773
Epoch 50/100 Iteration 190/234: loss=0.058655 lr=0.000020 grad_norm=0.402288
Epoch 50/100 Iteration 191/234: loss=0.054887 lr=0.000020 grad_norm=0.698919
Epoch 50/100 Iteration 192/234: loss=0.055786 lr=0.000020 grad_norm=1.207170
Epoch 50/100 Iteration 193/234: loss=0.055115 lr=0.000020 grad_norm=0.831333
Epoch 50/100 Iteration 194/234: loss=0.062980 lr=0.000020 grad_norm=0.588435
Epoch 50/100 Iteration 195/234: loss=0.062683 lr=0.000020 grad_norm=0.995043
Epoch 50/100 Iteration 196/234: loss=0.054868 lr=0.000020 grad_norm=0.504269
Epoch 50/100 Iteration 197/234: loss=0.058433 lr=0.000020 grad_norm=0.713495
Epoch 50/100 Iteration 198/234: loss=0.062110 lr=0.000020 grad_norm=1.056558
Epoch 50/100 Iteration 199/234: loss=0.049691 lr=0.000020 grad_norm=0.919622
Epoch 50/100 Iteration 200/234: loss=0.055883 lr=0.000020 grad_norm=0.459899
Epoch 50/100 Iteration 201/234: loss=0.057679 lr=0.000020 grad_norm=0.634358
Epoch 50/100 Iteration 202/234: loss=0.056513 lr=0.000020 grad_norm=0.608610
Epoch 50/100 Iteration 203/234: loss=0.054116 lr=0.000020 grad_norm=0.361603
Epoch 50/100 Iteration 204/234: loss=0.051790 lr=0.000020 grad_norm=0.618585
Epoch 50/100 Iteration 205/234: loss=0.058025 lr=0.000020 grad_norm=0.569184
Epoch 50/100 Iteration 206/234: loss=0.054368 lr=0.000020 grad_norm=0.326398
Epoch 50/100 Iteration 207/234: loss=0.051940 lr=0.000020 grad_norm=0.664445
Epoch 50/100 Iteration 208/234: loss=0.059348 lr=0.000020 grad_norm=0.548879
Epoch 50/100 Iteration 209/234: loss=0.054011 lr=0.000020 grad_norm=0.703631
Epoch 50/100 Iteration 210/234: loss=0.058482 lr=0.000020 grad_norm=1.581624
Epoch 50/100 Iteration 211/234: loss=0.054156 lr=0.000020 grad_norm=1.647199
Epoch 50/100 Iteration 212/234: loss=0.048808 lr=0.000020 grad_norm=0.644057
Epoch 50/100 Iteration 213/234: loss=0.062090 lr=0.000020 grad_norm=1.585327
Epoch 50/100 Iteration 214/234: loss=0.063192 lr=0.000020 grad_norm=2.996293
Epoch 50/100 Iteration 215/234: loss=0.055609 lr=0.000020 grad_norm=1.959054
Epoch 50/100 Iteration 216/234: loss=0.050002 lr=0.000020 grad_norm=0.863794
Epoch 50/100 Iteration 217/234: loss=0.059463 lr=0.000020 grad_norm=1.482423
Epoch 50/100 Iteration 218/234: loss=0.056925 lr=0.000020 grad_norm=0.666346
Epoch 50/100 Iteration 219/234: loss=0.058707 lr=0.000020 grad_norm=1.464852
Epoch 50/100 Iteration 220/234: loss=0.049069 lr=0.000020 grad_norm=0.748491
Epoch 50/100 Iteration 221/234: loss=0.046526 lr=0.000020 grad_norm=0.944856
Epoch 50/100 Iteration 222/234: loss=0.053361 lr=0.000020 grad_norm=1.224167
Epoch 50/100 Iteration 223/234: loss=0.049967 lr=0.000020 grad_norm=0.385455
Epoch 50/100 Iteration 224/234: loss=0.060863 lr=0.000020 grad_norm=1.388439
Epoch 50/100 Iteration 225/234: loss=0.059170 lr=0.000020 grad_norm=1.124773
Epoch 50/100 Iteration 226/234: loss=0.056453 lr=0.000020 grad_norm=1.090203
Epoch 50/100 Iteration 227/234: loss=0.067684 lr=0.000020 grad_norm=1.879504
Epoch 50/100 Iteration 228/234: loss=0.058141 lr=0.000020 grad_norm=0.677060
Epoch 50/100 Iteration 229/234: loss=0.053532 lr=0.000020 grad_norm=1.511864
Epoch 50/100 Iteration 230/234: loss=0.049765 lr=0.000020 grad_norm=1.584085
Epoch 50/100 Iteration 231/234: loss=0.050208 lr=0.000020 grad_norm=0.591960
Epoch 50/100 Iteration 232/234: loss=0.053066 lr=0.000020 grad_norm=1.551846
Epoch 50/100 Iteration 233/234: loss=0.058903 lr=0.000020 grad_norm=0.705975
Epoch 50/100 Iteration 234/234: loss=0.055248 lr=0.000020 grad_norm=0.958250
Epoch 50/100 finished. Avg Loss: 0.055391
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 51/100 Iteration 1/234: loss=0.053724 lr=0.000020 grad_norm=0.910791
Epoch 51/100 Iteration 2/234: loss=0.053225 lr=0.000020 grad_norm=0.641258
Epoch 51/100 Iteration 3/234: loss=0.056388 lr=0.000020 grad_norm=1.100523
Epoch 51/100 Iteration 4/234: loss=0.052627 lr=0.000020 grad_norm=0.507663
Epoch 51/100 Iteration 5/234: loss=0.057939 lr=0.000020 grad_norm=0.589140
Epoch 51/100 Iteration 6/234: loss=0.056851 lr=0.000020 grad_norm=0.744511
Epoch 51/100 Iteration 7/234: loss=0.055843 lr=0.000020 grad_norm=0.855437
Epoch 51/100 Iteration 8/234: loss=0.056372 lr=0.000020 grad_norm=0.687057
Epoch 51/100 Iteration 9/234: loss=0.052609 lr=0.000020 grad_norm=0.434147
Epoch 51/100 Iteration 10/234: loss=0.050897 lr=0.000020 grad_norm=1.215541
Epoch 51/100 Iteration 11/234: loss=0.062012 lr=0.000020 grad_norm=1.184050
Epoch 51/100 Iteration 12/234: loss=0.055795 lr=0.000020 grad_norm=0.962393
Epoch 51/100 Iteration 13/234: loss=0.053620 lr=0.000020 grad_norm=1.023208
Epoch 51/100 Iteration 14/234: loss=0.051069 lr=0.000020 grad_norm=1.165915
Epoch 51/100 Iteration 15/234: loss=0.059433 lr=0.000020 grad_norm=1.011455
Epoch 51/100 Iteration 16/234: loss=0.056078 lr=0.000020 grad_norm=0.939910
Epoch 51/100 Iteration 17/234: loss=0.063212 lr=0.000020 grad_norm=0.799418
Epoch 51/100 Iteration 18/234: loss=0.056182 lr=0.000020 grad_norm=0.488861
Epoch 51/100 Iteration 19/234: loss=0.058930 lr=0.000020 grad_norm=0.701047
Epoch 51/100 Iteration 20/234: loss=0.056456 lr=0.000020 grad_norm=0.618250
Epoch 51/100 Iteration 21/234: loss=0.054788 lr=0.000020 grad_norm=0.564890
Epoch 51/100 Iteration 22/234: loss=0.057595 lr=0.000020 grad_norm=0.664043
Epoch 51/100 Iteration 23/234: loss=0.061712 lr=0.000020 grad_norm=0.703627
Epoch 51/100 Iteration 24/234: loss=0.046245 lr=0.000020 grad_norm=0.597561
Epoch 51/100 Iteration 25/234: loss=0.055906 lr=0.000020 grad_norm=1.170832
Epoch 51/100 Iteration 26/234: loss=0.055346 lr=0.000020 grad_norm=0.881421
Epoch 51/100 Iteration 27/234: loss=0.059812 lr=0.000020 grad_norm=0.788621
Epoch 51/100 Iteration 28/234: loss=0.058083 lr=0.000020 grad_norm=1.477325
Epoch 51/100 Iteration 29/234: loss=0.050880 lr=0.000020 grad_norm=0.868638
Epoch 51/100 Iteration 30/234: loss=0.053046 lr=0.000020 grad_norm=0.961510
Epoch 51/100 Iteration 31/234: loss=0.057019 lr=0.000020 grad_norm=1.454162
Epoch 51/100 Iteration 32/234: loss=0.055731 lr=0.000020 grad_norm=0.970698
Epoch 51/100 Iteration 33/234: loss=0.047225 lr=0.000020 grad_norm=1.210514
Epoch 51/100 Iteration 34/234: loss=0.058199 lr=0.000020 grad_norm=1.315554
Epoch 51/100 Iteration 35/234: loss=0.059134 lr=0.000020 grad_norm=1.199552
Epoch 51/100 Iteration 36/234: loss=0.052047 lr=0.000020 grad_norm=1.190197
Epoch 51/100 Iteration 37/234: loss=0.059269 lr=0.000020 grad_norm=1.105902
Epoch 51/100 Iteration 38/234: loss=0.045669 lr=0.000020 grad_norm=1.474332
Epoch 51/100 Iteration 39/234: loss=0.057422 lr=0.000020 grad_norm=1.537951
Epoch 51/100 Iteration 40/234: loss=0.068873 lr=0.000020 grad_norm=1.233774
Epoch 51/100 Iteration 41/234: loss=0.055558 lr=0.000020 grad_norm=1.062724
Epoch 51/100 Iteration 42/234: loss=0.050189 lr=0.000020 grad_norm=0.853454
Epoch 51/100 Iteration 43/234: loss=0.056463 lr=0.000020 grad_norm=1.084842
Epoch 51/100 Iteration 44/234: loss=0.051037 lr=0.000020 grad_norm=0.606756
Epoch 51/100 Iteration 45/234: loss=0.051321 lr=0.000020 grad_norm=0.829385
Epoch 51/100 Iteration 46/234: loss=0.048659 lr=0.000020 grad_norm=1.405530
Epoch 51/100 Iteration 47/234: loss=0.055330 lr=0.000020 grad_norm=1.777457
Epoch 51/100 Iteration 48/234: loss=0.053924 lr=0.000020 grad_norm=1.158150
Epoch 51/100 Iteration 49/234: loss=0.056850 lr=0.000020 grad_norm=0.690033
Epoch 51/100 Iteration 50/234: loss=0.055630 lr=0.000020 grad_norm=0.954775
Epoch 51/100 Iteration 51/234: loss=0.052688 lr=0.000020 grad_norm=0.823820
Epoch 51/100 Iteration 52/234: loss=0.055235 lr=0.000020 grad_norm=1.189966
Epoch 51/100 Iteration 53/234: loss=0.056045 lr=0.000020 grad_norm=1.720973
Epoch 51/100 Iteration 54/234: loss=0.058437 lr=0.000020 grad_norm=1.490847
Epoch 51/100 Iteration 55/234: loss=0.051068 lr=0.000020 grad_norm=0.736280
Epoch 51/100 Iteration 56/234: loss=0.059716 lr=0.000020 grad_norm=0.961844
Epoch 51/100 Iteration 57/234: loss=0.047048 lr=0.000020 grad_norm=0.887033
Epoch 51/100 Iteration 58/234: loss=0.050763 lr=0.000020 grad_norm=0.763365
Epoch 51/100 Iteration 59/234: loss=0.056406 lr=0.000020 grad_norm=0.900185
Epoch 51/100 Iteration 60/234: loss=0.058431 lr=0.000020 grad_norm=0.731222
Epoch 51/100 Iteration 61/234: loss=0.057548 lr=0.000020 grad_norm=0.880352
Epoch 51/100 Iteration 62/234: loss=0.058596 lr=0.000020 grad_norm=0.915362
Epoch 51/100 Iteration 63/234: loss=0.056068 lr=0.000020 grad_norm=0.709631
Epoch 51/100 Iteration 64/234: loss=0.046581 lr=0.000020 grad_norm=0.818734
Epoch 51/100 Iteration 65/234: loss=0.045220 lr=0.000020 grad_norm=0.566134
Epoch 51/100 Iteration 66/234: loss=0.056066 lr=0.000020 grad_norm=0.832814
Epoch 51/100 Iteration 67/234: loss=0.053078 lr=0.000020 grad_norm=0.556533
Epoch 51/100 Iteration 68/234: loss=0.061173 lr=0.000020 grad_norm=0.622737
Epoch 51/100 Iteration 69/234: loss=0.052131 lr=0.000020 grad_norm=0.808440
Epoch 51/100 Iteration 70/234: loss=0.051835 lr=0.000020 grad_norm=0.470130
Epoch 51/100 Iteration 71/234: loss=0.046948 lr=0.000020 grad_norm=0.618687
Epoch 51/100 Iteration 72/234: loss=0.049460 lr=0.000020 grad_norm=0.669616
Epoch 51/100 Iteration 73/234: loss=0.060617 lr=0.000020 grad_norm=0.400406
Epoch 51/100 Iteration 74/234: loss=0.058762 lr=0.000020 grad_norm=0.745930
Epoch 51/100 Iteration 75/234: loss=0.050192 lr=0.000020 grad_norm=0.912478
Epoch 51/100 Iteration 76/234: loss=0.063640 lr=0.000020 grad_norm=0.775448
Epoch 51/100 Iteration 77/234: loss=0.057504 lr=0.000020 grad_norm=0.496411
Epoch 51/100 Iteration 78/234: loss=0.051530 lr=0.000020 grad_norm=0.748282
Epoch 51/100 Iteration 79/234: loss=0.058986 lr=0.000020 grad_norm=1.280327
Epoch 51/100 Iteration 80/234: loss=0.051793 lr=0.000020 grad_norm=1.435592
Epoch 51/100 Iteration 81/234: loss=0.045467 lr=0.000020 grad_norm=0.741805
Epoch 51/100 Iteration 82/234: loss=0.055056 lr=0.000020 grad_norm=0.529299
Epoch 51/100 Iteration 83/234: loss=0.048362 lr=0.000020 grad_norm=0.811045
Epoch 51/100 Iteration 84/234: loss=0.052330 lr=0.000020 grad_norm=0.466063
Epoch 51/100 Iteration 85/234: loss=0.048614 lr=0.000020 grad_norm=0.781424
Epoch 51/100 Iteration 86/234: loss=0.058026 lr=0.000020 grad_norm=0.729073
Epoch 51/100 Iteration 87/234: loss=0.057111 lr=0.000020 grad_norm=0.604204
Epoch 51/100 Iteration 88/234: loss=0.060400 lr=0.000020 grad_norm=1.012039
Epoch 51/100 Iteration 89/234: loss=0.050752 lr=0.000020 grad_norm=0.900542
Epoch 51/100 Iteration 90/234: loss=0.057904 lr=0.000020 grad_norm=0.614775
Epoch 51/100 Iteration 91/234: loss=0.061549 lr=0.000020 grad_norm=0.701082
Epoch 51/100 Iteration 92/234: loss=0.054433 lr=0.000020 grad_norm=0.552268
Epoch 51/100 Iteration 93/234: loss=0.050480 lr=0.000020 grad_norm=0.596610
Epoch 51/100 Iteration 94/234: loss=0.049574 lr=0.000020 grad_norm=0.927676
Epoch 51/100 Iteration 95/234: loss=0.068187 lr=0.000020 grad_norm=0.619387
Epoch 51/100 Iteration 96/234: loss=0.057236 lr=0.000020 grad_norm=0.741369
Epoch 51/100 Iteration 97/234: loss=0.051586 lr=0.000020 grad_norm=0.952704
Epoch 51/100 Iteration 98/234: loss=0.053845 lr=0.000020 grad_norm=0.519261
Epoch 51/100 Iteration 99/234: loss=0.057737 lr=0.000020 grad_norm=0.983276
Epoch 51/100 Iteration 100/234: loss=0.056730 lr=0.000020 grad_norm=1.312083
Epoch 51/100 Iteration 101/234: loss=0.047397 lr=0.000020 grad_norm=0.930085
Epoch 51/100 Iteration 102/234: loss=0.061115 lr=0.000020 grad_norm=0.649239
Epoch 51/100 Iteration 103/234: loss=0.049587 lr=0.000020 grad_norm=1.638514
Epoch 51/100 Iteration 104/234: loss=0.053348 lr=0.000020 grad_norm=1.939191
Epoch 51/100 Iteration 105/234: loss=0.055908 lr=0.000020 grad_norm=1.491390
Epoch 51/100 Iteration 106/234: loss=0.049183 lr=0.000020 grad_norm=0.430852
Epoch 51/100 Iteration 107/234: loss=0.057600 lr=0.000020 grad_norm=1.250519
Epoch 51/100 Iteration 108/234: loss=0.046753 lr=0.000020 grad_norm=0.989337
Epoch 51/100 Iteration 109/234: loss=0.053311 lr=0.000020 grad_norm=0.671931
Epoch 51/100 Iteration 110/234: loss=0.047967 lr=0.000020 grad_norm=1.496202
Epoch 51/100 Iteration 111/234: loss=0.053378 lr=0.000020 grad_norm=0.857376
Epoch 51/100 Iteration 112/234: loss=0.053076 lr=0.000020 grad_norm=1.113060
Epoch 51/100 Iteration 113/234: loss=0.048658 lr=0.000020 grad_norm=2.012364
Epoch 51/100 Iteration 114/234: loss=0.056471 lr=0.000020 grad_norm=0.906882
Epoch 51/100 Iteration 115/234: loss=0.061466 lr=0.000020 grad_norm=1.446765
Epoch 51/100 Iteration 116/234: loss=0.051007 lr=0.000020 grad_norm=2.102099
Epoch 51/100 Iteration 117/234: loss=0.054538 lr=0.000020 grad_norm=0.864334
Epoch 51/100 Iteration 118/234: loss=0.057450 lr=0.000020 grad_norm=0.995563
Epoch 51/100 Iteration 119/234: loss=0.048957 lr=0.000020 grad_norm=1.338190
Epoch 51/100 Iteration 120/234: loss=0.059596 lr=0.000020 grad_norm=0.781406
Epoch 51/100 Iteration 121/234: loss=0.056883 lr=0.000020 grad_norm=0.988370
Epoch 51/100 Iteration 122/234: loss=0.054293 lr=0.000020 grad_norm=1.113941
Epoch 51/100 Iteration 123/234: loss=0.051329 lr=0.000020 grad_norm=0.737063
Epoch 51/100 Iteration 124/234: loss=0.060859 lr=0.000020 grad_norm=1.246699
Epoch 51/100 Iteration 125/234: loss=0.051676 lr=0.000020 grad_norm=0.829080
Epoch 51/100 Iteration 126/234: loss=0.049993 lr=0.000020 grad_norm=0.731698
Epoch 51/100 Iteration 127/234: loss=0.059389 lr=0.000020 grad_norm=1.179650
Epoch 51/100 Iteration 128/234: loss=0.045339 lr=0.000020 grad_norm=0.863043
Epoch 51/100 Iteration 129/234: loss=0.054327 lr=0.000020 grad_norm=0.674771
Epoch 51/100 Iteration 130/234: loss=0.055458 lr=0.000020 grad_norm=0.896417
Epoch 51/100 Iteration 131/234: loss=0.054356 lr=0.000020 grad_norm=0.460313
Epoch 51/100 Iteration 132/234: loss=0.058820 lr=0.000020 grad_norm=0.719038
Epoch 51/100 Iteration 133/234: loss=0.061456 lr=0.000020 grad_norm=0.773430
Epoch 51/100 Iteration 134/234: loss=0.056889 lr=0.000020 grad_norm=0.430724
Epoch 51/100 Iteration 135/234: loss=0.050338 lr=0.000020 grad_norm=0.559253
Epoch 51/100 Iteration 136/234: loss=0.055265 lr=0.000020 grad_norm=0.584362
Epoch 51/100 Iteration 137/234: loss=0.059840 lr=0.000020 grad_norm=1.011325
Epoch 51/100 Iteration 138/234: loss=0.051787 lr=0.000020 grad_norm=1.164678
Epoch 51/100 Iteration 139/234: loss=0.051277 lr=0.000020 grad_norm=0.810469
Epoch 51/100 Iteration 140/234: loss=0.054063 lr=0.000020 grad_norm=0.893969
Epoch 51/100 Iteration 141/234: loss=0.055145 lr=0.000020 grad_norm=1.752823
Epoch 51/100 Iteration 142/234: loss=0.062103 lr=0.000020 grad_norm=1.764619
Epoch 51/100 Iteration 143/234: loss=0.054139 lr=0.000020 grad_norm=0.756309
Epoch 51/100 Iteration 144/234: loss=0.064863 lr=0.000020 grad_norm=1.297354
Epoch 51/100 Iteration 145/234: loss=0.053518 lr=0.000020 grad_norm=1.570653
Epoch 51/100 Iteration 146/234: loss=0.055340 lr=0.000020 grad_norm=0.596621
Epoch 51/100 Iteration 147/234: loss=0.055295 lr=0.000020 grad_norm=1.569000
Epoch 51/100 Iteration 148/234: loss=0.061934 lr=0.000020 grad_norm=1.367172
Epoch 51/100 Iteration 149/234: loss=0.052890 lr=0.000020 grad_norm=0.627856
Epoch 51/100 Iteration 150/234: loss=0.065006 lr=0.000020 grad_norm=1.508548
Epoch 51/100 Iteration 151/234: loss=0.057218 lr=0.000020 grad_norm=1.142660
Epoch 51/100 Iteration 152/234: loss=0.062621 lr=0.000020 grad_norm=0.949196
Epoch 51/100 Iteration 153/234: loss=0.050952 lr=0.000020 grad_norm=1.010966
Epoch 51/100 Iteration 154/234: loss=0.057464 lr=0.000020 grad_norm=0.697683
Epoch 51/100 Iteration 155/234: loss=0.050747 lr=0.000020 grad_norm=1.537035
Epoch 51/100 Iteration 156/234: loss=0.047208 lr=0.000020 grad_norm=1.141312
Epoch 51/100 Iteration 157/234: loss=0.052578 lr=0.000020 grad_norm=1.146991
Epoch 51/100 Iteration 158/234: loss=0.052662 lr=0.000020 grad_norm=2.073126
Epoch 51/100 Iteration 159/234: loss=0.055075 lr=0.000020 grad_norm=0.631680
Epoch 51/100 Iteration 160/234: loss=0.048778 lr=0.000020 grad_norm=1.593741
Epoch 51/100 Iteration 161/234: loss=0.067933 lr=0.000020 grad_norm=1.630810
Epoch 51/100 Iteration 162/234: loss=0.052394 lr=0.000020 grad_norm=0.898021
Epoch 51/100 Iteration 163/234: loss=0.048081 lr=0.000020 grad_norm=1.337851
Epoch 51/100 Iteration 164/234: loss=0.056178 lr=0.000020 grad_norm=0.467454
Epoch 51/100 Iteration 165/234: loss=0.052922 lr=0.000020 grad_norm=0.991088
Epoch 51/100 Iteration 166/234: loss=0.055379 lr=0.000020 grad_norm=0.711837
Epoch 51/100 Iteration 167/234: loss=0.055741 lr=0.000020 grad_norm=0.659635
Epoch 51/100 Iteration 168/234: loss=0.053764 lr=0.000020 grad_norm=0.883211
Epoch 51/100 Iteration 169/234: loss=0.048752 lr=0.000020 grad_norm=0.747399
Epoch 51/100 Iteration 170/234: loss=0.053235 lr=0.000020 grad_norm=0.874189
Epoch 51/100 Iteration 171/234: loss=0.058423 lr=0.000020 grad_norm=0.643458
Epoch 51/100 Iteration 172/234: loss=0.052926 lr=0.000020 grad_norm=0.847056
Epoch 51/100 Iteration 173/234: loss=0.054050 lr=0.000020 grad_norm=1.016628
Epoch 51/100 Iteration 174/234: loss=0.052752 lr=0.000020 grad_norm=0.653085
Epoch 51/100 Iteration 175/234: loss=0.053596 lr=0.000020 grad_norm=0.706611
Epoch 51/100 Iteration 176/234: loss=0.055871 lr=0.000020 grad_norm=0.861694
Epoch 51/100 Iteration 177/234: loss=0.054299 lr=0.000020 grad_norm=0.440049
Epoch 51/100 Iteration 178/234: loss=0.060149 lr=0.000020 grad_norm=0.992480
Epoch 51/100 Iteration 179/234: loss=0.054020 lr=0.000020 grad_norm=1.140808
Epoch 51/100 Iteration 180/234: loss=0.051136 lr=0.000020 grad_norm=0.516149
Epoch 51/100 Iteration 181/234: loss=0.053784 lr=0.000020 grad_norm=0.628032
Epoch 51/100 Iteration 182/234: loss=0.059490 lr=0.000020 grad_norm=0.837698
Epoch 51/100 Iteration 183/234: loss=0.053856 lr=0.000020 grad_norm=0.560943
Epoch 51/100 Iteration 184/234: loss=0.057247 lr=0.000020 grad_norm=0.929544
Epoch 51/100 Iteration 185/234: loss=0.052075 lr=0.000020 grad_norm=0.995520
Epoch 51/100 Iteration 186/234: loss=0.060294 lr=0.000020 grad_norm=0.803406
Epoch 51/100 Iteration 187/234: loss=0.056213 lr=0.000020 grad_norm=0.794114
Epoch 51/100 Iteration 188/234: loss=0.050931 lr=0.000020 grad_norm=0.782758
Epoch 51/100 Iteration 189/234: loss=0.059414 lr=0.000020 grad_norm=0.804337
Epoch 51/100 Iteration 190/234: loss=0.053219 lr=0.000020 grad_norm=0.434024
Epoch 51/100 Iteration 191/234: loss=0.060103 lr=0.000020 grad_norm=0.869643
Epoch 51/100 Iteration 192/234: loss=0.053021 lr=0.000020 grad_norm=0.885096
Epoch 51/100 Iteration 193/234: loss=0.058180 lr=0.000020 grad_norm=0.588848
Epoch 51/100 Iteration 194/234: loss=0.055983 lr=0.000020 grad_norm=0.579735
Epoch 51/100 Iteration 195/234: loss=0.056614 lr=0.000020 grad_norm=0.574344
Epoch 51/100 Iteration 196/234: loss=0.047278 lr=0.000020 grad_norm=0.663966
Epoch 51/100 Iteration 197/234: loss=0.053726 lr=0.000020 grad_norm=0.861374
Epoch 51/100 Iteration 198/234: loss=0.063960 lr=0.000020 grad_norm=0.570765
Epoch 51/100 Iteration 199/234: loss=0.051525 lr=0.000020 grad_norm=0.444462
Epoch 51/100 Iteration 200/234: loss=0.057745 lr=0.000020 grad_norm=0.722278
Epoch 51/100 Iteration 201/234: loss=0.058878 lr=0.000020 grad_norm=0.524870
Epoch 51/100 Iteration 202/234: loss=0.052546 lr=0.000020 grad_norm=0.734048
Epoch 51/100 Iteration 203/234: loss=0.048760 lr=0.000020 grad_norm=0.902484
Epoch 51/100 Iteration 204/234: loss=0.048405 lr=0.000020 grad_norm=0.714240
Epoch 51/100 Iteration 205/234: loss=0.057750 lr=0.000020 grad_norm=0.842387
Epoch 51/100 Iteration 206/234: loss=0.051177 lr=0.000020 grad_norm=0.861360
Epoch 51/100 Iteration 207/234: loss=0.050671 lr=0.000020 grad_norm=0.690236
Epoch 51/100 Iteration 208/234: loss=0.054888 lr=0.000020 grad_norm=0.574761
Epoch 51/100 Iteration 209/234: loss=0.052118 lr=0.000020 grad_norm=1.296965
Epoch 51/100 Iteration 210/234: loss=0.060009 lr=0.000020 grad_norm=1.576004
Epoch 51/100 Iteration 211/234: loss=0.056618 lr=0.000020 grad_norm=0.830655
Epoch 51/100 Iteration 212/234: loss=0.058595 lr=0.000020 grad_norm=0.794095
Epoch 51/100 Iteration 213/234: loss=0.059146 lr=0.000020 grad_norm=1.100764
Epoch 51/100 Iteration 214/234: loss=0.049342 lr=0.000020 grad_norm=0.792515
Epoch 51/100 Iteration 215/234: loss=0.048441 lr=0.000020 grad_norm=0.509106
Epoch 51/100 Iteration 216/234: loss=0.061645 lr=0.000020 grad_norm=1.098261
Epoch 51/100 Iteration 217/234: loss=0.054100 lr=0.000020 grad_norm=1.547275
Epoch 51/100 Iteration 218/234: loss=0.053329 lr=0.000020 grad_norm=1.408699
Epoch 51/100 Iteration 219/234: loss=0.058281 lr=0.000020 grad_norm=1.015108
Epoch 51/100 Iteration 220/234: loss=0.057842 lr=0.000020 grad_norm=0.691282
Epoch 51/100 Iteration 221/234: loss=0.057773 lr=0.000020 grad_norm=0.474817
Epoch 51/100 Iteration 222/234: loss=0.056315 lr=0.000020 grad_norm=0.398920
Epoch 51/100 Iteration 223/234: loss=0.061015 lr=0.000020 grad_norm=0.539120
Epoch 51/100 Iteration 224/234: loss=0.057869 lr=0.000020 grad_norm=0.532937
Epoch 51/100 Iteration 225/234: loss=0.059448 lr=0.000020 grad_norm=0.450277
Epoch 51/100 Iteration 226/234: loss=0.051892 lr=0.000020 grad_norm=0.594002
Epoch 51/100 Iteration 227/234: loss=0.052592 lr=0.000020 grad_norm=0.675397
Epoch 51/100 Iteration 228/234: loss=0.048912 lr=0.000020 grad_norm=0.448234
Epoch 51/100 Iteration 229/234: loss=0.050733 lr=0.000020 grad_norm=0.765540
Epoch 51/100 Iteration 230/234: loss=0.048390 lr=0.000020 grad_norm=0.884008
Epoch 51/100 Iteration 231/234: loss=0.053667 lr=0.000020 grad_norm=0.376970
Epoch 51/100 Iteration 232/234: loss=0.058132 lr=0.000020 grad_norm=1.077026
Epoch 51/100 Iteration 233/234: loss=0.051851 lr=0.000020 grad_norm=1.570854
Epoch 51/100 Iteration 234/234: loss=0.051714 lr=0.000020 grad_norm=0.479471
Epoch 51/100 finished. Avg Loss: 0.054839
Epoch 52/100 Iteration 1/234: loss=0.055932 lr=0.000020 grad_norm=1.309237
Epoch 52/100 Iteration 2/234: loss=0.049032 lr=0.000020 grad_norm=1.680797
Epoch 52/100 Iteration 3/234: loss=0.045788 lr=0.000020 grad_norm=0.652225
Epoch 52/100 Iteration 4/234: loss=0.050778 lr=0.000020 grad_norm=1.023195
Epoch 52/100 Iteration 5/234: loss=0.055698 lr=0.000020 grad_norm=1.417127
Epoch 52/100 Iteration 6/234: loss=0.050958 lr=0.000020 grad_norm=0.765890
Epoch 52/100 Iteration 7/234: loss=0.059950 lr=0.000020 grad_norm=1.110493
Epoch 52/100 Iteration 8/234: loss=0.051925 lr=0.000020 grad_norm=1.594442
Epoch 52/100 Iteration 9/234: loss=0.048571 lr=0.000020 grad_norm=0.552450
Epoch 52/100 Iteration 10/234: loss=0.057153 lr=0.000020 grad_norm=1.549541
Epoch 52/100 Iteration 11/234: loss=0.054865 lr=0.000020 grad_norm=1.602107
Epoch 52/100 Iteration 12/234: loss=0.055412 lr=0.000020 grad_norm=1.036161
Epoch 52/100 Iteration 13/234: loss=0.060710 lr=0.000020 grad_norm=3.446087
Epoch 52/100 Iteration 14/234: loss=0.042123 lr=0.000020 grad_norm=2.378125
Epoch 52/100 Iteration 15/234: loss=0.064129 lr=0.000020 grad_norm=2.065063
Epoch 52/100 Iteration 16/234: loss=0.053307 lr=0.000020 grad_norm=3.941015
Epoch 52/100 Iteration 17/234: loss=0.046399 lr=0.000020 grad_norm=0.948496
Epoch 52/100 Iteration 18/234: loss=0.052483 lr=0.000020 grad_norm=2.333583
Epoch 52/100 Iteration 19/234: loss=0.055110 lr=0.000020 grad_norm=1.339710
Epoch 52/100 Iteration 20/234: loss=0.055742 lr=0.000020 grad_norm=2.384631
Epoch 52/100 Iteration 21/234: loss=0.057803 lr=0.000020 grad_norm=0.981858
Epoch 52/100 Iteration 22/234: loss=0.055856 lr=0.000020 grad_norm=2.329823
Epoch 52/100 Iteration 23/234: loss=0.052987 lr=0.000020 grad_norm=0.666417
Epoch 52/100 Iteration 24/234: loss=0.057019 lr=0.000020 grad_norm=2.147871
Epoch 52/100 Iteration 25/234: loss=0.057738 lr=0.000020 grad_norm=0.704128
Epoch 52/100 Iteration 26/234: loss=0.055707 lr=0.000020 grad_norm=1.375748
Epoch 52/100 Iteration 27/234: loss=0.053570 lr=0.000020 grad_norm=1.049065
Epoch 52/100 Iteration 28/234: loss=0.054741 lr=0.000020 grad_norm=1.501269
Epoch 52/100 Iteration 29/234: loss=0.049395 lr=0.000020 grad_norm=0.680611
Epoch 52/100 Iteration 30/234: loss=0.051066 lr=0.000020 grad_norm=1.203543
Epoch 52/100 Iteration 31/234: loss=0.052495 lr=0.000020 grad_norm=0.516342
Epoch 52/100 Iteration 32/234: loss=0.050775 lr=0.000020 grad_norm=1.233110
Epoch 52/100 Iteration 33/234: loss=0.057656 lr=0.000020 grad_norm=0.580801
Epoch 52/100 Iteration 34/234: loss=0.049743 lr=0.000020 grad_norm=1.102780
Epoch 52/100 Iteration 35/234: loss=0.051734 lr=0.000020 grad_norm=0.979842
Epoch 52/100 Iteration 36/234: loss=0.055769 lr=0.000020 grad_norm=1.966566
Epoch 52/100 Iteration 37/234: loss=0.057467 lr=0.000020 grad_norm=0.545321
Epoch 52/100 Iteration 38/234: loss=0.054854 lr=0.000020 grad_norm=1.287636
Epoch 52/100 Iteration 39/234: loss=0.059394 lr=0.000020 grad_norm=0.600286
Epoch 52/100 Iteration 40/234: loss=0.058357 lr=0.000020 grad_norm=1.134817
Epoch 52/100 Iteration 41/234: loss=0.051248 lr=0.000020 grad_norm=0.495562
Epoch 52/100 Iteration 42/234: loss=0.060930 lr=0.000020 grad_norm=0.844279
Epoch 52/100 Iteration 43/234: loss=0.051282 lr=0.000020 grad_norm=0.520111
Epoch 52/100 Iteration 44/234: loss=0.058178 lr=0.000020 grad_norm=0.911010
Epoch 52/100 Iteration 45/234: loss=0.054409 lr=0.000020 grad_norm=0.589616
Epoch 52/100 Iteration 46/234: loss=0.051896 lr=0.000020 grad_norm=1.102710
Epoch 52/100 Iteration 47/234: loss=0.053298 lr=0.000020 grad_norm=0.681862
Epoch 52/100 Iteration 48/234: loss=0.051444 lr=0.000020 grad_norm=1.051381
Epoch 52/100 Iteration 49/234: loss=0.055226 lr=0.000020 grad_norm=1.286809
Epoch 52/100 Iteration 50/234: loss=0.048420 lr=0.000020 grad_norm=0.487707
Epoch 52/100 Iteration 51/234: loss=0.055854 lr=0.000020 grad_norm=1.079415
Epoch 52/100 Iteration 52/234: loss=0.061225 lr=0.000020 grad_norm=0.722479
Epoch 52/100 Iteration 53/234: loss=0.057423 lr=0.000020 grad_norm=1.262973
Epoch 52/100 Iteration 54/234: loss=0.052785 lr=0.000020 grad_norm=1.007994
Epoch 52/100 Iteration 55/234: loss=0.058639 lr=0.000020 grad_norm=0.713804
Epoch 52/100 Iteration 56/234: loss=0.054672 lr=0.000020 grad_norm=0.756973
Epoch 52/100 Iteration 57/234: loss=0.052832 lr=0.000020 grad_norm=0.533290
Epoch 52/100 Iteration 58/234: loss=0.058868 lr=0.000020 grad_norm=0.699015
Epoch 52/100 Iteration 59/234: loss=0.045527 lr=0.000020 grad_norm=0.491786
Epoch 52/100 Iteration 60/234: loss=0.058150 lr=0.000020 grad_norm=1.002743
Epoch 52/100 Iteration 61/234: loss=0.054425 lr=0.000020 grad_norm=0.613024
Epoch 52/100 Iteration 62/234: loss=0.053319 lr=0.000020 grad_norm=0.611981
Epoch 52/100 Iteration 63/234: loss=0.051367 lr=0.000020 grad_norm=0.758111
Epoch 52/100 Iteration 64/234: loss=0.050762 lr=0.000020 grad_norm=0.463949
Epoch 52/100 Iteration 65/234: loss=0.056126 lr=0.000020 grad_norm=0.983172
Epoch 52/100 Iteration 66/234: loss=0.055251 lr=0.000020 grad_norm=0.723758
Epoch 52/100 Iteration 67/234: loss=0.051258 lr=0.000020 grad_norm=0.579234
Epoch 52/100 Iteration 68/234: loss=0.051848 lr=0.000020 grad_norm=0.729984
Epoch 52/100 Iteration 69/234: loss=0.052920 lr=0.000020 grad_norm=0.664435
Epoch 52/100 Iteration 70/234: loss=0.051634 lr=0.000020 grad_norm=0.986232
Epoch 52/100 Iteration 71/234: loss=0.057580 lr=0.000020 grad_norm=0.590803
Epoch 52/100 Iteration 72/234: loss=0.056412 lr=0.000020 grad_norm=0.736942
Epoch 52/100 Iteration 73/234: loss=0.052732 lr=0.000020 grad_norm=0.691580
Epoch 52/100 Iteration 74/234: loss=0.050783 lr=0.000020 grad_norm=0.477902
Epoch 52/100 Iteration 75/234: loss=0.057434 lr=0.000020 grad_norm=0.481378
Epoch 52/100 Iteration 76/234: loss=0.054005 lr=0.000020 grad_norm=0.389956
Epoch 52/100 Iteration 77/234: loss=0.059269 lr=0.000020 grad_norm=0.407867
Epoch 52/100 Iteration 78/234: loss=0.056070 lr=0.000020 grad_norm=0.420811
Epoch 52/100 Iteration 79/234: loss=0.057702 lr=0.000020 grad_norm=0.426090
Epoch 52/100 Iteration 80/234: loss=0.056728 lr=0.000020 grad_norm=0.584297
Epoch 52/100 Iteration 81/234: loss=0.053038 lr=0.000020 grad_norm=0.727705
Epoch 52/100 Iteration 82/234: loss=0.059054 lr=0.000020 grad_norm=0.498374
Epoch 52/100 Iteration 83/234: loss=0.054223 lr=0.000020 grad_norm=0.586717
Epoch 52/100 Iteration 84/234: loss=0.047118 lr=0.000020 grad_norm=0.368286
Epoch 52/100 Iteration 85/234: loss=0.044621 lr=0.000020 grad_norm=0.494006
Epoch 52/100 Iteration 86/234: loss=0.054695 lr=0.000020 grad_norm=0.546386
Epoch 52/100 Iteration 87/234: loss=0.049915 lr=0.000020 grad_norm=0.575790
Epoch 52/100 Iteration 88/234: loss=0.056362 lr=0.000020 grad_norm=0.452991
Epoch 52/100 Iteration 89/234: loss=0.049401 lr=0.000020 grad_norm=0.521817
Epoch 52/100 Iteration 90/234: loss=0.051244 lr=0.000020 grad_norm=0.764141
Epoch 52/100 Iteration 91/234: loss=0.053565 lr=0.000020 grad_norm=0.568878
Epoch 52/100 Iteration 92/234: loss=0.052098 lr=0.000020 grad_norm=0.593134
Epoch 52/100 Iteration 93/234: loss=0.050553 lr=0.000020 grad_norm=0.876462
Epoch 52/100 Iteration 94/234: loss=0.056625 lr=0.000020 grad_norm=1.220975
Epoch 52/100 Iteration 95/234: loss=0.056976 lr=0.000020 grad_norm=1.186997
Epoch 52/100 Iteration 96/234: loss=0.053527 lr=0.000020 grad_norm=0.979823
Epoch 52/100 Iteration 97/234: loss=0.050944 lr=0.000020 grad_norm=1.597743
Epoch 52/100 Iteration 98/234: loss=0.060221 lr=0.000020 grad_norm=1.711536
Epoch 52/100 Iteration 99/234: loss=0.049216 lr=0.000020 grad_norm=0.909082
Epoch 52/100 Iteration 100/234: loss=0.056463 lr=0.000020 grad_norm=0.674118
Epoch 52/100 Iteration 101/234: loss=0.057681 lr=0.000020 grad_norm=1.119861
Epoch 52/100 Iteration 102/234: loss=0.059477 lr=0.000020 grad_norm=0.631643
Epoch 52/100 Iteration 103/234: loss=0.058475 lr=0.000020 grad_norm=1.018253
Epoch 52/100 Iteration 104/234: loss=0.047067 lr=0.000020 grad_norm=1.064225
Epoch 52/100 Iteration 105/234: loss=0.047017 lr=0.000020 grad_norm=0.478920
Epoch 52/100 Iteration 106/234: loss=0.051236 lr=0.000020 grad_norm=0.443628
Epoch 52/100 Iteration 107/234: loss=0.053054 lr=0.000020 grad_norm=0.555196
Epoch 52/100 Iteration 108/234: loss=0.058651 lr=0.000020 grad_norm=0.975809
Epoch 52/100 Iteration 109/234: loss=0.054847 lr=0.000020 grad_norm=0.500480
Epoch 52/100 Iteration 110/234: loss=0.054280 lr=0.000020 grad_norm=0.772078
Epoch 52/100 Iteration 111/234: loss=0.051271 lr=0.000020 grad_norm=0.728590
Epoch 52/100 Iteration 112/234: loss=0.056521 lr=0.000020 grad_norm=0.418556
Epoch 52/100 Iteration 113/234: loss=0.053020 lr=0.000020 grad_norm=0.713686
Epoch 52/100 Iteration 114/234: loss=0.056799 lr=0.000020 grad_norm=0.530107
Epoch 52/100 Iteration 115/234: loss=0.052510 lr=0.000020 grad_norm=0.621534
Epoch 52/100 Iteration 116/234: loss=0.048010 lr=0.000020 grad_norm=0.708634
Epoch 52/100 Iteration 117/234: loss=0.056467 lr=0.000020 grad_norm=0.672207
Epoch 52/100 Iteration 118/234: loss=0.048096 lr=0.000020 grad_norm=0.586964
Epoch 52/100 Iteration 119/234: loss=0.053794 lr=0.000020 grad_norm=0.624584
Epoch 52/100 Iteration 120/234: loss=0.053252 lr=0.000020 grad_norm=0.573352
Epoch 52/100 Iteration 121/234: loss=0.050210 lr=0.000020 grad_norm=0.750348
Epoch 52/100 Iteration 122/234: loss=0.052048 lr=0.000020 grad_norm=0.609413
Epoch 52/100 Iteration 123/234: loss=0.049480 lr=0.000020 grad_norm=0.834394
Epoch 52/100 Iteration 124/234: loss=0.043089 lr=0.000020 grad_norm=0.483803
Epoch 52/100 Iteration 125/234: loss=0.052711 lr=0.000020 grad_norm=1.315451
Epoch 52/100 Iteration 126/234: loss=0.053409 lr=0.000020 grad_norm=1.793240
Epoch 52/100 Iteration 127/234: loss=0.054404 lr=0.000020 grad_norm=0.516337
Epoch 52/100 Iteration 128/234: loss=0.049181 lr=0.000020 grad_norm=1.743798
Epoch 52/100 Iteration 129/234: loss=0.056885 lr=0.000020 grad_norm=1.916744
Epoch 52/100 Iteration 130/234: loss=0.052518 lr=0.000020 grad_norm=0.798564
Epoch 52/100 Iteration 131/234: loss=0.051425 lr=0.000020 grad_norm=0.941532
Epoch 52/100 Iteration 132/234: loss=0.053456 lr=0.000020 grad_norm=0.898429
Epoch 52/100 Iteration 133/234: loss=0.049203 lr=0.000020 grad_norm=0.642166
Epoch 52/100 Iteration 134/234: loss=0.049765 lr=0.000020 grad_norm=0.652160
Epoch 52/100 Iteration 135/234: loss=0.047959 lr=0.000020 grad_norm=0.524308
Epoch 52/100 Iteration 136/234: loss=0.058814 lr=0.000020 grad_norm=0.883453
Epoch 52/100 Iteration 137/234: loss=0.048316 lr=0.000020 grad_norm=0.676699
Epoch 52/100 Iteration 138/234: loss=0.054372 lr=0.000020 grad_norm=0.716834
Epoch 52/100 Iteration 139/234: loss=0.046969 lr=0.000020 grad_norm=0.763997
Epoch 52/100 Iteration 140/234: loss=0.048721 lr=0.000020 grad_norm=0.422819
Epoch 52/100 Iteration 141/234: loss=0.056530 lr=0.000020 grad_norm=0.775564
Epoch 52/100 Iteration 142/234: loss=0.058904 lr=0.000020 grad_norm=0.526823
Epoch 52/100 Iteration 143/234: loss=0.055380 lr=0.000020 grad_norm=0.578902
Epoch 52/100 Iteration 144/234: loss=0.049781 lr=0.000020 grad_norm=0.674603
Epoch 52/100 Iteration 145/234: loss=0.053478 lr=0.000020 grad_norm=0.380400
Epoch 52/100 Iteration 146/234: loss=0.056564 lr=0.000020 grad_norm=1.104782
Epoch 52/100 Iteration 147/234: loss=0.053471 lr=0.000020 grad_norm=1.433536
Epoch 52/100 Iteration 148/234: loss=0.054848 lr=0.000020 grad_norm=0.649229
Epoch 52/100 Iteration 149/234: loss=0.056036 lr=0.000020 grad_norm=1.038608
Epoch 52/100 Iteration 150/234: loss=0.052977 lr=0.000020 grad_norm=1.609794
Epoch 52/100 Iteration 151/234: loss=0.053527 lr=0.000020 grad_norm=1.069051
Epoch 52/100 Iteration 152/234: loss=0.055198 lr=0.000020 grad_norm=0.694195
Epoch 52/100 Iteration 153/234: loss=0.043415 lr=0.000020 grad_norm=0.860270
Epoch 52/100 Iteration 154/234: loss=0.053504 lr=0.000020 grad_norm=0.769841
Epoch 52/100 Iteration 155/234: loss=0.051751 lr=0.000020 grad_norm=0.828173
Epoch 52/100 Iteration 156/234: loss=0.055230 lr=0.000020 grad_norm=0.915043
Epoch 52/100 Iteration 157/234: loss=0.057408 lr=0.000020 grad_norm=0.457668
Epoch 52/100 Iteration 158/234: loss=0.060873 lr=0.000020 grad_norm=0.872837
Epoch 52/100 Iteration 159/234: loss=0.052825 lr=0.000020 grad_norm=0.753252
Epoch 52/100 Iteration 160/234: loss=0.054549 lr=0.000020 grad_norm=0.519409
Epoch 52/100 Iteration 161/234: loss=0.054313 lr=0.000020 grad_norm=0.563486
Epoch 52/100 Iteration 162/234: loss=0.050757 lr=0.000020 grad_norm=0.792089
Epoch 52/100 Iteration 163/234: loss=0.053964 lr=0.000020 grad_norm=0.824089
Epoch 52/100 Iteration 164/234: loss=0.051209 lr=0.000020 grad_norm=0.567642
Epoch 52/100 Iteration 165/234: loss=0.057139 lr=0.000020 grad_norm=1.090733
Epoch 52/100 Iteration 166/234: loss=0.060796 lr=0.000020 grad_norm=1.086609
Epoch 52/100 Iteration 167/234: loss=0.060481 lr=0.000020 grad_norm=0.537929
Epoch 52/100 Iteration 168/234: loss=0.054064 lr=0.000020 grad_norm=0.711658
Epoch 52/100 Iteration 169/234: loss=0.049245 lr=0.000020 grad_norm=0.917949
Epoch 52/100 Iteration 170/234: loss=0.062014 lr=0.000020 grad_norm=0.563337
Epoch 52/100 Iteration 171/234: loss=0.056266 lr=0.000020 grad_norm=1.652650
Epoch 52/100 Iteration 172/234: loss=0.057280 lr=0.000020 grad_norm=1.964551
Epoch 52/100 Iteration 173/234: loss=0.049737 lr=0.000020 grad_norm=1.071531
Epoch 52/100 Iteration 174/234: loss=0.056950 lr=0.000020 grad_norm=1.037513
Epoch 52/100 Iteration 175/234: loss=0.061955 lr=0.000020 grad_norm=1.780457
Epoch 52/100 Iteration 176/234: loss=0.058155 lr=0.000020 grad_norm=1.369493
Epoch 52/100 Iteration 177/234: loss=0.048895 lr=0.000020 grad_norm=0.961783
Epoch 52/100 Iteration 178/234: loss=0.052264 lr=0.000020 grad_norm=0.691138
Epoch 52/100 Iteration 179/234: loss=0.052275 lr=0.000020 grad_norm=0.943329
Epoch 52/100 Iteration 180/234: loss=0.052737 lr=0.000020 grad_norm=1.072705
Epoch 52/100 Iteration 181/234: loss=0.061737 lr=0.000020 grad_norm=0.836898
Epoch 52/100 Iteration 182/234: loss=0.061404 lr=0.000020 grad_norm=1.089661
Epoch 52/100 Iteration 183/234: loss=0.060508 lr=0.000020 grad_norm=1.458376
Epoch 52/100 Iteration 184/234: loss=0.060591 lr=0.000020 grad_norm=1.698853
Epoch 52/100 Iteration 185/234: loss=0.053372 lr=0.000020 grad_norm=0.955848
Epoch 52/100 Iteration 186/234: loss=0.059358 lr=0.000020 grad_norm=0.685592
Epoch 52/100 Iteration 187/234: loss=0.051947 lr=0.000020 grad_norm=0.966525
Epoch 52/100 Iteration 188/234: loss=0.051835 lr=0.000020 grad_norm=0.575231
Epoch 52/100 Iteration 189/234: loss=0.058326 lr=0.000020 grad_norm=0.997107
Epoch 52/100 Iteration 190/234: loss=0.050568 lr=0.000020 grad_norm=1.576227
Epoch 52/100 Iteration 191/234: loss=0.047721 lr=0.000020 grad_norm=0.819130
Epoch 52/100 Iteration 192/234: loss=0.050199 lr=0.000020 grad_norm=1.151221
Epoch 52/100 Iteration 193/234: loss=0.056535 lr=0.000020 grad_norm=1.910409
Epoch 52/100 Iteration 194/234: loss=0.046631 lr=0.000020 grad_norm=0.783620
Epoch 52/100 Iteration 195/234: loss=0.055285 lr=0.000020 grad_norm=1.815342
Epoch 52/100 Iteration 196/234: loss=0.056856 lr=0.000020 grad_norm=2.573839
Epoch 52/100 Iteration 197/234: loss=0.047954 lr=0.000020 grad_norm=0.951194
Epoch 52/100 Iteration 198/234: loss=0.046448 lr=0.000020 grad_norm=1.268826
Epoch 52/100 Iteration 199/234: loss=0.050284 lr=0.000020 grad_norm=1.235807
Epoch 52/100 Iteration 200/234: loss=0.054676 lr=0.000020 grad_norm=1.016307
Epoch 52/100 Iteration 201/234: loss=0.046515 lr=0.000020 grad_norm=1.615878
Epoch 52/100 Iteration 202/234: loss=0.048514 lr=0.000020 grad_norm=0.489703
Epoch 52/100 Iteration 203/234: loss=0.053133 lr=0.000020 grad_norm=1.036241
Epoch 52/100 Iteration 204/234: loss=0.059228 lr=0.000020 grad_norm=0.857332
Epoch 52/100 Iteration 205/234: loss=0.051738 lr=0.000020 grad_norm=0.571450
Epoch 52/100 Iteration 206/234: loss=0.050891 lr=0.000020 grad_norm=1.014445
Epoch 52/100 Iteration 207/234: loss=0.060135 lr=0.000020 grad_norm=0.918997
Epoch 52/100 Iteration 208/234: loss=0.055162 lr=0.000020 grad_norm=0.636343
Epoch 52/100 Iteration 209/234: loss=0.056747 lr=0.000020 grad_norm=0.556010
Epoch 52/100 Iteration 210/234: loss=0.058795 lr=0.000020 grad_norm=0.782991
Epoch 52/100 Iteration 211/234: loss=0.052379 lr=0.000020 grad_norm=0.610722
Epoch 52/100 Iteration 212/234: loss=0.053343 lr=0.000020 grad_norm=0.663548
Epoch 52/100 Iteration 213/234: loss=0.060444 lr=0.000020 grad_norm=0.482517
Epoch 52/100 Iteration 214/234: loss=0.048981 lr=0.000020 grad_norm=0.846852
Epoch 52/100 Iteration 215/234: loss=0.053057 lr=0.000020 grad_norm=0.716397
Epoch 52/100 Iteration 216/234: loss=0.053114 lr=0.000020 grad_norm=0.445035
Epoch 52/100 Iteration 217/234: loss=0.048414 lr=0.000020 grad_norm=0.379003
Epoch 52/100 Iteration 218/234: loss=0.056594 lr=0.000020 grad_norm=0.451289
Epoch 52/100 Iteration 219/234: loss=0.049838 lr=0.000020 grad_norm=0.669761
Epoch 52/100 Iteration 220/234: loss=0.048329 lr=0.000020 grad_norm=0.521542
Epoch 52/100 Iteration 221/234: loss=0.050007 lr=0.000020 grad_norm=0.669992
Epoch 52/100 Iteration 222/234: loss=0.049019 lr=0.000020 grad_norm=0.854114
Epoch 52/100 Iteration 223/234: loss=0.053589 lr=0.000020 grad_norm=0.593430
Epoch 52/100 Iteration 224/234: loss=0.052225 lr=0.000020 grad_norm=1.384617
Epoch 52/100 Iteration 225/234: loss=0.053008 lr=0.000020 grad_norm=1.809968
Epoch 52/100 Iteration 226/234: loss=0.048392 lr=0.000020 grad_norm=0.849445
Epoch 52/100 Iteration 227/234: loss=0.056111 lr=0.000020 grad_norm=0.688737
Epoch 52/100 Iteration 228/234: loss=0.057134 lr=0.000020 grad_norm=0.981707
Epoch 52/100 Iteration 229/234: loss=0.059498 lr=0.000020 grad_norm=0.397987
Epoch 52/100 Iteration 230/234: loss=0.057859 lr=0.000020 grad_norm=0.739039
Epoch 52/100 Iteration 231/234: loss=0.052925 lr=0.000020 grad_norm=0.829439
Epoch 52/100 Iteration 232/234: loss=0.056061 lr=0.000020 grad_norm=0.470517
Epoch 52/100 Iteration 233/234: loss=0.058572 lr=0.000020 grad_norm=0.503971
Epoch 52/100 Iteration 234/234: loss=0.052755 lr=0.000020 grad_norm=0.442536
Epoch 52/100 finished. Avg Loss: 0.053786
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 53/100 Iteration 1/234: loss=0.052396 lr=0.000020 grad_norm=0.370201
Epoch 53/100 Iteration 2/234: loss=0.052966 lr=0.000020 grad_norm=0.585213
Epoch 53/100 Iteration 3/234: loss=0.056626 lr=0.000020 grad_norm=0.808101
Epoch 53/100 Iteration 4/234: loss=0.048120 lr=0.000020 grad_norm=0.366150
Epoch 53/100 Iteration 5/234: loss=0.054839 lr=0.000020 grad_norm=1.046160
Epoch 53/100 Iteration 6/234: loss=0.054497 lr=0.000020 grad_norm=1.064401
Epoch 53/100 Iteration 7/234: loss=0.057918 lr=0.000020 grad_norm=0.361473
Epoch 53/100 Iteration 8/234: loss=0.048907 lr=0.000020 grad_norm=0.975790
Epoch 53/100 Iteration 9/234: loss=0.051664 lr=0.000020 grad_norm=0.978828
Epoch 53/100 Iteration 10/234: loss=0.051873 lr=0.000020 grad_norm=0.371598
Epoch 53/100 Iteration 11/234: loss=0.047972 lr=0.000020 grad_norm=1.100088
Epoch 53/100 Iteration 12/234: loss=0.056309 lr=0.000020 grad_norm=1.135206
Epoch 53/100 Iteration 13/234: loss=0.050934 lr=0.000020 grad_norm=0.371833
Epoch 53/100 Iteration 14/234: loss=0.053902 lr=0.000020 grad_norm=1.114752
Epoch 53/100 Iteration 15/234: loss=0.064194 lr=0.000020 grad_norm=1.277647
Epoch 53/100 Iteration 16/234: loss=0.058929 lr=0.000020 grad_norm=0.697191
Epoch 53/100 Iteration 17/234: loss=0.051323 lr=0.000020 grad_norm=0.568282
Epoch 53/100 Iteration 18/234: loss=0.047336 lr=0.000020 grad_norm=0.299588
Epoch 53/100 Iteration 19/234: loss=0.052944 lr=0.000020 grad_norm=0.651580
Epoch 53/100 Iteration 20/234: loss=0.048460 lr=0.000020 grad_norm=0.520277
Epoch 53/100 Iteration 21/234: loss=0.049543 lr=0.000020 grad_norm=0.588605
Epoch 53/100 Iteration 22/234: loss=0.060698 lr=0.000020 grad_norm=0.790600
Epoch 53/100 Iteration 23/234: loss=0.055518 lr=0.000020 grad_norm=0.535509
Epoch 53/100 Iteration 24/234: loss=0.045217 lr=0.000020 grad_norm=0.493309
Epoch 53/100 Iteration 25/234: loss=0.048296 lr=0.000020 grad_norm=0.509757
Epoch 53/100 Iteration 26/234: loss=0.050265 lr=0.000020 grad_norm=0.338719
Epoch 53/100 Iteration 27/234: loss=0.058898 lr=0.000020 grad_norm=0.850029
Epoch 53/100 Iteration 28/234: loss=0.054641 lr=0.000020 grad_norm=0.993732
Epoch 53/100 Iteration 29/234: loss=0.048075 lr=0.000020 grad_norm=0.439747
Epoch 53/100 Iteration 30/234: loss=0.052838 lr=0.000020 grad_norm=0.913628
Epoch 53/100 Iteration 31/234: loss=0.049222 lr=0.000020 grad_norm=1.482214
Epoch 53/100 Iteration 32/234: loss=0.046936 lr=0.000020 grad_norm=1.232985
Epoch 53/100 Iteration 33/234: loss=0.049798 lr=0.000020 grad_norm=0.775980
Epoch 53/100 Iteration 34/234: loss=0.055614 lr=0.000020 grad_norm=0.836615
Epoch 53/100 Iteration 35/234: loss=0.054022 lr=0.000020 grad_norm=0.738814
Epoch 53/100 Iteration 36/234: loss=0.053226 lr=0.000020 grad_norm=0.723464
Epoch 53/100 Iteration 37/234: loss=0.054940 lr=0.000020 grad_norm=0.538430
Epoch 53/100 Iteration 38/234: loss=0.048061 lr=0.000020 grad_norm=0.786810
Epoch 53/100 Iteration 39/234: loss=0.052084 lr=0.000020 grad_norm=0.788971
Epoch 53/100 Iteration 40/234: loss=0.056842 lr=0.000020 grad_norm=0.449774
Epoch 53/100 Iteration 41/234: loss=0.055415 lr=0.000020 grad_norm=0.672197
Epoch 53/100 Iteration 42/234: loss=0.049315 lr=0.000020 grad_norm=0.911665
Epoch 53/100 Iteration 43/234: loss=0.048760 lr=0.000020 grad_norm=0.692645
Epoch 53/100 Iteration 44/234: loss=0.049148 lr=0.000020 grad_norm=0.432651
Epoch 53/100 Iteration 45/234: loss=0.051554 lr=0.000020 grad_norm=0.776655
Epoch 53/100 Iteration 46/234: loss=0.051426 lr=0.000020 grad_norm=0.677960
Epoch 53/100 Iteration 47/234: loss=0.050986 lr=0.000020 grad_norm=0.488026
Epoch 53/100 Iteration 48/234: loss=0.052734 lr=0.000020 grad_norm=0.535593
Epoch 53/100 Iteration 49/234: loss=0.054401 lr=0.000020 grad_norm=1.083815
Epoch 53/100 Iteration 50/234: loss=0.058322 lr=0.000020 grad_norm=1.432452
Epoch 53/100 Iteration 51/234: loss=0.058262 lr=0.000020 grad_norm=1.312808
Epoch 53/100 Iteration 52/234: loss=0.044335 lr=0.000020 grad_norm=0.914187
Epoch 53/100 Iteration 53/234: loss=0.054916 lr=0.000020 grad_norm=0.631091
Epoch 53/100 Iteration 54/234: loss=0.058025 lr=0.000020 grad_norm=1.138212
Epoch 53/100 Iteration 55/234: loss=0.047662 lr=0.000020 grad_norm=1.107928
Epoch 53/100 Iteration 56/234: loss=0.058454 lr=0.000020 grad_norm=0.566183
Epoch 53/100 Iteration 57/234: loss=0.052533 lr=0.000020 grad_norm=1.253920
Epoch 53/100 Iteration 58/234: loss=0.054853 lr=0.000020 grad_norm=1.284758
Epoch 53/100 Iteration 59/234: loss=0.052592 lr=0.000020 grad_norm=0.788168
Epoch 53/100 Iteration 60/234: loss=0.056015 lr=0.000020 grad_norm=0.351916
Epoch 53/100 Iteration 61/234: loss=0.048988 lr=0.000020 grad_norm=0.702885
Epoch 53/100 Iteration 62/234: loss=0.049816 lr=0.000020 grad_norm=0.559595
Epoch 53/100 Iteration 63/234: loss=0.054863 lr=0.000020 grad_norm=0.465712
Epoch 53/100 Iteration 64/234: loss=0.051539 lr=0.000020 grad_norm=0.547773
Epoch 53/100 Iteration 65/234: loss=0.054598 lr=0.000020 grad_norm=0.415473
Epoch 53/100 Iteration 66/234: loss=0.054462 lr=0.000020 grad_norm=0.446017
Epoch 53/100 Iteration 67/234: loss=0.054037 lr=0.000020 grad_norm=0.421796
Epoch 53/100 Iteration 68/234: loss=0.048944 lr=0.000020 grad_norm=0.479509
Epoch 53/100 Iteration 69/234: loss=0.050207 lr=0.000020 grad_norm=0.497227
Epoch 53/100 Iteration 70/234: loss=0.057869 lr=0.000020 grad_norm=0.561637
Epoch 53/100 Iteration 71/234: loss=0.060102 lr=0.000020 grad_norm=0.687927
Epoch 53/100 Iteration 72/234: loss=0.047919 lr=0.000020 grad_norm=0.565290
Epoch 53/100 Iteration 73/234: loss=0.059513 lr=0.000020 grad_norm=0.602318
Epoch 53/100 Iteration 74/234: loss=0.049653 lr=0.000020 grad_norm=1.237415
Epoch 53/100 Iteration 75/234: loss=0.052730 lr=0.000020 grad_norm=1.048436
Epoch 53/100 Iteration 76/234: loss=0.056094 lr=0.000020 grad_norm=0.674913
Epoch 53/100 Iteration 77/234: loss=0.046601 lr=0.000020 grad_norm=1.542140
Epoch 53/100 Iteration 78/234: loss=0.056052 lr=0.000020 grad_norm=1.577370
Epoch 53/100 Iteration 79/234: loss=0.057088 lr=0.000020 grad_norm=0.900565
Epoch 53/100 Iteration 80/234: loss=0.049418 lr=0.000020 grad_norm=0.846200
Epoch 53/100 Iteration 81/234: loss=0.049557 lr=0.000020 grad_norm=0.835182
Epoch 53/100 Iteration 82/234: loss=0.049653 lr=0.000020 grad_norm=0.963546
Epoch 53/100 Iteration 83/234: loss=0.049462 lr=0.000020 grad_norm=1.177020
Epoch 53/100 Iteration 84/234: loss=0.051913 lr=0.000020 grad_norm=1.272458
Epoch 53/100 Iteration 85/234: loss=0.052137 lr=0.000020 grad_norm=0.787249
Epoch 53/100 Iteration 86/234: loss=0.052714 lr=0.000020 grad_norm=0.873935
Epoch 53/100 Iteration 87/234: loss=0.051110 lr=0.000020 grad_norm=0.995921
Epoch 53/100 Iteration 88/234: loss=0.047773 lr=0.000020 grad_norm=0.574483
Epoch 53/100 Iteration 89/234: loss=0.053664 lr=0.000020 grad_norm=1.092099
Epoch 53/100 Iteration 90/234: loss=0.063001 lr=0.000020 grad_norm=1.042524
Epoch 53/100 Iteration 91/234: loss=0.049550 lr=0.000020 grad_norm=0.372770
Epoch 53/100 Iteration 92/234: loss=0.055206 lr=0.000020 grad_norm=0.515004
Epoch 53/100 Iteration 93/234: loss=0.061842 lr=0.000020 grad_norm=0.826893
Epoch 53/100 Iteration 94/234: loss=0.049896 lr=0.000020 grad_norm=1.062831
Epoch 53/100 Iteration 95/234: loss=0.047993 lr=0.000020 grad_norm=0.682809
Epoch 53/100 Iteration 96/234: loss=0.052788 lr=0.000020 grad_norm=0.707458
Epoch 53/100 Iteration 97/234: loss=0.062753 lr=0.000020 grad_norm=0.846806
Epoch 53/100 Iteration 98/234: loss=0.047306 lr=0.000020 grad_norm=0.587526
Epoch 53/100 Iteration 99/234: loss=0.052205 lr=0.000020 grad_norm=0.779057
Epoch 53/100 Iteration 100/234: loss=0.054228 lr=0.000020 grad_norm=1.232288
Epoch 53/100 Iteration 101/234: loss=0.048824 lr=0.000020 grad_norm=0.874917
Epoch 53/100 Iteration 102/234: loss=0.053888 lr=0.000020 grad_norm=0.580828
Epoch 53/100 Iteration 103/234: loss=0.049561 lr=0.000020 grad_norm=1.083822
Epoch 53/100 Iteration 104/234: loss=0.047697 lr=0.000020 grad_norm=1.082400
Epoch 53/100 Iteration 105/234: loss=0.049623 lr=0.000020 grad_norm=0.483710
Epoch 53/100 Iteration 106/234: loss=0.052886 lr=0.000020 grad_norm=1.314644
Epoch 53/100 Iteration 107/234: loss=0.061018 lr=0.000020 grad_norm=1.185272
Epoch 53/100 Iteration 108/234: loss=0.041314 lr=0.000020 grad_norm=0.583938
Epoch 53/100 Iteration 109/234: loss=0.058786 lr=0.000020 grad_norm=0.944490
Epoch 53/100 Iteration 110/234: loss=0.054325 lr=0.000020 grad_norm=0.947194
Epoch 53/100 Iteration 111/234: loss=0.050451 lr=0.000020 grad_norm=0.599237
Epoch 53/100 Iteration 112/234: loss=0.047485 lr=0.000020 grad_norm=0.997619
Epoch 53/100 Iteration 113/234: loss=0.054506 lr=0.000020 grad_norm=1.145007
Epoch 53/100 Iteration 114/234: loss=0.053186 lr=0.000020 grad_norm=0.726129
Epoch 53/100 Iteration 115/234: loss=0.046653 lr=0.000020 grad_norm=0.601827
Epoch 53/100 Iteration 116/234: loss=0.049229 lr=0.000020 grad_norm=0.861358
Epoch 53/100 Iteration 117/234: loss=0.052054 lr=0.000020 grad_norm=0.376933
Epoch 53/100 Iteration 118/234: loss=0.048687 lr=0.000020 grad_norm=0.780746
Epoch 53/100 Iteration 119/234: loss=0.057522 lr=0.000020 grad_norm=1.054054
Epoch 53/100 Iteration 120/234: loss=0.051589 lr=0.000020 grad_norm=0.519857
Epoch 53/100 Iteration 121/234: loss=0.044863 lr=0.000020 grad_norm=0.903853
Epoch 53/100 Iteration 122/234: loss=0.056574 lr=0.000020 grad_norm=1.123017
Epoch 53/100 Iteration 123/234: loss=0.050730 lr=0.000020 grad_norm=0.578973
Epoch 53/100 Iteration 124/234: loss=0.056701 lr=0.000020 grad_norm=1.469512
Epoch 53/100 Iteration 125/234: loss=0.054425 lr=0.000020 grad_norm=1.565275
Epoch 53/100 Iteration 126/234: loss=0.055669 lr=0.000020 grad_norm=0.579880
Epoch 53/100 Iteration 127/234: loss=0.056548 lr=0.000020 grad_norm=1.525043
Epoch 53/100 Iteration 128/234: loss=0.050202 lr=0.000020 grad_norm=1.544123
Epoch 53/100 Iteration 129/234: loss=0.054997 lr=0.000020 grad_norm=0.543551
Epoch 53/100 Iteration 130/234: loss=0.048318 lr=0.000020 grad_norm=1.303404
Epoch 53/100 Iteration 131/234: loss=0.055417 lr=0.000020 grad_norm=1.413681
Epoch 53/100 Iteration 132/234: loss=0.049668 lr=0.000020 grad_norm=0.541822
Epoch 53/100 Iteration 133/234: loss=0.044423 lr=0.000020 grad_norm=1.415651
Epoch 53/100 Iteration 134/234: loss=0.045257 lr=0.000020 grad_norm=1.020369
Epoch 53/100 Iteration 135/234: loss=0.054323 lr=0.000020 grad_norm=0.830665
Epoch 53/100 Iteration 136/234: loss=0.051368 lr=0.000020 grad_norm=1.571058
Epoch 53/100 Iteration 137/234: loss=0.049517 lr=0.000020 grad_norm=1.106218
Epoch 53/100 Iteration 138/234: loss=0.054908 lr=0.000020 grad_norm=0.856210
Epoch 53/100 Iteration 139/234: loss=0.057910 lr=0.000020 grad_norm=0.910908
Epoch 53/100 Iteration 140/234: loss=0.055987 lr=0.000020 grad_norm=0.479931
Epoch 53/100 Iteration 141/234: loss=0.051646 lr=0.000020 grad_norm=0.769352
Epoch 53/100 Iteration 142/234: loss=0.057106 lr=0.000020 grad_norm=1.025475
Epoch 53/100 Iteration 143/234: loss=0.051543 lr=0.000020 grad_norm=0.420877
Epoch 53/100 Iteration 144/234: loss=0.047848 lr=0.000020 grad_norm=0.808596
Epoch 53/100 Iteration 145/234: loss=0.051081 lr=0.000020 grad_norm=0.621684
Epoch 53/100 Iteration 146/234: loss=0.051633 lr=0.000020 grad_norm=0.694749
Epoch 53/100 Iteration 147/234: loss=0.056276 lr=0.000020 grad_norm=1.296518
Epoch 53/100 Iteration 148/234: loss=0.053165 lr=0.000020 grad_norm=1.471698
Epoch 53/100 Iteration 149/234: loss=0.055151 lr=0.000020 grad_norm=0.940507
Epoch 53/100 Iteration 150/234: loss=0.053430 lr=0.000020 grad_norm=0.652395
Epoch 53/100 Iteration 151/234: loss=0.045865 lr=0.000020 grad_norm=0.726098
Epoch 53/100 Iteration 152/234: loss=0.052352 lr=0.000020 grad_norm=0.721468
Epoch 53/100 Iteration 153/234: loss=0.050573 lr=0.000020 grad_norm=0.690514
Epoch 53/100 Iteration 154/234: loss=0.050224 lr=0.000020 grad_norm=0.616642
Epoch 53/100 Iteration 155/234: loss=0.051238 lr=0.000020 grad_norm=0.547445
Epoch 53/100 Iteration 156/234: loss=0.045395 lr=0.000020 grad_norm=0.836005
Epoch 53/100 Iteration 157/234: loss=0.044774 lr=0.000020 grad_norm=0.648940
Epoch 53/100 Iteration 158/234: loss=0.055842 lr=0.000020 grad_norm=1.020442
Epoch 53/100 Iteration 159/234: loss=0.060641 lr=0.000020 grad_norm=2.053822
Epoch 53/100 Iteration 160/234: loss=0.049353 lr=0.000020 grad_norm=2.479672
Epoch 53/100 Iteration 161/234: loss=0.053178 lr=0.000020 grad_norm=1.617237
Epoch 53/100 Iteration 162/234: loss=0.049934 lr=0.000020 grad_norm=0.319648
Epoch 53/100 Iteration 163/234: loss=0.052149 lr=0.000020 grad_norm=1.171860
Epoch 53/100 Iteration 164/234: loss=0.052094 lr=0.000020 grad_norm=1.035709
Epoch 53/100 Iteration 165/234: loss=0.049158 lr=0.000020 grad_norm=0.503380
Epoch 53/100 Iteration 166/234: loss=0.054996 lr=0.000020 grad_norm=0.748376
Epoch 53/100 Iteration 167/234: loss=0.046759 lr=0.000020 grad_norm=1.273167
Epoch 53/100 Iteration 168/234: loss=0.048162 lr=0.000020 grad_norm=1.145920
Epoch 53/100 Iteration 169/234: loss=0.057276 lr=0.000020 grad_norm=0.504482
Epoch 53/100 Iteration 170/234: loss=0.049033 lr=0.000020 grad_norm=1.305176
Epoch 53/100 Iteration 171/234: loss=0.051312 lr=0.000020 grad_norm=1.317659
Epoch 53/100 Iteration 172/234: loss=0.060408 lr=0.000020 grad_norm=0.502377
Epoch 53/100 Iteration 173/234: loss=0.050716 lr=0.000020 grad_norm=2.120274
Epoch 53/100 Iteration 174/234: loss=0.056565 lr=0.000020 grad_norm=1.932465
Epoch 53/100 Iteration 175/234: loss=0.052759 lr=0.000020 grad_norm=0.751066
Epoch 53/100 Iteration 176/234: loss=0.050902 lr=0.000020 grad_norm=2.211088
Epoch 53/100 Iteration 177/234: loss=0.045084 lr=0.000020 grad_norm=1.083804
Epoch 53/100 Iteration 178/234: loss=0.046759 lr=0.000020 grad_norm=1.124007
Epoch 53/100 Iteration 179/234: loss=0.060294 lr=0.000020 grad_norm=2.077986
Epoch 53/100 Iteration 180/234: loss=0.053090 lr=0.000020 grad_norm=1.307797
Epoch 53/100 Iteration 181/234: loss=0.057776 lr=0.000020 grad_norm=1.021901
Epoch 53/100 Iteration 182/234: loss=0.058371 lr=0.000020 grad_norm=1.247423
Epoch 53/100 Iteration 183/234: loss=0.049982 lr=0.000020 grad_norm=1.076537
Epoch 53/100 Iteration 184/234: loss=0.049402 lr=0.000020 grad_norm=0.953628
Epoch 53/100 Iteration 185/234: loss=0.049358 lr=0.000020 grad_norm=0.858504
Epoch 53/100 Iteration 186/234: loss=0.055844 lr=0.000020 grad_norm=1.225172
Epoch 53/100 Iteration 187/234: loss=0.059606 lr=0.000020 grad_norm=1.079947
Epoch 53/100 Iteration 188/234: loss=0.050238 lr=0.000020 grad_norm=0.910465
Epoch 53/100 Iteration 189/234: loss=0.059149 lr=0.000020 grad_norm=0.870970
Epoch 53/100 Iteration 190/234: loss=0.056556 lr=0.000020 grad_norm=0.889316
Epoch 53/100 Iteration 191/234: loss=0.052200 lr=0.000020 grad_norm=0.872162
Epoch 53/100 Iteration 192/234: loss=0.049203 lr=0.000020 grad_norm=0.400597
Epoch 53/100 Iteration 193/234: loss=0.052884 lr=0.000020 grad_norm=1.036535
Epoch 53/100 Iteration 194/234: loss=0.046112 lr=0.000020 grad_norm=0.839999
Epoch 53/100 Iteration 195/234: loss=0.048834 lr=0.000020 grad_norm=0.763969
Epoch 53/100 Iteration 196/234: loss=0.056300 lr=0.000020 grad_norm=1.038070
Epoch 53/100 Iteration 197/234: loss=0.056322 lr=0.000020 grad_norm=0.826092
Epoch 53/100 Iteration 198/234: loss=0.061607 lr=0.000020 grad_norm=0.600628
Epoch 53/100 Iteration 199/234: loss=0.048040 lr=0.000020 grad_norm=0.930775
Epoch 53/100 Iteration 200/234: loss=0.056970 lr=0.000020 grad_norm=0.554541
Epoch 53/100 Iteration 201/234: loss=0.050128 lr=0.000020 grad_norm=0.954704
Epoch 53/100 Iteration 202/234: loss=0.044679 lr=0.000020 grad_norm=0.984027
Epoch 53/100 Iteration 203/234: loss=0.055717 lr=0.000020 grad_norm=0.579167
Epoch 53/100 Iteration 204/234: loss=0.059898 lr=0.000020 grad_norm=0.430624
Epoch 53/100 Iteration 205/234: loss=0.056335 lr=0.000020 grad_norm=0.743250
Epoch 53/100 Iteration 206/234: loss=0.051902 lr=0.000020 grad_norm=0.529410
Epoch 53/100 Iteration 207/234: loss=0.057682 lr=0.000020 grad_norm=0.601957
Epoch 53/100 Iteration 208/234: loss=0.050531 lr=0.000020 grad_norm=0.413989
Epoch 53/100 Iteration 209/234: loss=0.049404 lr=0.000020 grad_norm=0.650444
Epoch 53/100 Iteration 210/234: loss=0.060202 lr=0.000020 grad_norm=0.407770
Epoch 53/100 Iteration 211/234: loss=0.055803 lr=0.000020 grad_norm=0.712384
Epoch 53/100 Iteration 212/234: loss=0.040942 lr=0.000020 grad_norm=0.724971
Epoch 53/100 Iteration 213/234: loss=0.050600 lr=0.000020 grad_norm=0.750776
Epoch 53/100 Iteration 214/234: loss=0.057364 lr=0.000020 grad_norm=0.632563
Epoch 53/100 Iteration 215/234: loss=0.054764 lr=0.000020 grad_norm=0.635070
Epoch 53/100 Iteration 216/234: loss=0.052016 lr=0.000020 grad_norm=0.934406
Epoch 53/100 Iteration 217/234: loss=0.053449 lr=0.000020 grad_norm=0.605535
Epoch 53/100 Iteration 218/234: loss=0.054748 lr=0.000020 grad_norm=0.828605
Epoch 53/100 Iteration 219/234: loss=0.046433 lr=0.000020 grad_norm=1.507149
Epoch 53/100 Iteration 220/234: loss=0.051054 lr=0.000020 grad_norm=1.020053
Epoch 53/100 Iteration 221/234: loss=0.056544 lr=0.000020 grad_norm=0.653258
Epoch 53/100 Iteration 222/234: loss=0.058174 lr=0.000020 grad_norm=1.650915
Epoch 53/100 Iteration 223/234: loss=0.062791 lr=0.000020 grad_norm=1.490715
Epoch 53/100 Iteration 224/234: loss=0.056687 lr=0.000020 grad_norm=0.461514
Epoch 53/100 Iteration 225/234: loss=0.057511 lr=0.000020 grad_norm=1.852684
Epoch 53/100 Iteration 226/234: loss=0.055168 lr=0.000020 grad_norm=1.800139
Epoch 53/100 Iteration 227/234: loss=0.058248 lr=0.000020 grad_norm=0.502555
Epoch 53/100 Iteration 228/234: loss=0.048801 lr=0.000020 grad_norm=1.666779
Epoch 53/100 Iteration 229/234: loss=0.054866 lr=0.000020 grad_norm=0.846743
Epoch 53/100 Iteration 230/234: loss=0.050724 lr=0.000020 grad_norm=1.047461
Epoch 53/100 Iteration 231/234: loss=0.048949 lr=0.000020 grad_norm=1.476415
Epoch 53/100 Iteration 232/234: loss=0.055494 lr=0.000020 grad_norm=0.710558
Epoch 53/100 Iteration 233/234: loss=0.051050 lr=0.000020 grad_norm=0.765531
Epoch 53/100 Iteration 234/234: loss=0.060081 lr=0.000020 grad_norm=0.845644
Epoch 53/100 finished. Avg Loss: 0.052727
Epoch 54/100 Iteration 1/234: loss=0.053726 lr=0.000020 grad_norm=0.483289
Epoch 54/100 Iteration 2/234: loss=0.050734 lr=0.000020 grad_norm=0.616436
Epoch 54/100 Iteration 3/234: loss=0.056206 lr=0.000020 grad_norm=0.622390
Epoch 54/100 Iteration 4/234: loss=0.048746 lr=0.000020 grad_norm=0.374310
Epoch 54/100 Iteration 5/234: loss=0.050301 lr=0.000020 grad_norm=0.529888
Epoch 54/100 Iteration 6/234: loss=0.051236 lr=0.000020 grad_norm=0.762973
Epoch 54/100 Iteration 7/234: loss=0.049359 lr=0.000020 grad_norm=0.586748
Epoch 54/100 Iteration 8/234: loss=0.050066 lr=0.000020 grad_norm=0.661840
Epoch 54/100 Iteration 9/234: loss=0.056578 lr=0.000020 grad_norm=1.417792
Epoch 54/100 Iteration 10/234: loss=0.053046 lr=0.000020 grad_norm=0.908755
Epoch 54/100 Iteration 11/234: loss=0.048735 lr=0.000020 grad_norm=0.535037
Epoch 54/100 Iteration 12/234: loss=0.055379 lr=0.000020 grad_norm=0.874887
Epoch 54/100 Iteration 13/234: loss=0.048010 lr=0.000020 grad_norm=0.682245
Epoch 54/100 Iteration 14/234: loss=0.051746 lr=0.000020 grad_norm=0.727852
Epoch 54/100 Iteration 15/234: loss=0.053166 lr=0.000020 grad_norm=0.874581
Epoch 54/100 Iteration 16/234: loss=0.045804 lr=0.000020 grad_norm=0.615553
Epoch 54/100 Iteration 17/234: loss=0.057557 lr=0.000020 grad_norm=0.563758
Epoch 54/100 Iteration 18/234: loss=0.052189 lr=0.000020 grad_norm=0.673978
Epoch 54/100 Iteration 19/234: loss=0.055597 lr=0.000020 grad_norm=0.713597
Epoch 54/100 Iteration 20/234: loss=0.050422 lr=0.000020 grad_norm=0.437235
Epoch 54/100 Iteration 21/234: loss=0.058579 lr=0.000020 grad_norm=0.503472
Epoch 54/100 Iteration 22/234: loss=0.052740 lr=0.000020 grad_norm=0.821583
Epoch 54/100 Iteration 23/234: loss=0.050052 lr=0.000020 grad_norm=0.821638
Epoch 54/100 Iteration 24/234: loss=0.057790 lr=0.000020 grad_norm=0.534811
Epoch 54/100 Iteration 25/234: loss=0.053967 lr=0.000020 grad_norm=0.790278
Epoch 54/100 Iteration 26/234: loss=0.054563 lr=0.000020 grad_norm=0.859294
Epoch 54/100 Iteration 27/234: loss=0.047278 lr=0.000020 grad_norm=0.472834
Epoch 54/100 Iteration 28/234: loss=0.052060 lr=0.000020 grad_norm=1.095163
Epoch 54/100 Iteration 29/234: loss=0.045199 lr=0.000020 grad_norm=0.856516
Epoch 54/100 Iteration 30/234: loss=0.058414 lr=0.000020 grad_norm=1.044837
Epoch 54/100 Iteration 31/234: loss=0.051219 lr=0.000020 grad_norm=2.054235
Epoch 54/100 Iteration 32/234: loss=0.058570 lr=0.000020 grad_norm=1.594245
Epoch 54/100 Iteration 33/234: loss=0.052044 lr=0.000020 grad_norm=0.558276
Epoch 54/100 Iteration 34/234: loss=0.053834 lr=0.000020 grad_norm=1.121858
Epoch 54/100 Iteration 35/234: loss=0.059559 lr=0.000020 grad_norm=1.249727
Epoch 54/100 Iteration 36/234: loss=0.048929 lr=0.000020 grad_norm=0.824228
Epoch 54/100 Iteration 37/234: loss=0.056701 lr=0.000020 grad_norm=0.712178
Epoch 54/100 Iteration 38/234: loss=0.050157 lr=0.000020 grad_norm=0.782159
Epoch 54/100 Iteration 39/234: loss=0.052675 lr=0.000020 grad_norm=0.801178
Epoch 54/100 Iteration 40/234: loss=0.049152 lr=0.000020 grad_norm=0.687035
Epoch 54/100 Iteration 41/234: loss=0.041690 lr=0.000020 grad_norm=0.500288
Epoch 54/100 Iteration 42/234: loss=0.048563 lr=0.000020 grad_norm=0.646093
Epoch 54/100 Iteration 43/234: loss=0.049579 lr=0.000020 grad_norm=0.648247
Epoch 54/100 Iteration 44/234: loss=0.059782 lr=0.000020 grad_norm=0.684823
Epoch 54/100 Iteration 45/234: loss=0.049196 lr=0.000020 grad_norm=0.459923
Epoch 54/100 Iteration 46/234: loss=0.056618 lr=0.000020 grad_norm=0.731509
Epoch 54/100 Iteration 47/234: loss=0.048863 lr=0.000020 grad_norm=0.966160
Epoch 54/100 Iteration 48/234: loss=0.054925 lr=0.000020 grad_norm=1.192034
Epoch 54/100 Iteration 49/234: loss=0.051189 lr=0.000020 grad_norm=0.807727
Epoch 54/100 Iteration 50/234: loss=0.057167 lr=0.000020 grad_norm=0.466127
Epoch 54/100 Iteration 51/234: loss=0.051736 lr=0.000020 grad_norm=0.696418
Epoch 54/100 Iteration 52/234: loss=0.052862 lr=0.000020 grad_norm=0.674997
Epoch 54/100 Iteration 53/234: loss=0.057888 lr=0.000020 grad_norm=0.479114
Epoch 54/100 Iteration 54/234: loss=0.059629 lr=0.000020 grad_norm=0.478104
Epoch 54/100 Iteration 55/234: loss=0.051352 lr=0.000020 grad_norm=0.445957
Epoch 54/100 Iteration 56/234: loss=0.059472 lr=0.000020 grad_norm=0.661986
Epoch 54/100 Iteration 57/234: loss=0.052111 lr=0.000020 grad_norm=0.972710
Epoch 54/100 Iteration 58/234: loss=0.062653 lr=0.000020 grad_norm=0.634728
Epoch 54/100 Iteration 59/234: loss=0.049467 lr=0.000020 grad_norm=0.734376
Epoch 54/100 Iteration 60/234: loss=0.048683 lr=0.000020 grad_norm=0.859009
Epoch 54/100 Iteration 61/234: loss=0.050684 lr=0.000020 grad_norm=0.409738
Epoch 54/100 Iteration 62/234: loss=0.055698 lr=0.000020 grad_norm=1.013823
Epoch 54/100 Iteration 63/234: loss=0.051788 lr=0.000020 grad_norm=1.094874
Epoch 54/100 Iteration 64/234: loss=0.051294 lr=0.000020 grad_norm=0.761112
Epoch 54/100 Iteration 65/234: loss=0.054685 lr=0.000020 grad_norm=0.658977
Epoch 54/100 Iteration 66/234: loss=0.051185 lr=0.000020 grad_norm=0.732385
Epoch 54/100 Iteration 67/234: loss=0.049644 lr=0.000020 grad_norm=0.429824
Epoch 54/100 Iteration 68/234: loss=0.044390 lr=0.000020 grad_norm=0.695501
Epoch 54/100 Iteration 69/234: loss=0.045606 lr=0.000020 grad_norm=0.749235
Epoch 54/100 Iteration 70/234: loss=0.046701 lr=0.000020 grad_norm=0.648686
Epoch 54/100 Iteration 71/234: loss=0.056045 lr=0.000020 grad_norm=0.626474
Epoch 54/100 Iteration 72/234: loss=0.051357 lr=0.000020 grad_norm=0.551151
Epoch 54/100 Iteration 73/234: loss=0.049997 lr=0.000020 grad_norm=0.694143
Epoch 54/100 Iteration 74/234: loss=0.056879 lr=0.000020 grad_norm=0.833933
Epoch 54/100 Iteration 75/234: loss=0.044692 lr=0.000020 grad_norm=1.102933
Epoch 54/100 Iteration 76/234: loss=0.047046 lr=0.000020 grad_norm=1.342399
Epoch 54/100 Iteration 77/234: loss=0.059788 lr=0.000020 grad_norm=1.367529
Epoch 54/100 Iteration 78/234: loss=0.049792 lr=0.000020 grad_norm=0.838231
Epoch 54/100 Iteration 79/234: loss=0.052508 lr=0.000020 grad_norm=0.990465
Epoch 54/100 Iteration 80/234: loss=0.052755 lr=0.000020 grad_norm=1.391609
Epoch 54/100 Iteration 81/234: loss=0.044847 lr=0.000020 grad_norm=0.697381
Epoch 54/100 Iteration 82/234: loss=0.048323 lr=0.000020 grad_norm=1.390946
Epoch 54/100 Iteration 83/234: loss=0.055879 lr=0.000020 grad_norm=2.708410
Epoch 54/100 Iteration 84/234: loss=0.056056 lr=0.000020 grad_norm=2.173404
Epoch 54/100 Iteration 85/234: loss=0.058122 lr=0.000020 grad_norm=0.552560
Epoch 54/100 Iteration 86/234: loss=0.050424 lr=0.000020 grad_norm=1.484512
Epoch 54/100 Iteration 87/234: loss=0.044021 lr=0.000020 grad_norm=1.115228
Epoch 54/100 Iteration 88/234: loss=0.056272 lr=0.000020 grad_norm=0.855798
Epoch 54/100 Iteration 89/234: loss=0.056701 lr=0.000020 grad_norm=1.255021
Epoch 54/100 Iteration 90/234: loss=0.057441 lr=0.000020 grad_norm=0.846855
Epoch 54/100 Iteration 91/234: loss=0.055245 lr=0.000020 grad_norm=0.718461
Epoch 54/100 Iteration 92/234: loss=0.045091 lr=0.000020 grad_norm=0.854800
Epoch 54/100 Iteration 93/234: loss=0.053665 lr=0.000020 grad_norm=0.641793
Epoch 54/100 Iteration 94/234: loss=0.057704 lr=0.000020 grad_norm=1.032405
Epoch 54/100 Iteration 95/234: loss=0.053404 lr=0.000020 grad_norm=0.759885
Epoch 54/100 Iteration 96/234: loss=0.048479 lr=0.000020 grad_norm=0.487700
Epoch 54/100 Iteration 97/234: loss=0.057954 lr=0.000020 grad_norm=0.631341
Epoch 54/100 Iteration 98/234: loss=0.048969 lr=0.000020 grad_norm=0.614461
Epoch 54/100 Iteration 99/234: loss=0.044242 lr=0.000020 grad_norm=0.462521
Epoch 54/100 Iteration 100/234: loss=0.049981 lr=0.000020 grad_norm=0.579936
Epoch 54/100 Iteration 101/234: loss=0.049069 lr=0.000020 grad_norm=0.750481
Epoch 54/100 Iteration 102/234: loss=0.066109 lr=0.000020 grad_norm=0.913436
Epoch 54/100 Iteration 103/234: loss=0.053614 lr=0.000020 grad_norm=0.649143
Epoch 54/100 Iteration 104/234: loss=0.050725 lr=0.000020 grad_norm=1.089624
Epoch 54/100 Iteration 105/234: loss=0.055308 lr=0.000020 grad_norm=1.303257
Epoch 54/100 Iteration 106/234: loss=0.058847 lr=0.000020 grad_norm=1.404528
Epoch 54/100 Iteration 107/234: loss=0.047188 lr=0.000020 grad_norm=0.770078
Epoch 54/100 Iteration 108/234: loss=0.052792 lr=0.000020 grad_norm=0.804380
Epoch 54/100 Iteration 109/234: loss=0.053021 lr=0.000020 grad_norm=1.203024
Epoch 54/100 Iteration 110/234: loss=0.044055 lr=0.000020 grad_norm=0.942177
Epoch 54/100 Iteration 111/234: loss=0.053574 lr=0.000020 grad_norm=0.611587
Epoch 54/100 Iteration 112/234: loss=0.048078 lr=0.000020 grad_norm=0.940356
Epoch 54/100 Iteration 113/234: loss=0.049033 lr=0.000020 grad_norm=0.588003
Epoch 54/100 Iteration 114/234: loss=0.051355 lr=0.000020 grad_norm=0.595779
Epoch 54/100 Iteration 115/234: loss=0.055588 lr=0.000020 grad_norm=0.790393
Epoch 54/100 Iteration 116/234: loss=0.055113 lr=0.000020 grad_norm=0.829028
Epoch 54/100 Iteration 117/234: loss=0.039689 lr=0.000020 grad_norm=0.521127
Epoch 54/100 Iteration 118/234: loss=0.050853 lr=0.000020 grad_norm=0.825685
Epoch 54/100 Iteration 119/234: loss=0.051870 lr=0.000020 grad_norm=1.319353
Epoch 54/100 Iteration 120/234: loss=0.053689 lr=0.000020 grad_norm=1.230298
Epoch 54/100 Iteration 121/234: loss=0.053978 lr=0.000020 grad_norm=0.814735
Epoch 54/100 Iteration 122/234: loss=0.047828 lr=0.000020 grad_norm=0.625568
Epoch 54/100 Iteration 123/234: loss=0.052367 lr=0.000020 grad_norm=0.811363
Epoch 54/100 Iteration 124/234: loss=0.051619 lr=0.000020 grad_norm=0.526031
Epoch 54/100 Iteration 125/234: loss=0.047590 lr=0.000020 grad_norm=0.711227
Epoch 54/100 Iteration 126/234: loss=0.049715 lr=0.000020 grad_norm=0.490813
Epoch 54/100 Iteration 127/234: loss=0.052946 lr=0.000020 grad_norm=0.594052
Epoch 54/100 Iteration 128/234: loss=0.052828 lr=0.000020 grad_norm=0.603013
Epoch 54/100 Iteration 129/234: loss=0.053788 lr=0.000020 grad_norm=0.666854
Epoch 54/100 Iteration 130/234: loss=0.055633 lr=0.000020 grad_norm=0.830181
Epoch 54/100 Iteration 131/234: loss=0.055385 lr=0.000020 grad_norm=0.771226
Epoch 54/100 Iteration 132/234: loss=0.048170 lr=0.000020 grad_norm=0.493456
Epoch 54/100 Iteration 133/234: loss=0.057070 lr=0.000020 grad_norm=0.798728
Epoch 54/100 Iteration 134/234: loss=0.056685 lr=0.000020 grad_norm=1.138627
Epoch 54/100 Iteration 135/234: loss=0.049346 lr=0.000020 grad_norm=1.129487
Epoch 54/100 Iteration 136/234: loss=0.050609 lr=0.000020 grad_norm=0.632659
Epoch 54/100 Iteration 137/234: loss=0.049436 lr=0.000020 grad_norm=1.078904
Epoch 54/100 Iteration 138/234: loss=0.046907 lr=0.000020 grad_norm=1.750790
Epoch 54/100 Iteration 139/234: loss=0.054421 lr=0.000020 grad_norm=1.251302
Epoch 54/100 Iteration 140/234: loss=0.051546 lr=0.000020 grad_norm=1.065894
Epoch 54/100 Iteration 141/234: loss=0.050269 lr=0.000020 grad_norm=0.680420
Epoch 54/100 Iteration 142/234: loss=0.049426 lr=0.000020 grad_norm=0.857442
Epoch 54/100 Iteration 143/234: loss=0.049516 lr=0.000020 grad_norm=1.415559
Epoch 54/100 Iteration 144/234: loss=0.045595 lr=0.000020 grad_norm=1.116758
Epoch 54/100 Iteration 145/234: loss=0.054142 lr=0.000020 grad_norm=0.795016
Epoch 54/100 Iteration 146/234: loss=0.056599 lr=0.000020 grad_norm=0.941753
Epoch 54/100 Iteration 147/234: loss=0.054066 lr=0.000020 grad_norm=0.516746
Epoch 54/100 Iteration 148/234: loss=0.048229 lr=0.000020 grad_norm=0.546503
Epoch 54/100 Iteration 149/234: loss=0.043084 lr=0.000020 grad_norm=0.747794
Epoch 54/100 Iteration 150/234: loss=0.052821 lr=0.000020 grad_norm=0.612767
Epoch 54/100 Iteration 151/234: loss=0.046714 lr=0.000020 grad_norm=0.782368
Epoch 54/100 Iteration 152/234: loss=0.049275 lr=0.000020 grad_norm=0.563968
Epoch 54/100 Iteration 153/234: loss=0.048121 lr=0.000020 grad_norm=0.989254
Epoch 54/100 Iteration 154/234: loss=0.046410 lr=0.000020 grad_norm=0.947434
Epoch 54/100 Iteration 155/234: loss=0.050551 lr=0.000020 grad_norm=0.582063
Epoch 54/100 Iteration 156/234: loss=0.052176 lr=0.000020 grad_norm=1.446243
Epoch 54/100 Iteration 157/234: loss=0.054761 lr=0.000020 grad_norm=1.510507
Epoch 54/100 Iteration 158/234: loss=0.053312 lr=0.000020 grad_norm=0.436920
Epoch 54/100 Iteration 159/234: loss=0.057551 lr=0.000020 grad_norm=1.598133
Epoch 54/100 Iteration 160/234: loss=0.056109 lr=0.000020 grad_norm=1.768191
Epoch 54/100 Iteration 161/234: loss=0.054287 lr=0.000020 grad_norm=0.453087
Epoch 54/100 Iteration 162/234: loss=0.048942 lr=0.000020 grad_norm=1.161010
Epoch 54/100 Iteration 163/234: loss=0.055787 lr=0.000020 grad_norm=0.845067
Epoch 54/100 Iteration 164/234: loss=0.051218 lr=0.000020 grad_norm=0.946024
Epoch 54/100 Iteration 165/234: loss=0.053211 lr=0.000020 grad_norm=1.123109
Epoch 54/100 Iteration 166/234: loss=0.046327 lr=0.000020 grad_norm=0.584052
Epoch 54/100 Iteration 167/234: loss=0.054202 lr=0.000020 grad_norm=1.013734
Epoch 54/100 Iteration 168/234: loss=0.051243 lr=0.000020 grad_norm=0.896766
Epoch 54/100 Iteration 169/234: loss=0.050562 lr=0.000020 grad_norm=0.532065
Epoch 54/100 Iteration 170/234: loss=0.051338 lr=0.000020 grad_norm=0.705555
Epoch 54/100 Iteration 171/234: loss=0.049545 lr=0.000020 grad_norm=0.683736
Epoch 54/100 Iteration 172/234: loss=0.048061 lr=0.000020 grad_norm=0.555144
Epoch 54/100 Iteration 173/234: loss=0.048429 lr=0.000020 grad_norm=0.934897
Epoch 54/100 Iteration 174/234: loss=0.050493 lr=0.000020 grad_norm=0.603323
Epoch 54/100 Iteration 175/234: loss=0.052257 lr=0.000020 grad_norm=0.998664
Epoch 54/100 Iteration 176/234: loss=0.046484 lr=0.000020 grad_norm=0.800936
Epoch 54/100 Iteration 177/234: loss=0.046515 lr=0.000020 grad_norm=0.734495
Epoch 54/100 Iteration 178/234: loss=0.045562 lr=0.000020 grad_norm=1.349354
Epoch 54/100 Iteration 179/234: loss=0.047602 lr=0.000020 grad_norm=0.835468
Epoch 54/100 Iteration 180/234: loss=0.055382 lr=0.000020 grad_norm=0.786517
Epoch 54/100 Iteration 181/234: loss=0.049264 lr=0.000020 grad_norm=1.035714
Epoch 54/100 Iteration 182/234: loss=0.051901 lr=0.000020 grad_norm=0.625550
Epoch 54/100 Iteration 183/234: loss=0.049008 lr=0.000020 grad_norm=0.934454
Epoch 54/100 Iteration 184/234: loss=0.048883 lr=0.000020 grad_norm=1.050964
Epoch 54/100 Iteration 185/234: loss=0.046319 lr=0.000020 grad_norm=0.804663
Epoch 54/100 Iteration 186/234: loss=0.050384 lr=0.000020 grad_norm=0.633361
Epoch 54/100 Iteration 187/234: loss=0.053352 lr=0.000020 grad_norm=0.684910
Epoch 54/100 Iteration 188/234: loss=0.050753 lr=0.000020 grad_norm=0.602913
Epoch 54/100 Iteration 189/234: loss=0.054540 lr=0.000020 grad_norm=1.032784
Epoch 54/100 Iteration 190/234: loss=0.062295 lr=0.000020 grad_norm=0.871445
Epoch 54/100 Iteration 191/234: loss=0.053411 lr=0.000020 grad_norm=0.359365
Epoch 54/100 Iteration 192/234: loss=0.048783 lr=0.000020 grad_norm=0.657256
Epoch 54/100 Iteration 193/234: loss=0.053030 lr=0.000020 grad_norm=0.551777
Epoch 54/100 Iteration 194/234: loss=0.047426 lr=0.000020 grad_norm=0.393264
Epoch 54/100 Iteration 195/234: loss=0.048161 lr=0.000020 grad_norm=0.507794
Epoch 54/100 Iteration 196/234: loss=0.043828 lr=0.000020 grad_norm=0.661363
Epoch 54/100 Iteration 197/234: loss=0.045499 lr=0.000020 grad_norm=0.547536
Epoch 54/100 Iteration 198/234: loss=0.058516 lr=0.000020 grad_norm=0.781960
Epoch 54/100 Iteration 199/234: loss=0.051456 lr=0.000020 grad_norm=0.815509
Epoch 54/100 Iteration 200/234: loss=0.049984 lr=0.000020 grad_norm=0.797114
Epoch 54/100 Iteration 201/234: loss=0.057795 lr=0.000020 grad_norm=0.835081
Epoch 54/100 Iteration 202/234: loss=0.047257 lr=0.000020 grad_norm=0.493740
Epoch 54/100 Iteration 203/234: loss=0.051812 lr=0.000020 grad_norm=0.453239
Epoch 54/100 Iteration 204/234: loss=0.050774 lr=0.000020 grad_norm=0.666442
Epoch 54/100 Iteration 205/234: loss=0.050546 lr=0.000020 grad_norm=0.721958
Epoch 54/100 Iteration 206/234: loss=0.049308 lr=0.000020 grad_norm=0.537674
Epoch 54/100 Iteration 207/234: loss=0.049821 lr=0.000020 grad_norm=0.493153
Epoch 54/100 Iteration 208/234: loss=0.052653 lr=0.000020 grad_norm=0.949748
Epoch 54/100 Iteration 209/234: loss=0.049812 lr=0.000020 grad_norm=1.008240
Epoch 54/100 Iteration 210/234: loss=0.045564 lr=0.000020 grad_norm=0.659431
Epoch 54/100 Iteration 211/234: loss=0.051299 lr=0.000020 grad_norm=0.530749
Epoch 54/100 Iteration 212/234: loss=0.050605 lr=0.000020 grad_norm=0.771120
Epoch 54/100 Iteration 213/234: loss=0.049465 lr=0.000020 grad_norm=0.802420
Epoch 54/100 Iteration 214/234: loss=0.044432 lr=0.000020 grad_norm=0.509373
Epoch 54/100 Iteration 215/234: loss=0.052640 lr=0.000020 grad_norm=0.589116
Epoch 54/100 Iteration 216/234: loss=0.051001 lr=0.000020 grad_norm=0.738411
Epoch 54/100 Iteration 217/234: loss=0.051530 lr=0.000020 grad_norm=0.621184
Epoch 54/100 Iteration 218/234: loss=0.056260 lr=0.000020 grad_norm=0.652186
Epoch 54/100 Iteration 219/234: loss=0.050313 lr=0.000020 grad_norm=0.852267
Epoch 54/100 Iteration 220/234: loss=0.056524 lr=0.000020 grad_norm=0.708647
Epoch 54/100 Iteration 221/234: loss=0.045867 lr=0.000020 grad_norm=0.399426
Epoch 54/100 Iteration 222/234: loss=0.054882 lr=0.000020 grad_norm=0.584116
Epoch 54/100 Iteration 223/234: loss=0.052697 lr=0.000020 grad_norm=0.771344
Epoch 54/100 Iteration 224/234: loss=0.055085 lr=0.000020 grad_norm=0.729126
Epoch 54/100 Iteration 225/234: loss=0.047780 lr=0.000020 grad_norm=0.632295
Epoch 54/100 Iteration 226/234: loss=0.050393 lr=0.000020 grad_norm=0.456416
Epoch 54/100 Iteration 227/234: loss=0.056287 lr=0.000020 grad_norm=0.798013
Epoch 54/100 Iteration 228/234: loss=0.053755 lr=0.000020 grad_norm=1.180342
Epoch 54/100 Iteration 229/234: loss=0.046686 lr=0.000020 grad_norm=0.800658
Epoch 54/100 Iteration 230/234: loss=0.050714 lr=0.000020 grad_norm=0.566430
Epoch 54/100 Iteration 231/234: loss=0.053334 lr=0.000020 grad_norm=1.187816
Epoch 54/100 Iteration 232/234: loss=0.050895 lr=0.000020 grad_norm=1.532376
Epoch 54/100 Iteration 233/234: loss=0.049926 lr=0.000020 grad_norm=1.063424
Epoch 54/100 Iteration 234/234: loss=0.052550 lr=0.000020 grad_norm=0.549513
Epoch 54/100 finished. Avg Loss: 0.051673
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 55/100 Iteration 1/234: loss=0.051229 lr=0.000020 grad_norm=1.284161
Epoch 55/100 Iteration 2/234: loss=0.048345 lr=0.000020 grad_norm=1.312598
Epoch 55/100 Iteration 3/234: loss=0.050809 lr=0.000020 grad_norm=0.746109
Epoch 55/100 Iteration 4/234: loss=0.051491 lr=0.000020 grad_norm=1.467610
Epoch 55/100 Iteration 5/234: loss=0.049989 lr=0.000020 grad_norm=1.368990
Epoch 55/100 Iteration 6/234: loss=0.050018 lr=0.000020 grad_norm=0.776033
Epoch 55/100 Iteration 7/234: loss=0.053910 lr=0.000020 grad_norm=0.577033
Epoch 55/100 Iteration 8/234: loss=0.045659 lr=0.000020 grad_norm=0.836743
Epoch 55/100 Iteration 9/234: loss=0.048888 lr=0.000020 grad_norm=0.571091
Epoch 55/100 Iteration 10/234: loss=0.051639 lr=0.000020 grad_norm=0.669257
Epoch 55/100 Iteration 11/234: loss=0.044600 lr=0.000020 grad_norm=0.648841
Epoch 55/100 Iteration 12/234: loss=0.045061 lr=0.000020 grad_norm=0.692700
Epoch 55/100 Iteration 13/234: loss=0.048515 lr=0.000020 grad_norm=0.789510
Epoch 55/100 Iteration 14/234: loss=0.041429 lr=0.000020 grad_norm=0.649479
Epoch 55/100 Iteration 15/234: loss=0.057306 lr=0.000020 grad_norm=0.795886
Epoch 55/100 Iteration 16/234: loss=0.051356 lr=0.000020 grad_norm=1.061433
Epoch 55/100 Iteration 17/234: loss=0.044204 lr=0.000020 grad_norm=1.046906
Epoch 55/100 Iteration 18/234: loss=0.052081 lr=0.000020 grad_norm=0.870780
Epoch 55/100 Iteration 19/234: loss=0.063574 lr=0.000020 grad_norm=0.871174
Epoch 55/100 Iteration 20/234: loss=0.051305 lr=0.000020 grad_norm=1.414096
Epoch 55/100 Iteration 21/234: loss=0.057511 lr=0.000020 grad_norm=0.993914
Epoch 55/100 Iteration 22/234: loss=0.051951 lr=0.000020 grad_norm=0.602078
Epoch 55/100 Iteration 23/234: loss=0.051460 lr=0.000020 grad_norm=0.664865
Epoch 55/100 Iteration 24/234: loss=0.057325 lr=0.000020 grad_norm=0.406732
Epoch 55/100 Iteration 25/234: loss=0.046544 lr=0.000020 grad_norm=0.591754
Epoch 55/100 Iteration 26/234: loss=0.048430 lr=0.000020 grad_norm=1.269506
Epoch 55/100 Iteration 27/234: loss=0.056996 lr=0.000020 grad_norm=1.778476
Epoch 55/100 Iteration 28/234: loss=0.052995 lr=0.000020 grad_norm=1.123879
Epoch 55/100 Iteration 29/234: loss=0.051830 lr=0.000020 grad_norm=0.607805
Epoch 55/100 Iteration 30/234: loss=0.052328 lr=0.000020 grad_norm=1.557130
Epoch 55/100 Iteration 31/234: loss=0.056647 lr=0.000020 grad_norm=1.908633
Epoch 55/100 Iteration 32/234: loss=0.057226 lr=0.000020 grad_norm=1.176205
Epoch 55/100 Iteration 33/234: loss=0.053082 lr=0.000020 grad_norm=0.838058
Epoch 55/100 Iteration 34/234: loss=0.046632 lr=0.000020 grad_norm=1.008240
Epoch 55/100 Iteration 35/234: loss=0.051124 lr=0.000020 grad_norm=1.321126
Epoch 55/100 Iteration 36/234: loss=0.046647 lr=0.000020 grad_norm=0.542620
Epoch 55/100 Iteration 37/234: loss=0.048467 lr=0.000020 grad_norm=1.089117
Epoch 55/100 Iteration 38/234: loss=0.049185 lr=0.000020 grad_norm=1.514312
Epoch 55/100 Iteration 39/234: loss=0.048689 lr=0.000020 grad_norm=0.533146
Epoch 55/100 Iteration 40/234: loss=0.048788 lr=0.000020 grad_norm=1.210747
Epoch 55/100 Iteration 41/234: loss=0.055529 lr=0.000020 grad_norm=1.667225
Epoch 55/100 Iteration 42/234: loss=0.049659 lr=0.000020 grad_norm=1.158094
Epoch 55/100 Iteration 43/234: loss=0.057841 lr=0.000020 grad_norm=0.430369
Epoch 55/100 Iteration 44/234: loss=0.057713 lr=0.000020 grad_norm=1.027259
Epoch 55/100 Iteration 45/234: loss=0.049086 lr=0.000020 grad_norm=0.919300
Epoch 55/100 Iteration 46/234: loss=0.055750 lr=0.000020 grad_norm=0.456113
Epoch 55/100 Iteration 47/234: loss=0.055297 lr=0.000020 grad_norm=0.984969
Epoch 55/100 Iteration 48/234: loss=0.052019 lr=0.000020 grad_norm=1.038762
Epoch 55/100 Iteration 49/234: loss=0.058493 lr=0.000020 grad_norm=0.575176
Epoch 55/100 Iteration 50/234: loss=0.042451 lr=0.000020 grad_norm=0.465037
Epoch 55/100 Iteration 51/234: loss=0.049436 lr=0.000020 grad_norm=0.498305
Epoch 55/100 Iteration 52/234: loss=0.046430 lr=0.000020 grad_norm=0.347911
Epoch 55/100 Iteration 53/234: loss=0.047322 lr=0.000020 grad_norm=0.640901
Epoch 55/100 Iteration 54/234: loss=0.055105 lr=0.000020 grad_norm=0.627752
Epoch 55/100 Iteration 55/234: loss=0.050210 lr=0.000020 grad_norm=0.494517
Epoch 55/100 Iteration 56/234: loss=0.051484 lr=0.000020 grad_norm=0.664838
Epoch 55/100 Iteration 57/234: loss=0.045852 lr=0.000020 grad_norm=1.194245
Epoch 55/100 Iteration 58/234: loss=0.051532 lr=0.000020 grad_norm=1.142242
Epoch 55/100 Iteration 59/234: loss=0.045714 lr=0.000020 grad_norm=0.487962
Epoch 55/100 Iteration 60/234: loss=0.050885 lr=0.000020 grad_norm=0.976446
Epoch 55/100 Iteration 61/234: loss=0.050795 lr=0.000020 grad_norm=1.747990
Epoch 55/100 Iteration 62/234: loss=0.053141 lr=0.000020 grad_norm=1.752983
Epoch 55/100 Iteration 63/234: loss=0.051574 lr=0.000020 grad_norm=0.858861
Epoch 55/100 Iteration 64/234: loss=0.046653 lr=0.000020 grad_norm=0.515203
Epoch 55/100 Iteration 65/234: loss=0.052174 lr=0.000020 grad_norm=0.764502
Epoch 55/100 Iteration 66/234: loss=0.054577 lr=0.000020 grad_norm=0.503011
Epoch 55/100 Iteration 67/234: loss=0.051176 lr=0.000020 grad_norm=0.725324
Epoch 55/100 Iteration 68/234: loss=0.056670 lr=0.000020 grad_norm=1.627255
Epoch 55/100 Iteration 69/234: loss=0.051659 lr=0.000020 grad_norm=2.001262
Epoch 55/100 Iteration 70/234: loss=0.058591 lr=0.000020 grad_norm=1.712839
Epoch 55/100 Iteration 71/234: loss=0.046716 lr=0.000020 grad_norm=0.759894
Epoch 55/100 Iteration 72/234: loss=0.048998 lr=0.000020 grad_norm=0.802115
Epoch 55/100 Iteration 73/234: loss=0.054494 lr=0.000020 grad_norm=1.026428
Epoch 55/100 Iteration 74/234: loss=0.049775 lr=0.000020 grad_norm=0.393755
Epoch 55/100 Iteration 75/234: loss=0.043692 lr=0.000020 grad_norm=1.094714
Epoch 55/100 Iteration 76/234: loss=0.045292 lr=0.000020 grad_norm=0.902601
Epoch 55/100 Iteration 77/234: loss=0.050798 lr=0.000020 grad_norm=0.543204
Epoch 55/100 Iteration 78/234: loss=0.049099 lr=0.000020 grad_norm=0.959798
Epoch 55/100 Iteration 79/234: loss=0.046407 lr=0.000020 grad_norm=0.716123
Epoch 55/100 Iteration 80/234: loss=0.054214 lr=0.000020 grad_norm=0.899675
Epoch 55/100 Iteration 81/234: loss=0.052494 lr=0.000020 grad_norm=1.273789
Epoch 55/100 Iteration 82/234: loss=0.054407 lr=0.000020 grad_norm=1.031398
Epoch 55/100 Iteration 83/234: loss=0.049610 lr=0.000020 grad_norm=0.528623
Epoch 55/100 Iteration 84/234: loss=0.055599 lr=0.000020 grad_norm=0.955288
Epoch 55/100 Iteration 85/234: loss=0.050264 lr=0.000020 grad_norm=0.941256
Epoch 55/100 Iteration 86/234: loss=0.048106 lr=0.000020 grad_norm=0.656391
Epoch 55/100 Iteration 87/234: loss=0.053118 lr=0.000020 grad_norm=1.009567
Epoch 55/100 Iteration 88/234: loss=0.049177 lr=0.000020 grad_norm=1.046275
Epoch 55/100 Iteration 89/234: loss=0.046556 lr=0.000020 grad_norm=0.709891
Epoch 55/100 Iteration 90/234: loss=0.052562 lr=0.000020 grad_norm=0.873991
Epoch 55/100 Iteration 91/234: loss=0.051765 lr=0.000020 grad_norm=0.926417
Epoch 55/100 Iteration 92/234: loss=0.050892 lr=0.000020 grad_norm=0.893467
Epoch 55/100 Iteration 93/234: loss=0.048877 lr=0.000020 grad_norm=0.839758
Epoch 55/100 Iteration 94/234: loss=0.050712 lr=0.000020 grad_norm=0.940220
Epoch 55/100 Iteration 95/234: loss=0.047172 lr=0.000020 grad_norm=0.477099
Epoch 55/100 Iteration 96/234: loss=0.059168 lr=0.000020 grad_norm=1.372478
Epoch 55/100 Iteration 97/234: loss=0.049240 lr=0.000020 grad_norm=2.213988
Epoch 55/100 Iteration 98/234: loss=0.050852 lr=0.000020 grad_norm=1.489516
Epoch 55/100 Iteration 99/234: loss=0.058654 lr=0.000020 grad_norm=1.384978
Epoch 55/100 Iteration 100/234: loss=0.050262 lr=0.000020 grad_norm=2.421785
Epoch 55/100 Iteration 101/234: loss=0.046650 lr=0.000020 grad_norm=0.893519
Epoch 55/100 Iteration 102/234: loss=0.051164 lr=0.000020 grad_norm=1.366750
Epoch 55/100 Iteration 103/234: loss=0.050588 lr=0.000020 grad_norm=1.686935
Epoch 55/100 Iteration 104/234: loss=0.050735 lr=0.000020 grad_norm=0.499497
Epoch 55/100 Iteration 105/234: loss=0.051968 lr=0.000020 grad_norm=0.954012
Epoch 55/100 Iteration 106/234: loss=0.047720 lr=0.000020 grad_norm=0.595642
Epoch 55/100 Iteration 107/234: loss=0.044386 lr=0.000020 grad_norm=0.617605
Epoch 55/100 Iteration 108/234: loss=0.054001 lr=0.000020 grad_norm=0.905441
Epoch 55/100 Iteration 109/234: loss=0.047754 lr=0.000020 grad_norm=0.538691
Epoch 55/100 Iteration 110/234: loss=0.049600 lr=0.000020 grad_norm=1.177811
Epoch 55/100 Iteration 111/234: loss=0.047157 lr=0.000020 grad_norm=1.372530
Epoch 55/100 Iteration 112/234: loss=0.046437 lr=0.000020 grad_norm=0.442749
Epoch 55/100 Iteration 113/234: loss=0.050143 lr=0.000020 grad_norm=1.632927
Epoch 55/100 Iteration 114/234: loss=0.049854 lr=0.000020 grad_norm=0.688149
Epoch 55/100 Iteration 115/234: loss=0.046069 lr=0.000020 grad_norm=1.744394
Epoch 55/100 Iteration 116/234: loss=0.052454 lr=0.000020 grad_norm=1.641910
Epoch 55/100 Iteration 117/234: loss=0.051480 lr=0.000020 grad_norm=0.931883
Epoch 55/100 Iteration 118/234: loss=0.046650 lr=0.000020 grad_norm=1.942698
Epoch 55/100 Iteration 119/234: loss=0.047915 lr=0.000020 grad_norm=0.641304
Epoch 55/100 Iteration 120/234: loss=0.045901 lr=0.000020 grad_norm=1.609971
Epoch 55/100 Iteration 121/234: loss=0.043706 lr=0.000020 grad_norm=1.472251
Epoch 55/100 Iteration 122/234: loss=0.049398 lr=0.000020 grad_norm=0.848684
Epoch 55/100 Iteration 123/234: loss=0.053154 lr=0.000020 grad_norm=1.688929
Epoch 55/100 Iteration 124/234: loss=0.050881 lr=0.000020 grad_norm=0.463957
Epoch 55/100 Iteration 125/234: loss=0.051334 lr=0.000020 grad_norm=1.326935
Epoch 55/100 Iteration 126/234: loss=0.046633 lr=0.000020 grad_norm=1.038422
Epoch 55/100 Iteration 127/234: loss=0.051959 lr=0.000020 grad_norm=0.932714
Epoch 55/100 Iteration 128/234: loss=0.053361 lr=0.000020 grad_norm=1.564380
Epoch 55/100 Iteration 129/234: loss=0.050981 lr=0.000020 grad_norm=0.680662
Epoch 55/100 Iteration 130/234: loss=0.045947 lr=0.000020 grad_norm=0.602998
Epoch 55/100 Iteration 131/234: loss=0.057904 lr=0.000020 grad_norm=0.502604
Epoch 55/100 Iteration 132/234: loss=0.044292 lr=0.000020 grad_norm=0.668961
Epoch 55/100 Iteration 133/234: loss=0.057309 lr=0.000020 grad_norm=1.047025
Epoch 55/100 Iteration 134/234: loss=0.047061 lr=0.000020 grad_norm=0.679763
Epoch 55/100 Iteration 135/234: loss=0.046107 lr=0.000020 grad_norm=0.667599
Epoch 55/100 Iteration 136/234: loss=0.044521 lr=0.000020 grad_norm=0.449513
Epoch 55/100 Iteration 137/234: loss=0.055298 lr=0.000020 grad_norm=0.959841
Epoch 55/100 Iteration 138/234: loss=0.047483 lr=0.000020 grad_norm=0.922801
Epoch 55/100 Iteration 139/234: loss=0.048863 lr=0.000020 grad_norm=0.831007
Epoch 55/100 Iteration 140/234: loss=0.049083 lr=0.000020 grad_norm=1.035923
Epoch 55/100 Iteration 141/234: loss=0.055039 lr=0.000020 grad_norm=0.629188
Epoch 55/100 Iteration 142/234: loss=0.050702 lr=0.000020 grad_norm=1.748671
Epoch 55/100 Iteration 143/234: loss=0.049531 lr=0.000020 grad_norm=1.396153
Epoch 55/100 Iteration 144/234: loss=0.047898 lr=0.000020 grad_norm=0.654139
Epoch 55/100 Iteration 145/234: loss=0.049306 lr=0.000020 grad_norm=1.607195
Epoch 55/100 Iteration 146/234: loss=0.052656 lr=0.000020 grad_norm=1.274156
Epoch 55/100 Iteration 147/234: loss=0.046851 lr=0.000020 grad_norm=0.908131
Epoch 55/100 Iteration 148/234: loss=0.051116 lr=0.000020 grad_norm=0.688949
Epoch 55/100 Iteration 149/234: loss=0.049131 lr=0.000020 grad_norm=0.826623
Epoch 55/100 Iteration 150/234: loss=0.047345 lr=0.000020 grad_norm=0.625902
Epoch 55/100 Iteration 151/234: loss=0.052275 lr=0.000020 grad_norm=0.723757
Epoch 55/100 Iteration 152/234: loss=0.054548 lr=0.000020 grad_norm=0.941940
Epoch 55/100 Iteration 153/234: loss=0.056267 lr=0.000020 grad_norm=0.753154
Epoch 55/100 Iteration 154/234: loss=0.052622 lr=0.000020 grad_norm=0.367578
Epoch 55/100 Iteration 155/234: loss=0.053469 lr=0.000020 grad_norm=0.749172
Epoch 55/100 Iteration 156/234: loss=0.050922 lr=0.000020 grad_norm=1.214875
Epoch 55/100 Iteration 157/234: loss=0.048240 lr=0.000020 grad_norm=0.606445
Epoch 55/100 Iteration 158/234: loss=0.048466 lr=0.000020 grad_norm=0.717459
Epoch 55/100 Iteration 159/234: loss=0.055208 lr=0.000020 grad_norm=1.001903
Epoch 55/100 Iteration 160/234: loss=0.048276 lr=0.000020 grad_norm=0.749255
Epoch 55/100 Iteration 161/234: loss=0.040677 lr=0.000020 grad_norm=0.475948
Epoch 55/100 Iteration 162/234: loss=0.053915 lr=0.000020 grad_norm=0.461558
Epoch 55/100 Iteration 163/234: loss=0.052395 lr=0.000020 grad_norm=0.423295
Epoch 55/100 Iteration 164/234: loss=0.046447 lr=0.000020 grad_norm=0.394334
Epoch 55/100 Iteration 165/234: loss=0.050504 lr=0.000020 grad_norm=0.546660
Epoch 55/100 Iteration 166/234: loss=0.042178 lr=0.000020 grad_norm=0.490906
Epoch 55/100 Iteration 167/234: loss=0.051172 lr=0.000020 grad_norm=0.507619
Epoch 55/100 Iteration 168/234: loss=0.047137 lr=0.000020 grad_norm=0.741666
Epoch 55/100 Iteration 169/234: loss=0.046698 lr=0.000020 grad_norm=1.160072
Epoch 55/100 Iteration 170/234: loss=0.050528 lr=0.000020 grad_norm=0.668704
Epoch 55/100 Iteration 171/234: loss=0.050177 lr=0.000020 grad_norm=0.941702
Epoch 55/100 Iteration 172/234: loss=0.045197 lr=0.000020 grad_norm=0.738277
Epoch 55/100 Iteration 173/234: loss=0.050856 lr=0.000020 grad_norm=0.703458
Epoch 55/100 Iteration 174/234: loss=0.050432 lr=0.000020 grad_norm=1.237644
Epoch 55/100 Iteration 175/234: loss=0.045508 lr=0.000020 grad_norm=0.595864
Epoch 55/100 Iteration 176/234: loss=0.044578 lr=0.000020 grad_norm=0.669280
Epoch 55/100 Iteration 177/234: loss=0.048254 lr=0.000020 grad_norm=0.753398
Epoch 55/100 Iteration 178/234: loss=0.053336 lr=0.000020 grad_norm=0.431748
Epoch 55/100 Iteration 179/234: loss=0.042766 lr=0.000020 grad_norm=0.820359
Epoch 55/100 Iteration 180/234: loss=0.046894 lr=0.000020 grad_norm=0.839184
Epoch 55/100 Iteration 181/234: loss=0.055128 lr=0.000020 grad_norm=0.775093
Epoch 55/100 Iteration 182/234: loss=0.046564 lr=0.000020 grad_norm=0.952127
Epoch 55/100 Iteration 183/234: loss=0.046778 lr=0.000020 grad_norm=0.845348
Epoch 55/100 Iteration 184/234: loss=0.048916 lr=0.000020 grad_norm=0.608746
Epoch 55/100 Iteration 185/234: loss=0.055599 lr=0.000020 grad_norm=0.914899
Epoch 55/100 Iteration 186/234: loss=0.052258 lr=0.000020 grad_norm=0.830303
Epoch 55/100 Iteration 187/234: loss=0.054374 lr=0.000020 grad_norm=0.540495
Epoch 55/100 Iteration 188/234: loss=0.050670 lr=0.000020 grad_norm=0.915112
Epoch 55/100 Iteration 189/234: loss=0.053724 lr=0.000020 grad_norm=0.946821
Epoch 55/100 Iteration 190/234: loss=0.052011 lr=0.000020 grad_norm=2.406382
Epoch 55/100 Iteration 191/234: loss=0.049822 lr=0.000020 grad_norm=2.903184
Epoch 55/100 Iteration 192/234: loss=0.047621 lr=0.000020 grad_norm=0.882501
Epoch 55/100 Iteration 193/234: loss=0.047684 lr=0.000020 grad_norm=1.645844
Epoch 55/100 Iteration 194/234: loss=0.052659 lr=0.000020 grad_norm=1.390749
Epoch 55/100 Iteration 195/234: loss=0.049432 lr=0.000020 grad_norm=0.921440
Epoch 55/100 Iteration 196/234: loss=0.051019 lr=0.000020 grad_norm=1.894853
Epoch 55/100 Iteration 197/234: loss=0.052118 lr=0.000020 grad_norm=1.045663
Epoch 55/100 Iteration 198/234: loss=0.048739 lr=0.000020 grad_norm=1.259394
Epoch 55/100 Iteration 199/234: loss=0.044759 lr=0.000020 grad_norm=0.923652
Epoch 55/100 Iteration 200/234: loss=0.050887 lr=0.000020 grad_norm=0.807719
Epoch 55/100 Iteration 201/234: loss=0.052042 lr=0.000020 grad_norm=0.982969
Epoch 55/100 Iteration 202/234: loss=0.049169 lr=0.000020 grad_norm=0.781020
Epoch 55/100 Iteration 203/234: loss=0.049186 lr=0.000020 grad_norm=0.664646
Epoch 55/100 Iteration 204/234: loss=0.059897 lr=0.000020 grad_norm=0.690953
Epoch 55/100 Iteration 205/234: loss=0.048300 lr=0.000020 grad_norm=0.592478
Epoch 55/100 Iteration 206/234: loss=0.047798 lr=0.000020 grad_norm=0.423746
Epoch 55/100 Iteration 207/234: loss=0.052470 lr=0.000020 grad_norm=0.735121
Epoch 55/100 Iteration 208/234: loss=0.047276 lr=0.000020 grad_norm=0.722768
Epoch 55/100 Iteration 209/234: loss=0.055298 lr=0.000020 grad_norm=0.445683
Epoch 55/100 Iteration 210/234: loss=0.042329 lr=0.000020 grad_norm=0.524675
Epoch 55/100 Iteration 211/234: loss=0.056927 lr=0.000020 grad_norm=0.651499
Epoch 55/100 Iteration 212/234: loss=0.047284 lr=0.000020 grad_norm=0.729936
Epoch 55/100 Iteration 213/234: loss=0.044308 lr=0.000020 grad_norm=0.955942
Epoch 55/100 Iteration 214/234: loss=0.057437 lr=0.000020 grad_norm=0.820819
Epoch 55/100 Iteration 215/234: loss=0.051707 lr=0.000020 grad_norm=0.426069
Epoch 55/100 Iteration 216/234: loss=0.047188 lr=0.000020 grad_norm=0.681961
Epoch 55/100 Iteration 217/234: loss=0.050714 lr=0.000020 grad_norm=0.856047
Epoch 55/100 Iteration 218/234: loss=0.052934 lr=0.000020 grad_norm=0.605407
Epoch 55/100 Iteration 219/234: loss=0.050263 lr=0.000020 grad_norm=0.484851
Epoch 55/100 Iteration 220/234: loss=0.050001 lr=0.000020 grad_norm=0.670420
Epoch 55/100 Iteration 221/234: loss=0.048674 lr=0.000020 grad_norm=0.722935
Epoch 55/100 Iteration 222/234: loss=0.045550 lr=0.000020 grad_norm=0.659116
Epoch 55/100 Iteration 223/234: loss=0.055863 lr=0.000020 grad_norm=0.567532
Epoch 55/100 Iteration 224/234: loss=0.053052 lr=0.000020 grad_norm=0.585920
Epoch 55/100 Iteration 225/234: loss=0.053653 lr=0.000020 grad_norm=1.014606
Epoch 55/100 Iteration 226/234: loss=0.052382 lr=0.000020 grad_norm=1.175447
Epoch 55/100 Iteration 227/234: loss=0.047561 lr=0.000020 grad_norm=0.930992
Epoch 55/100 Iteration 228/234: loss=0.054003 lr=0.000020 grad_norm=0.636151
Epoch 55/100 Iteration 229/234: loss=0.053530 lr=0.000020 grad_norm=0.551723
Epoch 55/100 Iteration 230/234: loss=0.059147 lr=0.000020 grad_norm=0.596235
Epoch 55/100 Iteration 231/234: loss=0.048916 lr=0.000020 grad_norm=0.522951
Epoch 55/100 Iteration 232/234: loss=0.047546 lr=0.000020 grad_norm=0.630537
Epoch 55/100 Iteration 233/234: loss=0.052727 lr=0.000020 grad_norm=0.702645
Epoch 55/100 Iteration 234/234: loss=0.047654 lr=0.000020 grad_norm=0.674198
Epoch 55/100 finished. Avg Loss: 0.050453
Epoch 56/100 Iteration 1/234: loss=0.050281 lr=0.000020 grad_norm=0.825425
Epoch 56/100 Iteration 2/234: loss=0.050812 lr=0.000020 grad_norm=0.766664
Epoch 56/100 Iteration 3/234: loss=0.052949 lr=0.000020 grad_norm=0.883717
Epoch 56/100 Iteration 4/234: loss=0.045979 lr=0.000020 grad_norm=1.215975
Epoch 56/100 Iteration 5/234: loss=0.044504 lr=0.000020 grad_norm=0.822543
Epoch 56/100 Iteration 6/234: loss=0.054448 lr=0.000020 grad_norm=0.492190
Epoch 56/100 Iteration 7/234: loss=0.045997 lr=0.000020 grad_norm=0.771434
Epoch 56/100 Iteration 8/234: loss=0.046910 lr=0.000020 grad_norm=0.769331
Epoch 56/100 Iteration 9/234: loss=0.053270 lr=0.000020 grad_norm=0.484350
Epoch 56/100 Iteration 10/234: loss=0.051224 lr=0.000020 grad_norm=0.755583
Epoch 56/100 Iteration 11/234: loss=0.054802 lr=0.000020 grad_norm=0.947374
Epoch 56/100 Iteration 12/234: loss=0.051040 lr=0.000020 grad_norm=1.153325
Epoch 56/100 Iteration 13/234: loss=0.053638 lr=0.000020 grad_norm=0.777684
Epoch 56/100 Iteration 14/234: loss=0.052162 lr=0.000020 grad_norm=0.652974
Epoch 56/100 Iteration 15/234: loss=0.055393 lr=0.000020 grad_norm=1.200061
Epoch 56/100 Iteration 16/234: loss=0.047533 lr=0.000020 grad_norm=1.406585
Epoch 56/100 Iteration 17/234: loss=0.050325 lr=0.000020 grad_norm=0.829424
Epoch 56/100 Iteration 18/234: loss=0.055860 lr=0.000020 grad_norm=1.295108
Epoch 56/100 Iteration 19/234: loss=0.044624 lr=0.000020 grad_norm=2.293871
Epoch 56/100 Iteration 20/234: loss=0.046008 lr=0.000020 grad_norm=1.597983
Epoch 56/100 Iteration 21/234: loss=0.049324 lr=0.000020 grad_norm=0.826801
Epoch 56/100 Iteration 22/234: loss=0.049870 lr=0.000020 grad_norm=1.933354
Epoch 56/100 Iteration 23/234: loss=0.048748 lr=0.000020 grad_norm=1.147290
Epoch 56/100 Iteration 24/234: loss=0.050909 lr=0.000020 grad_norm=1.264487
Epoch 56/100 Iteration 25/234: loss=0.050120 lr=0.000020 grad_norm=1.708042
Epoch 56/100 Iteration 26/234: loss=0.049956 lr=0.000020 grad_norm=1.111208
Epoch 56/100 Iteration 27/234: loss=0.049159 lr=0.000020 grad_norm=1.042613
Epoch 56/100 Iteration 28/234: loss=0.051167 lr=0.000020 grad_norm=0.762088
Epoch 56/100 Iteration 29/234: loss=0.042368 lr=0.000020 grad_norm=1.064663
Epoch 56/100 Iteration 30/234: loss=0.053582 lr=0.000020 grad_norm=0.737576
Epoch 56/100 Iteration 31/234: loss=0.052032 lr=0.000020 grad_norm=0.885125
Epoch 56/100 Iteration 32/234: loss=0.050104 lr=0.000020 grad_norm=1.279681
Epoch 56/100 Iteration 33/234: loss=0.048729 lr=0.000020 grad_norm=0.753063
Epoch 56/100 Iteration 34/234: loss=0.055780 lr=0.000020 grad_norm=0.875607
Epoch 56/100 Iteration 35/234: loss=0.048706 lr=0.000020 grad_norm=1.336995
Epoch 56/100 Iteration 36/234: loss=0.051387 lr=0.000020 grad_norm=0.955081
Epoch 56/100 Iteration 37/234: loss=0.048089 lr=0.000020 grad_norm=1.057339
Epoch 56/100 Iteration 38/234: loss=0.052867 lr=0.000020 grad_norm=1.057463
Epoch 56/100 Iteration 39/234: loss=0.049738 lr=0.000020 grad_norm=0.644835
Epoch 56/100 Iteration 40/234: loss=0.053933 lr=0.000020 grad_norm=0.893274
Epoch 56/100 Iteration 41/234: loss=0.050787 lr=0.000020 grad_norm=1.086662
Epoch 56/100 Iteration 42/234: loss=0.046340 lr=0.000020 grad_norm=0.802516
Epoch 56/100 Iteration 43/234: loss=0.045317 lr=0.000020 grad_norm=1.163361
Epoch 56/100 Iteration 44/234: loss=0.047137 lr=0.000020 grad_norm=0.815297
Epoch 56/100 Iteration 45/234: loss=0.055834 lr=0.000020 grad_norm=0.606418
Epoch 56/100 Iteration 46/234: loss=0.053309 lr=0.000020 grad_norm=0.728501
Epoch 56/100 Iteration 47/234: loss=0.051886 lr=0.000020 grad_norm=0.917826
Epoch 56/100 Iteration 48/234: loss=0.056707 lr=0.000020 grad_norm=0.794269
Epoch 56/100 Iteration 49/234: loss=0.055213 lr=0.000020 grad_norm=0.549763
Epoch 56/100 Iteration 50/234: loss=0.051325 lr=0.000020 grad_norm=1.402969
Epoch 56/100 Iteration 51/234: loss=0.051656 lr=0.000020 grad_norm=2.731049
Epoch 56/100 Iteration 52/234: loss=0.048792 lr=0.000020 grad_norm=2.819836
Epoch 56/100 Iteration 53/234: loss=0.059202 lr=0.000020 grad_norm=0.796175
Epoch 56/100 Iteration 54/234: loss=0.049837 lr=0.000020 grad_norm=2.449461
Epoch 56/100 Iteration 55/234: loss=0.045723 lr=0.000020 grad_norm=1.143201
Epoch 56/100 Iteration 56/234: loss=0.045438 lr=0.000020 grad_norm=1.572292
Epoch 56/100 Iteration 57/234: loss=0.052194 lr=0.000020 grad_norm=1.155427
Epoch 56/100 Iteration 58/234: loss=0.059935 lr=0.000020 grad_norm=1.915058
Epoch 56/100 Iteration 59/234: loss=0.048385 lr=0.000020 grad_norm=1.489484
Epoch 56/100 Iteration 60/234: loss=0.045186 lr=0.000020 grad_norm=1.021954
Epoch 56/100 Iteration 61/234: loss=0.059034 lr=0.000020 grad_norm=1.357686
Epoch 56/100 Iteration 62/234: loss=0.050405 lr=0.000020 grad_norm=0.855331
Epoch 56/100 Iteration 63/234: loss=0.043767 lr=0.000020 grad_norm=1.331300
Epoch 56/100 Iteration 64/234: loss=0.047549 lr=0.000020 grad_norm=0.400971
Epoch 56/100 Iteration 65/234: loss=0.046561 lr=0.000020 grad_norm=1.164653
Epoch 56/100 Iteration 66/234: loss=0.047938 lr=0.000020 grad_norm=0.555435
Epoch 56/100 Iteration 67/234: loss=0.049587 lr=0.000020 grad_norm=1.021040
Epoch 56/100 Iteration 68/234: loss=0.053534 lr=0.000020 grad_norm=1.260681
Epoch 56/100 Iteration 69/234: loss=0.050720 lr=0.000020 grad_norm=0.523025
Epoch 56/100 Iteration 70/234: loss=0.043736 lr=0.000020 grad_norm=0.750143
Epoch 56/100 Iteration 71/234: loss=0.056008 lr=0.000020 grad_norm=0.589776
Epoch 56/100 Iteration 72/234: loss=0.051077 lr=0.000020 grad_norm=0.498048
Epoch 56/100 Iteration 73/234: loss=0.047587 lr=0.000020 grad_norm=0.755823
Epoch 56/100 Iteration 74/234: loss=0.046510 lr=0.000020 grad_norm=0.752711
Epoch 56/100 Iteration 75/234: loss=0.048309 lr=0.000020 grad_norm=0.698357
Epoch 56/100 Iteration 76/234: loss=0.048783 lr=0.000020 grad_norm=0.786305
Epoch 56/100 Iteration 77/234: loss=0.045734 lr=0.000020 grad_norm=1.170144
Epoch 56/100 Iteration 78/234: loss=0.049254 lr=0.000020 grad_norm=0.870177
Epoch 56/100 Iteration 79/234: loss=0.045863 lr=0.000020 grad_norm=0.493063
Epoch 56/100 Iteration 80/234: loss=0.051321 lr=0.000020 grad_norm=1.332525
Epoch 56/100 Iteration 81/234: loss=0.046519 lr=0.000020 grad_norm=0.825456
Epoch 56/100 Iteration 82/234: loss=0.046241 lr=0.000020 grad_norm=0.872604
Epoch 56/100 Iteration 83/234: loss=0.057601 lr=0.000020 grad_norm=1.452357
Epoch 56/100 Iteration 84/234: loss=0.050768 lr=0.000020 grad_norm=0.738557
Epoch 56/100 Iteration 85/234: loss=0.051695 lr=0.000020 grad_norm=0.781430
Epoch 56/100 Iteration 86/234: loss=0.050062 lr=0.000020 grad_norm=1.022752
Epoch 56/100 Iteration 87/234: loss=0.051204 lr=0.000020 grad_norm=0.454018
Epoch 56/100 Iteration 88/234: loss=0.045429 lr=0.000020 grad_norm=0.987709
Epoch 56/100 Iteration 89/234: loss=0.048420 lr=0.000020 grad_norm=0.674208
Epoch 56/100 Iteration 90/234: loss=0.047791 lr=0.000020 grad_norm=0.697250
Epoch 56/100 Iteration 91/234: loss=0.054135 lr=0.000020 grad_norm=0.645485
Epoch 56/100 Iteration 92/234: loss=0.049547 lr=0.000020 grad_norm=0.818126
Epoch 56/100 Iteration 93/234: loss=0.048309 lr=0.000020 grad_norm=0.453585
Epoch 56/100 Iteration 94/234: loss=0.048966 lr=0.000020 grad_norm=1.128548
Epoch 56/100 Iteration 95/234: loss=0.049709 lr=0.000020 grad_norm=1.038560
Epoch 56/100 Iteration 96/234: loss=0.053255 lr=0.000020 grad_norm=0.389512
Epoch 56/100 Iteration 97/234: loss=0.050709 lr=0.000020 grad_norm=0.667711
Epoch 56/100 Iteration 98/234: loss=0.055791 lr=0.000020 grad_norm=0.748608
Epoch 56/100 Iteration 99/234: loss=0.051115 lr=0.000020 grad_norm=0.675764
Epoch 56/100 Iteration 100/234: loss=0.062519 lr=0.000020 grad_norm=0.722939
Epoch 56/100 Iteration 101/234: loss=0.056519 lr=0.000020 grad_norm=0.597248
Epoch 56/100 Iteration 102/234: loss=0.053940 lr=0.000020 grad_norm=0.627959
Epoch 56/100 Iteration 103/234: loss=0.054407 lr=0.000020 grad_norm=0.928078
Epoch 56/100 Iteration 104/234: loss=0.054495 lr=0.000020 grad_norm=1.685832
Epoch 56/100 Iteration 105/234: loss=0.048783 lr=0.000020 grad_norm=1.667363
Epoch 56/100 Iteration 106/234: loss=0.049878 lr=0.000020 grad_norm=0.472627
Epoch 56/100 Iteration 107/234: loss=0.052245 lr=0.000020 grad_norm=1.283950
Epoch 56/100 Iteration 108/234: loss=0.057698 lr=0.000020 grad_norm=1.145421
Epoch 56/100 Iteration 109/234: loss=0.048999 lr=0.000020 grad_norm=0.726414
Epoch 56/100 Iteration 110/234: loss=0.046789 lr=0.000020 grad_norm=0.981580
Epoch 56/100 Iteration 111/234: loss=0.043286 lr=0.000020 grad_norm=0.861469
Epoch 56/100 Iteration 112/234: loss=0.051946 lr=0.000020 grad_norm=0.642837
Epoch 56/100 Iteration 113/234: loss=0.044738 lr=0.000020 grad_norm=0.637968
Epoch 56/100 Iteration 114/234: loss=0.053513 lr=0.000020 grad_norm=0.671633
Epoch 56/100 Iteration 115/234: loss=0.040793 lr=0.000020 grad_norm=0.363138
Epoch 56/100 Iteration 116/234: loss=0.059068 lr=0.000020 grad_norm=0.720175
Epoch 56/100 Iteration 117/234: loss=0.052301 lr=0.000020 grad_norm=0.832170
Epoch 56/100 Iteration 118/234: loss=0.047369 lr=0.000020 grad_norm=0.792887
Epoch 56/100 Iteration 119/234: loss=0.041053 lr=0.000020 grad_norm=0.624685
Epoch 56/100 Iteration 120/234: loss=0.051974 lr=0.000020 grad_norm=0.667100
Epoch 56/100 Iteration 121/234: loss=0.050051 lr=0.000020 grad_norm=1.053207
Epoch 56/100 Iteration 122/234: loss=0.052095 lr=0.000020 grad_norm=0.601433
Epoch 56/100 Iteration 123/234: loss=0.048577 lr=0.000020 grad_norm=1.006303
Epoch 56/100 Iteration 124/234: loss=0.052796 lr=0.000020 grad_norm=1.753870
Epoch 56/100 Iteration 125/234: loss=0.047767 lr=0.000020 grad_norm=1.208423
Epoch 56/100 Iteration 126/234: loss=0.044616 lr=0.000020 grad_norm=0.614800
Epoch 56/100 Iteration 127/234: loss=0.048839 lr=0.000020 grad_norm=1.175261
Epoch 56/100 Iteration 128/234: loss=0.053800 lr=0.000020 grad_norm=0.555529
Epoch 56/100 Iteration 129/234: loss=0.051540 lr=0.000020 grad_norm=0.601958
Epoch 56/100 Iteration 130/234: loss=0.045568 lr=0.000020 grad_norm=1.141665
Epoch 56/100 Iteration 131/234: loss=0.046706 lr=0.000020 grad_norm=0.578454
Epoch 56/100 Iteration 132/234: loss=0.050724 lr=0.000020 grad_norm=0.936252
Epoch 56/100 Iteration 133/234: loss=0.047728 lr=0.000020 grad_norm=1.016032
Epoch 56/100 Iteration 134/234: loss=0.052084 lr=0.000020 grad_norm=0.486346
Epoch 56/100 Iteration 135/234: loss=0.049223 lr=0.000020 grad_norm=0.825165
Epoch 56/100 Iteration 136/234: loss=0.045420 lr=0.000020 grad_norm=0.565972
Epoch 56/100 Iteration 137/234: loss=0.051947 lr=0.000020 grad_norm=0.984557
Epoch 56/100 Iteration 138/234: loss=0.049969 lr=0.000020 grad_norm=1.279179
Epoch 56/100 Iteration 139/234: loss=0.043313 lr=0.000020 grad_norm=0.667372
Epoch 56/100 Iteration 140/234: loss=0.056663 lr=0.000020 grad_norm=1.020287
Epoch 56/100 Iteration 141/234: loss=0.047493 lr=0.000020 grad_norm=1.400176
Epoch 56/100 Iteration 142/234: loss=0.046837 lr=0.000020 grad_norm=0.845993
Epoch 56/100 Iteration 143/234: loss=0.047263 lr=0.000020 grad_norm=0.566325
Epoch 56/100 Iteration 144/234: loss=0.053870 lr=0.000020 grad_norm=0.643045
Epoch 56/100 Iteration 145/234: loss=0.044783 lr=0.000020 grad_norm=0.758808
Epoch 56/100 Iteration 146/234: loss=0.046269 lr=0.000020 grad_norm=0.649527
Epoch 56/100 Iteration 147/234: loss=0.044318 lr=0.000020 grad_norm=0.645923
Epoch 56/100 Iteration 148/234: loss=0.043390 lr=0.000020 grad_norm=0.453506
Epoch 56/100 Iteration 149/234: loss=0.047986 lr=0.000020 grad_norm=0.779815
Epoch 56/100 Iteration 150/234: loss=0.058412 lr=0.000020 grad_norm=0.859572
Epoch 56/100 Iteration 151/234: loss=0.047946 lr=0.000020 grad_norm=0.417434
Epoch 56/100 Iteration 152/234: loss=0.054799 lr=0.000020 grad_norm=0.601906
Epoch 56/100 Iteration 153/234: loss=0.048898 lr=0.000020 grad_norm=0.638552
Epoch 56/100 Iteration 154/234: loss=0.049082 lr=0.000020 grad_norm=0.539748
Epoch 56/100 Iteration 155/234: loss=0.052670 lr=0.000020 grad_norm=1.045838
Epoch 56/100 Iteration 156/234: loss=0.050764 lr=0.000020 grad_norm=0.703732
Epoch 56/100 Iteration 157/234: loss=0.049657 lr=0.000020 grad_norm=0.763095
Epoch 56/100 Iteration 158/234: loss=0.041840 lr=0.000020 grad_norm=1.347965
Epoch 56/100 Iteration 159/234: loss=0.054767 lr=0.000020 grad_norm=0.910790
Epoch 56/100 Iteration 160/234: loss=0.046575 lr=0.000020 grad_norm=0.526421
Epoch 56/100 Iteration 161/234: loss=0.047901 lr=0.000020 grad_norm=1.168581
Epoch 56/100 Iteration 162/234: loss=0.053993 lr=0.000020 grad_norm=0.852217
Epoch 56/100 Iteration 163/234: loss=0.054762 lr=0.000020 grad_norm=0.833795
Epoch 56/100 Iteration 164/234: loss=0.058698 lr=0.000020 grad_norm=1.476861
Epoch 56/100 Iteration 165/234: loss=0.057183 lr=0.000020 grad_norm=1.449667
Epoch 56/100 Iteration 166/234: loss=0.052198 lr=0.000020 grad_norm=1.004773
Epoch 56/100 Iteration 167/234: loss=0.058169 lr=0.000020 grad_norm=0.685282
Epoch 56/100 Iteration 168/234: loss=0.056908 lr=0.000020 grad_norm=0.508068
Epoch 56/100 Iteration 169/234: loss=0.048243 lr=0.000020 grad_norm=0.636408
Epoch 56/100 Iteration 170/234: loss=0.051271 lr=0.000020 grad_norm=0.807324
Epoch 56/100 Iteration 171/234: loss=0.042159 lr=0.000020 grad_norm=0.584791
Epoch 56/100 Iteration 172/234: loss=0.049355 lr=0.000020 grad_norm=0.857430
Epoch 56/100 Iteration 173/234: loss=0.044184 lr=0.000020 grad_norm=1.043682
Epoch 56/100 Iteration 174/234: loss=0.051264 lr=0.000020 grad_norm=0.848069
Epoch 56/100 Iteration 175/234: loss=0.053598 lr=0.000020 grad_norm=0.976238
Epoch 56/100 Iteration 176/234: loss=0.051477 lr=0.000020 grad_norm=1.051940
Epoch 56/100 Iteration 177/234: loss=0.051130 lr=0.000020 grad_norm=1.121488
Epoch 56/100 Iteration 178/234: loss=0.052576 lr=0.000020 grad_norm=0.864437
Epoch 56/100 Iteration 179/234: loss=0.053424 lr=0.000020 grad_norm=0.416918
Epoch 56/100 Iteration 180/234: loss=0.052371 lr=0.000020 grad_norm=1.153162
Epoch 56/100 Iteration 181/234: loss=0.048484 lr=0.000020 grad_norm=1.642039
Epoch 56/100 Iteration 182/234: loss=0.057157 lr=0.000020 grad_norm=1.504773
Epoch 56/100 Iteration 183/234: loss=0.043533 lr=0.000020 grad_norm=0.874135
Epoch 56/100 Iteration 184/234: loss=0.045100 lr=0.000020 grad_norm=0.487681
Epoch 56/100 Iteration 185/234: loss=0.051148 lr=0.000020 grad_norm=0.567676
Epoch 56/100 Iteration 186/234: loss=0.048569 lr=0.000020 grad_norm=0.599129
Epoch 56/100 Iteration 187/234: loss=0.050918 lr=0.000020 grad_norm=0.426843
Epoch 56/100 Iteration 188/234: loss=0.048538 lr=0.000020 grad_norm=0.553727
Epoch 56/100 Iteration 189/234: loss=0.049371 lr=0.000020 grad_norm=0.807386
Epoch 56/100 Iteration 190/234: loss=0.048677 lr=0.000020 grad_norm=0.498343
Epoch 56/100 Iteration 191/234: loss=0.044148 lr=0.000020 grad_norm=0.503364
Epoch 56/100 Iteration 192/234: loss=0.050244 lr=0.000020 grad_norm=0.560428
Epoch 56/100 Iteration 193/234: loss=0.049648 lr=0.000020 grad_norm=0.585502
Epoch 56/100 Iteration 194/234: loss=0.056876 lr=0.000020 grad_norm=1.730896
Epoch 56/100 Iteration 195/234: loss=0.054067 lr=0.000020 grad_norm=1.733158
Epoch 56/100 Iteration 196/234: loss=0.049279 lr=0.000020 grad_norm=0.448882
Epoch 56/100 Iteration 197/234: loss=0.049716 lr=0.000020 grad_norm=1.483336
Epoch 56/100 Iteration 198/234: loss=0.051212 lr=0.000020 grad_norm=1.830726
Epoch 56/100 Iteration 199/234: loss=0.049720 lr=0.000020 grad_norm=0.812243
Epoch 56/100 Iteration 200/234: loss=0.048539 lr=0.000020 grad_norm=1.155657
Epoch 56/100 Iteration 201/234: loss=0.047232 lr=0.000020 grad_norm=1.189963
Epoch 56/100 Iteration 202/234: loss=0.046730 lr=0.000020 grad_norm=0.648411
Epoch 56/100 Iteration 203/234: loss=0.051196 lr=0.000020 grad_norm=1.533814
Epoch 56/100 Iteration 204/234: loss=0.047299 lr=0.000020 grad_norm=1.098339
Epoch 56/100 Iteration 205/234: loss=0.045173 lr=0.000020 grad_norm=0.960463
Epoch 56/100 Iteration 206/234: loss=0.052888 lr=0.000020 grad_norm=1.130632
Epoch 56/100 Iteration 207/234: loss=0.044778 lr=0.000020 grad_norm=0.766823
Epoch 56/100 Iteration 208/234: loss=0.051353 lr=0.000020 grad_norm=0.724260
Epoch 56/100 Iteration 209/234: loss=0.052274 lr=0.000020 grad_norm=1.117333
Epoch 56/100 Iteration 210/234: loss=0.054139 lr=0.000020 grad_norm=1.387113
Epoch 56/100 Iteration 211/234: loss=0.049113 lr=0.000020 grad_norm=0.963383
Epoch 56/100 Iteration 212/234: loss=0.048330 lr=0.000020 grad_norm=0.961989
Epoch 56/100 Iteration 213/234: loss=0.050455 lr=0.000020 grad_norm=1.409977
Epoch 56/100 Iteration 214/234: loss=0.050205 lr=0.000020 grad_norm=1.104452
Epoch 56/100 Iteration 215/234: loss=0.049968 lr=0.000020 grad_norm=1.393798
Epoch 56/100 Iteration 216/234: loss=0.052172 lr=0.000020 grad_norm=2.336086
Epoch 56/100 Iteration 217/234: loss=0.045906 lr=0.000020 grad_norm=2.041561
Epoch 56/100 Iteration 218/234: loss=0.045234 lr=0.000020 grad_norm=0.699790
Epoch 56/100 Iteration 219/234: loss=0.055936 lr=0.000020 grad_norm=1.357058
Epoch 56/100 Iteration 220/234: loss=0.047447 lr=0.000020 grad_norm=1.555888
Epoch 56/100 Iteration 221/234: loss=0.049071 lr=0.000020 grad_norm=0.728816
Epoch 56/100 Iteration 222/234: loss=0.059324 lr=0.000020 grad_norm=1.902945
Epoch 56/100 Iteration 223/234: loss=0.047733 lr=0.000020 grad_norm=1.782943
Epoch 56/100 Iteration 224/234: loss=0.052799 lr=0.000020 grad_norm=0.541136
Epoch 56/100 Iteration 225/234: loss=0.049734 lr=0.000020 grad_norm=1.517144
Epoch 56/100 Iteration 226/234: loss=0.044655 lr=0.000020 grad_norm=0.757275
Epoch 56/100 Iteration 227/234: loss=0.048155 lr=0.000020 grad_norm=1.476008
Epoch 56/100 Iteration 228/234: loss=0.050463 lr=0.000020 grad_norm=1.226615
Epoch 56/100 Iteration 229/234: loss=0.054381 lr=0.000020 grad_norm=0.778484
Epoch 56/100 Iteration 230/234: loss=0.045646 lr=0.000020 grad_norm=1.429866
Epoch 56/100 Iteration 231/234: loss=0.051034 lr=0.000020 grad_norm=0.853646
Epoch 56/100 Iteration 232/234: loss=0.046229 lr=0.000020 grad_norm=1.076243
Epoch 56/100 Iteration 233/234: loss=0.055777 lr=0.000020 grad_norm=1.180421
Epoch 56/100 Iteration 234/234: loss=0.046919 lr=0.000020 grad_norm=0.618008
Epoch 56/100 finished. Avg Loss: 0.050171
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 57/100 Iteration 1/234: loss=0.048087 lr=0.000020 grad_norm=1.162199
Epoch 57/100 Iteration 2/234: loss=0.048341 lr=0.000020 grad_norm=1.087047
Epoch 57/100 Iteration 3/234: loss=0.050936 lr=0.000020 grad_norm=0.562226
Epoch 57/100 Iteration 4/234: loss=0.047561 lr=0.000020 grad_norm=0.734020
Epoch 57/100 Iteration 5/234: loss=0.049920 lr=0.000020 grad_norm=0.774272
Epoch 57/100 Iteration 6/234: loss=0.058061 lr=0.000020 grad_norm=0.475552
Epoch 57/100 Iteration 7/234: loss=0.049056 lr=0.000020 grad_norm=0.728315
Epoch 57/100 Iteration 8/234: loss=0.046305 lr=0.000020 grad_norm=0.886524
Epoch 57/100 Iteration 9/234: loss=0.047690 lr=0.000020 grad_norm=0.672816
Epoch 57/100 Iteration 10/234: loss=0.051882 lr=0.000020 grad_norm=0.540752
Epoch 57/100 Iteration 11/234: loss=0.050659 lr=0.000020 grad_norm=0.607784
Epoch 57/100 Iteration 12/234: loss=0.051874 lr=0.000020 grad_norm=0.858247
Epoch 57/100 Iteration 13/234: loss=0.043871 lr=0.000020 grad_norm=0.865628
Epoch 57/100 Iteration 14/234: loss=0.046743 lr=0.000020 grad_norm=0.621924
Epoch 57/100 Iteration 15/234: loss=0.050067 lr=0.000020 grad_norm=0.611548
Epoch 57/100 Iteration 16/234: loss=0.053943 lr=0.000020 grad_norm=0.982670
Epoch 57/100 Iteration 17/234: loss=0.047487 lr=0.000020 grad_norm=0.657446
Epoch 57/100 Iteration 18/234: loss=0.038829 lr=0.000020 grad_norm=0.526385
Epoch 57/100 Iteration 19/234: loss=0.056593 lr=0.000020 grad_norm=1.178009
Epoch 57/100 Iteration 20/234: loss=0.045290 lr=0.000020 grad_norm=1.170996
Epoch 57/100 Iteration 21/234: loss=0.049681 lr=0.000020 grad_norm=0.656651
Epoch 57/100 Iteration 22/234: loss=0.046857 lr=0.000020 grad_norm=0.405497
Epoch 57/100 Iteration 23/234: loss=0.050866 lr=0.000020 grad_norm=0.446017
Epoch 57/100 Iteration 24/234: loss=0.053440 lr=0.000020 grad_norm=0.792110
Epoch 57/100 Iteration 25/234: loss=0.051873 lr=0.000020 grad_norm=1.121198
Epoch 57/100 Iteration 26/234: loss=0.049863 lr=0.000020 grad_norm=0.741668
Epoch 57/100 Iteration 27/234: loss=0.052578 lr=0.000020 grad_norm=0.571093
Epoch 57/100 Iteration 28/234: loss=0.047486 lr=0.000020 grad_norm=1.257696
Epoch 57/100 Iteration 29/234: loss=0.053460 lr=0.000020 grad_norm=1.027469
Epoch 57/100 Iteration 30/234: loss=0.048486 lr=0.000020 grad_norm=0.403129
Epoch 57/100 Iteration 31/234: loss=0.046512 lr=0.000020 grad_norm=0.647379
Epoch 57/100 Iteration 32/234: loss=0.054241 lr=0.000020 grad_norm=1.113550
Epoch 57/100 Iteration 33/234: loss=0.053476 lr=0.000020 grad_norm=0.900344
Epoch 57/100 Iteration 34/234: loss=0.051172 lr=0.000020 grad_norm=0.565206
Epoch 57/100 Iteration 35/234: loss=0.050192 lr=0.000020 grad_norm=0.491006
Epoch 57/100 Iteration 36/234: loss=0.044616 lr=0.000020 grad_norm=0.783107
Epoch 57/100 Iteration 37/234: loss=0.046201 lr=0.000020 grad_norm=0.444043
Epoch 57/100 Iteration 38/234: loss=0.046880 lr=0.000020 grad_norm=0.450822
Epoch 57/100 Iteration 39/234: loss=0.050698 lr=0.000020 grad_norm=0.564510
Epoch 57/100 Iteration 40/234: loss=0.046573 lr=0.000020 grad_norm=0.481122
Epoch 57/100 Iteration 41/234: loss=0.040103 lr=0.000020 grad_norm=0.327539
Epoch 57/100 Iteration 42/234: loss=0.051581 lr=0.000020 grad_norm=0.465314
Epoch 57/100 Iteration 43/234: loss=0.044342 lr=0.000020 grad_norm=0.490977
Epoch 57/100 Iteration 44/234: loss=0.056983 lr=0.000020 grad_norm=0.594616
Epoch 57/100 Iteration 45/234: loss=0.050276 lr=0.000020 grad_norm=0.349659
Epoch 57/100 Iteration 46/234: loss=0.051718 lr=0.000020 grad_norm=0.612193
Epoch 57/100 Iteration 47/234: loss=0.050883 lr=0.000020 grad_norm=0.866775
Epoch 57/100 Iteration 48/234: loss=0.047078 lr=0.000020 grad_norm=0.642675
Epoch 57/100 Iteration 49/234: loss=0.047276 lr=0.000020 grad_norm=0.508073
Epoch 57/100 Iteration 50/234: loss=0.051016 lr=0.000020 grad_norm=1.178775
Epoch 57/100 Iteration 51/234: loss=0.053503 lr=0.000020 grad_norm=1.067592
Epoch 57/100 Iteration 52/234: loss=0.046737 lr=0.000020 grad_norm=0.366543
Epoch 57/100 Iteration 53/234: loss=0.050209 lr=0.000020 grad_norm=1.463697
Epoch 57/100 Iteration 54/234: loss=0.048200 lr=0.000020 grad_norm=1.825506
Epoch 57/100 Iteration 55/234: loss=0.050671 lr=0.000020 grad_norm=0.639149
Epoch 57/100 Iteration 56/234: loss=0.052443 lr=0.000020 grad_norm=1.205560
Epoch 57/100 Iteration 57/234: loss=0.054358 lr=0.000020 grad_norm=1.398512
Epoch 57/100 Iteration 58/234: loss=0.049771 lr=0.000020 grad_norm=0.445617
Epoch 57/100 Iteration 59/234: loss=0.047093 lr=0.000020 grad_norm=1.381884
Epoch 57/100 Iteration 60/234: loss=0.052988 lr=0.000020 grad_norm=1.152007
Epoch 57/100 Iteration 61/234: loss=0.052726 lr=0.000020 grad_norm=0.538845
Epoch 57/100 Iteration 62/234: loss=0.052745 lr=0.000020 grad_norm=0.923674
Epoch 57/100 Iteration 63/234: loss=0.053614 lr=0.000020 grad_norm=0.761232
Epoch 57/100 Iteration 64/234: loss=0.042451 lr=0.000020 grad_norm=0.420504
Epoch 57/100 Iteration 65/234: loss=0.047609 lr=0.000020 grad_norm=0.955165
Epoch 57/100 Iteration 66/234: loss=0.051022 lr=0.000020 grad_norm=1.007208
Epoch 57/100 Iteration 67/234: loss=0.044655 lr=0.000020 grad_norm=0.660009
Epoch 57/100 Iteration 68/234: loss=0.051795 lr=0.000020 grad_norm=0.399821
Epoch 57/100 Iteration 69/234: loss=0.050033 lr=0.000020 grad_norm=0.578452
Epoch 57/100 Iteration 70/234: loss=0.052729 lr=0.000020 grad_norm=0.535007
Epoch 57/100 Iteration 71/234: loss=0.057057 lr=0.000020 grad_norm=0.485992
Epoch 57/100 Iteration 72/234: loss=0.047417 lr=0.000020 grad_norm=0.393914
Epoch 57/100 Iteration 73/234: loss=0.042897 lr=0.000020 grad_norm=0.393995
Epoch 57/100 Iteration 74/234: loss=0.051847 lr=0.000020 grad_norm=0.711356
Epoch 57/100 Iteration 75/234: loss=0.050984 lr=0.000020 grad_norm=0.943373
Epoch 57/100 Iteration 76/234: loss=0.051750 lr=0.000020 grad_norm=0.973764
Epoch 57/100 Iteration 77/234: loss=0.044809 lr=0.000020 grad_norm=0.720396
Epoch 57/100 Iteration 78/234: loss=0.051156 lr=0.000020 grad_norm=0.532409
Epoch 57/100 Iteration 79/234: loss=0.041552 lr=0.000020 grad_norm=0.676885
Epoch 57/100 Iteration 80/234: loss=0.047606 lr=0.000020 grad_norm=0.532849
Epoch 57/100 Iteration 81/234: loss=0.052093 lr=0.000020 grad_norm=0.702233
Epoch 57/100 Iteration 82/234: loss=0.053427 lr=0.000020 grad_norm=0.384160
Epoch 57/100 Iteration 83/234: loss=0.043525 lr=0.000020 grad_norm=1.083956
Epoch 57/100 Iteration 84/234: loss=0.043902 lr=0.000020 grad_norm=1.663750
Epoch 57/100 Iteration 85/234: loss=0.055081 lr=0.000020 grad_norm=0.799373
Epoch 57/100 Iteration 86/234: loss=0.043564 lr=0.000020 grad_norm=0.780531
Epoch 57/100 Iteration 87/234: loss=0.048048 lr=0.000020 grad_norm=1.179735
Epoch 57/100 Iteration 88/234: loss=0.047181 lr=0.000020 grad_norm=0.826719
Epoch 57/100 Iteration 89/234: loss=0.047004 lr=0.000020 grad_norm=0.442043
Epoch 57/100 Iteration 90/234: loss=0.040700 lr=0.000020 grad_norm=0.666527
Epoch 57/100 Iteration 91/234: loss=0.053335 lr=0.000020 grad_norm=0.441842
Epoch 57/100 Iteration 92/234: loss=0.045665 lr=0.000020 grad_norm=0.550848
Epoch 57/100 Iteration 93/234: loss=0.053551 lr=0.000020 grad_norm=0.577050
Epoch 57/100 Iteration 94/234: loss=0.048832 lr=0.000020 grad_norm=0.507347
Epoch 57/100 Iteration 95/234: loss=0.045043 lr=0.000020 grad_norm=0.502220
Epoch 57/100 Iteration 96/234: loss=0.041794 lr=0.000020 grad_norm=0.720183
Epoch 57/100 Iteration 97/234: loss=0.051320 lr=0.000020 grad_norm=0.439634
Epoch 57/100 Iteration 98/234: loss=0.048501 lr=0.000020 grad_norm=0.770665
Epoch 57/100 Iteration 99/234: loss=0.047133 lr=0.000020 grad_norm=0.626000
Epoch 57/100 Iteration 100/234: loss=0.050657 lr=0.000020 grad_norm=0.466077
Epoch 57/100 Iteration 101/234: loss=0.052714 lr=0.000020 grad_norm=0.462042
Epoch 57/100 Iteration 102/234: loss=0.049708 lr=0.000020 grad_norm=0.443026
Epoch 57/100 Iteration 103/234: loss=0.042299 lr=0.000020 grad_norm=0.553199
Epoch 57/100 Iteration 104/234: loss=0.044805 lr=0.000020 grad_norm=0.570579
Epoch 57/100 Iteration 105/234: loss=0.047457 lr=0.000020 grad_norm=0.507790
Epoch 57/100 Iteration 106/234: loss=0.044877 lr=0.000020 grad_norm=0.722414
Epoch 57/100 Iteration 107/234: loss=0.049150 lr=0.000020 grad_norm=1.095205
Epoch 57/100 Iteration 108/234: loss=0.046904 lr=0.000020 grad_norm=1.586492
Epoch 57/100 Iteration 109/234: loss=0.048865 lr=0.000020 grad_norm=1.456572
Epoch 57/100 Iteration 110/234: loss=0.047501 lr=0.000020 grad_norm=0.596987
Epoch 57/100 Iteration 111/234: loss=0.051622 lr=0.000020 grad_norm=0.816561
Epoch 57/100 Iteration 112/234: loss=0.043451 lr=0.000020 grad_norm=0.425442
Epoch 57/100 Iteration 113/234: loss=0.049992 lr=0.000020 grad_norm=0.757986
Epoch 57/100 Iteration 114/234: loss=0.053243 lr=0.000020 grad_norm=0.664150
Epoch 57/100 Iteration 115/234: loss=0.047918 lr=0.000020 grad_norm=0.568995
Epoch 57/100 Iteration 116/234: loss=0.040613 lr=0.000020 grad_norm=0.522294
Epoch 57/100 Iteration 117/234: loss=0.051085 lr=0.000020 grad_norm=0.596126
Epoch 57/100 Iteration 118/234: loss=0.055877 lr=0.000020 grad_norm=0.577172
Epoch 57/100 Iteration 119/234: loss=0.051411 lr=0.000020 grad_norm=0.818958
Epoch 57/100 Iteration 120/234: loss=0.051184 lr=0.000020 grad_norm=0.772360
Epoch 57/100 Iteration 121/234: loss=0.044936 lr=0.000020 grad_norm=0.499027
Epoch 57/100 Iteration 122/234: loss=0.046923 lr=0.000020 grad_norm=1.331439
Epoch 57/100 Iteration 123/234: loss=0.051542 lr=0.000020 grad_norm=1.868072
Epoch 57/100 Iteration 124/234: loss=0.053929 lr=0.000020 grad_norm=1.734605
Epoch 57/100 Iteration 125/234: loss=0.052025 lr=0.000020 grad_norm=1.594585
Epoch 57/100 Iteration 126/234: loss=0.052954 lr=0.000020 grad_norm=1.322267
Epoch 57/100 Iteration 127/234: loss=0.047683 lr=0.000020 grad_norm=0.643867
Epoch 57/100 Iteration 128/234: loss=0.048881 lr=0.000020 grad_norm=0.671153
Epoch 57/100 Iteration 129/234: loss=0.049059 lr=0.000020 grad_norm=1.250003
Epoch 57/100 Iteration 130/234: loss=0.051062 lr=0.000020 grad_norm=1.070212
Epoch 57/100 Iteration 131/234: loss=0.050841 lr=0.000020 grad_norm=1.034054
Epoch 57/100 Iteration 132/234: loss=0.048557 lr=0.000020 grad_norm=1.167532
Epoch 57/100 Iteration 133/234: loss=0.051236 lr=0.000020 grad_norm=0.637127
Epoch 57/100 Iteration 134/234: loss=0.048604 lr=0.000020 grad_norm=0.811368
Epoch 57/100 Iteration 135/234: loss=0.042827 lr=0.000020 grad_norm=1.200561
Epoch 57/100 Iteration 136/234: loss=0.051857 lr=0.000020 grad_norm=0.831482
Epoch 57/100 Iteration 137/234: loss=0.049477 lr=0.000020 grad_norm=0.664258
Epoch 57/100 Iteration 138/234: loss=0.041539 lr=0.000020 grad_norm=0.908006
Epoch 57/100 Iteration 139/234: loss=0.045398 lr=0.000020 grad_norm=0.735183
Epoch 57/100 Iteration 140/234: loss=0.046525 lr=0.000020 grad_norm=0.635495
Epoch 57/100 Iteration 141/234: loss=0.047714 lr=0.000020 grad_norm=0.803832
Epoch 57/100 Iteration 142/234: loss=0.049418 lr=0.000020 grad_norm=1.101649
Epoch 57/100 Iteration 143/234: loss=0.051885 lr=0.000020 grad_norm=1.105079
Epoch 57/100 Iteration 144/234: loss=0.050510 lr=0.000020 grad_norm=0.829432
Epoch 57/100 Iteration 145/234: loss=0.055357 lr=0.000020 grad_norm=0.612773
Epoch 57/100 Iteration 146/234: loss=0.052803 lr=0.000020 grad_norm=0.622357
Epoch 57/100 Iteration 147/234: loss=0.051768 lr=0.000020 grad_norm=0.658617
Epoch 57/100 Iteration 148/234: loss=0.047554 lr=0.000020 grad_norm=0.685329
Epoch 57/100 Iteration 149/234: loss=0.050333 lr=0.000020 grad_norm=0.663120
Epoch 57/100 Iteration 150/234: loss=0.045554 lr=0.000020 grad_norm=0.452368
Epoch 57/100 Iteration 151/234: loss=0.051570 lr=0.000020 grad_norm=0.563587
Epoch 57/100 Iteration 152/234: loss=0.044509 lr=0.000020 grad_norm=0.681361
Epoch 57/100 Iteration 153/234: loss=0.048776 lr=0.000020 grad_norm=0.781936
Epoch 57/100 Iteration 154/234: loss=0.050248 lr=0.000020 grad_norm=0.569040
Epoch 57/100 Iteration 155/234: loss=0.048880 lr=0.000020 grad_norm=0.620386
Epoch 57/100 Iteration 156/234: loss=0.049751 lr=0.000020 grad_norm=0.930847
Epoch 57/100 Iteration 157/234: loss=0.041671 lr=0.000020 grad_norm=0.579463
Epoch 57/100 Iteration 158/234: loss=0.048960 lr=0.000020 grad_norm=0.543142
Epoch 57/100 Iteration 159/234: loss=0.048998 lr=0.000020 grad_norm=0.833627
Epoch 57/100 Iteration 160/234: loss=0.045672 lr=0.000020 grad_norm=0.478781
Epoch 57/100 Iteration 161/234: loss=0.051485 lr=0.000020 grad_norm=0.753814
Epoch 57/100 Iteration 162/234: loss=0.049985 lr=0.000020 grad_norm=0.622836
Epoch 57/100 Iteration 163/234: loss=0.045033 lr=0.000020 grad_norm=0.481532
Epoch 57/100 Iteration 164/234: loss=0.055190 lr=0.000020 grad_norm=0.594528
Epoch 57/100 Iteration 165/234: loss=0.044882 lr=0.000020 grad_norm=0.578071
Epoch 57/100 Iteration 166/234: loss=0.052925 lr=0.000020 grad_norm=0.600801
Epoch 57/100 Iteration 167/234: loss=0.047723 lr=0.000020 grad_norm=0.584158
Epoch 57/100 Iteration 168/234: loss=0.053907 lr=0.000020 grad_norm=0.682259
Epoch 57/100 Iteration 169/234: loss=0.043476 lr=0.000020 grad_norm=0.716780
Epoch 57/100 Iteration 170/234: loss=0.043159 lr=0.000020 grad_norm=0.872108
Epoch 57/100 Iteration 171/234: loss=0.046541 lr=0.000020 grad_norm=0.701342
Epoch 57/100 Iteration 172/234: loss=0.051265 lr=0.000020 grad_norm=0.921587
Epoch 57/100 Iteration 173/234: loss=0.049845 lr=0.000020 grad_norm=1.104662
Epoch 57/100 Iteration 174/234: loss=0.053292 lr=0.000020 grad_norm=0.482411
Epoch 57/100 Iteration 175/234: loss=0.047630 lr=0.000020 grad_norm=1.167178
Epoch 57/100 Iteration 176/234: loss=0.041919 lr=0.000020 grad_norm=1.307083
Epoch 57/100 Iteration 177/234: loss=0.053305 lr=0.000020 grad_norm=0.461719
Epoch 57/100 Iteration 178/234: loss=0.052369 lr=0.000020 grad_norm=1.049427
Epoch 57/100 Iteration 179/234: loss=0.043440 lr=0.000020 grad_norm=1.044012
Epoch 57/100 Iteration 180/234: loss=0.045469 lr=0.000020 grad_norm=0.421532
Epoch 57/100 Iteration 181/234: loss=0.044564 lr=0.000020 grad_norm=1.166389
Epoch 57/100 Iteration 182/234: loss=0.052581 lr=0.000020 grad_norm=1.540218
Epoch 57/100 Iteration 183/234: loss=0.050672 lr=0.000020 grad_norm=1.210788
Epoch 57/100 Iteration 184/234: loss=0.040108 lr=0.000020 grad_norm=0.354210
Epoch 57/100 Iteration 185/234: loss=0.044816 lr=0.000020 grad_norm=0.849915
Epoch 57/100 Iteration 186/234: loss=0.043046 lr=0.000020 grad_norm=0.637583
Epoch 57/100 Iteration 187/234: loss=0.046535 lr=0.000020 grad_norm=0.632933
Epoch 57/100 Iteration 188/234: loss=0.045933 lr=0.000020 grad_norm=0.891080
Epoch 57/100 Iteration 189/234: loss=0.055303 lr=0.000020 grad_norm=0.397223
Epoch 57/100 Iteration 190/234: loss=0.045379 lr=0.000020 grad_norm=0.821306
Epoch 57/100 Iteration 191/234: loss=0.054667 lr=0.000020 grad_norm=1.090759
Epoch 57/100 Iteration 192/234: loss=0.055577 lr=0.000020 grad_norm=1.269616
Epoch 57/100 Iteration 193/234: loss=0.044105 lr=0.000020 grad_norm=1.039125
Epoch 57/100 Iteration 194/234: loss=0.049862 lr=0.000020 grad_norm=0.788465
Epoch 57/100 Iteration 195/234: loss=0.043143 lr=0.000020 grad_norm=1.378373
Epoch 57/100 Iteration 196/234: loss=0.049134 lr=0.000020 grad_norm=1.340520
Epoch 57/100 Iteration 197/234: loss=0.050410 lr=0.000020 grad_norm=1.070947
Epoch 57/100 Iteration 198/234: loss=0.054467 lr=0.000020 grad_norm=1.148163
Epoch 57/100 Iteration 199/234: loss=0.051714 lr=0.000020 grad_norm=1.254558
Epoch 57/100 Iteration 200/234: loss=0.047093 lr=0.000020 grad_norm=1.075223
Epoch 57/100 Iteration 201/234: loss=0.050330 lr=0.000020 grad_norm=0.669282
Epoch 57/100 Iteration 202/234: loss=0.044982 lr=0.000020 grad_norm=0.647387
Epoch 57/100 Iteration 203/234: loss=0.047067 lr=0.000020 grad_norm=0.431511
Epoch 57/100 Iteration 204/234: loss=0.039643 lr=0.000020 grad_norm=0.392506
Epoch 57/100 Iteration 205/234: loss=0.042749 lr=0.000020 grad_norm=0.576272
Epoch 57/100 Iteration 206/234: loss=0.057132 lr=0.000020 grad_norm=0.726743
Epoch 57/100 Iteration 207/234: loss=0.051412 lr=0.000020 grad_norm=0.884217
Epoch 57/100 Iteration 208/234: loss=0.049154 lr=0.000020 grad_norm=1.109943
Epoch 57/100 Iteration 209/234: loss=0.052569 lr=0.000020 grad_norm=1.418033
Epoch 57/100 Iteration 210/234: loss=0.041230 lr=0.000020 grad_norm=0.835334
Epoch 57/100 Iteration 211/234: loss=0.042263 lr=0.000020 grad_norm=0.576445
Epoch 57/100 Iteration 212/234: loss=0.052280 lr=0.000020 grad_norm=0.680340
Epoch 57/100 Iteration 213/234: loss=0.045863 lr=0.000020 grad_norm=0.567729
Epoch 57/100 Iteration 214/234: loss=0.048049 lr=0.000020 grad_norm=1.077911
Epoch 57/100 Iteration 215/234: loss=0.047002 lr=0.000020 grad_norm=0.823577
Epoch 57/100 Iteration 216/234: loss=0.048895 lr=0.000020 grad_norm=0.547887
Epoch 57/100 Iteration 217/234: loss=0.054691 lr=0.000020 grad_norm=0.876780
Epoch 57/100 Iteration 218/234: loss=0.045212 lr=0.000020 grad_norm=0.536601
Epoch 57/100 Iteration 219/234: loss=0.048241 lr=0.000020 grad_norm=0.789742
Epoch 57/100 Iteration 220/234: loss=0.048072 lr=0.000020 grad_norm=0.752982
Epoch 57/100 Iteration 221/234: loss=0.057022 lr=0.000020 grad_norm=1.022078
Epoch 57/100 Iteration 222/234: loss=0.052032 lr=0.000020 grad_norm=0.970091
Epoch 57/100 Iteration 223/234: loss=0.047331 lr=0.000020 grad_norm=1.176049
Epoch 57/100 Iteration 224/234: loss=0.058902 lr=0.000020 grad_norm=1.388135
Epoch 57/100 Iteration 225/234: loss=0.045442 lr=0.000020 grad_norm=0.824729
Epoch 57/100 Iteration 226/234: loss=0.047564 lr=0.000020 grad_norm=0.657326
Epoch 57/100 Iteration 227/234: loss=0.048690 lr=0.000020 grad_norm=0.658917
Epoch 57/100 Iteration 228/234: loss=0.047730 lr=0.000020 grad_norm=0.551474
Epoch 57/100 Iteration 229/234: loss=0.046473 lr=0.000020 grad_norm=1.196229
Epoch 57/100 Iteration 230/234: loss=0.051846 lr=0.000020 grad_norm=1.572286
Epoch 57/100 Iteration 231/234: loss=0.042883 lr=0.000020 grad_norm=1.162701
Epoch 57/100 Iteration 232/234: loss=0.039550 lr=0.000020 grad_norm=0.452953
Epoch 57/100 Iteration 233/234: loss=0.044726 lr=0.000020 grad_norm=0.760120
Epoch 57/100 Iteration 234/234: loss=0.049876 lr=0.000020 grad_norm=0.843735
Epoch 57/100 finished. Avg Loss: 0.048845
Epoch 58/100 Iteration 1/234: loss=0.048961 lr=0.000020 grad_norm=0.823550
Epoch 58/100 Iteration 2/234: loss=0.049978 lr=0.000020 grad_norm=0.762381
Epoch 58/100 Iteration 3/234: loss=0.047826 lr=0.000020 grad_norm=0.774153
Epoch 58/100 Iteration 4/234: loss=0.054649 lr=0.000020 grad_norm=0.809931
Epoch 58/100 Iteration 5/234: loss=0.050624 lr=0.000020 grad_norm=0.581941
Epoch 58/100 Iteration 6/234: loss=0.050783 lr=0.000020 grad_norm=0.546863
Epoch 58/100 Iteration 7/234: loss=0.047696 lr=0.000020 grad_norm=0.838368
Epoch 58/100 Iteration 8/234: loss=0.042852 lr=0.000020 grad_norm=0.894156
Epoch 58/100 Iteration 9/234: loss=0.052923 lr=0.000020 grad_norm=0.849123
Epoch 58/100 Iteration 10/234: loss=0.049272 lr=0.000020 grad_norm=0.565949
Epoch 58/100 Iteration 11/234: loss=0.043526 lr=0.000020 grad_norm=1.611404
Epoch 58/100 Iteration 12/234: loss=0.044172 lr=0.000020 grad_norm=1.418428
Epoch 58/100 Iteration 13/234: loss=0.047955 lr=0.000020 grad_norm=0.455719
Epoch 58/100 Iteration 14/234: loss=0.061842 lr=0.000020 grad_norm=1.389053
Epoch 58/100 Iteration 15/234: loss=0.053249 lr=0.000020 grad_norm=1.564568
Epoch 58/100 Iteration 16/234: loss=0.053570 lr=0.000020 grad_norm=0.707117
Epoch 58/100 Iteration 17/234: loss=0.046658 lr=0.000020 grad_norm=0.907993
Epoch 58/100 Iteration 18/234: loss=0.043108 lr=0.000020 grad_norm=1.489191
Epoch 58/100 Iteration 19/234: loss=0.048237 lr=0.000020 grad_norm=1.040283
Epoch 58/100 Iteration 20/234: loss=0.051880 lr=0.000020 grad_norm=0.386446
Epoch 58/100 Iteration 21/234: loss=0.057221 lr=0.000020 grad_norm=1.246134
Epoch 58/100 Iteration 22/234: loss=0.053256 lr=0.000020 grad_norm=1.331499
Epoch 58/100 Iteration 23/234: loss=0.043866 lr=0.000020 grad_norm=0.854509
Epoch 58/100 Iteration 24/234: loss=0.044837 lr=0.000020 grad_norm=0.516606
Epoch 58/100 Iteration 25/234: loss=0.049408 lr=0.000020 grad_norm=0.916554
Epoch 58/100 Iteration 26/234: loss=0.046679 lr=0.000020 grad_norm=0.956792
Epoch 58/100 Iteration 27/234: loss=0.055749 lr=0.000020 grad_norm=0.636264
Epoch 58/100 Iteration 28/234: loss=0.048192 lr=0.000020 grad_norm=1.346893
Epoch 58/100 Iteration 29/234: loss=0.046582 lr=0.000020 grad_norm=1.308636
Epoch 58/100 Iteration 30/234: loss=0.047076 lr=0.000020 grad_norm=0.608754
Epoch 58/100 Iteration 31/234: loss=0.048859 lr=0.000020 grad_norm=1.476548
Epoch 58/100 Iteration 32/234: loss=0.047726 lr=0.000020 grad_norm=1.325687
Epoch 58/100 Iteration 33/234: loss=0.048309 lr=0.000020 grad_norm=0.519529
Epoch 58/100 Iteration 34/234: loss=0.050901 lr=0.000020 grad_norm=1.725927
Epoch 58/100 Iteration 35/234: loss=0.041890 lr=0.000020 grad_norm=1.468559
Epoch 58/100 Iteration 36/234: loss=0.048500 lr=0.000020 grad_norm=0.470508
Epoch 58/100 Iteration 37/234: loss=0.049551 lr=0.000020 grad_norm=1.657766
Epoch 58/100 Iteration 38/234: loss=0.049153 lr=0.000020 grad_norm=1.233492
Epoch 58/100 Iteration 39/234: loss=0.048518 lr=0.000020 grad_norm=0.935670
Epoch 58/100 Iteration 40/234: loss=0.051218 lr=0.000020 grad_norm=2.180206
Epoch 58/100 Iteration 41/234: loss=0.053411 lr=0.000020 grad_norm=1.278870
Epoch 58/100 Iteration 42/234: loss=0.044685 lr=0.000020 grad_norm=0.766519
Epoch 58/100 Iteration 43/234: loss=0.053137 lr=0.000020 grad_norm=1.604062
Epoch 58/100 Iteration 44/234: loss=0.045360 lr=0.000020 grad_norm=0.634600
Epoch 58/100 Iteration 45/234: loss=0.045299 lr=0.000020 grad_norm=1.232419
Epoch 58/100 Iteration 46/234: loss=0.047612 lr=0.000020 grad_norm=0.695862
Epoch 58/100 Iteration 47/234: loss=0.044935 lr=0.000020 grad_norm=0.998519
Epoch 58/100 Iteration 48/234: loss=0.053734 lr=0.000020 grad_norm=1.004619
Epoch 58/100 Iteration 49/234: loss=0.053499 lr=0.000020 grad_norm=0.572906
Epoch 58/100 Iteration 50/234: loss=0.050628 lr=0.000020 grad_norm=1.003381
Epoch 58/100 Iteration 51/234: loss=0.044683 lr=0.000020 grad_norm=1.228743
Epoch 58/100 Iteration 52/234: loss=0.043082 lr=0.000020 grad_norm=0.439350
Epoch 58/100 Iteration 53/234: loss=0.048634 lr=0.000020 grad_norm=1.704839
Epoch 58/100 Iteration 54/234: loss=0.044340 lr=0.000020 grad_norm=1.530761
Epoch 58/100 Iteration 55/234: loss=0.046254 lr=0.000020 grad_norm=0.733095
Epoch 58/100 Iteration 56/234: loss=0.050509 lr=0.000020 grad_norm=1.806916
Epoch 58/100 Iteration 57/234: loss=0.052279 lr=0.000020 grad_norm=0.839448
Epoch 58/100 Iteration 58/234: loss=0.043206 lr=0.000020 grad_norm=1.021660
Epoch 58/100 Iteration 59/234: loss=0.050497 lr=0.000020 grad_norm=1.129054
Epoch 58/100 Iteration 60/234: loss=0.045918 lr=0.000020 grad_norm=0.427470
Epoch 58/100 Iteration 61/234: loss=0.043919 lr=0.000020 grad_norm=0.759766
Epoch 58/100 Iteration 62/234: loss=0.044890 lr=0.000020 grad_norm=0.641975
Epoch 58/100 Iteration 63/234: loss=0.056353 lr=0.000020 grad_norm=0.575365
Epoch 58/100 Iteration 64/234: loss=0.049131 lr=0.000020 grad_norm=0.797203
Epoch 58/100 Iteration 65/234: loss=0.049560 lr=0.000020 grad_norm=0.507205
Epoch 58/100 Iteration 66/234: loss=0.047334 lr=0.000020 grad_norm=0.629574
Epoch 58/100 Iteration 67/234: loss=0.045256 lr=0.000020 grad_norm=0.545296
Epoch 58/100 Iteration 68/234: loss=0.053182 lr=0.000020 grad_norm=0.776597
Epoch 58/100 Iteration 69/234: loss=0.039214 lr=0.000020 grad_norm=1.156066
Epoch 58/100 Iteration 70/234: loss=0.043378 lr=0.000020 grad_norm=0.652982
Epoch 58/100 Iteration 71/234: loss=0.052642 lr=0.000020 grad_norm=0.792051
Epoch 58/100 Iteration 72/234: loss=0.045297 lr=0.000020 grad_norm=1.226970
Epoch 58/100 Iteration 73/234: loss=0.052359 lr=0.000020 grad_norm=0.894980
Epoch 58/100 Iteration 74/234: loss=0.043892 lr=0.000020 grad_norm=0.580857
Epoch 58/100 Iteration 75/234: loss=0.051049 lr=0.000020 grad_norm=0.697486
Epoch 58/100 Iteration 76/234: loss=0.044174 lr=0.000020 grad_norm=0.427900
Epoch 58/100 Iteration 77/234: loss=0.049018 lr=0.000020 grad_norm=0.601559
Epoch 58/100 Iteration 78/234: loss=0.047076 lr=0.000020 grad_norm=0.508672
Epoch 58/100 Iteration 79/234: loss=0.046909 lr=0.000020 grad_norm=0.953953
Epoch 58/100 Iteration 80/234: loss=0.049681 lr=0.000020 grad_norm=0.553475
Epoch 58/100 Iteration 81/234: loss=0.049131 lr=0.000020 grad_norm=0.837281
Epoch 58/100 Iteration 82/234: loss=0.056655 lr=0.000020 grad_norm=1.213421
Epoch 58/100 Iteration 83/234: loss=0.042874 lr=0.000020 grad_norm=0.972112
Epoch 58/100 Iteration 84/234: loss=0.044609 lr=0.000020 grad_norm=0.491904
Epoch 58/100 Iteration 85/234: loss=0.042931 lr=0.000020 grad_norm=0.925056
Epoch 58/100 Iteration 86/234: loss=0.043597 lr=0.000020 grad_norm=1.047130
Epoch 58/100 Iteration 87/234: loss=0.047760 lr=0.000020 grad_norm=0.495322
Epoch 58/100 Iteration 88/234: loss=0.046259 lr=0.000020 grad_norm=0.999481
Epoch 58/100 Iteration 89/234: loss=0.048486 lr=0.000020 grad_norm=0.568003
Epoch 58/100 Iteration 90/234: loss=0.042775 lr=0.000020 grad_norm=0.630719
Epoch 58/100 Iteration 91/234: loss=0.055869 lr=0.000020 grad_norm=1.394944
Epoch 58/100 Iteration 92/234: loss=0.045483 lr=0.000020 grad_norm=1.703766
Epoch 58/100 Iteration 93/234: loss=0.047866 lr=0.000020 grad_norm=1.163136
Epoch 58/100 Iteration 94/234: loss=0.054987 lr=0.000020 grad_norm=0.421697
Epoch 58/100 Iteration 95/234: loss=0.049216 lr=0.000020 grad_norm=1.010128
Epoch 58/100 Iteration 96/234: loss=0.047124 lr=0.000020 grad_norm=1.035506
Epoch 58/100 Iteration 97/234: loss=0.046286 lr=0.000020 grad_norm=0.545410
Epoch 58/100 Iteration 98/234: loss=0.051732 lr=0.000020 grad_norm=1.001808
Epoch 58/100 Iteration 99/234: loss=0.046965 lr=0.000020 grad_norm=0.745120
Epoch 58/100 Iteration 100/234: loss=0.047438 lr=0.000020 grad_norm=0.933129
Epoch 58/100 Iteration 101/234: loss=0.053879 lr=0.000020 grad_norm=1.383711
Epoch 58/100 Iteration 102/234: loss=0.052932 lr=0.000020 grad_norm=1.042916
Epoch 58/100 Iteration 103/234: loss=0.043504 lr=0.000020 grad_norm=0.716178
Epoch 58/100 Iteration 104/234: loss=0.044707 lr=0.000020 grad_norm=0.642864
Epoch 58/100 Iteration 105/234: loss=0.042667 lr=0.000020 grad_norm=0.665170
Epoch 58/100 Iteration 106/234: loss=0.049686 lr=0.000020 grad_norm=0.627649
Epoch 58/100 Iteration 107/234: loss=0.043296 lr=0.000020 grad_norm=0.407610
Epoch 58/100 Iteration 108/234: loss=0.049566 lr=0.000020 grad_norm=0.633841
Epoch 58/100 Iteration 109/234: loss=0.045553 lr=0.000020 grad_norm=0.423163
Epoch 58/100 Iteration 110/234: loss=0.050003 lr=0.000020 grad_norm=0.877536
Epoch 58/100 Iteration 111/234: loss=0.049280 lr=0.000020 grad_norm=1.119588
Epoch 58/100 Iteration 112/234: loss=0.049921 lr=0.000020 grad_norm=0.720269
Epoch 58/100 Iteration 113/234: loss=0.042528 lr=0.000020 grad_norm=0.527261
Epoch 58/100 Iteration 114/234: loss=0.046685 lr=0.000020 grad_norm=0.730210
Epoch 58/100 Iteration 115/234: loss=0.044792 lr=0.000020 grad_norm=0.546109
Epoch 58/100 Iteration 116/234: loss=0.046966 lr=0.000020 grad_norm=0.853737
Epoch 58/100 Iteration 117/234: loss=0.041955 lr=0.000020 grad_norm=0.765426
Epoch 58/100 Iteration 118/234: loss=0.047705 lr=0.000020 grad_norm=0.472637
Epoch 58/100 Iteration 119/234: loss=0.047081 lr=0.000020 grad_norm=0.797116
Epoch 58/100 Iteration 120/234: loss=0.049559 lr=0.000020 grad_norm=0.621826
Epoch 58/100 Iteration 121/234: loss=0.049632 lr=0.000020 grad_norm=0.763876
Epoch 58/100 Iteration 122/234: loss=0.045642 lr=0.000020 grad_norm=0.948746
Epoch 58/100 Iteration 123/234: loss=0.048009 lr=0.000020 grad_norm=0.510942
Epoch 58/100 Iteration 124/234: loss=0.049994 lr=0.000020 grad_norm=0.558115
Epoch 58/100 Iteration 125/234: loss=0.050942 lr=0.000020 grad_norm=0.791223
Epoch 58/100 Iteration 126/234: loss=0.048845 lr=0.000020 grad_norm=0.641761
Epoch 58/100 Iteration 127/234: loss=0.047705 lr=0.000020 grad_norm=0.845594
Epoch 58/100 Iteration 128/234: loss=0.044548 lr=0.000020 grad_norm=0.889196
Epoch 58/100 Iteration 129/234: loss=0.048970 lr=0.000020 grad_norm=0.568168
Epoch 58/100 Iteration 130/234: loss=0.047863 lr=0.000020 grad_norm=0.602843
Epoch 58/100 Iteration 131/234: loss=0.050592 lr=0.000020 grad_norm=0.585936
Epoch 58/100 Iteration 132/234: loss=0.047934 lr=0.000020 grad_norm=0.458299
Epoch 58/100 Iteration 133/234: loss=0.050221 lr=0.000020 grad_norm=0.540949
Epoch 58/100 Iteration 134/234: loss=0.048398 lr=0.000020 grad_norm=0.509430
Epoch 58/100 Iteration 135/234: loss=0.049148 lr=0.000020 grad_norm=0.414291
Epoch 58/100 Iteration 136/234: loss=0.047514 lr=0.000020 grad_norm=0.441209
Epoch 58/100 Iteration 137/234: loss=0.049439 lr=0.000020 grad_norm=0.838139
Epoch 58/100 Iteration 138/234: loss=0.053168 lr=0.000020 grad_norm=0.959441
Epoch 58/100 Iteration 139/234: loss=0.049794 lr=0.000020 grad_norm=0.967714
Epoch 58/100 Iteration 140/234: loss=0.053954 lr=0.000020 grad_norm=0.860145
Epoch 58/100 Iteration 141/234: loss=0.044124 lr=0.000020 grad_norm=0.441883
Epoch 58/100 Iteration 142/234: loss=0.046767 lr=0.000020 grad_norm=0.915655
Epoch 58/100 Iteration 143/234: loss=0.045271 lr=0.000020 grad_norm=0.724627
Epoch 58/100 Iteration 144/234: loss=0.049682 lr=0.000020 grad_norm=0.642109
Epoch 58/100 Iteration 145/234: loss=0.045082 lr=0.000020 grad_norm=0.791459
Epoch 58/100 Iteration 146/234: loss=0.052721 lr=0.000020 grad_norm=1.119308
Epoch 58/100 Iteration 147/234: loss=0.047937 lr=0.000020 grad_norm=1.566977
Epoch 58/100 Iteration 148/234: loss=0.057237 lr=0.000020 grad_norm=1.937342
Epoch 58/100 Iteration 149/234: loss=0.045063 lr=0.000020 grad_norm=1.544752
Epoch 58/100 Iteration 150/234: loss=0.044995 lr=0.000020 grad_norm=0.610996
Epoch 58/100 Iteration 151/234: loss=0.051756 lr=0.000020 grad_norm=1.516405
Epoch 58/100 Iteration 152/234: loss=0.046988 lr=0.000020 grad_norm=1.896546
Epoch 58/100 Iteration 153/234: loss=0.050643 lr=0.000020 grad_norm=0.720300
Epoch 58/100 Iteration 154/234: loss=0.048370 lr=0.000020 grad_norm=1.729034
Epoch 58/100 Iteration 155/234: loss=0.053471 lr=0.000020 grad_norm=2.140615
Epoch 58/100 Iteration 156/234: loss=0.046786 lr=0.000020 grad_norm=0.703757
Epoch 58/100 Iteration 157/234: loss=0.043828 lr=0.000020 grad_norm=1.377519
Epoch 58/100 Iteration 158/234: loss=0.050618 lr=0.000020 grad_norm=1.200838
Epoch 58/100 Iteration 159/234: loss=0.053320 lr=0.000020 grad_norm=0.874612
Epoch 58/100 Iteration 160/234: loss=0.045237 lr=0.000020 grad_norm=1.334421
Epoch 58/100 Iteration 161/234: loss=0.053743 lr=0.000020 grad_norm=0.533236
Epoch 58/100 Iteration 162/234: loss=0.050660 lr=0.000020 grad_norm=1.107204
Epoch 58/100 Iteration 163/234: loss=0.049825 lr=0.000020 grad_norm=0.902634
Epoch 58/100 Iteration 164/234: loss=0.052596 lr=0.000020 grad_norm=0.530481
Epoch 58/100 Iteration 165/234: loss=0.048962 lr=0.000020 grad_norm=0.800800
Epoch 58/100 Iteration 166/234: loss=0.050978 lr=0.000020 grad_norm=0.831225
Epoch 58/100 Iteration 167/234: loss=0.049007 lr=0.000020 grad_norm=0.476396
Epoch 58/100 Iteration 168/234: loss=0.045056 lr=0.000020 grad_norm=0.619972
Epoch 58/100 Iteration 169/234: loss=0.046857 lr=0.000020 grad_norm=0.722896
Epoch 58/100 Iteration 170/234: loss=0.046890 lr=0.000020 grad_norm=0.593277
Epoch 58/100 Iteration 171/234: loss=0.049011 lr=0.000020 grad_norm=0.514323
Epoch 58/100 Iteration 172/234: loss=0.051525 lr=0.000020 grad_norm=0.995348
Epoch 58/100 Iteration 173/234: loss=0.050317 lr=0.000020 grad_norm=1.262216
Epoch 58/100 Iteration 174/234: loss=0.054268 lr=0.000020 grad_norm=0.745161
Epoch 58/100 Iteration 175/234: loss=0.049918 lr=0.000020 grad_norm=0.662865
Epoch 58/100 Iteration 176/234: loss=0.047196 lr=0.000020 grad_norm=0.679883
Epoch 58/100 Iteration 177/234: loss=0.046953 lr=0.000020 grad_norm=0.427314
Epoch 58/100 Iteration 178/234: loss=0.047957 lr=0.000020 grad_norm=0.587494
Epoch 58/100 Iteration 179/234: loss=0.048272 lr=0.000020 grad_norm=0.912420
Epoch 58/100 Iteration 180/234: loss=0.053773 lr=0.000020 grad_norm=0.725179
Epoch 58/100 Iteration 181/234: loss=0.051590 lr=0.000020 grad_norm=0.606570
Epoch 58/100 Iteration 182/234: loss=0.049840 lr=0.000020 grad_norm=0.404916
Epoch 58/100 Iteration 183/234: loss=0.043635 lr=0.000020 grad_norm=0.522858
Epoch 58/100 Iteration 184/234: loss=0.043610 lr=0.000020 grad_norm=0.554958
Epoch 58/100 Iteration 185/234: loss=0.049511 lr=0.000020 grad_norm=0.756334
Epoch 58/100 Iteration 186/234: loss=0.048457 lr=0.000020 grad_norm=0.389605
Epoch 58/100 Iteration 187/234: loss=0.051123 lr=0.000020 grad_norm=0.599634
Epoch 58/100 Iteration 188/234: loss=0.052708 lr=0.000020 grad_norm=0.619756
Epoch 58/100 Iteration 189/234: loss=0.049065 lr=0.000020 grad_norm=0.727209
Epoch 58/100 Iteration 190/234: loss=0.046634 lr=0.000020 grad_norm=0.519547
Epoch 58/100 Iteration 191/234: loss=0.045764 lr=0.000020 grad_norm=0.506432
Epoch 58/100 Iteration 192/234: loss=0.049638 lr=0.000020 grad_norm=0.578855
Epoch 58/100 Iteration 193/234: loss=0.053639 lr=0.000020 grad_norm=0.403076
Epoch 58/100 Iteration 194/234: loss=0.043254 lr=0.000020 grad_norm=0.561119
Epoch 58/100 Iteration 195/234: loss=0.053483 lr=0.000020 grad_norm=0.563195
Epoch 58/100 Iteration 196/234: loss=0.044170 lr=0.000020 grad_norm=0.518982
Epoch 58/100 Iteration 197/234: loss=0.049590 lr=0.000020 grad_norm=0.910124
Epoch 58/100 Iteration 198/234: loss=0.047508 lr=0.000020 grad_norm=0.592892
Epoch 58/100 Iteration 199/234: loss=0.045961 lr=0.000020 grad_norm=0.564114
Epoch 58/100 Iteration 200/234: loss=0.053124 lr=0.000020 grad_norm=0.964931
Epoch 58/100 Iteration 201/234: loss=0.048325 lr=0.000020 grad_norm=0.791263
Epoch 58/100 Iteration 202/234: loss=0.046225 lr=0.000020 grad_norm=0.635873
Epoch 58/100 Iteration 203/234: loss=0.048874 lr=0.000020 grad_norm=0.996895
Epoch 58/100 Iteration 204/234: loss=0.049824 lr=0.000020 grad_norm=1.106508
Epoch 58/100 Iteration 205/234: loss=0.042229 lr=0.000020 grad_norm=0.680311
Epoch 58/100 Iteration 206/234: loss=0.053434 lr=0.000020 grad_norm=0.614483
Epoch 58/100 Iteration 207/234: loss=0.050602 lr=0.000020 grad_norm=1.152807
Epoch 58/100 Iteration 208/234: loss=0.053132 lr=0.000020 grad_norm=1.272745
Epoch 58/100 Iteration 209/234: loss=0.048994 lr=0.000020 grad_norm=0.988576
Epoch 58/100 Iteration 210/234: loss=0.048723 lr=0.000020 grad_norm=0.433961
Epoch 58/100 Iteration 211/234: loss=0.049467 lr=0.000020 grad_norm=0.687241
Epoch 58/100 Iteration 212/234: loss=0.042851 lr=0.000020 grad_norm=0.713041
Epoch 58/100 Iteration 213/234: loss=0.045112 lr=0.000020 grad_norm=0.387021
Epoch 58/100 Iteration 214/234: loss=0.053155 lr=0.000020 grad_norm=0.872914
Epoch 58/100 Iteration 215/234: loss=0.048873 lr=0.000020 grad_norm=1.172821
Epoch 58/100 Iteration 216/234: loss=0.052865 lr=0.000020 grad_norm=1.375488
Epoch 58/100 Iteration 217/234: loss=0.046006 lr=0.000020 grad_norm=1.489686
Epoch 58/100 Iteration 218/234: loss=0.059137 lr=0.000020 grad_norm=1.194915
Epoch 58/100 Iteration 219/234: loss=0.046755 lr=0.000020 grad_norm=0.619398
Epoch 58/100 Iteration 220/234: loss=0.047268 lr=0.000020 grad_norm=0.608622
Epoch 58/100 Iteration 221/234: loss=0.046669 lr=0.000020 grad_norm=0.821211
Epoch 58/100 Iteration 222/234: loss=0.045422 lr=0.000020 grad_norm=0.451648
Epoch 58/100 Iteration 223/234: loss=0.051925 lr=0.000020 grad_norm=0.560881
Epoch 58/100 Iteration 224/234: loss=0.049227 lr=0.000020 grad_norm=0.745444
Epoch 58/100 Iteration 225/234: loss=0.046711 lr=0.000020 grad_norm=0.992268
Epoch 58/100 Iteration 226/234: loss=0.051427 lr=0.000020 grad_norm=1.730867
Epoch 58/100 Iteration 227/234: loss=0.043012 lr=0.000020 grad_norm=1.176248
Epoch 58/100 Iteration 228/234: loss=0.048749 lr=0.000020 grad_norm=0.574707
Epoch 58/100 Iteration 229/234: loss=0.043891 lr=0.000020 grad_norm=1.032653
Epoch 58/100 Iteration 230/234: loss=0.047186 lr=0.000020 grad_norm=0.602958
Epoch 58/100 Iteration 231/234: loss=0.046258 lr=0.000020 grad_norm=1.725581
Epoch 58/100 Iteration 232/234: loss=0.042990 lr=0.000020 grad_norm=2.059810
Epoch 58/100 Iteration 233/234: loss=0.040962 lr=0.000020 grad_norm=0.583137
Epoch 58/100 Iteration 234/234: loss=0.052255 lr=0.000020 grad_norm=1.970674
Epoch 58/100 finished. Avg Loss: 0.048463
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 59/100 Iteration 1/234: loss=0.048131 lr=0.000020 grad_norm=2.520960
Epoch 59/100 Iteration 2/234: loss=0.052790 lr=0.000020 grad_norm=1.011715
Epoch 59/100 Iteration 3/234: loss=0.047686 lr=0.000020 grad_norm=2.038236
Epoch 59/100 Iteration 4/234: loss=0.046196 lr=0.000020 grad_norm=1.519676
Epoch 59/100 Iteration 5/234: loss=0.047062 lr=0.000020 grad_norm=1.356417
Epoch 59/100 Iteration 6/234: loss=0.052567 lr=0.000020 grad_norm=1.945956
Epoch 59/100 Iteration 7/234: loss=0.040959 lr=0.000020 grad_norm=0.599351
Epoch 59/100 Iteration 8/234: loss=0.041414 lr=0.000020 grad_norm=1.770907
Epoch 59/100 Iteration 9/234: loss=0.050586 lr=0.000020 grad_norm=0.464865
Epoch 59/100 Iteration 10/234: loss=0.047655 lr=0.000020 grad_norm=1.690609
Epoch 59/100 Iteration 11/234: loss=0.050083 lr=0.000020 grad_norm=0.709227
Epoch 59/100 Iteration 12/234: loss=0.051134 lr=0.000020 grad_norm=1.217145
Epoch 59/100 Iteration 13/234: loss=0.050961 lr=0.000020 grad_norm=1.045098
Epoch 59/100 Iteration 14/234: loss=0.050792 lr=0.000020 grad_norm=0.843553
Epoch 59/100 Iteration 15/234: loss=0.047306 lr=0.000020 grad_norm=1.485779
Epoch 59/100 Iteration 16/234: loss=0.052130 lr=0.000020 grad_norm=0.810625
Epoch 59/100 Iteration 17/234: loss=0.046413 lr=0.000020 grad_norm=0.838132
Epoch 59/100 Iteration 18/234: loss=0.048382 lr=0.000020 grad_norm=1.115987
Epoch 59/100 Iteration 19/234: loss=0.045661 lr=0.000020 grad_norm=0.426264
Epoch 59/100 Iteration 20/234: loss=0.044553 lr=0.000020 grad_norm=0.616799
Epoch 59/100 Iteration 21/234: loss=0.054146 lr=0.000020 grad_norm=0.630453
Epoch 59/100 Iteration 22/234: loss=0.049474 lr=0.000020 grad_norm=0.485787
Epoch 59/100 Iteration 23/234: loss=0.052619 lr=0.000020 grad_norm=0.490285
Epoch 59/100 Iteration 24/234: loss=0.044868 lr=0.000020 grad_norm=0.458623
Epoch 59/100 Iteration 25/234: loss=0.049324 lr=0.000020 grad_norm=0.746227
Epoch 59/100 Iteration 26/234: loss=0.047795 lr=0.000020 grad_norm=0.656458
Epoch 59/100 Iteration 27/234: loss=0.055566 lr=0.000020 grad_norm=0.661688
Epoch 59/100 Iteration 28/234: loss=0.046212 lr=0.000020 grad_norm=0.603948
Epoch 59/100 Iteration 29/234: loss=0.050311 lr=0.000020 grad_norm=0.635180
Epoch 59/100 Iteration 30/234: loss=0.052793 lr=0.000020 grad_norm=0.706762
Epoch 59/100 Iteration 31/234: loss=0.041161 lr=0.000020 grad_norm=0.661413
Epoch 59/100 Iteration 32/234: loss=0.044224 lr=0.000020 grad_norm=0.574319
Epoch 59/100 Iteration 33/234: loss=0.046275 lr=0.000020 grad_norm=0.397928
Epoch 59/100 Iteration 34/234: loss=0.044187 lr=0.000020 grad_norm=0.469602
Epoch 59/100 Iteration 35/234: loss=0.046306 lr=0.000020 grad_norm=0.828362
Epoch 59/100 Iteration 36/234: loss=0.052676 lr=0.000020 grad_norm=1.007805
Epoch 59/100 Iteration 37/234: loss=0.050997 lr=0.000020 grad_norm=1.023904
Epoch 59/100 Iteration 38/234: loss=0.056659 lr=0.000020 grad_norm=1.440374
Epoch 59/100 Iteration 39/234: loss=0.048258 lr=0.000020 grad_norm=1.328908
Epoch 59/100 Iteration 40/234: loss=0.048827 lr=0.000020 grad_norm=0.638751
Epoch 59/100 Iteration 41/234: loss=0.041895 lr=0.000020 grad_norm=0.822758
Epoch 59/100 Iteration 42/234: loss=0.055372 lr=0.000020 grad_norm=1.024847
Epoch 59/100 Iteration 43/234: loss=0.045481 lr=0.000020 grad_norm=0.667312
Epoch 59/100 Iteration 44/234: loss=0.049854 lr=0.000020 grad_norm=0.865732
Epoch 59/100 Iteration 45/234: loss=0.047565 lr=0.000020 grad_norm=1.041123
Epoch 59/100 Iteration 46/234: loss=0.046884 lr=0.000020 grad_norm=0.788030
Epoch 59/100 Iteration 47/234: loss=0.049896 lr=0.000020 grad_norm=0.618525
Epoch 59/100 Iteration 48/234: loss=0.053355 lr=0.000020 grad_norm=0.542942
Epoch 59/100 Iteration 49/234: loss=0.049148 lr=0.000020 grad_norm=0.431072
Epoch 59/100 Iteration 50/234: loss=0.047593 lr=0.000020 grad_norm=0.454612
Epoch 59/100 Iteration 51/234: loss=0.045737 lr=0.000020 grad_norm=0.678013
Epoch 59/100 Iteration 52/234: loss=0.047941 lr=0.000020 grad_norm=0.792630
Epoch 59/100 Iteration 53/234: loss=0.049617 lr=0.000020 grad_norm=0.691177
Epoch 59/100 Iteration 54/234: loss=0.048932 lr=0.000020 grad_norm=0.648170
Epoch 59/100 Iteration 55/234: loss=0.046530 lr=0.000020 grad_norm=0.701640
Epoch 59/100 Iteration 56/234: loss=0.045389 lr=0.000020 grad_norm=0.500310
Epoch 59/100 Iteration 57/234: loss=0.045548 lr=0.000020 grad_norm=0.821702
Epoch 59/100 Iteration 58/234: loss=0.049878 lr=0.000020 grad_norm=0.826865
Epoch 59/100 Iteration 59/234: loss=0.047926 lr=0.000020 grad_norm=0.559305
Epoch 59/100 Iteration 60/234: loss=0.047402 lr=0.000020 grad_norm=0.756744
Epoch 59/100 Iteration 61/234: loss=0.047884 lr=0.000020 grad_norm=0.817485
Epoch 59/100 Iteration 62/234: loss=0.049290 lr=0.000020 grad_norm=0.679379
Epoch 59/100 Iteration 63/234: loss=0.051608 lr=0.000020 grad_norm=0.916491
Epoch 59/100 Iteration 64/234: loss=0.048717 lr=0.000020 grad_norm=0.784760
Epoch 59/100 Iteration 65/234: loss=0.045146 lr=0.000020 grad_norm=0.656230
Epoch 59/100 Iteration 66/234: loss=0.049431 lr=0.000020 grad_norm=0.928837
Epoch 59/100 Iteration 67/234: loss=0.044826 lr=0.000020 grad_norm=0.626235
Epoch 59/100 Iteration 68/234: loss=0.044924 lr=0.000020 grad_norm=0.377527
Epoch 59/100 Iteration 69/234: loss=0.046242 lr=0.000020 grad_norm=1.134757
Epoch 59/100 Iteration 70/234: loss=0.049837 lr=0.000020 grad_norm=1.322618
Epoch 59/100 Iteration 71/234: loss=0.036407 lr=0.000020 grad_norm=0.547126
Epoch 59/100 Iteration 72/234: loss=0.044653 lr=0.000020 grad_norm=0.953929
Epoch 59/100 Iteration 73/234: loss=0.056779 lr=0.000020 grad_norm=1.117862
Epoch 59/100 Iteration 74/234: loss=0.046705 lr=0.000020 grad_norm=0.731073
Epoch 59/100 Iteration 75/234: loss=0.048510 lr=0.000020 grad_norm=0.953683
Epoch 59/100 Iteration 76/234: loss=0.044725 lr=0.000020 grad_norm=0.561370
Epoch 59/100 Iteration 77/234: loss=0.044089 lr=0.000020 grad_norm=0.758226
Epoch 59/100 Iteration 78/234: loss=0.043339 lr=0.000020 grad_norm=1.167810
Epoch 59/100 Iteration 79/234: loss=0.046970 lr=0.000020 grad_norm=0.682360
Epoch 59/100 Iteration 80/234: loss=0.044801 lr=0.000020 grad_norm=0.967210
Epoch 59/100 Iteration 81/234: loss=0.046632 lr=0.000020 grad_norm=1.303298
Epoch 59/100 Iteration 82/234: loss=0.053843 lr=0.000020 grad_norm=1.132021
Epoch 59/100 Iteration 83/234: loss=0.048291 lr=0.000020 grad_norm=0.857731
Epoch 59/100 Iteration 84/234: loss=0.048471 lr=0.000020 grad_norm=1.338383
Epoch 59/100 Iteration 85/234: loss=0.050603 lr=0.000020 grad_norm=0.974625
Epoch 59/100 Iteration 86/234: loss=0.041018 lr=0.000020 grad_norm=0.814781
Epoch 59/100 Iteration 87/234: loss=0.048625 lr=0.000020 grad_norm=0.884644
Epoch 59/100 Iteration 88/234: loss=0.044152 lr=0.000020 grad_norm=0.386976
Epoch 59/100 Iteration 89/234: loss=0.053149 lr=0.000020 grad_norm=1.082701
Epoch 59/100 Iteration 90/234: loss=0.052468 lr=0.000020 grad_norm=1.223905
Epoch 59/100 Iteration 91/234: loss=0.051816 lr=0.000020 grad_norm=0.491746
Epoch 59/100 Iteration 92/234: loss=0.048886 lr=0.000020 grad_norm=1.066612
Epoch 59/100 Iteration 93/234: loss=0.047148 lr=0.000020 grad_norm=0.592446
Epoch 59/100 Iteration 94/234: loss=0.046893 lr=0.000020 grad_norm=0.786251
Epoch 59/100 Iteration 95/234: loss=0.050668 lr=0.000020 grad_norm=0.909282
Epoch 59/100 Iteration 96/234: loss=0.040801 lr=0.000020 grad_norm=0.492271
Epoch 59/100 Iteration 97/234: loss=0.050428 lr=0.000020 grad_norm=1.040466
Epoch 59/100 Iteration 98/234: loss=0.045424 lr=0.000020 grad_norm=1.180899
Epoch 59/100 Iteration 99/234: loss=0.045777 lr=0.000020 grad_norm=0.466289
Epoch 59/100 Iteration 100/234: loss=0.044130 lr=0.000020 grad_norm=1.155020
Epoch 59/100 Iteration 101/234: loss=0.045324 lr=0.000020 grad_norm=1.180959
Epoch 59/100 Iteration 102/234: loss=0.048830 lr=0.000020 grad_norm=1.104639
Epoch 59/100 Iteration 103/234: loss=0.048004 lr=0.000020 grad_norm=2.653727
Epoch 59/100 Iteration 104/234: loss=0.046178 lr=0.000020 grad_norm=1.855715
Epoch 59/100 Iteration 105/234: loss=0.043928 lr=0.000020 grad_norm=0.778486
Epoch 59/100 Iteration 106/234: loss=0.043187 lr=0.000020 grad_norm=1.552624
Epoch 59/100 Iteration 107/234: loss=0.046868 lr=0.000020 grad_norm=0.803684
Epoch 59/100 Iteration 108/234: loss=0.051497 lr=0.000020 grad_norm=1.279466
Epoch 59/100 Iteration 109/234: loss=0.049862 lr=0.000020 grad_norm=1.203648
Epoch 59/100 Iteration 110/234: loss=0.047859 lr=0.000020 grad_norm=0.750718
Epoch 59/100 Iteration 111/234: loss=0.041919 lr=0.000020 grad_norm=1.201254
Epoch 59/100 Iteration 112/234: loss=0.047162 lr=0.000020 grad_norm=0.475402
Epoch 59/100 Iteration 113/234: loss=0.047372 lr=0.000020 grad_norm=0.924524
Epoch 59/100 Iteration 114/234: loss=0.048640 lr=0.000020 grad_norm=1.065973
Epoch 59/100 Iteration 115/234: loss=0.047054 lr=0.000020 grad_norm=0.456837
Epoch 59/100 Iteration 116/234: loss=0.044374 lr=0.000020 grad_norm=0.972220
Epoch 59/100 Iteration 117/234: loss=0.051216 lr=0.000020 grad_norm=0.493940
Epoch 59/100 Iteration 118/234: loss=0.048401 lr=0.000020 grad_norm=0.982149
Epoch 59/100 Iteration 119/234: loss=0.044816 lr=0.000020 grad_norm=0.831841
Epoch 59/100 Iteration 120/234: loss=0.047994 lr=0.000020 grad_norm=1.003532
Epoch 59/100 Iteration 121/234: loss=0.047087 lr=0.000020 grad_norm=1.153638
Epoch 59/100 Iteration 122/234: loss=0.049893 lr=0.000020 grad_norm=0.808537
Epoch 59/100 Iteration 123/234: loss=0.048117 lr=0.000020 grad_norm=1.850422
Epoch 59/100 Iteration 124/234: loss=0.047920 lr=0.000020 grad_norm=1.334703
Epoch 59/100 Iteration 125/234: loss=0.052141 lr=0.000020 grad_norm=0.567978
Epoch 59/100 Iteration 126/234: loss=0.050127 lr=0.000020 grad_norm=1.196865
Epoch 59/100 Iteration 127/234: loss=0.045823 lr=0.000020 grad_norm=0.843097
Epoch 59/100 Iteration 128/234: loss=0.047001 lr=0.000020 grad_norm=0.843651
Epoch 59/100 Iteration 129/234: loss=0.041426 lr=0.000020 grad_norm=0.897370
Epoch 59/100 Iteration 130/234: loss=0.054100 lr=0.000020 grad_norm=0.939958
Epoch 59/100 Iteration 131/234: loss=0.047913 lr=0.000020 grad_norm=1.004602
Epoch 59/100 Iteration 132/234: loss=0.050940 lr=0.000020 grad_norm=0.775637
Epoch 59/100 Iteration 133/234: loss=0.051489 lr=0.000020 grad_norm=0.737530
Epoch 59/100 Iteration 134/234: loss=0.044712 lr=0.000020 grad_norm=1.154129
Epoch 59/100 Iteration 135/234: loss=0.048924 lr=0.000020 grad_norm=0.842772
Epoch 59/100 Iteration 136/234: loss=0.053523 lr=0.000020 grad_norm=1.194416
Epoch 59/100 Iteration 137/234: loss=0.045474 lr=0.000020 grad_norm=1.264297
Epoch 59/100 Iteration 138/234: loss=0.053280 lr=0.000020 grad_norm=0.568459
Epoch 59/100 Iteration 139/234: loss=0.049142 lr=0.000020 grad_norm=1.086285
Epoch 59/100 Iteration 140/234: loss=0.049013 lr=0.000020 grad_norm=1.242095
Epoch 59/100 Iteration 141/234: loss=0.049783 lr=0.000020 grad_norm=0.730922
Epoch 59/100 Iteration 142/234: loss=0.044137 lr=0.000020 grad_norm=0.839378
Epoch 59/100 Iteration 143/234: loss=0.054158 lr=0.000020 grad_norm=1.377363
Epoch 59/100 Iteration 144/234: loss=0.047535 lr=0.000020 grad_norm=1.334927
Epoch 59/100 Iteration 145/234: loss=0.056540 lr=0.000020 grad_norm=0.548313
Epoch 59/100 Iteration 146/234: loss=0.050017 lr=0.000020 grad_norm=1.966914
Epoch 59/100 Iteration 147/234: loss=0.043485 lr=0.000020 grad_norm=2.074829
Epoch 59/100 Iteration 148/234: loss=0.038015 lr=0.000020 grad_norm=0.436691
Epoch 59/100 Iteration 149/234: loss=0.047308 lr=0.000020 grad_norm=1.478008
Epoch 59/100 Iteration 150/234: loss=0.039560 lr=0.000020 grad_norm=0.928688
Epoch 59/100 Iteration 151/234: loss=0.044960 lr=0.000020 grad_norm=1.185468
Epoch 59/100 Iteration 152/234: loss=0.048851 lr=0.000020 grad_norm=1.720475
Epoch 59/100 Iteration 153/234: loss=0.047487 lr=0.000020 grad_norm=0.676615
Epoch 59/100 Iteration 154/234: loss=0.046699 lr=0.000020 grad_norm=1.358105
Epoch 59/100 Iteration 155/234: loss=0.044608 lr=0.000020 grad_norm=1.926941
Epoch 59/100 Iteration 156/234: loss=0.045232 lr=0.000020 grad_norm=1.042547
Epoch 59/100 Iteration 157/234: loss=0.041124 lr=0.000020 grad_norm=1.079468
Epoch 59/100 Iteration 158/234: loss=0.049232 lr=0.000020 grad_norm=1.562740
Epoch 59/100 Iteration 159/234: loss=0.043433 lr=0.000020 grad_norm=0.918917
Epoch 59/100 Iteration 160/234: loss=0.048715 lr=0.000020 grad_norm=0.802588
Epoch 59/100 Iteration 161/234: loss=0.046585 lr=0.000020 grad_norm=1.546753
Epoch 59/100 Iteration 162/234: loss=0.048369 lr=0.000020 grad_norm=0.733416
Epoch 59/100 Iteration 163/234: loss=0.047189 lr=0.000020 grad_norm=0.689709
Epoch 59/100 Iteration 164/234: loss=0.040119 lr=0.000020 grad_norm=0.718510
Epoch 59/100 Iteration 165/234: loss=0.042678 lr=0.000020 grad_norm=0.451309
Epoch 59/100 Iteration 166/234: loss=0.046264 lr=0.000020 grad_norm=0.714697
Epoch 59/100 Iteration 167/234: loss=0.047706 lr=0.000020 grad_norm=0.703533
Epoch 59/100 Iteration 168/234: loss=0.050191 lr=0.000020 grad_norm=1.066317
Epoch 59/100 Iteration 169/234: loss=0.047045 lr=0.000020 grad_norm=0.998849
Epoch 59/100 Iteration 170/234: loss=0.050846 lr=0.000020 grad_norm=0.559547
Epoch 59/100 Iteration 171/234: loss=0.049732 lr=0.000020 grad_norm=0.989038
Epoch 59/100 Iteration 172/234: loss=0.052811 lr=0.000020 grad_norm=1.008179
Epoch 59/100 Iteration 173/234: loss=0.048234 lr=0.000020 grad_norm=0.728394
Epoch 59/100 Iteration 174/234: loss=0.038017 lr=0.000020 grad_norm=0.940239
Epoch 59/100 Iteration 175/234: loss=0.046654 lr=0.000020 grad_norm=0.748696
Epoch 59/100 Iteration 176/234: loss=0.047706 lr=0.000020 grad_norm=0.848568
Epoch 59/100 Iteration 177/234: loss=0.042047 lr=0.000020 grad_norm=1.343465
Epoch 59/100 Iteration 178/234: loss=0.047793 lr=0.000020 grad_norm=0.977637
Epoch 59/100 Iteration 179/234: loss=0.050624 lr=0.000020 grad_norm=0.810013
Epoch 59/100 Iteration 180/234: loss=0.042235 lr=0.000020 grad_norm=1.202482
Epoch 59/100 Iteration 181/234: loss=0.039426 lr=0.000020 grad_norm=0.414637
Epoch 59/100 Iteration 182/234: loss=0.042705 lr=0.000020 grad_norm=0.975328
Epoch 59/100 Iteration 183/234: loss=0.046933 lr=0.000020 grad_norm=0.941621
Epoch 59/100 Iteration 184/234: loss=0.041397 lr=0.000020 grad_norm=0.325102
Epoch 59/100 Iteration 185/234: loss=0.044527 lr=0.000020 grad_norm=1.117038
Epoch 59/100 Iteration 186/234: loss=0.047329 lr=0.000020 grad_norm=1.284356
Epoch 59/100 Iteration 187/234: loss=0.051555 lr=0.000020 grad_norm=0.708515
Epoch 59/100 Iteration 188/234: loss=0.043169 lr=0.000020 grad_norm=0.501674
Epoch 59/100 Iteration 189/234: loss=0.039105 lr=0.000020 grad_norm=0.667959
Epoch 59/100 Iteration 190/234: loss=0.049835 lr=0.000020 grad_norm=0.588647
Epoch 59/100 Iteration 191/234: loss=0.048281 lr=0.000020 grad_norm=0.766847
Epoch 59/100 Iteration 192/234: loss=0.049178 lr=0.000020 grad_norm=0.455877
Epoch 59/100 Iteration 193/234: loss=0.044468 lr=0.000020 grad_norm=0.648113
Epoch 59/100 Iteration 194/234: loss=0.052056 lr=0.000020 grad_norm=0.711444
Epoch 59/100 Iteration 195/234: loss=0.050449 lr=0.000020 grad_norm=0.531511
Epoch 59/100 Iteration 196/234: loss=0.042950 lr=0.000020 grad_norm=0.783256
Epoch 59/100 Iteration 197/234: loss=0.044089 lr=0.000020 grad_norm=0.586325
Epoch 59/100 Iteration 198/234: loss=0.042980 lr=0.000020 grad_norm=0.391906
Epoch 59/100 Iteration 199/234: loss=0.046706 lr=0.000020 grad_norm=0.582968
Epoch 59/100 Iteration 200/234: loss=0.049144 lr=0.000020 grad_norm=0.474213
Epoch 59/100 Iteration 201/234: loss=0.045759 lr=0.000020 grad_norm=1.223474
Epoch 59/100 Iteration 202/234: loss=0.046320 lr=0.000020 grad_norm=1.473382
Epoch 59/100 Iteration 203/234: loss=0.050807 lr=0.000020 grad_norm=0.860712
Epoch 59/100 Iteration 204/234: loss=0.043354 lr=0.000020 grad_norm=0.586860
Epoch 59/100 Iteration 205/234: loss=0.048558 lr=0.000020 grad_norm=0.428673
Epoch 59/100 Iteration 206/234: loss=0.045446 lr=0.000020 grad_norm=0.780603
Epoch 59/100 Iteration 207/234: loss=0.047610 lr=0.000020 grad_norm=1.101089
Epoch 59/100 Iteration 208/234: loss=0.048705 lr=0.000020 grad_norm=0.895601
Epoch 59/100 Iteration 209/234: loss=0.048069 lr=0.000020 grad_norm=0.424129
Epoch 59/100 Iteration 210/234: loss=0.043080 lr=0.000020 grad_norm=0.970702
Epoch 59/100 Iteration 211/234: loss=0.050477 lr=0.000020 grad_norm=0.776470
Epoch 59/100 Iteration 212/234: loss=0.043037 lr=0.000020 grad_norm=0.567711
Epoch 59/100 Iteration 213/234: loss=0.047921 lr=0.000020 grad_norm=1.159423
Epoch 59/100 Iteration 214/234: loss=0.047814 lr=0.000020 grad_norm=0.726596
Epoch 59/100 Iteration 215/234: loss=0.048167 lr=0.000020 grad_norm=1.000953
Epoch 59/100 Iteration 216/234: loss=0.047140 lr=0.000020 grad_norm=1.539939
Epoch 59/100 Iteration 217/234: loss=0.048249 lr=0.000020 grad_norm=0.643663
Epoch 59/100 Iteration 218/234: loss=0.045302 lr=0.000020 grad_norm=2.209889
Epoch 59/100 Iteration 219/234: loss=0.052997 lr=0.000020 grad_norm=2.509574
Epoch 59/100 Iteration 220/234: loss=0.040670 lr=0.000020 grad_norm=0.563231
Epoch 59/100 Iteration 221/234: loss=0.043851 lr=0.000020 grad_norm=1.888141
Epoch 59/100 Iteration 222/234: loss=0.050839 lr=0.000020 grad_norm=1.421832
Epoch 59/100 Iteration 223/234: loss=0.042365 lr=0.000020 grad_norm=0.880509
Epoch 59/100 Iteration 224/234: loss=0.042762 lr=0.000020 grad_norm=1.232258
Epoch 59/100 Iteration 225/234: loss=0.045197 lr=0.000020 grad_norm=1.019329
Epoch 59/100 Iteration 226/234: loss=0.053629 lr=0.000020 grad_norm=1.691823
Epoch 59/100 Iteration 227/234: loss=0.047022 lr=0.000020 grad_norm=0.777802
Epoch 59/100 Iteration 228/234: loss=0.050514 lr=0.000020 grad_norm=1.410586
Epoch 59/100 Iteration 229/234: loss=0.044717 lr=0.000020 grad_norm=0.925078
Epoch 59/100 Iteration 230/234: loss=0.050858 lr=0.000020 grad_norm=1.363542
Epoch 59/100 Iteration 231/234: loss=0.049854 lr=0.000020 grad_norm=1.890341
Epoch 59/100 Iteration 232/234: loss=0.044507 lr=0.000020 grad_norm=1.178012
Epoch 59/100 Iteration 233/234: loss=0.047290 lr=0.000020 grad_norm=0.938960
Epoch 59/100 Iteration 234/234: loss=0.051701 lr=0.000020 grad_norm=1.192596
Epoch 59/100 finished. Avg Loss: 0.047454
Epoch 60/100 Iteration 1/234: loss=0.047740 lr=0.000020 grad_norm=0.827538
Epoch 60/100 Iteration 2/234: loss=0.046744 lr=0.000020 grad_norm=0.848230
Epoch 60/100 Iteration 3/234: loss=0.048182 lr=0.000020 grad_norm=0.537480
Epoch 60/100 Iteration 4/234: loss=0.048259 lr=0.000020 grad_norm=0.783243
Epoch 60/100 Iteration 5/234: loss=0.049973 lr=0.000020 grad_norm=0.688751
Epoch 60/100 Iteration 6/234: loss=0.049556 lr=0.000020 grad_norm=0.615911
Epoch 60/100 Iteration 7/234: loss=0.046283 lr=0.000020 grad_norm=0.676354
Epoch 60/100 Iteration 8/234: loss=0.048913 lr=0.000020 grad_norm=0.538185
Epoch 60/100 Iteration 9/234: loss=0.044250 lr=0.000020 grad_norm=0.824165
Epoch 60/100 Iteration 10/234: loss=0.049171 lr=0.000020 grad_norm=0.865566
Epoch 60/100 Iteration 11/234: loss=0.051367 lr=0.000020 grad_norm=0.941345
Epoch 60/100 Iteration 12/234: loss=0.047692 lr=0.000020 grad_norm=1.551764
Epoch 60/100 Iteration 13/234: loss=0.049748 lr=0.000020 grad_norm=1.161116
Epoch 60/100 Iteration 14/234: loss=0.047830 lr=0.000020 grad_norm=0.568154
Epoch 60/100 Iteration 15/234: loss=0.049626 lr=0.000020 grad_norm=1.215529
Epoch 60/100 Iteration 16/234: loss=0.043756 lr=0.000020 grad_norm=1.025844
Epoch 60/100 Iteration 17/234: loss=0.045061 lr=0.000020 grad_norm=0.582212
Epoch 60/100 Iteration 18/234: loss=0.045783 lr=0.000020 grad_norm=1.290261
Epoch 60/100 Iteration 19/234: loss=0.049320 lr=0.000020 grad_norm=0.904093
Epoch 60/100 Iteration 20/234: loss=0.048573 lr=0.000020 grad_norm=0.404799
Epoch 60/100 Iteration 21/234: loss=0.044029 lr=0.000020 grad_norm=0.710998
Epoch 60/100 Iteration 22/234: loss=0.050211 lr=0.000020 grad_norm=0.765903
Epoch 60/100 Iteration 23/234: loss=0.047490 lr=0.000020 grad_norm=0.707075
Epoch 60/100 Iteration 24/234: loss=0.050470 lr=0.000020 grad_norm=1.017825
Epoch 60/100 Iteration 25/234: loss=0.047843 lr=0.000020 grad_norm=0.334984
Epoch 60/100 Iteration 26/234: loss=0.038502 lr=0.000020 grad_norm=1.080183
Epoch 60/100 Iteration 27/234: loss=0.045693 lr=0.000020 grad_norm=0.847127
Epoch 60/100 Iteration 28/234: loss=0.039145 lr=0.000020 grad_norm=0.511662
Epoch 60/100 Iteration 29/234: loss=0.047144 lr=0.000020 grad_norm=0.910400
Epoch 60/100 Iteration 30/234: loss=0.044932 lr=0.000020 grad_norm=0.723966
Epoch 60/100 Iteration 31/234: loss=0.055503 lr=0.000020 grad_norm=2.004042
Epoch 60/100 Iteration 32/234: loss=0.047895 lr=0.000020 grad_norm=2.246208
Epoch 60/100 Iteration 33/234: loss=0.048033 lr=0.000020 grad_norm=0.723170
Epoch 60/100 Iteration 34/234: loss=0.051486 lr=0.000020 grad_norm=1.326576
Epoch 60/100 Iteration 35/234: loss=0.052716 lr=0.000020 grad_norm=1.196203
Epoch 60/100 Iteration 36/234: loss=0.049136 lr=0.000020 grad_norm=0.637043
Epoch 60/100 Iteration 37/234: loss=0.048394 lr=0.000020 grad_norm=0.989045
Epoch 60/100 Iteration 38/234: loss=0.044226 lr=0.000020 grad_norm=0.711050
Epoch 60/100 Iteration 39/234: loss=0.041138 lr=0.000020 grad_norm=1.019937
Epoch 60/100 Iteration 40/234: loss=0.047027 lr=0.000020 grad_norm=1.050737
Epoch 60/100 Iteration 41/234: loss=0.054339 lr=0.000020 grad_norm=0.729722
Epoch 60/100 Iteration 42/234: loss=0.049411 lr=0.000020 grad_norm=1.164358
Epoch 60/100 Iteration 43/234: loss=0.045625 lr=0.000020 grad_norm=0.936639
Epoch 60/100 Iteration 44/234: loss=0.049454 lr=0.000020 grad_norm=0.660561
Epoch 60/100 Iteration 45/234: loss=0.045916 lr=0.000020 grad_norm=0.978375
Epoch 60/100 Iteration 46/234: loss=0.047054 lr=0.000020 grad_norm=0.535510
Epoch 60/100 Iteration 47/234: loss=0.038081 lr=0.000020 grad_norm=0.644824
Epoch 60/100 Iteration 48/234: loss=0.050908 lr=0.000020 grad_norm=0.868798
Epoch 60/100 Iteration 49/234: loss=0.043244 lr=0.000020 grad_norm=0.880951
Epoch 60/100 Iteration 50/234: loss=0.048196 lr=0.000020 grad_norm=0.754246
Epoch 60/100 Iteration 51/234: loss=0.041754 lr=0.000020 grad_norm=0.440671
Epoch 60/100 Iteration 52/234: loss=0.044790 lr=0.000020 grad_norm=0.831206
Epoch 60/100 Iteration 53/234: loss=0.047647 lr=0.000020 grad_norm=0.649359
Epoch 60/100 Iteration 54/234: loss=0.047908 lr=0.000020 grad_norm=0.537608
Epoch 60/100 Iteration 55/234: loss=0.048252 lr=0.000020 grad_norm=0.713688
Epoch 60/100 Iteration 56/234: loss=0.049155 lr=0.000020 grad_norm=0.736679
Epoch 60/100 Iteration 57/234: loss=0.045172 lr=0.000020 grad_norm=0.526345
Epoch 60/100 Iteration 58/234: loss=0.046630 lr=0.000020 grad_norm=0.617402
Epoch 60/100 Iteration 59/234: loss=0.043401 lr=0.000020 grad_norm=0.820170
Epoch 60/100 Iteration 60/234: loss=0.046226 lr=0.000020 grad_norm=0.687500
Epoch 60/100 Iteration 61/234: loss=0.043422 lr=0.000020 grad_norm=0.555466
Epoch 60/100 Iteration 62/234: loss=0.041056 lr=0.000020 grad_norm=0.705622
Epoch 60/100 Iteration 63/234: loss=0.047165 lr=0.000020 grad_norm=0.581559
Epoch 60/100 Iteration 64/234: loss=0.041818 lr=0.000020 grad_norm=0.789656
Epoch 60/100 Iteration 65/234: loss=0.041952 lr=0.000020 grad_norm=0.751055
Epoch 60/100 Iteration 66/234: loss=0.044878 lr=0.000020 grad_norm=0.423854
Epoch 60/100 Iteration 67/234: loss=0.048116 lr=0.000020 grad_norm=0.766274
Epoch 60/100 Iteration 68/234: loss=0.050327 lr=0.000020 grad_norm=0.938329
Epoch 60/100 Iteration 69/234: loss=0.049721 lr=0.000020 grad_norm=0.840254
Epoch 60/100 Iteration 70/234: loss=0.048561 lr=0.000020 grad_norm=0.496466
Epoch 60/100 Iteration 71/234: loss=0.048637 lr=0.000020 grad_norm=1.055977
Epoch 60/100 Iteration 72/234: loss=0.046219 lr=0.000020 grad_norm=1.376061
Epoch 60/100 Iteration 73/234: loss=0.045148 lr=0.000020 grad_norm=0.819864
Epoch 60/100 Iteration 74/234: loss=0.047019 lr=0.000020 grad_norm=0.535295
Epoch 60/100 Iteration 75/234: loss=0.042355 lr=0.000020 grad_norm=1.174981
Epoch 60/100 Iteration 76/234: loss=0.049564 lr=0.000020 grad_norm=0.860722
Epoch 60/100 Iteration 77/234: loss=0.046303 lr=0.000020 grad_norm=0.797355
Epoch 60/100 Iteration 78/234: loss=0.046490 lr=0.000020 grad_norm=0.938021
Epoch 60/100 Iteration 79/234: loss=0.046406 lr=0.000020 grad_norm=0.524626
Epoch 60/100 Iteration 80/234: loss=0.051967 lr=0.000020 grad_norm=0.908163
Epoch 60/100 Iteration 81/234: loss=0.045763 lr=0.000020 grad_norm=0.558902
Epoch 60/100 Iteration 82/234: loss=0.044081 lr=0.000020 grad_norm=0.791366
Epoch 60/100 Iteration 83/234: loss=0.051807 lr=0.000020 grad_norm=1.536585
Epoch 60/100 Iteration 84/234: loss=0.054512 lr=0.000020 grad_norm=1.582270
Epoch 60/100 Iteration 85/234: loss=0.042092 lr=0.000020 grad_norm=0.606432
Epoch 60/100 Iteration 86/234: loss=0.041900 lr=0.000020 grad_norm=0.646459
Epoch 60/100 Iteration 87/234: loss=0.048836 lr=0.000020 grad_norm=0.887268
Epoch 60/100 Iteration 88/234: loss=0.040854 lr=0.000020 grad_norm=0.521788
Epoch 60/100 Iteration 89/234: loss=0.044977 lr=0.000020 grad_norm=0.631212
Epoch 60/100 Iteration 90/234: loss=0.048023 lr=0.000020 grad_norm=0.748871
Epoch 60/100 Iteration 91/234: loss=0.044033 lr=0.000020 grad_norm=0.620439
Epoch 60/100 Iteration 92/234: loss=0.043196 lr=0.000020 grad_norm=0.735861
Epoch 60/100 Iteration 93/234: loss=0.045340 lr=0.000020 grad_norm=1.017376
Epoch 60/100 Iteration 94/234: loss=0.047957 lr=0.000020 grad_norm=0.959851
Epoch 60/100 Iteration 95/234: loss=0.049422 lr=0.000020 grad_norm=0.602999
Epoch 60/100 Iteration 96/234: loss=0.053659 lr=0.000020 grad_norm=0.863849
Epoch 60/100 Iteration 97/234: loss=0.050537 lr=0.000020 grad_norm=1.394847
Epoch 60/100 Iteration 98/234: loss=0.040883 lr=0.000020 grad_norm=1.038943
Epoch 60/100 Iteration 99/234: loss=0.045146 lr=0.000020 grad_norm=0.430564
Epoch 60/100 Iteration 100/234: loss=0.047995 lr=0.000020 grad_norm=1.428195
Epoch 60/100 Iteration 101/234: loss=0.041114 lr=0.000020 grad_norm=1.142821
Epoch 60/100 Iteration 102/234: loss=0.045029 lr=0.000020 grad_norm=0.704580
Epoch 60/100 Iteration 103/234: loss=0.048691 lr=0.000020 grad_norm=1.345916
Epoch 60/100 Iteration 104/234: loss=0.055598 lr=0.000020 grad_norm=0.603749
Epoch 60/100 Iteration 105/234: loss=0.050646 lr=0.000020 grad_norm=0.923467
Epoch 60/100 Iteration 106/234: loss=0.054378 lr=0.000020 grad_norm=1.445492
Epoch 60/100 Iteration 107/234: loss=0.048265 lr=0.000020 grad_norm=1.112338
Epoch 60/100 Iteration 108/234: loss=0.042956 lr=0.000020 grad_norm=0.426392
Epoch 60/100 Iteration 109/234: loss=0.046390 lr=0.000020 grad_norm=0.651318
Epoch 60/100 Iteration 110/234: loss=0.044663 lr=0.000020 grad_norm=0.703309
Epoch 60/100 Iteration 111/234: loss=0.042013 lr=0.000020 grad_norm=0.542295
Epoch 60/100 Iteration 112/234: loss=0.047266 lr=0.000020 grad_norm=0.548244
Epoch 60/100 Iteration 113/234: loss=0.048611 lr=0.000020 grad_norm=0.679099
Epoch 60/100 Iteration 114/234: loss=0.057568 lr=0.000020 grad_norm=0.608685
Epoch 60/100 Iteration 115/234: loss=0.043145 lr=0.000020 grad_norm=0.517499
Epoch 60/100 Iteration 116/234: loss=0.050911 lr=0.000020 grad_norm=0.521554
Epoch 60/100 Iteration 117/234: loss=0.046398 lr=0.000020 grad_norm=0.664739
Epoch 60/100 Iteration 118/234: loss=0.047623 lr=0.000020 grad_norm=0.464628
Epoch 60/100 Iteration 119/234: loss=0.050538 lr=0.000020 grad_norm=0.635687
Epoch 60/100 Iteration 120/234: loss=0.049920 lr=0.000020 grad_norm=0.990491
Epoch 60/100 Iteration 121/234: loss=0.045906 lr=0.000020 grad_norm=1.085546
Epoch 60/100 Iteration 122/234: loss=0.041334 lr=0.000020 grad_norm=0.755490
Epoch 60/100 Iteration 123/234: loss=0.049133 lr=0.000020 grad_norm=0.475966
Epoch 60/100 Iteration 124/234: loss=0.049357 lr=0.000020 grad_norm=1.233997
Epoch 60/100 Iteration 125/234: loss=0.052519 lr=0.000020 grad_norm=1.541912
Epoch 60/100 Iteration 126/234: loss=0.046035 lr=0.000020 grad_norm=0.785061
Epoch 60/100 Iteration 127/234: loss=0.047911 lr=0.000020 grad_norm=0.679580
Epoch 60/100 Iteration 128/234: loss=0.048460 lr=0.000020 grad_norm=0.871664
Epoch 60/100 Iteration 129/234: loss=0.045884 lr=0.000020 grad_norm=0.557157
Epoch 60/100 Iteration 130/234: loss=0.049822 lr=0.000020 grad_norm=0.362300
Epoch 60/100 Iteration 131/234: loss=0.046941 lr=0.000020 grad_norm=0.498243
Epoch 60/100 Iteration 132/234: loss=0.048775 lr=0.000020 grad_norm=0.474529
Epoch 60/100 Iteration 133/234: loss=0.050815 lr=0.000020 grad_norm=0.644496
Epoch 60/100 Iteration 134/234: loss=0.047177 lr=0.000020 grad_norm=1.176175
Epoch 60/100 Iteration 135/234: loss=0.038901 lr=0.000020 grad_norm=0.971063
Epoch 60/100 Iteration 136/234: loss=0.046981 lr=0.000020 grad_norm=0.531014
Epoch 60/100 Iteration 137/234: loss=0.047629 lr=0.000020 grad_norm=1.272789
Epoch 60/100 Iteration 138/234: loss=0.043209 lr=0.000020 grad_norm=0.839517
Epoch 60/100 Iteration 139/234: loss=0.050222 lr=0.000020 grad_norm=0.916105
Epoch 60/100 Iteration 140/234: loss=0.047118 lr=0.000020 grad_norm=1.882071
Epoch 60/100 Iteration 141/234: loss=0.046587 lr=0.000020 grad_norm=1.457392
Epoch 60/100 Iteration 142/234: loss=0.044493 lr=0.000020 grad_norm=0.582167
Epoch 60/100 Iteration 143/234: loss=0.045362 lr=0.000020 grad_norm=0.693760
Epoch 60/100 Iteration 144/234: loss=0.046757 lr=0.000020 grad_norm=0.788454
Epoch 60/100 Iteration 145/234: loss=0.046401 lr=0.000020 grad_norm=0.999030
Epoch 60/100 Iteration 146/234: loss=0.045489 lr=0.000020 grad_norm=1.143106
Epoch 60/100 Iteration 147/234: loss=0.046600 lr=0.000020 grad_norm=0.589213
Epoch 60/100 Iteration 148/234: loss=0.047978 lr=0.000020 grad_norm=1.343521
Epoch 60/100 Iteration 149/234: loss=0.040170 lr=0.000020 grad_norm=1.618775
Epoch 60/100 Iteration 150/234: loss=0.052170 lr=0.000020 grad_norm=0.726063
Epoch 60/100 Iteration 151/234: loss=0.040978 lr=0.000020 grad_norm=1.587192
Epoch 60/100 Iteration 152/234: loss=0.050557 lr=0.000020 grad_norm=1.007438
Epoch 60/100 Iteration 153/234: loss=0.049496 lr=0.000020 grad_norm=1.002001
Epoch 60/100 Iteration 154/234: loss=0.056657 lr=0.000020 grad_norm=1.567205
Epoch 60/100 Iteration 155/234: loss=0.044606 lr=0.000020 grad_norm=0.967591
Epoch 60/100 Iteration 156/234: loss=0.049646 lr=0.000020 grad_norm=0.575351
Epoch 60/100 Iteration 157/234: loss=0.043710 lr=0.000020 grad_norm=1.105829
Epoch 60/100 Iteration 158/234: loss=0.047790 lr=0.000020 grad_norm=0.533890
Epoch 60/100 Iteration 159/234: loss=0.041951 lr=0.000020 grad_norm=0.851605
Epoch 60/100 Iteration 160/234: loss=0.053634 lr=0.000020 grad_norm=1.082226
Epoch 60/100 Iteration 161/234: loss=0.043216 lr=0.000020 grad_norm=0.405601
Epoch 60/100 Iteration 162/234: loss=0.043219 lr=0.000020 grad_norm=0.945218
Epoch 60/100 Iteration 163/234: loss=0.046345 lr=0.000020 grad_norm=0.842419
Epoch 60/100 Iteration 164/234: loss=0.049346 lr=0.000020 grad_norm=0.872516
Epoch 60/100 Iteration 165/234: loss=0.043915 lr=0.000020 grad_norm=1.020542
Epoch 60/100 Iteration 166/234: loss=0.040496 lr=0.000020 grad_norm=0.645224
Epoch 60/100 Iteration 167/234: loss=0.044885 lr=0.000020 grad_norm=1.038842
Epoch 60/100 Iteration 168/234: loss=0.043898 lr=0.000020 grad_norm=1.057627
Epoch 60/100 Iteration 169/234: loss=0.050906 lr=0.000020 grad_norm=0.539270
Epoch 60/100 Iteration 170/234: loss=0.051886 lr=0.000020 grad_norm=1.122989
Epoch 60/100 Iteration 171/234: loss=0.048471 lr=0.000020 grad_norm=1.138138
Epoch 60/100 Iteration 172/234: loss=0.048718 lr=0.000020 grad_norm=0.517960
Epoch 60/100 Iteration 173/234: loss=0.038459 lr=0.000020 grad_norm=0.789309
Epoch 60/100 Iteration 174/234: loss=0.041785 lr=0.000020 grad_norm=0.674718
Epoch 60/100 Iteration 175/234: loss=0.044762 lr=0.000020 grad_norm=0.489442
Epoch 60/100 Iteration 176/234: loss=0.051355 lr=0.000020 grad_norm=0.484084
Epoch 60/100 Iteration 177/234: loss=0.049106 lr=0.000020 grad_norm=1.273492
Epoch 60/100 Iteration 178/234: loss=0.049621 lr=0.000020 grad_norm=1.858371
Epoch 60/100 Iteration 179/234: loss=0.043333 lr=0.000020 grad_norm=1.419524
Epoch 60/100 Iteration 180/234: loss=0.047675 lr=0.000020 grad_norm=0.469115
Epoch 60/100 Iteration 181/234: loss=0.039380 lr=0.000020 grad_norm=0.918865
Epoch 60/100 Iteration 182/234: loss=0.049310 lr=0.000020 grad_norm=0.720331
Epoch 60/100 Iteration 183/234: loss=0.046360 lr=0.000020 grad_norm=1.092013
Epoch 60/100 Iteration 184/234: loss=0.049773 lr=0.000020 grad_norm=1.401565
Epoch 60/100 Iteration 185/234: loss=0.048899 lr=0.000020 grad_norm=0.867633
Epoch 60/100 Iteration 186/234: loss=0.046586 lr=0.000020 grad_norm=0.397110
Epoch 60/100 Iteration 187/234: loss=0.045776 lr=0.000020 grad_norm=0.740895
Epoch 60/100 Iteration 188/234: loss=0.048502 lr=0.000020 grad_norm=0.905673
Epoch 60/100 Iteration 189/234: loss=0.048744 lr=0.000020 grad_norm=0.552871
Epoch 60/100 Iteration 190/234: loss=0.044483 lr=0.000020 grad_norm=0.727359
Epoch 60/100 Iteration 191/234: loss=0.046889 lr=0.000020 grad_norm=0.911082
Epoch 60/100 Iteration 192/234: loss=0.043420 lr=0.000020 grad_norm=0.645843
Epoch 60/100 Iteration 193/234: loss=0.046607 lr=0.000020 grad_norm=0.735838
Epoch 60/100 Iteration 194/234: loss=0.049603 lr=0.000020 grad_norm=1.384152
Epoch 60/100 Iteration 195/234: loss=0.043282 lr=0.000020 grad_norm=1.209870
Epoch 60/100 Iteration 196/234: loss=0.043447 lr=0.000020 grad_norm=0.594335
Epoch 60/100 Iteration 197/234: loss=0.047123 lr=0.000020 grad_norm=0.991990
Epoch 60/100 Iteration 198/234: loss=0.047788 lr=0.000020 grad_norm=0.767832
Epoch 60/100 Iteration 199/234: loss=0.052433 lr=0.000020 grad_norm=0.724398
Epoch 60/100 Iteration 200/234: loss=0.045828 lr=0.000020 grad_norm=1.040883
Epoch 60/100 Iteration 201/234: loss=0.047156 lr=0.000020 grad_norm=0.914447
Epoch 60/100 Iteration 202/234: loss=0.047007 lr=0.000020 grad_norm=0.565995
Epoch 60/100 Iteration 203/234: loss=0.042467 lr=0.000020 grad_norm=0.788657
Epoch 60/100 Iteration 204/234: loss=0.051011 lr=0.000020 grad_norm=0.641188
Epoch 60/100 Iteration 205/234: loss=0.043052 lr=0.000020 grad_norm=0.394560
Epoch 60/100 Iteration 206/234: loss=0.044121 lr=0.000020 grad_norm=0.592878
Epoch 60/100 Iteration 207/234: loss=0.049020 lr=0.000020 grad_norm=0.526599
Epoch 60/100 Iteration 208/234: loss=0.039058 lr=0.000020 grad_norm=0.405041
Epoch 60/100 Iteration 209/234: loss=0.051041 lr=0.000020 grad_norm=0.558544
Epoch 60/100 Iteration 210/234: loss=0.047845 lr=0.000020 grad_norm=0.561138
Epoch 60/100 Iteration 211/234: loss=0.037895 lr=0.000020 grad_norm=0.599399
Epoch 60/100 Iteration 212/234: loss=0.047313 lr=0.000020 grad_norm=0.713453
Epoch 60/100 Iteration 213/234: loss=0.052424 lr=0.000020 grad_norm=0.725062
Epoch 60/100 Iteration 214/234: loss=0.048380 lr=0.000020 grad_norm=0.525613
Epoch 60/100 Iteration 215/234: loss=0.039631 lr=0.000020 grad_norm=0.402826
Epoch 60/100 Iteration 216/234: loss=0.045753 lr=0.000020 grad_norm=0.506988
Epoch 60/100 Iteration 217/234: loss=0.043972 lr=0.000020 grad_norm=0.405848
Epoch 60/100 Iteration 218/234: loss=0.048583 lr=0.000020 grad_norm=0.545059
Epoch 60/100 Iteration 219/234: loss=0.049402 lr=0.000020 grad_norm=0.441875
Epoch 60/100 Iteration 220/234: loss=0.047522 lr=0.000020 grad_norm=0.686651
Epoch 60/100 Iteration 221/234: loss=0.046579 lr=0.000020 grad_norm=0.776031
Epoch 60/100 Iteration 222/234: loss=0.043132 lr=0.000020 grad_norm=0.474618
Epoch 60/100 Iteration 223/234: loss=0.050936 lr=0.000020 grad_norm=0.570932
Epoch 60/100 Iteration 224/234: loss=0.047562 lr=0.000020 grad_norm=0.888208
Epoch 60/100 Iteration 225/234: loss=0.043569 lr=0.000020 grad_norm=0.842513
Epoch 60/100 Iteration 226/234: loss=0.044836 lr=0.000020 grad_norm=0.481025
Epoch 60/100 Iteration 227/234: loss=0.043746 lr=0.000020 grad_norm=0.427326
Epoch 60/100 Iteration 228/234: loss=0.051222 lr=0.000020 grad_norm=0.645188
Epoch 60/100 Iteration 229/234: loss=0.046903 lr=0.000020 grad_norm=0.833126
Epoch 60/100 Iteration 230/234: loss=0.042881 lr=0.000020 grad_norm=0.669689
Epoch 60/100 Iteration 231/234: loss=0.050036 lr=0.000020 grad_norm=0.468032
Epoch 60/100 Iteration 232/234: loss=0.047260 lr=0.000020 grad_norm=0.435429
Epoch 60/100 Iteration 233/234: loss=0.045552 lr=0.000020 grad_norm=0.728113
Epoch 60/100 Iteration 234/234: loss=0.049190 lr=0.000020 grad_norm=0.810394
Epoch 60/100 finished. Avg Loss: 0.046875
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 61/100 Iteration 1/234: loss=0.046649 lr=0.000020 grad_norm=0.467196
Epoch 61/100 Iteration 2/234: loss=0.047918 lr=0.000020 grad_norm=0.678388
Epoch 61/100 Iteration 3/234: loss=0.040079 lr=0.000020 grad_norm=1.000085
Epoch 61/100 Iteration 4/234: loss=0.044342 lr=0.000020 grad_norm=0.475603
Epoch 61/100 Iteration 5/234: loss=0.052438 lr=0.000020 grad_norm=1.235184
Epoch 61/100 Iteration 6/234: loss=0.043126 lr=0.000020 grad_norm=1.737556
Epoch 61/100 Iteration 7/234: loss=0.051331 lr=0.000020 grad_norm=1.140186
Epoch 61/100 Iteration 8/234: loss=0.050284 lr=0.000020 grad_norm=0.631543
Epoch 61/100 Iteration 9/234: loss=0.042592 lr=0.000020 grad_norm=0.630662
Epoch 61/100 Iteration 10/234: loss=0.034952 lr=0.000020 grad_norm=0.443598
Epoch 61/100 Iteration 11/234: loss=0.047167 lr=0.000020 grad_norm=0.566204
Epoch 61/100 Iteration 12/234: loss=0.044684 lr=0.000020 grad_norm=0.913943
Epoch 61/100 Iteration 13/234: loss=0.047808 lr=0.000020 grad_norm=0.819171
Epoch 61/100 Iteration 14/234: loss=0.043023 lr=0.000020 grad_norm=0.479249
Epoch 61/100 Iteration 15/234: loss=0.048346 lr=0.000020 grad_norm=0.484206
Epoch 61/100 Iteration 16/234: loss=0.042493 lr=0.000020 grad_norm=0.778127
Epoch 61/100 Iteration 17/234: loss=0.050067 lr=0.000020 grad_norm=0.577547
Epoch 61/100 Iteration 18/234: loss=0.045381 lr=0.000020 grad_norm=0.482598
Epoch 61/100 Iteration 19/234: loss=0.043554 lr=0.000020 grad_norm=0.682312
Epoch 61/100 Iteration 20/234: loss=0.051396 lr=0.000020 grad_norm=0.543880
Epoch 61/100 Iteration 21/234: loss=0.045782 lr=0.000020 grad_norm=1.304487
Epoch 61/100 Iteration 22/234: loss=0.047918 lr=0.000020 grad_norm=1.518434
Epoch 61/100 Iteration 23/234: loss=0.045513 lr=0.000020 grad_norm=0.974496
Epoch 61/100 Iteration 24/234: loss=0.049024 lr=0.000020 grad_norm=0.326483
Epoch 61/100 Iteration 25/234: loss=0.049721 lr=0.000020 grad_norm=0.648366
Epoch 61/100 Iteration 26/234: loss=0.044587 lr=0.000020 grad_norm=0.531753
Epoch 61/100 Iteration 27/234: loss=0.047350 lr=0.000020 grad_norm=0.435763
Epoch 61/100 Iteration 28/234: loss=0.043169 lr=0.000020 grad_norm=0.406824
Epoch 61/100 Iteration 29/234: loss=0.045029 lr=0.000020 grad_norm=0.343698
Epoch 61/100 Iteration 30/234: loss=0.036782 lr=0.000020 grad_norm=0.368012
Epoch 61/100 Iteration 31/234: loss=0.043832 lr=0.000020 grad_norm=0.354199
Epoch 61/100 Iteration 32/234: loss=0.043499 lr=0.000020 grad_norm=0.821376
Epoch 61/100 Iteration 33/234: loss=0.046597 lr=0.000020 grad_norm=0.881738
Epoch 61/100 Iteration 34/234: loss=0.045096 lr=0.000020 grad_norm=0.454930
Epoch 61/100 Iteration 35/234: loss=0.050968 lr=0.000020 grad_norm=0.557003
Epoch 61/100 Iteration 36/234: loss=0.039705 lr=0.000020 grad_norm=0.567466
Epoch 61/100 Iteration 37/234: loss=0.047534 lr=0.000020 grad_norm=0.553113
Epoch 61/100 Iteration 38/234: loss=0.051981 lr=0.000020 grad_norm=1.056756
Epoch 61/100 Iteration 39/234: loss=0.048323 lr=0.000020 grad_norm=1.282234
Epoch 61/100 Iteration 40/234: loss=0.046558 lr=0.000020 grad_norm=1.075088
Epoch 61/100 Iteration 41/234: loss=0.044922 lr=0.000020 grad_norm=0.724896
Epoch 61/100 Iteration 42/234: loss=0.042395 lr=0.000020 grad_norm=0.742019
Epoch 61/100 Iteration 43/234: loss=0.046151 lr=0.000020 grad_norm=0.979956
Epoch 61/100 Iteration 44/234: loss=0.045736 lr=0.000020 grad_norm=0.604263
Epoch 61/100 Iteration 45/234: loss=0.040532 lr=0.000020 grad_norm=1.011981
Epoch 61/100 Iteration 46/234: loss=0.047298 lr=0.000020 grad_norm=1.826728
Epoch 61/100 Iteration 47/234: loss=0.045486 lr=0.000020 grad_norm=1.670735
Epoch 61/100 Iteration 48/234: loss=0.045067 lr=0.000020 grad_norm=0.714329
Epoch 61/100 Iteration 49/234: loss=0.044416 lr=0.000020 grad_norm=1.194398
Epoch 61/100 Iteration 50/234: loss=0.047093 lr=0.000020 grad_norm=1.999980
Epoch 61/100 Iteration 51/234: loss=0.046790 lr=0.000020 grad_norm=1.664440
Epoch 61/100 Iteration 52/234: loss=0.044764 lr=0.000020 grad_norm=0.661102
Epoch 61/100 Iteration 53/234: loss=0.050183 lr=0.000020 grad_norm=1.134262
Epoch 61/100 Iteration 54/234: loss=0.045271 lr=0.000020 grad_norm=1.672969
Epoch 61/100 Iteration 55/234: loss=0.047506 lr=0.000020 grad_norm=0.984756
Epoch 61/100 Iteration 56/234: loss=0.048316 lr=0.000020 grad_norm=1.021030
Epoch 61/100 Iteration 57/234: loss=0.045481 lr=0.000020 grad_norm=1.199835
Epoch 61/100 Iteration 58/234: loss=0.052852 lr=0.000020 grad_norm=0.847236
Epoch 61/100 Iteration 59/234: loss=0.051416 lr=0.000020 grad_norm=0.988131
Epoch 61/100 Iteration 60/234: loss=0.042943 lr=0.000020 grad_norm=1.317278
Epoch 61/100 Iteration 61/234: loss=0.046964 lr=0.000020 grad_norm=1.137532
Epoch 61/100 Iteration 62/234: loss=0.040119 lr=0.000020 grad_norm=1.103625
Epoch 61/100 Iteration 63/234: loss=0.047826 lr=0.000020 grad_norm=1.263897
Epoch 61/100 Iteration 64/234: loss=0.040593 lr=0.000020 grad_norm=0.989584
Epoch 61/100 Iteration 65/234: loss=0.040771 lr=0.000020 grad_norm=1.441208
Epoch 61/100 Iteration 66/234: loss=0.049223 lr=0.000020 grad_norm=1.460215
Epoch 61/100 Iteration 67/234: loss=0.045276 lr=0.000020 grad_norm=0.567361
Epoch 61/100 Iteration 68/234: loss=0.045049 lr=0.000020 grad_norm=1.526998
Epoch 61/100 Iteration 69/234: loss=0.044345 lr=0.000020 grad_norm=1.301849
Epoch 61/100 Iteration 70/234: loss=0.049402 lr=0.000020 grad_norm=0.613863
Epoch 61/100 Iteration 71/234: loss=0.048531 lr=0.000020 grad_norm=1.393729
Epoch 61/100 Iteration 72/234: loss=0.047724 lr=0.000020 grad_norm=0.967965
Epoch 61/100 Iteration 73/234: loss=0.048125 lr=0.000020 grad_norm=0.693706
Epoch 61/100 Iteration 74/234: loss=0.048275 lr=0.000020 grad_norm=1.306532
Epoch 61/100 Iteration 75/234: loss=0.049942 lr=0.000020 grad_norm=1.121119
Epoch 61/100 Iteration 76/234: loss=0.043551 lr=0.000020 grad_norm=0.523595
Epoch 61/100 Iteration 77/234: loss=0.044856 lr=0.000020 grad_norm=0.935388
Epoch 61/100 Iteration 78/234: loss=0.046049 lr=0.000020 grad_norm=1.141696
Epoch 61/100 Iteration 79/234: loss=0.056080 lr=0.000020 grad_norm=0.805849
Epoch 61/100 Iteration 80/234: loss=0.047756 lr=0.000020 grad_norm=0.985994
Epoch 61/100 Iteration 81/234: loss=0.051727 lr=0.000020 grad_norm=1.558098
Epoch 61/100 Iteration 82/234: loss=0.048103 lr=0.000020 grad_norm=1.359565
Epoch 61/100 Iteration 83/234: loss=0.040865 lr=0.000020 grad_norm=0.599275
Epoch 61/100 Iteration 84/234: loss=0.049281 lr=0.000020 grad_norm=0.899911
Epoch 61/100 Iteration 85/234: loss=0.044822 lr=0.000020 grad_norm=0.912505
Epoch 61/100 Iteration 86/234: loss=0.050148 lr=0.000020 grad_norm=0.629365
Epoch 61/100 Iteration 87/234: loss=0.037394 lr=0.000020 grad_norm=0.978274
Epoch 61/100 Iteration 88/234: loss=0.043974 lr=0.000020 grad_norm=0.652643
Epoch 61/100 Iteration 89/234: loss=0.046999 lr=0.000020 grad_norm=0.655266
Epoch 61/100 Iteration 90/234: loss=0.047434 lr=0.000020 grad_norm=0.751876
Epoch 61/100 Iteration 91/234: loss=0.048061 lr=0.000020 grad_norm=0.693486
Epoch 61/100 Iteration 92/234: loss=0.042469 lr=0.000020 grad_norm=0.421158
Epoch 61/100 Iteration 93/234: loss=0.047644 lr=0.000020 grad_norm=0.910062
Epoch 61/100 Iteration 94/234: loss=0.047653 lr=0.000020 grad_norm=0.897444
Epoch 61/100 Iteration 95/234: loss=0.046667 lr=0.000020 grad_norm=0.548801
Epoch 61/100 Iteration 96/234: loss=0.045662 lr=0.000020 grad_norm=1.130165
Epoch 61/100 Iteration 97/234: loss=0.046738 lr=0.000020 grad_norm=1.573327
Epoch 61/100 Iteration 98/234: loss=0.041642 lr=0.000020 grad_norm=1.025327
Epoch 61/100 Iteration 99/234: loss=0.042410 lr=0.000020 grad_norm=0.653174
Epoch 61/100 Iteration 100/234: loss=0.051990 lr=0.000020 grad_norm=1.655222
Epoch 61/100 Iteration 101/234: loss=0.040454 lr=0.000020 grad_norm=1.402351
Epoch 61/100 Iteration 102/234: loss=0.047098 lr=0.000020 grad_norm=0.459378
Epoch 61/100 Iteration 103/234: loss=0.044614 lr=0.000020 grad_norm=1.060309
Epoch 61/100 Iteration 104/234: loss=0.041038 lr=0.000020 grad_norm=1.089231
Epoch 61/100 Iteration 105/234: loss=0.049520 lr=0.000020 grad_norm=0.539884
Epoch 61/100 Iteration 106/234: loss=0.045589 lr=0.000020 grad_norm=0.840645
Epoch 61/100 Iteration 107/234: loss=0.046461 lr=0.000020 grad_norm=0.929710
Epoch 61/100 Iteration 108/234: loss=0.045068 lr=0.000020 grad_norm=0.453845
Epoch 61/100 Iteration 109/234: loss=0.050226 lr=0.000020 grad_norm=0.976865
Epoch 61/100 Iteration 110/234: loss=0.048660 lr=0.000020 grad_norm=1.323821
Epoch 61/100 Iteration 111/234: loss=0.046680 lr=0.000020 grad_norm=1.183049
Epoch 61/100 Iteration 112/234: loss=0.046494 lr=0.000020 grad_norm=0.538011
Epoch 61/100 Iteration 113/234: loss=0.053177 lr=0.000020 grad_norm=1.172309
Epoch 61/100 Iteration 114/234: loss=0.048751 lr=0.000020 grad_norm=1.645780
Epoch 61/100 Iteration 115/234: loss=0.046624 lr=0.000020 grad_norm=1.330693
Epoch 61/100 Iteration 116/234: loss=0.050498 lr=0.000020 grad_norm=0.540675
Epoch 61/100 Iteration 117/234: loss=0.038114 lr=0.000020 grad_norm=0.836327
Epoch 61/100 Iteration 118/234: loss=0.046310 lr=0.000020 grad_norm=1.075736
Epoch 61/100 Iteration 119/234: loss=0.044521 lr=0.000020 grad_norm=0.373258
Epoch 61/100 Iteration 120/234: loss=0.040688 lr=0.000020 grad_norm=0.688791
Epoch 61/100 Iteration 121/234: loss=0.049590 lr=0.000020 grad_norm=0.647872
Epoch 61/100 Iteration 122/234: loss=0.044281 lr=0.000020 grad_norm=0.432433
Epoch 61/100 Iteration 123/234: loss=0.044469 lr=0.000020 grad_norm=0.450062
Epoch 61/100 Iteration 124/234: loss=0.041695 lr=0.000020 grad_norm=0.689877
Epoch 61/100 Iteration 125/234: loss=0.047138 lr=0.000020 grad_norm=0.725735
Epoch 61/100 Iteration 126/234: loss=0.043023 lr=0.000020 grad_norm=0.438551
Epoch 61/100 Iteration 127/234: loss=0.045152 lr=0.000020 grad_norm=0.429484
Epoch 61/100 Iteration 128/234: loss=0.045996 lr=0.000020 grad_norm=0.760667
Epoch 61/100 Iteration 129/234: loss=0.042178 lr=0.000020 grad_norm=0.706944
Epoch 61/100 Iteration 130/234: loss=0.047415 lr=0.000020 grad_norm=0.468807
Epoch 61/100 Iteration 131/234: loss=0.049205 lr=0.000020 grad_norm=0.836510
Epoch 61/100 Iteration 132/234: loss=0.040241 lr=0.000020 grad_norm=0.870868
Epoch 61/100 Iteration 133/234: loss=0.046127 lr=0.000020 grad_norm=0.362685
Epoch 61/100 Iteration 134/234: loss=0.041268 lr=0.000020 grad_norm=0.768388
Epoch 61/100 Iteration 135/234: loss=0.043130 lr=0.000020 grad_norm=0.532535
Epoch 61/100 Iteration 136/234: loss=0.038335 lr=0.000020 grad_norm=0.557749
Epoch 61/100 Iteration 137/234: loss=0.038538 lr=0.000020 grad_norm=0.618891
Epoch 61/100 Iteration 138/234: loss=0.047073 lr=0.000020 grad_norm=0.476887
Epoch 61/100 Iteration 139/234: loss=0.049313 lr=0.000020 grad_norm=0.601214
Epoch 61/100 Iteration 140/234: loss=0.047175 lr=0.000020 grad_norm=0.456006
Epoch 61/100 Iteration 141/234: loss=0.043045 lr=0.000020 grad_norm=0.559815
Epoch 61/100 Iteration 142/234: loss=0.048737 lr=0.000020 grad_norm=0.501190
Epoch 61/100 Iteration 143/234: loss=0.043477 lr=0.000020 grad_norm=0.509445
Epoch 61/100 Iteration 144/234: loss=0.051277 lr=0.000020 grad_norm=0.425448
Epoch 61/100 Iteration 145/234: loss=0.045725 lr=0.000020 grad_norm=0.407396
Epoch 61/100 Iteration 146/234: loss=0.045970 lr=0.000020 grad_norm=0.346848
Epoch 61/100 Iteration 147/234: loss=0.048514 lr=0.000020 grad_norm=0.690421
Epoch 61/100 Iteration 148/234: loss=0.044043 lr=0.000020 grad_norm=1.023930
Epoch 61/100 Iteration 149/234: loss=0.043717 lr=0.000020 grad_norm=0.984561
Epoch 61/100 Iteration 150/234: loss=0.042401 lr=0.000020 grad_norm=0.549410
Epoch 61/100 Iteration 151/234: loss=0.039868 lr=0.000020 grad_norm=0.559369
Epoch 61/100 Iteration 152/234: loss=0.046076 lr=0.000020 grad_norm=0.468051
Epoch 61/100 Iteration 153/234: loss=0.041631 lr=0.000020 grad_norm=0.748850
Epoch 61/100 Iteration 154/234: loss=0.041722 lr=0.000020 grad_norm=0.463376
Epoch 61/100 Iteration 155/234: loss=0.042281 lr=0.000020 grad_norm=0.887688
Epoch 61/100 Iteration 156/234: loss=0.047615 lr=0.000020 grad_norm=1.539468
Epoch 61/100 Iteration 157/234: loss=0.045449 lr=0.000020 grad_norm=1.404596
Epoch 61/100 Iteration 158/234: loss=0.051287 lr=0.000020 grad_norm=0.948363
Epoch 61/100 Iteration 159/234: loss=0.043215 lr=0.000020 grad_norm=1.239664
Epoch 61/100 Iteration 160/234: loss=0.045566 lr=0.000020 grad_norm=1.632799
Epoch 61/100 Iteration 161/234: loss=0.044340 lr=0.000020 grad_norm=1.249668
Epoch 61/100 Iteration 162/234: loss=0.044887 lr=0.000020 grad_norm=0.764879
Epoch 61/100 Iteration 163/234: loss=0.043117 lr=0.000020 grad_norm=1.004517
Epoch 61/100 Iteration 164/234: loss=0.052406 lr=0.000020 grad_norm=1.092722
Epoch 61/100 Iteration 165/234: loss=0.045763 lr=0.000020 grad_norm=0.647347
Epoch 61/100 Iteration 166/234: loss=0.042089 lr=0.000020 grad_norm=0.983128
Epoch 61/100 Iteration 167/234: loss=0.049448 lr=0.000020 grad_norm=0.803543
Epoch 61/100 Iteration 168/234: loss=0.048024 lr=0.000020 grad_norm=0.506011
Epoch 61/100 Iteration 169/234: loss=0.040457 lr=0.000020 grad_norm=0.851891
Epoch 61/100 Iteration 170/234: loss=0.046092 lr=0.000020 grad_norm=0.617396
Epoch 61/100 Iteration 171/234: loss=0.048997 lr=0.000020 grad_norm=0.824190
Epoch 61/100 Iteration 172/234: loss=0.050351 lr=0.000020 grad_norm=0.890148
Epoch 61/100 Iteration 173/234: loss=0.047819 lr=0.000020 grad_norm=0.659218
Epoch 61/100 Iteration 174/234: loss=0.045892 lr=0.000020 grad_norm=0.794004
Epoch 61/100 Iteration 175/234: loss=0.045292 lr=0.000020 grad_norm=0.726761
Epoch 61/100 Iteration 176/234: loss=0.050084 lr=0.000020 grad_norm=0.598785
Epoch 61/100 Iteration 177/234: loss=0.043848 lr=0.000020 grad_norm=0.694075
Epoch 61/100 Iteration 178/234: loss=0.044054 lr=0.000020 grad_norm=0.633387
Epoch 61/100 Iteration 179/234: loss=0.040029 lr=0.000020 grad_norm=0.624835
Epoch 61/100 Iteration 180/234: loss=0.047448 lr=0.000020 grad_norm=0.603457
Epoch 61/100 Iteration 181/234: loss=0.044064 lr=0.000020 grad_norm=0.994526
Epoch 61/100 Iteration 182/234: loss=0.046410 lr=0.000020 grad_norm=1.382465
Epoch 61/100 Iteration 183/234: loss=0.049632 lr=0.000020 grad_norm=0.938407
Epoch 61/100 Iteration 184/234: loss=0.044761 lr=0.000020 grad_norm=0.781032
Epoch 61/100 Iteration 185/234: loss=0.042865 lr=0.000020 grad_norm=0.615204
Epoch 61/100 Iteration 186/234: loss=0.043337 lr=0.000020 grad_norm=1.194745
Epoch 61/100 Iteration 187/234: loss=0.045788 lr=0.000020 grad_norm=0.465860
Epoch 61/100 Iteration 188/234: loss=0.046808 lr=0.000020 grad_norm=1.162512
Epoch 61/100 Iteration 189/234: loss=0.045171 lr=0.000020 grad_norm=0.890992
Epoch 61/100 Iteration 190/234: loss=0.046921 lr=0.000020 grad_norm=0.691711
Epoch 61/100 Iteration 191/234: loss=0.043993 lr=0.000020 grad_norm=0.739481
Epoch 61/100 Iteration 192/234: loss=0.044473 lr=0.000020 grad_norm=0.811449
Epoch 61/100 Iteration 193/234: loss=0.043812 lr=0.000020 grad_norm=0.879098
Epoch 61/100 Iteration 194/234: loss=0.043976 lr=0.000020 grad_norm=0.470484
Epoch 61/100 Iteration 195/234: loss=0.045790 lr=0.000020 grad_norm=0.676289
Epoch 61/100 Iteration 196/234: loss=0.039960 lr=0.000020 grad_norm=0.686878
Epoch 61/100 Iteration 197/234: loss=0.041801 lr=0.000020 grad_norm=0.608967
Epoch 61/100 Iteration 198/234: loss=0.042911 lr=0.000020 grad_norm=0.739474
Epoch 61/100 Iteration 199/234: loss=0.046107 lr=0.000020 grad_norm=0.694419
Epoch 61/100 Iteration 200/234: loss=0.038153 lr=0.000020 grad_norm=0.490229
Epoch 61/100 Iteration 201/234: loss=0.046290 lr=0.000020 grad_norm=0.654659
Epoch 61/100 Iteration 202/234: loss=0.039318 lr=0.000020 grad_norm=0.587323
Epoch 61/100 Iteration 203/234: loss=0.039886 lr=0.000020 grad_norm=0.782333
Epoch 61/100 Iteration 204/234: loss=0.043905 lr=0.000020 grad_norm=1.349167
Epoch 61/100 Iteration 205/234: loss=0.044804 lr=0.000020 grad_norm=0.943146
Epoch 61/100 Iteration 206/234: loss=0.045175 lr=0.000020 grad_norm=0.579114
Epoch 61/100 Iteration 207/234: loss=0.048722 lr=0.000020 grad_norm=1.241096
Epoch 61/100 Iteration 208/234: loss=0.045203 lr=0.000020 grad_norm=0.755704
Epoch 61/100 Iteration 209/234: loss=0.045627 lr=0.000020 grad_norm=0.536410
Epoch 61/100 Iteration 210/234: loss=0.044522 lr=0.000020 grad_norm=0.886658
Epoch 61/100 Iteration 211/234: loss=0.042792 lr=0.000020 grad_norm=0.431047
Epoch 61/100 Iteration 212/234: loss=0.043578 lr=0.000020 grad_norm=1.013559
Epoch 61/100 Iteration 213/234: loss=0.042496 lr=0.000020 grad_norm=1.241776
Epoch 61/100 Iteration 214/234: loss=0.045713 lr=0.000020 grad_norm=0.631483
Epoch 61/100 Iteration 215/234: loss=0.045771 lr=0.000020 grad_norm=0.919437
Epoch 61/100 Iteration 216/234: loss=0.045817 lr=0.000020 grad_norm=0.595517
Epoch 61/100 Iteration 217/234: loss=0.049639 lr=0.000020 grad_norm=0.516965
Epoch 61/100 Iteration 218/234: loss=0.055626 lr=0.000020 grad_norm=0.726558
Epoch 61/100 Iteration 219/234: loss=0.044821 lr=0.000020 grad_norm=0.440322
Epoch 61/100 Iteration 220/234: loss=0.046768 lr=0.000020 grad_norm=0.601946
Epoch 61/100 Iteration 221/234: loss=0.043614 lr=0.000020 grad_norm=0.596280
Epoch 61/100 Iteration 222/234: loss=0.047752 lr=0.000020 grad_norm=0.729956
Epoch 61/100 Iteration 223/234: loss=0.048190 lr=0.000020 grad_norm=1.321617
Epoch 61/100 Iteration 224/234: loss=0.042607 lr=0.000020 grad_norm=1.504780
Epoch 61/100 Iteration 225/234: loss=0.046118 lr=0.000020 grad_norm=0.385857
Epoch 61/100 Iteration 226/234: loss=0.047931 lr=0.000020 grad_norm=1.570503
Epoch 61/100 Iteration 227/234: loss=0.041434 lr=0.000020 grad_norm=1.501837
Epoch 61/100 Iteration 228/234: loss=0.043636 lr=0.000020 grad_norm=0.389039
Epoch 61/100 Iteration 229/234: loss=0.041377 lr=0.000020 grad_norm=1.217425
Epoch 61/100 Iteration 230/234: loss=0.046493 lr=0.000020 grad_norm=0.861904
Epoch 61/100 Iteration 231/234: loss=0.051465 lr=0.000020 grad_norm=0.422498
Epoch 61/100 Iteration 232/234: loss=0.047864 lr=0.000020 grad_norm=0.692335
Epoch 61/100 Iteration 233/234: loss=0.043312 lr=0.000020 grad_norm=0.845015
Epoch 61/100 Iteration 234/234: loss=0.040743 lr=0.000020 grad_norm=1.015821
Epoch 61/100 finished. Avg Loss: 0.045611
Epoch 62/100 Iteration 1/234: loss=0.046436 lr=0.000020 grad_norm=0.853346
Epoch 62/100 Iteration 2/234: loss=0.046054 lr=0.000020 grad_norm=0.463120
Epoch 62/100 Iteration 3/234: loss=0.045461 lr=0.000020 grad_norm=0.487489
Epoch 62/100 Iteration 4/234: loss=0.046361 lr=0.000020 grad_norm=0.426528
Epoch 62/100 Iteration 5/234: loss=0.047212 lr=0.000020 grad_norm=0.589873
Epoch 62/100 Iteration 6/234: loss=0.048776 lr=0.000020 grad_norm=0.716761
Epoch 62/100 Iteration 7/234: loss=0.050758 lr=0.000020 grad_norm=0.955714
Epoch 62/100 Iteration 8/234: loss=0.041312 lr=0.000020 grad_norm=0.832751
Epoch 62/100 Iteration 9/234: loss=0.035699 lr=0.000020 grad_norm=0.550074
Epoch 62/100 Iteration 10/234: loss=0.043428 lr=0.000020 grad_norm=0.516255
Epoch 62/100 Iteration 11/234: loss=0.046464 lr=0.000020 grad_norm=0.392697
Epoch 62/100 Iteration 12/234: loss=0.047126 lr=0.000020 grad_norm=0.623819
Epoch 62/100 Iteration 13/234: loss=0.042742 lr=0.000020 grad_norm=0.841986
Epoch 62/100 Iteration 14/234: loss=0.053145 lr=0.000020 grad_norm=0.947120
Epoch 62/100 Iteration 15/234: loss=0.047412 lr=0.000020 grad_norm=0.698714
Epoch 62/100 Iteration 16/234: loss=0.050660 lr=0.000020 grad_norm=0.585988
Epoch 62/100 Iteration 17/234: loss=0.046502 lr=0.000020 grad_norm=1.070682
Epoch 62/100 Iteration 18/234: loss=0.045805 lr=0.000020 grad_norm=1.141151
Epoch 62/100 Iteration 19/234: loss=0.045766 lr=0.000020 grad_norm=0.822222
Epoch 62/100 Iteration 20/234: loss=0.045029 lr=0.000020 grad_norm=1.192178
Epoch 62/100 Iteration 21/234: loss=0.048816 lr=0.000020 grad_norm=1.548143
Epoch 62/100 Iteration 22/234: loss=0.042648 lr=0.000020 grad_norm=1.660626
Epoch 62/100 Iteration 23/234: loss=0.048880 lr=0.000020 grad_norm=1.020080
Epoch 62/100 Iteration 24/234: loss=0.050544 lr=0.000020 grad_norm=1.183448
Epoch 62/100 Iteration 25/234: loss=0.034564 lr=0.000020 grad_norm=0.926451
Epoch 62/100 Iteration 26/234: loss=0.045037 lr=0.000020 grad_norm=1.029168
Epoch 62/100 Iteration 27/234: loss=0.050898 lr=0.000020 grad_norm=1.764103
Epoch 62/100 Iteration 28/234: loss=0.045466 lr=0.000020 grad_norm=1.101383
Epoch 62/100 Iteration 29/234: loss=0.045974 lr=0.000020 grad_norm=0.879250
Epoch 62/100 Iteration 30/234: loss=0.041318 lr=0.000020 grad_norm=1.757490
Epoch 62/100 Iteration 31/234: loss=0.043865 lr=0.000020 grad_norm=1.706734
Epoch 62/100 Iteration 32/234: loss=0.042954 lr=0.000020 grad_norm=0.925969
Epoch 62/100 Iteration 33/234: loss=0.045040 lr=0.000020 grad_norm=1.526930
Epoch 62/100 Iteration 34/234: loss=0.044779 lr=0.000020 grad_norm=1.168736
Epoch 62/100 Iteration 35/234: loss=0.042606 lr=0.000020 grad_norm=0.870172
Epoch 62/100 Iteration 36/234: loss=0.046785 lr=0.000020 grad_norm=0.914439
Epoch 62/100 Iteration 37/234: loss=0.045967 lr=0.000020 grad_norm=0.989193
Epoch 62/100 Iteration 38/234: loss=0.046802 lr=0.000020 grad_norm=1.043604
Epoch 62/100 Iteration 39/234: loss=0.044791 lr=0.000020 grad_norm=0.652823
Epoch 62/100 Iteration 40/234: loss=0.043879 lr=0.000020 grad_norm=0.775184
Epoch 62/100 Iteration 41/234: loss=0.042387 lr=0.000020 grad_norm=1.013130
Epoch 62/100 Iteration 42/234: loss=0.042726 lr=0.000020 grad_norm=0.777116
Epoch 62/100 Iteration 43/234: loss=0.052143 lr=0.000020 grad_norm=0.986517
Epoch 62/100 Iteration 44/234: loss=0.047560 lr=0.000020 grad_norm=0.948897
Epoch 62/100 Iteration 45/234: loss=0.049261 lr=0.000020 grad_norm=0.427376
Epoch 62/100 Iteration 46/234: loss=0.050109 lr=0.000020 grad_norm=1.205164
Epoch 62/100 Iteration 47/234: loss=0.042842 lr=0.000020 grad_norm=1.142588
Epoch 62/100 Iteration 48/234: loss=0.047549 lr=0.000020 grad_norm=0.710472
Epoch 62/100 Iteration 49/234: loss=0.044263 lr=0.000020 grad_norm=1.120685
Epoch 62/100 Iteration 50/234: loss=0.047046 lr=0.000020 grad_norm=0.960397
Epoch 62/100 Iteration 51/234: loss=0.046702 lr=0.000020 grad_norm=0.520386
Epoch 62/100 Iteration 52/234: loss=0.047628 lr=0.000020 grad_norm=0.938031
Epoch 62/100 Iteration 53/234: loss=0.043308 lr=0.000020 grad_norm=1.017960
Epoch 62/100 Iteration 54/234: loss=0.048634 lr=0.000020 grad_norm=0.480077
Epoch 62/100 Iteration 55/234: loss=0.044143 lr=0.000020 grad_norm=0.693456
Epoch 62/100 Iteration 56/234: loss=0.049089 lr=0.000020 grad_norm=0.751327
Epoch 62/100 Iteration 57/234: loss=0.044836 lr=0.000020 grad_norm=0.508744
Epoch 62/100 Iteration 58/234: loss=0.047149 lr=0.000020 grad_norm=0.790276
Epoch 62/100 Iteration 59/234: loss=0.046819 lr=0.000020 grad_norm=0.769359
Epoch 62/100 Iteration 60/234: loss=0.048107 lr=0.000020 grad_norm=0.466625
Epoch 62/100 Iteration 61/234: loss=0.044694 lr=0.000020 grad_norm=0.789379
Epoch 62/100 Iteration 62/234: loss=0.039205 lr=0.000020 grad_norm=0.961277
Epoch 62/100 Iteration 63/234: loss=0.048152 lr=0.000020 grad_norm=0.828564
Epoch 62/100 Iteration 64/234: loss=0.050761 lr=0.000020 grad_norm=0.511860
Epoch 62/100 Iteration 65/234: loss=0.045745 lr=0.000020 grad_norm=0.782155
Epoch 62/100 Iteration 66/234: loss=0.043646 lr=0.000020 grad_norm=0.897440
Epoch 62/100 Iteration 67/234: loss=0.041318 lr=0.000020 grad_norm=0.759452
Epoch 62/100 Iteration 68/234: loss=0.048018 lr=0.000020 grad_norm=0.594286
Epoch 62/100 Iteration 69/234: loss=0.045608 lr=0.000020 grad_norm=0.826786
Epoch 62/100 Iteration 70/234: loss=0.048804 lr=0.000020 grad_norm=1.111833
Epoch 62/100 Iteration 71/234: loss=0.039750 lr=0.000020 grad_norm=0.734432
Epoch 62/100 Iteration 72/234: loss=0.046778 lr=0.000020 grad_norm=0.683304
Epoch 62/100 Iteration 73/234: loss=0.039235 lr=0.000020 grad_norm=0.622099
Epoch 62/100 Iteration 74/234: loss=0.044836 lr=0.000020 grad_norm=0.705694
Epoch 62/100 Iteration 75/234: loss=0.049276 lr=0.000020 grad_norm=0.649059
Epoch 62/100 Iteration 76/234: loss=0.039701 lr=0.000020 grad_norm=0.587502
Epoch 62/100 Iteration 77/234: loss=0.047246 lr=0.000020 grad_norm=0.633176
Epoch 62/100 Iteration 78/234: loss=0.043656 lr=0.000020 grad_norm=0.517071
Epoch 62/100 Iteration 79/234: loss=0.041813 lr=0.000020 grad_norm=0.859891
Epoch 62/100 Iteration 80/234: loss=0.046607 lr=0.000020 grad_norm=0.809420
Epoch 62/100 Iteration 81/234: loss=0.051132 lr=0.000020 grad_norm=0.778384
Epoch 62/100 Iteration 82/234: loss=0.048242 lr=0.000020 grad_norm=0.889597
Epoch 62/100 Iteration 83/234: loss=0.046782 lr=0.000020 grad_norm=0.820991
Epoch 62/100 Iteration 84/234: loss=0.041446 lr=0.000020 grad_norm=0.967565
Epoch 62/100 Iteration 85/234: loss=0.041754 lr=0.000020 grad_norm=0.562099
Epoch 62/100 Iteration 86/234: loss=0.055324 lr=0.000020 grad_norm=0.539817
Epoch 62/100 Iteration 87/234: loss=0.049202 lr=0.000020 grad_norm=0.745396
Epoch 62/100 Iteration 88/234: loss=0.040622 lr=0.000020 grad_norm=0.536322
Epoch 62/100 Iteration 89/234: loss=0.041870 lr=0.000020 grad_norm=0.489432
Epoch 62/100 Iteration 90/234: loss=0.042271 lr=0.000020 grad_norm=0.760429
Epoch 62/100 Iteration 91/234: loss=0.047974 lr=0.000020 grad_norm=1.011608
Epoch 62/100 Iteration 92/234: loss=0.043054 lr=0.000020 grad_norm=0.831326
Epoch 62/100 Iteration 93/234: loss=0.048435 lr=0.000020 grad_norm=0.519059
Epoch 62/100 Iteration 94/234: loss=0.042937 lr=0.000020 grad_norm=0.387394
Epoch 62/100 Iteration 95/234: loss=0.039667 lr=0.000020 grad_norm=0.505485
Epoch 62/100 Iteration 96/234: loss=0.048719 lr=0.000020 grad_norm=0.437008
Epoch 62/100 Iteration 97/234: loss=0.045175 lr=0.000020 grad_norm=0.524090
Epoch 62/100 Iteration 98/234: loss=0.046622 lr=0.000020 grad_norm=0.463086
Epoch 62/100 Iteration 99/234: loss=0.042308 lr=0.000020 grad_norm=0.413079
Epoch 62/100 Iteration 100/234: loss=0.042201 lr=0.000020 grad_norm=0.581516
Epoch 62/100 Iteration 101/234: loss=0.046348 lr=0.000020 grad_norm=0.737123
Epoch 62/100 Iteration 102/234: loss=0.050620 lr=0.000020 grad_norm=0.795804
Epoch 62/100 Iteration 103/234: loss=0.044158 lr=0.000020 grad_norm=0.943790
Epoch 62/100 Iteration 104/234: loss=0.040906 lr=0.000020 grad_norm=1.112469
Epoch 62/100 Iteration 105/234: loss=0.046579 lr=0.000020 grad_norm=0.527093
Epoch 62/100 Iteration 106/234: loss=0.041968 lr=0.000020 grad_norm=0.776946
Epoch 62/100 Iteration 107/234: loss=0.038753 lr=0.000020 grad_norm=0.976364
Epoch 62/100 Iteration 108/234: loss=0.039810 lr=0.000020 grad_norm=0.426031
Epoch 62/100 Iteration 109/234: loss=0.045778 lr=0.000020 grad_norm=0.842619
Epoch 62/100 Iteration 110/234: loss=0.045641 lr=0.000020 grad_norm=1.710106
Epoch 62/100 Iteration 111/234: loss=0.044377 lr=0.000020 grad_norm=2.057361
Epoch 62/100 Iteration 112/234: loss=0.045257 lr=0.000020 grad_norm=1.272359
Epoch 62/100 Iteration 113/234: loss=0.041993 lr=0.000020 grad_norm=0.652617
Epoch 62/100 Iteration 114/234: loss=0.041681 lr=0.000020 grad_norm=1.466702
Epoch 62/100 Iteration 115/234: loss=0.043090 lr=0.000020 grad_norm=1.151070
Epoch 62/100 Iteration 116/234: loss=0.044127 lr=0.000020 grad_norm=0.469885
Epoch 62/100 Iteration 117/234: loss=0.039361 lr=0.000020 grad_norm=1.119452
Epoch 62/100 Iteration 118/234: loss=0.046513 lr=0.000020 grad_norm=1.161034
Epoch 62/100 Iteration 119/234: loss=0.040578 lr=0.000020 grad_norm=0.511505
Epoch 62/100 Iteration 120/234: loss=0.041341 lr=0.000020 grad_norm=0.874998
Epoch 62/100 Iteration 121/234: loss=0.047984 lr=0.000020 grad_norm=0.889892
Epoch 62/100 Iteration 122/234: loss=0.041456 lr=0.000020 grad_norm=0.508341
Epoch 62/100 Iteration 123/234: loss=0.040029 lr=0.000020 grad_norm=1.031753
Epoch 62/100 Iteration 124/234: loss=0.047143 lr=0.000020 grad_norm=0.861077
Epoch 62/100 Iteration 125/234: loss=0.046018 lr=0.000020 grad_norm=0.439111
Epoch 62/100 Iteration 126/234: loss=0.039228 lr=0.000020 grad_norm=1.118253
Epoch 62/100 Iteration 127/234: loss=0.048188 lr=0.000020 grad_norm=0.565839
Epoch 62/100 Iteration 128/234: loss=0.049136 lr=0.000020 grad_norm=1.185106
Epoch 62/100 Iteration 129/234: loss=0.045026 lr=0.000020 grad_norm=1.866291
Epoch 62/100 Iteration 130/234: loss=0.042896 lr=0.000020 grad_norm=1.270681
Epoch 62/100 Iteration 131/234: loss=0.047627 lr=0.000020 grad_norm=0.940986
Epoch 62/100 Iteration 132/234: loss=0.045572 lr=0.000020 grad_norm=1.835386
Epoch 62/100 Iteration 133/234: loss=0.043201 lr=0.000020 grad_norm=1.243535
Epoch 62/100 Iteration 134/234: loss=0.044419 lr=0.000020 grad_norm=0.599301
Epoch 62/100 Iteration 135/234: loss=0.044686 lr=0.000020 grad_norm=1.089842
Epoch 62/100 Iteration 136/234: loss=0.040425 lr=0.000020 grad_norm=0.425338
Epoch 62/100 Iteration 137/234: loss=0.047771 lr=0.000020 grad_norm=1.198248
Epoch 62/100 Iteration 138/234: loss=0.049398 lr=0.000020 grad_norm=1.150812
Epoch 62/100 Iteration 139/234: loss=0.043313 lr=0.000020 grad_norm=0.438462
Epoch 62/100 Iteration 140/234: loss=0.047202 lr=0.000020 grad_norm=1.072584
Epoch 62/100 Iteration 141/234: loss=0.045347 lr=0.000020 grad_norm=1.044049
Epoch 62/100 Iteration 142/234: loss=0.046187 lr=0.000020 grad_norm=0.387957
Epoch 62/100 Iteration 143/234: loss=0.044705 lr=0.000020 grad_norm=0.854434
Epoch 62/100 Iteration 144/234: loss=0.041641 lr=0.000020 grad_norm=0.606555
Epoch 62/100 Iteration 145/234: loss=0.053826 lr=0.000020 grad_norm=0.909681
Epoch 62/100 Iteration 146/234: loss=0.047175 lr=0.000020 grad_norm=1.147471
Epoch 62/100 Iteration 147/234: loss=0.051715 lr=0.000020 grad_norm=0.493543
Epoch 62/100 Iteration 148/234: loss=0.049083 lr=0.000020 grad_norm=1.336447
Epoch 62/100 Iteration 149/234: loss=0.044004 lr=0.000020 grad_norm=1.463450
Epoch 62/100 Iteration 150/234: loss=0.040151 lr=0.000020 grad_norm=0.442412
Epoch 62/100 Iteration 151/234: loss=0.039889 lr=0.000020 grad_norm=1.190489
Epoch 62/100 Iteration 152/234: loss=0.043812 lr=0.000020 grad_norm=0.707863
Epoch 62/100 Iteration 153/234: loss=0.048485 lr=0.000020 grad_norm=1.242345
Epoch 62/100 Iteration 154/234: loss=0.047119 lr=0.000020 grad_norm=1.962860
Epoch 62/100 Iteration 155/234: loss=0.046890 lr=0.000020 grad_norm=1.152142
Epoch 62/100 Iteration 156/234: loss=0.049744 lr=0.000020 grad_norm=0.957521
Epoch 62/100 Iteration 157/234: loss=0.048965 lr=0.000020 grad_norm=1.552240
Epoch 62/100 Iteration 158/234: loss=0.050247 lr=0.000020 grad_norm=0.805438
Epoch 62/100 Iteration 159/234: loss=0.041895 lr=0.000020 grad_norm=0.973303
Epoch 62/100 Iteration 160/234: loss=0.044920 lr=0.000020 grad_norm=0.970547
Epoch 62/100 Iteration 161/234: loss=0.048972 lr=0.000020 grad_norm=0.626835
Epoch 62/100 Iteration 162/234: loss=0.043270 lr=0.000020 grad_norm=0.764803
Epoch 62/100 Iteration 163/234: loss=0.047390 lr=0.000020 grad_norm=0.458337
Epoch 62/100 Iteration 164/234: loss=0.049651 lr=0.000020 grad_norm=0.853643
Epoch 62/100 Iteration 165/234: loss=0.043343 lr=0.000020 grad_norm=0.717182
Epoch 62/100 Iteration 166/234: loss=0.045838 lr=0.000020 grad_norm=1.082548
Epoch 62/100 Iteration 167/234: loss=0.041795 lr=0.000020 grad_norm=1.298278
Epoch 62/100 Iteration 168/234: loss=0.042407 lr=0.000020 grad_norm=0.491230
Epoch 62/100 Iteration 169/234: loss=0.048990 lr=0.000020 grad_norm=1.624570
Epoch 62/100 Iteration 170/234: loss=0.045964 lr=0.000020 grad_norm=1.131122
Epoch 62/100 Iteration 171/234: loss=0.041979 lr=0.000020 grad_norm=0.616694
Epoch 62/100 Iteration 172/234: loss=0.041925 lr=0.000020 grad_norm=1.409015
Epoch 62/100 Iteration 173/234: loss=0.044731 lr=0.000020 grad_norm=0.736802
Epoch 62/100 Iteration 174/234: loss=0.040662 lr=0.000020 grad_norm=0.981940
Epoch 62/100 Iteration 175/234: loss=0.040576 lr=0.000020 grad_norm=1.053044
Epoch 62/100 Iteration 176/234: loss=0.037108 lr=0.000020 grad_norm=0.518378
Epoch 62/100 Iteration 177/234: loss=0.042625 lr=0.000020 grad_norm=0.740264
Epoch 62/100 Iteration 178/234: loss=0.046499 lr=0.000020 grad_norm=0.735256
Epoch 62/100 Iteration 179/234: loss=0.049880 lr=0.000020 grad_norm=0.523514
Epoch 62/100 Iteration 180/234: loss=0.043488 lr=0.000020 grad_norm=0.591906
Epoch 62/100 Iteration 181/234: loss=0.041713 lr=0.000020 grad_norm=0.537598
Epoch 62/100 Iteration 182/234: loss=0.037627 lr=0.000020 grad_norm=0.623285
Epoch 62/100 Iteration 183/234: loss=0.044752 lr=0.000020 grad_norm=0.536262
Epoch 62/100 Iteration 184/234: loss=0.047685 lr=0.000020 grad_norm=0.358078
Epoch 62/100 Iteration 185/234: loss=0.042974 lr=0.000020 grad_norm=0.627606
Epoch 62/100 Iteration 186/234: loss=0.042000 lr=0.000020 grad_norm=0.924960
Epoch 62/100 Iteration 187/234: loss=0.048150 lr=0.000020 grad_norm=1.156567
Epoch 62/100 Iteration 188/234: loss=0.048100 lr=0.000020 grad_norm=0.922926
Epoch 62/100 Iteration 189/234: loss=0.046851 lr=0.000020 grad_norm=0.467251
Epoch 62/100 Iteration 190/234: loss=0.043743 lr=0.000020 grad_norm=0.800104
Epoch 62/100 Iteration 191/234: loss=0.046192 lr=0.000020 grad_norm=1.000697
Epoch 62/100 Iteration 192/234: loss=0.046319 lr=0.000020 grad_norm=0.584533
Epoch 62/100 Iteration 193/234: loss=0.048541 lr=0.000020 grad_norm=0.802899
Epoch 62/100 Iteration 194/234: loss=0.043730 lr=0.000020 grad_norm=0.608568
Epoch 62/100 Iteration 195/234: loss=0.045338 lr=0.000020 grad_norm=0.804659
Epoch 62/100 Iteration 196/234: loss=0.043763 lr=0.000020 grad_norm=1.760841
Epoch 62/100 Iteration 197/234: loss=0.044864 lr=0.000020 grad_norm=1.736039
Epoch 62/100 Iteration 198/234: loss=0.045904 lr=0.000020 grad_norm=0.968682
Epoch 62/100 Iteration 199/234: loss=0.040572 lr=0.000020 grad_norm=1.134325
Epoch 62/100 Iteration 200/234: loss=0.041254 lr=0.000020 grad_norm=1.134050
Epoch 62/100 Iteration 201/234: loss=0.048115 lr=0.000020 grad_norm=1.473155
Epoch 62/100 Iteration 202/234: loss=0.049564 lr=0.000020 grad_norm=2.011922
Epoch 62/100 Iteration 203/234: loss=0.044423 lr=0.000020 grad_norm=1.624777
Epoch 62/100 Iteration 204/234: loss=0.048802 lr=0.000020 grad_norm=1.549609
Epoch 62/100 Iteration 205/234: loss=0.042326 lr=0.000020 grad_norm=1.771642
Epoch 62/100 Iteration 206/234: loss=0.040356 lr=0.000020 grad_norm=0.667033
Epoch 62/100 Iteration 207/234: loss=0.051715 lr=0.000020 grad_norm=1.668078
Epoch 62/100 Iteration 208/234: loss=0.047588 lr=0.000020 grad_norm=1.665234
Epoch 62/100 Iteration 209/234: loss=0.048740 lr=0.000020 grad_norm=0.896558
Epoch 62/100 Iteration 210/234: loss=0.048567 lr=0.000020 grad_norm=2.101140
Epoch 62/100 Iteration 211/234: loss=0.041461 lr=0.000020 grad_norm=0.840666
Epoch 62/100 Iteration 212/234: loss=0.047997 lr=0.000020 grad_norm=1.368389
Epoch 62/100 Iteration 213/234: loss=0.041935 lr=0.000020 grad_norm=1.413767
Epoch 62/100 Iteration 214/234: loss=0.051545 lr=0.000020 grad_norm=0.636598
Epoch 62/100 Iteration 215/234: loss=0.040967 lr=0.000020 grad_norm=1.451165
Epoch 62/100 Iteration 216/234: loss=0.047686 lr=0.000020 grad_norm=0.747237
Epoch 62/100 Iteration 217/234: loss=0.041503 lr=0.000020 grad_norm=0.905686
Epoch 62/100 Iteration 218/234: loss=0.043544 lr=0.000020 grad_norm=0.835718
Epoch 62/100 Iteration 219/234: loss=0.053711 lr=0.000020 grad_norm=0.966562
Epoch 62/100 Iteration 220/234: loss=0.048811 lr=0.000020 grad_norm=1.411821
Epoch 62/100 Iteration 221/234: loss=0.046730 lr=0.000020 grad_norm=0.596098
Epoch 62/100 Iteration 222/234: loss=0.045159 lr=0.000020 grad_norm=0.720086
Epoch 62/100 Iteration 223/234: loss=0.047717 lr=0.000020 grad_norm=0.754960
Epoch 62/100 Iteration 224/234: loss=0.037263 lr=0.000020 grad_norm=0.527225
Epoch 62/100 Iteration 225/234: loss=0.041792 lr=0.000020 grad_norm=0.718836
Epoch 62/100 Iteration 226/234: loss=0.046755 lr=0.000020 grad_norm=0.674223
Epoch 62/100 Iteration 227/234: loss=0.047357 lr=0.000020 grad_norm=0.528589
Epoch 62/100 Iteration 228/234: loss=0.046635 lr=0.000020 grad_norm=0.987533
Epoch 62/100 Iteration 229/234: loss=0.048424 lr=0.000020 grad_norm=1.128859
Epoch 62/100 Iteration 230/234: loss=0.046113 lr=0.000020 grad_norm=0.885613
Epoch 62/100 Iteration 231/234: loss=0.042511 lr=0.000020 grad_norm=0.569289
Epoch 62/100 Iteration 232/234: loss=0.040957 lr=0.000020 grad_norm=1.177240
Epoch 62/100 Iteration 233/234: loss=0.043713 lr=0.000020 grad_norm=0.816904
Epoch 62/100 Iteration 234/234: loss=0.049214 lr=0.000020 grad_norm=0.828836
Epoch 62/100 finished. Avg Loss: 0.045252
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 63/100 Iteration 1/234: loss=0.044474 lr=0.000020 grad_norm=1.439746
Epoch 63/100 Iteration 2/234: loss=0.048162 lr=0.000020 grad_norm=0.364941
Epoch 63/100 Iteration 3/234: loss=0.047283 lr=0.000020 grad_norm=1.750922
Epoch 63/100 Iteration 4/234: loss=0.046358 lr=0.000020 grad_norm=1.528713
Epoch 63/100 Iteration 5/234: loss=0.040948 lr=0.000020 grad_norm=0.503293
Epoch 63/100 Iteration 6/234: loss=0.042872 lr=0.000020 grad_norm=1.197037
Epoch 63/100 Iteration 7/234: loss=0.052921 lr=0.000020 grad_norm=1.081040
Epoch 63/100 Iteration 8/234: loss=0.046759 lr=0.000020 grad_norm=0.982386
Epoch 63/100 Iteration 9/234: loss=0.048937 lr=0.000020 grad_norm=0.761243
Epoch 63/100 Iteration 10/234: loss=0.039476 lr=0.000020 grad_norm=0.879231
Epoch 63/100 Iteration 11/234: loss=0.040322 lr=0.000020 grad_norm=0.882855
Epoch 63/100 Iteration 12/234: loss=0.041873 lr=0.000020 grad_norm=0.743041
Epoch 63/100 Iteration 13/234: loss=0.052926 lr=0.000020 grad_norm=1.353546
Epoch 63/100 Iteration 14/234: loss=0.041976 lr=0.000020 grad_norm=0.904360
Epoch 63/100 Iteration 15/234: loss=0.045775 lr=0.000020 grad_norm=0.928857
Epoch 63/100 Iteration 16/234: loss=0.045624 lr=0.000020 grad_norm=1.246882
Epoch 63/100 Iteration 17/234: loss=0.047280 lr=0.000020 grad_norm=0.778263
Epoch 63/100 Iteration 18/234: loss=0.049159 lr=0.000020 grad_norm=1.318614
Epoch 63/100 Iteration 19/234: loss=0.043397 lr=0.000020 grad_norm=0.819661
Epoch 63/100 Iteration 20/234: loss=0.043757 lr=0.000020 grad_norm=0.841175
Epoch 63/100 Iteration 21/234: loss=0.043793 lr=0.000020 grad_norm=1.168735
Epoch 63/100 Iteration 22/234: loss=0.047280 lr=0.000020 grad_norm=0.606768
Epoch 63/100 Iteration 23/234: loss=0.049264 lr=0.000020 grad_norm=1.400713
Epoch 63/100 Iteration 24/234: loss=0.048214 lr=0.000020 grad_norm=0.994792
Epoch 63/100 Iteration 25/234: loss=0.042716 lr=0.000020 grad_norm=0.469406
Epoch 63/100 Iteration 26/234: loss=0.048442 lr=0.000020 grad_norm=0.863795
Epoch 63/100 Iteration 27/234: loss=0.050622 lr=0.000020 grad_norm=0.531657
Epoch 63/100 Iteration 28/234: loss=0.053655 lr=0.000020 grad_norm=0.955930
Epoch 63/100 Iteration 29/234: loss=0.042093 lr=0.000020 grad_norm=1.200249
Epoch 63/100 Iteration 30/234: loss=0.047649 lr=0.000020 grad_norm=0.923725
Epoch 63/100 Iteration 31/234: loss=0.045267 lr=0.000020 grad_norm=0.692516
Epoch 63/100 Iteration 32/234: loss=0.042709 lr=0.000020 grad_norm=1.027403
Epoch 63/100 Iteration 33/234: loss=0.044120 lr=0.000020 grad_norm=0.772865
Epoch 63/100 Iteration 34/234: loss=0.047713 lr=0.000020 grad_norm=0.634019
Epoch 63/100 Iteration 35/234: loss=0.037479 lr=0.000020 grad_norm=0.631684
Epoch 63/100 Iteration 36/234: loss=0.048739 lr=0.000020 grad_norm=1.044139
Epoch 63/100 Iteration 37/234: loss=0.047369 lr=0.000020 grad_norm=1.177671
Epoch 63/100 Iteration 38/234: loss=0.047200 lr=0.000020 grad_norm=0.594125
Epoch 63/100 Iteration 39/234: loss=0.046882 lr=0.000020 grad_norm=0.920894
Epoch 63/100 Iteration 40/234: loss=0.047135 lr=0.000020 grad_norm=0.885856
Epoch 63/100 Iteration 41/234: loss=0.041115 lr=0.000020 grad_norm=0.380969
Epoch 63/100 Iteration 42/234: loss=0.039253 lr=0.000020 grad_norm=0.674721
Epoch 63/100 Iteration 43/234: loss=0.040224 lr=0.000020 grad_norm=0.610283
Epoch 63/100 Iteration 44/234: loss=0.045211 lr=0.000020 grad_norm=1.105503
Epoch 63/100 Iteration 45/234: loss=0.043993 lr=0.000020 grad_norm=1.833383
Epoch 63/100 Iteration 46/234: loss=0.042797 lr=0.000020 grad_norm=1.675736
Epoch 63/100 Iteration 47/234: loss=0.042418 lr=0.000020 grad_norm=0.717587
Epoch 63/100 Iteration 48/234: loss=0.045522 lr=0.000020 grad_norm=0.912256
Epoch 63/100 Iteration 49/234: loss=0.040564 lr=0.000020 grad_norm=0.955517
Epoch 63/100 Iteration 50/234: loss=0.042565 lr=0.000020 grad_norm=0.850461
Epoch 63/100 Iteration 51/234: loss=0.047484 lr=0.000020 grad_norm=1.742763
Epoch 63/100 Iteration 52/234: loss=0.043035 lr=0.000020 grad_norm=0.936698
Epoch 63/100 Iteration 53/234: loss=0.047149 lr=0.000020 grad_norm=0.922534
Epoch 63/100 Iteration 54/234: loss=0.046972 lr=0.000020 grad_norm=1.422234
Epoch 63/100 Iteration 55/234: loss=0.043936 lr=0.000020 grad_norm=1.157774
Epoch 63/100 Iteration 56/234: loss=0.051305 lr=0.000020 grad_norm=0.587427
Epoch 63/100 Iteration 57/234: loss=0.041867 lr=0.000020 grad_norm=1.337355
Epoch 63/100 Iteration 58/234: loss=0.055223 lr=0.000020 grad_norm=1.034045
Epoch 63/100 Iteration 59/234: loss=0.040191 lr=0.000020 grad_norm=0.496827
Epoch 63/100 Iteration 60/234: loss=0.048952 lr=0.000020 grad_norm=1.189419
Epoch 63/100 Iteration 61/234: loss=0.045210 lr=0.000020 grad_norm=1.276924
Epoch 63/100 Iteration 62/234: loss=0.047506 lr=0.000020 grad_norm=0.539141
Epoch 63/100 Iteration 63/234: loss=0.046796 lr=0.000020 grad_norm=1.218267
Epoch 63/100 Iteration 64/234: loss=0.045400 lr=0.000020 grad_norm=1.195371
Epoch 63/100 Iteration 65/234: loss=0.044774 lr=0.000020 grad_norm=1.131616
Epoch 63/100 Iteration 66/234: loss=0.042953 lr=0.000020 grad_norm=0.893440
Epoch 63/100 Iteration 67/234: loss=0.044608 lr=0.000020 grad_norm=1.468220
Epoch 63/100 Iteration 68/234: loss=0.041541 lr=0.000020 grad_norm=0.891200
Epoch 63/100 Iteration 69/234: loss=0.043195 lr=0.000020 grad_norm=0.972978
Epoch 63/100 Iteration 70/234: loss=0.043927 lr=0.000020 grad_norm=1.580286
Epoch 63/100 Iteration 71/234: loss=0.048562 lr=0.000020 grad_norm=0.792438
Epoch 63/100 Iteration 72/234: loss=0.041103 lr=0.000020 grad_norm=1.293063
Epoch 63/100 Iteration 73/234: loss=0.050187 lr=0.000020 grad_norm=1.412935
Epoch 63/100 Iteration 74/234: loss=0.043241 lr=0.000020 grad_norm=0.730533
Epoch 63/100 Iteration 75/234: loss=0.053010 lr=0.000020 grad_norm=1.367078
Epoch 63/100 Iteration 76/234: loss=0.044670 lr=0.000020 grad_norm=1.302927
Epoch 63/100 Iteration 77/234: loss=0.046461 lr=0.000020 grad_norm=0.681162
Epoch 63/100 Iteration 78/234: loss=0.045043 lr=0.000020 grad_norm=1.108644
Epoch 63/100 Iteration 79/234: loss=0.051787 lr=0.000020 grad_norm=0.999262
Epoch 63/100 Iteration 80/234: loss=0.049481 lr=0.000020 grad_norm=0.768679
Epoch 63/100 Iteration 81/234: loss=0.048169 lr=0.000020 grad_norm=1.034543
Epoch 63/100 Iteration 82/234: loss=0.052710 lr=0.000020 grad_norm=1.109830
Epoch 63/100 Iteration 83/234: loss=0.044550 lr=0.000020 grad_norm=0.996417
Epoch 63/100 Iteration 84/234: loss=0.043114 lr=0.000020 grad_norm=0.851779
Epoch 63/100 Iteration 85/234: loss=0.049512 lr=0.000020 grad_norm=1.385486
Epoch 63/100 Iteration 86/234: loss=0.043269 lr=0.000020 grad_norm=1.134908
Epoch 63/100 Iteration 87/234: loss=0.046000 lr=0.000020 grad_norm=0.587456
Epoch 63/100 Iteration 88/234: loss=0.037612 lr=0.000020 grad_norm=1.038543
Epoch 63/100 Iteration 89/234: loss=0.039839 lr=0.000020 grad_norm=0.555527
Epoch 63/100 Iteration 90/234: loss=0.039095 lr=0.000020 grad_norm=1.145296
Epoch 63/100 Iteration 91/234: loss=0.045202 lr=0.000020 grad_norm=1.327277
Epoch 63/100 Iteration 92/234: loss=0.045875 lr=0.000020 grad_norm=0.516188
Epoch 63/100 Iteration 93/234: loss=0.040193 lr=0.000020 grad_norm=0.579558
Epoch 63/100 Iteration 94/234: loss=0.041577 lr=0.000020 grad_norm=0.521218
Epoch 63/100 Iteration 95/234: loss=0.048123 lr=0.000020 grad_norm=0.833387
Epoch 63/100 Iteration 96/234: loss=0.047760 lr=0.000020 grad_norm=0.891964
Epoch 63/100 Iteration 97/234: loss=0.041250 lr=0.000020 grad_norm=0.524047
Epoch 63/100 Iteration 98/234: loss=0.044527 lr=0.000020 grad_norm=0.847272
Epoch 63/100 Iteration 99/234: loss=0.041929 lr=0.000020 grad_norm=0.765888
Epoch 63/100 Iteration 100/234: loss=0.047965 lr=0.000020 grad_norm=0.450552
Epoch 63/100 Iteration 101/234: loss=0.046740 lr=0.000020 grad_norm=0.515926
Epoch 63/100 Iteration 102/234: loss=0.044498 lr=0.000020 grad_norm=0.455355
Epoch 63/100 Iteration 103/234: loss=0.045646 lr=0.000020 grad_norm=0.973702
Epoch 63/100 Iteration 104/234: loss=0.045815 lr=0.000020 grad_norm=1.226116
Epoch 63/100 Iteration 105/234: loss=0.044615 lr=0.000020 grad_norm=0.812408
Epoch 63/100 Iteration 106/234: loss=0.046656 lr=0.000020 grad_norm=0.658887
Epoch 63/100 Iteration 107/234: loss=0.044955 lr=0.000020 grad_norm=0.885113
Epoch 63/100 Iteration 108/234: loss=0.040354 lr=0.000020 grad_norm=0.519826
Epoch 63/100 Iteration 109/234: loss=0.041189 lr=0.000020 grad_norm=0.502634
Epoch 63/100 Iteration 110/234: loss=0.042229 lr=0.000020 grad_norm=0.697570
Epoch 63/100 Iteration 111/234: loss=0.040633 lr=0.000020 grad_norm=0.676387
Epoch 63/100 Iteration 112/234: loss=0.046514 lr=0.000020 grad_norm=0.980646
Epoch 63/100 Iteration 113/234: loss=0.039188 lr=0.000020 grad_norm=0.672083
Epoch 63/100 Iteration 114/234: loss=0.042697 lr=0.000020 grad_norm=0.934544
Epoch 63/100 Iteration 115/234: loss=0.045857 lr=0.000020 grad_norm=0.944778
Epoch 63/100 Iteration 116/234: loss=0.046685 lr=0.000020 grad_norm=0.626200
Epoch 63/100 Iteration 117/234: loss=0.047329 lr=0.000020 grad_norm=0.985009
Epoch 63/100 Iteration 118/234: loss=0.045401 lr=0.000020 grad_norm=0.711028
Epoch 63/100 Iteration 119/234: loss=0.041181 lr=0.000020 grad_norm=0.797190
Epoch 63/100 Iteration 120/234: loss=0.043387 lr=0.000020 grad_norm=1.106489
Epoch 63/100 Iteration 121/234: loss=0.044732 lr=0.000020 grad_norm=0.803285
Epoch 63/100 Iteration 122/234: loss=0.041794 lr=0.000020 grad_norm=0.499865
Epoch 63/100 Iteration 123/234: loss=0.042690 lr=0.000020 grad_norm=0.696210
Epoch 63/100 Iteration 124/234: loss=0.052586 lr=0.000020 grad_norm=0.517519
Epoch 63/100 Iteration 125/234: loss=0.044723 lr=0.000020 grad_norm=0.623646
Epoch 63/100 Iteration 126/234: loss=0.041904 lr=0.000020 grad_norm=0.651935
Epoch 63/100 Iteration 127/234: loss=0.046350 lr=0.000020 grad_norm=0.407178
Epoch 63/100 Iteration 128/234: loss=0.048786 lr=0.000020 grad_norm=0.642227
Epoch 63/100 Iteration 129/234: loss=0.044878 lr=0.000020 grad_norm=0.812965
Epoch 63/100 Iteration 130/234: loss=0.045607 lr=0.000020 grad_norm=0.502156
Epoch 63/100 Iteration 131/234: loss=0.043158 lr=0.000020 grad_norm=0.546314
Epoch 63/100 Iteration 132/234: loss=0.046885 lr=0.000020 grad_norm=0.771114
Epoch 63/100 Iteration 133/234: loss=0.046204 lr=0.000020 grad_norm=0.657038
Epoch 63/100 Iteration 134/234: loss=0.044787 lr=0.000020 grad_norm=0.506803
Epoch 63/100 Iteration 135/234: loss=0.046594 lr=0.000020 grad_norm=0.541749
Epoch 63/100 Iteration 136/234: loss=0.042589 lr=0.000020 grad_norm=0.507733
Epoch 63/100 Iteration 137/234: loss=0.037509 lr=0.000020 grad_norm=0.428632
Epoch 63/100 Iteration 138/234: loss=0.039959 lr=0.000020 grad_norm=0.506276
Epoch 63/100 Iteration 139/234: loss=0.046042 lr=0.000020 grad_norm=0.610745
Epoch 63/100 Iteration 140/234: loss=0.045518 lr=0.000020 grad_norm=0.520599
Epoch 63/100 Iteration 141/234: loss=0.044161 lr=0.000020 grad_norm=0.396601
Epoch 63/100 Iteration 142/234: loss=0.043671 lr=0.000020 grad_norm=0.514930
Epoch 63/100 Iteration 143/234: loss=0.046257 lr=0.000020 grad_norm=0.559750
Epoch 63/100 Iteration 144/234: loss=0.041818 lr=0.000020 grad_norm=0.562588
Epoch 63/100 Iteration 145/234: loss=0.048354 lr=0.000020 grad_norm=0.549427
Epoch 63/100 Iteration 146/234: loss=0.043245 lr=0.000020 grad_norm=0.497893
Epoch 63/100 Iteration 147/234: loss=0.047165 lr=0.000020 grad_norm=0.738847
Epoch 63/100 Iteration 148/234: loss=0.038424 lr=0.000020 grad_norm=0.682957
Epoch 63/100 Iteration 149/234: loss=0.046194 lr=0.000020 grad_norm=0.331515
Epoch 63/100 Iteration 150/234: loss=0.048552 lr=0.000020 grad_norm=0.969154
Epoch 63/100 Iteration 151/234: loss=0.042654 lr=0.000020 grad_norm=0.810953
Epoch 63/100 Iteration 152/234: loss=0.039738 lr=0.000020 grad_norm=0.317009
Epoch 63/100 Iteration 153/234: loss=0.043677 lr=0.000020 grad_norm=1.007580
Epoch 63/100 Iteration 154/234: loss=0.048274 lr=0.000020 grad_norm=1.085503
Epoch 63/100 Iteration 155/234: loss=0.046432 lr=0.000020 grad_norm=0.819981
Epoch 63/100 Iteration 156/234: loss=0.040464 lr=0.000020 grad_norm=0.606827
Epoch 63/100 Iteration 157/234: loss=0.046312 lr=0.000020 grad_norm=0.898279
Epoch 63/100 Iteration 158/234: loss=0.038188 lr=0.000020 grad_norm=0.858722
Epoch 63/100 Iteration 159/234: loss=0.040736 lr=0.000020 grad_norm=0.491221
Epoch 63/100 Iteration 160/234: loss=0.042024 lr=0.000020 grad_norm=0.479944
Epoch 63/100 Iteration 161/234: loss=0.043550 lr=0.000020 grad_norm=0.604507
Epoch 63/100 Iteration 162/234: loss=0.037678 lr=0.000020 grad_norm=0.643005
Epoch 63/100 Iteration 163/234: loss=0.043793 lr=0.000020 grad_norm=0.777539
Epoch 63/100 Iteration 164/234: loss=0.042948 lr=0.000020 grad_norm=0.405824
Epoch 63/100 Iteration 165/234: loss=0.045146 lr=0.000020 grad_norm=1.210809
Epoch 63/100 Iteration 166/234: loss=0.046948 lr=0.000020 grad_norm=1.634353
Epoch 63/100 Iteration 167/234: loss=0.044751 lr=0.000020 grad_norm=1.169518
Epoch 63/100 Iteration 168/234: loss=0.047842 lr=0.000020 grad_norm=0.766352
Epoch 63/100 Iteration 169/234: loss=0.044290 lr=0.000020 grad_norm=1.421005
Epoch 63/100 Iteration 170/234: loss=0.044386 lr=0.000020 grad_norm=0.923003
Epoch 63/100 Iteration 171/234: loss=0.040221 lr=0.000020 grad_norm=0.817072
Epoch 63/100 Iteration 172/234: loss=0.044527 lr=0.000020 grad_norm=1.335229
Epoch 63/100 Iteration 173/234: loss=0.039964 lr=0.000020 grad_norm=0.906639
Epoch 63/100 Iteration 174/234: loss=0.046938 lr=0.000020 grad_norm=0.821136
Epoch 63/100 Iteration 175/234: loss=0.046698 lr=0.000020 grad_norm=0.897532
Epoch 63/100 Iteration 176/234: loss=0.038091 lr=0.000020 grad_norm=0.794514
Epoch 63/100 Iteration 177/234: loss=0.046792 lr=0.000020 grad_norm=1.056522
Epoch 63/100 Iteration 178/234: loss=0.044040 lr=0.000020 grad_norm=0.880548
Epoch 63/100 Iteration 179/234: loss=0.043573 lr=0.000020 grad_norm=0.759932
Epoch 63/100 Iteration 180/234: loss=0.043965 lr=0.000020 grad_norm=0.925307
Epoch 63/100 Iteration 181/234: loss=0.044843 lr=0.000020 grad_norm=0.685827
Epoch 63/100 Iteration 182/234: loss=0.044804 lr=0.000020 grad_norm=0.724638
Epoch 63/100 Iteration 183/234: loss=0.044679 lr=0.000020 grad_norm=0.819961
Epoch 63/100 Iteration 184/234: loss=0.046118 lr=0.000020 grad_norm=0.736031
Epoch 63/100 Iteration 185/234: loss=0.040792 lr=0.000020 grad_norm=0.668942
Epoch 63/100 Iteration 186/234: loss=0.044607 lr=0.000020 grad_norm=0.589956
Epoch 63/100 Iteration 187/234: loss=0.041454 lr=0.000020 grad_norm=0.787838
Epoch 63/100 Iteration 188/234: loss=0.041645 lr=0.000020 grad_norm=0.542173
Epoch 63/100 Iteration 189/234: loss=0.047741 lr=0.000020 grad_norm=0.533552
Epoch 63/100 Iteration 190/234: loss=0.048676 lr=0.000020 grad_norm=1.257313
Epoch 63/100 Iteration 191/234: loss=0.041019 lr=0.000020 grad_norm=1.456227
Epoch 63/100 Iteration 192/234: loss=0.042018 lr=0.000020 grad_norm=0.622656
Epoch 63/100 Iteration 193/234: loss=0.047715 lr=0.000020 grad_norm=1.442462
Epoch 63/100 Iteration 194/234: loss=0.045659 lr=0.000020 grad_norm=2.267778
Epoch 63/100 Iteration 195/234: loss=0.040424 lr=0.000020 grad_norm=1.337194
Epoch 63/100 Iteration 196/234: loss=0.040249 lr=0.000020 grad_norm=0.586148
Epoch 63/100 Iteration 197/234: loss=0.047998 lr=0.000020 grad_norm=1.403893
Epoch 63/100 Iteration 198/234: loss=0.040258 lr=0.000020 grad_norm=0.513074
Epoch 63/100 Iteration 199/234: loss=0.039361 lr=0.000020 grad_norm=1.195559
Epoch 63/100 Iteration 200/234: loss=0.047645 lr=0.000020 grad_norm=1.027559
Epoch 63/100 Iteration 201/234: loss=0.040232 lr=0.000020 grad_norm=0.703436
Epoch 63/100 Iteration 202/234: loss=0.044855 lr=0.000020 grad_norm=0.968615
Epoch 63/100 Iteration 203/234: loss=0.043134 lr=0.000020 grad_norm=0.588600
Epoch 63/100 Iteration 204/234: loss=0.045961 lr=0.000020 grad_norm=1.392112
Epoch 63/100 Iteration 205/234: loss=0.047817 lr=0.000020 grad_norm=1.610653
Epoch 63/100 Iteration 206/234: loss=0.042683 lr=0.000020 grad_norm=0.716628
Epoch 63/100 Iteration 207/234: loss=0.046255 lr=0.000020 grad_norm=1.081208
Epoch 63/100 Iteration 208/234: loss=0.039523 lr=0.000020 grad_norm=1.092742
Epoch 63/100 Iteration 209/234: loss=0.039478 lr=0.000020 grad_norm=0.919617
Epoch 63/100 Iteration 210/234: loss=0.042519 lr=0.000020 grad_norm=1.644753
Epoch 63/100 Iteration 211/234: loss=0.045440 lr=0.000020 grad_norm=0.948634
Epoch 63/100 Iteration 212/234: loss=0.046596 lr=0.000020 grad_norm=1.526683
Epoch 63/100 Iteration 213/234: loss=0.046041 lr=0.000020 grad_norm=1.305513
Epoch 63/100 Iteration 214/234: loss=0.044601 lr=0.000020 grad_norm=1.289757
Epoch 63/100 Iteration 215/234: loss=0.043974 lr=0.000020 grad_norm=1.970290
Epoch 63/100 Iteration 216/234: loss=0.045882 lr=0.000020 grad_norm=0.658579
Epoch 63/100 Iteration 217/234: loss=0.045630 lr=0.000020 grad_norm=1.774904
Epoch 63/100 Iteration 218/234: loss=0.047331 lr=0.000020 grad_norm=1.023305
Epoch 63/100 Iteration 219/234: loss=0.046797 lr=0.000020 grad_norm=1.619741
Epoch 63/100 Iteration 220/234: loss=0.040843 lr=0.000020 grad_norm=1.889051
Epoch 63/100 Iteration 221/234: loss=0.041215 lr=0.000020 grad_norm=1.153832
Epoch 63/100 Iteration 222/234: loss=0.047852 lr=0.000020 grad_norm=2.745406
Epoch 63/100 Iteration 223/234: loss=0.043054 lr=0.000020 grad_norm=1.248555
Epoch 63/100 Iteration 224/234: loss=0.041125 lr=0.000020 grad_norm=1.332841
Epoch 63/100 Iteration 225/234: loss=0.043523 lr=0.000020 grad_norm=1.058725
Epoch 63/100 Iteration 226/234: loss=0.041715 lr=0.000020 grad_norm=1.227715
Epoch 63/100 Iteration 227/234: loss=0.041209 lr=0.000020 grad_norm=0.982417
Epoch 63/100 Iteration 228/234: loss=0.041258 lr=0.000020 grad_norm=1.280962
Epoch 63/100 Iteration 229/234: loss=0.047446 lr=0.000020 grad_norm=1.272616
Epoch 63/100 Iteration 230/234: loss=0.045862 lr=0.000020 grad_norm=0.547176
Epoch 63/100 Iteration 231/234: loss=0.042644 lr=0.000020 grad_norm=0.975551
Epoch 63/100 Iteration 232/234: loss=0.045161 lr=0.000020 grad_norm=0.559813
Epoch 63/100 Iteration 233/234: loss=0.045299 lr=0.000020 grad_norm=1.192457
Epoch 63/100 Iteration 234/234: loss=0.047363 lr=0.000020 grad_norm=0.661152
Epoch 63/100 finished. Avg Loss: 0.044622
Epoch 64/100 Iteration 1/234: loss=0.036971 lr=0.000020 grad_norm=0.957602
Epoch 64/100 Iteration 2/234: loss=0.042767 lr=0.000020 grad_norm=0.739935
Epoch 64/100 Iteration 3/234: loss=0.040390 lr=0.000020 grad_norm=0.794921
Epoch 64/100 Iteration 4/234: loss=0.047293 lr=0.000020 grad_norm=0.975596
Epoch 64/100 Iteration 5/234: loss=0.044124 lr=0.000020 grad_norm=0.435287
Epoch 64/100 Iteration 6/234: loss=0.046522 lr=0.000020 grad_norm=0.671108
Epoch 64/100 Iteration 7/234: loss=0.042405 lr=0.000020 grad_norm=0.441554
Epoch 64/100 Iteration 8/234: loss=0.043252 lr=0.000020 grad_norm=0.572438
Epoch 64/100 Iteration 9/234: loss=0.048539 lr=0.000020 grad_norm=0.541375
Epoch 64/100 Iteration 10/234: loss=0.048813 lr=0.000020 grad_norm=0.702161
Epoch 64/100 Iteration 11/234: loss=0.046035 lr=0.000020 grad_norm=0.544792
Epoch 64/100 Iteration 12/234: loss=0.039484 lr=0.000020 grad_norm=0.513530
Epoch 64/100 Iteration 13/234: loss=0.042564 lr=0.000020 grad_norm=0.588340
Epoch 64/100 Iteration 14/234: loss=0.045829 lr=0.000020 grad_norm=0.527753
Epoch 64/100 Iteration 15/234: loss=0.038890 lr=0.000020 grad_norm=0.713189
Epoch 64/100 Iteration 16/234: loss=0.041114 lr=0.000020 grad_norm=0.647384
Epoch 64/100 Iteration 17/234: loss=0.040425 lr=0.000020 grad_norm=0.690095
Epoch 64/100 Iteration 18/234: loss=0.043263 lr=0.000020 grad_norm=0.649196
Epoch 64/100 Iteration 19/234: loss=0.043534 lr=0.000020 grad_norm=0.474841
Epoch 64/100 Iteration 20/234: loss=0.042556 lr=0.000020 grad_norm=0.613361
Epoch 64/100 Iteration 21/234: loss=0.047425 lr=0.000020 grad_norm=0.708049
Epoch 64/100 Iteration 22/234: loss=0.044555 lr=0.000020 grad_norm=0.711372
Epoch 64/100 Iteration 23/234: loss=0.043446 lr=0.000020 grad_norm=0.618135
Epoch 64/100 Iteration 24/234: loss=0.042266 lr=0.000020 grad_norm=0.362459
Epoch 64/100 Iteration 25/234: loss=0.045223 lr=0.000020 grad_norm=0.674703
Epoch 64/100 Iteration 26/234: loss=0.043476 lr=0.000020 grad_norm=1.129674
Epoch 64/100 Iteration 27/234: loss=0.046391 lr=0.000020 grad_norm=1.125002
Epoch 64/100 Iteration 28/234: loss=0.047752 lr=0.000020 grad_norm=0.464533
Epoch 64/100 Iteration 29/234: loss=0.043786 lr=0.000020 grad_norm=0.982076
Epoch 64/100 Iteration 30/234: loss=0.046832 lr=0.000020 grad_norm=0.894575
Epoch 64/100 Iteration 31/234: loss=0.044356 lr=0.000020 grad_norm=0.579255
Epoch 64/100 Iteration 32/234: loss=0.044019 lr=0.000020 grad_norm=1.175816
Epoch 64/100 Iteration 33/234: loss=0.042135 lr=0.000020 grad_norm=0.743902
Epoch 64/100 Iteration 34/234: loss=0.038061 lr=0.000020 grad_norm=0.965349
Epoch 64/100 Iteration 35/234: loss=0.036666 lr=0.000020 grad_norm=1.169875
Epoch 64/100 Iteration 36/234: loss=0.045671 lr=0.000020 grad_norm=0.642588
Epoch 64/100 Iteration 37/234: loss=0.046989 lr=0.000020 grad_norm=1.464869
Epoch 64/100 Iteration 38/234: loss=0.050621 lr=0.000020 grad_norm=1.054269
Epoch 64/100 Iteration 39/234: loss=0.044641 lr=0.000020 grad_norm=0.778597
Epoch 64/100 Iteration 40/234: loss=0.042302 lr=0.000020 grad_norm=0.923462
Epoch 64/100 Iteration 41/234: loss=0.040812 lr=0.000020 grad_norm=0.736661
Epoch 64/100 Iteration 42/234: loss=0.047073 lr=0.000020 grad_norm=0.664732
Epoch 64/100 Iteration 43/234: loss=0.043404 lr=0.000020 grad_norm=0.613174
Epoch 64/100 Iteration 44/234: loss=0.043412 lr=0.000020 grad_norm=0.915982
Epoch 64/100 Iteration 45/234: loss=0.042651 lr=0.000020 grad_norm=0.573391
Epoch 64/100 Iteration 46/234: loss=0.043312 lr=0.000020 grad_norm=0.606103
Epoch 64/100 Iteration 47/234: loss=0.045882 lr=0.000020 grad_norm=0.760906
Epoch 64/100 Iteration 48/234: loss=0.041618 lr=0.000020 grad_norm=0.514268
Epoch 64/100 Iteration 49/234: loss=0.039095 lr=0.000020 grad_norm=0.428651
Epoch 64/100 Iteration 50/234: loss=0.043224 lr=0.000020 grad_norm=0.666800
Epoch 64/100 Iteration 51/234: loss=0.041202 lr=0.000020 grad_norm=0.612767
Epoch 64/100 Iteration 52/234: loss=0.046415 lr=0.000020 grad_norm=0.962212
Epoch 64/100 Iteration 53/234: loss=0.047327 lr=0.000020 grad_norm=1.176886
Epoch 64/100 Iteration 54/234: loss=0.050063 lr=0.000020 grad_norm=0.894836
Epoch 64/100 Iteration 55/234: loss=0.043434 lr=0.000020 grad_norm=0.453200
Epoch 64/100 Iteration 56/234: loss=0.041875 lr=0.000020 grad_norm=0.582681
Epoch 64/100 Iteration 57/234: loss=0.042873 lr=0.000020 grad_norm=0.482493
Epoch 64/100 Iteration 58/234: loss=0.051074 lr=0.000020 grad_norm=0.535660
Epoch 64/100 Iteration 59/234: loss=0.045670 lr=0.000020 grad_norm=0.556668
Epoch 64/100 Iteration 60/234: loss=0.038300 lr=0.000020 grad_norm=0.575219
Epoch 64/100 Iteration 61/234: loss=0.041813 lr=0.000020 grad_norm=0.367282
Epoch 64/100 Iteration 62/234: loss=0.041150 lr=0.000020 grad_norm=0.511490
Epoch 64/100 Iteration 63/234: loss=0.043411 lr=0.000020 grad_norm=0.516838
Epoch 64/100 Iteration 64/234: loss=0.047204 lr=0.000020 grad_norm=0.796021
Epoch 64/100 Iteration 65/234: loss=0.039535 lr=0.000020 grad_norm=0.689756
Epoch 64/100 Iteration 66/234: loss=0.040328 lr=0.000020 grad_norm=0.402744
Epoch 64/100 Iteration 67/234: loss=0.049391 lr=0.000020 grad_norm=0.659953
Epoch 64/100 Iteration 68/234: loss=0.040380 lr=0.000020 grad_norm=0.583985
Epoch 64/100 Iteration 69/234: loss=0.043105 lr=0.000020 grad_norm=0.689061
Epoch 64/100 Iteration 70/234: loss=0.043432 lr=0.000020 grad_norm=0.999092
Epoch 64/100 Iteration 71/234: loss=0.047303 lr=0.000020 grad_norm=0.959060
Epoch 64/100 Iteration 72/234: loss=0.049820 lr=0.000020 grad_norm=0.557467
Epoch 64/100 Iteration 73/234: loss=0.045048 lr=0.000020 grad_norm=0.481050
Epoch 64/100 Iteration 74/234: loss=0.045967 lr=0.000020 grad_norm=0.897907
Epoch 64/100 Iteration 75/234: loss=0.042408 lr=0.000020 grad_norm=1.143864
Epoch 64/100 Iteration 76/234: loss=0.039987 lr=0.000020 grad_norm=0.808570
Epoch 64/100 Iteration 77/234: loss=0.042349 lr=0.000020 grad_norm=0.833866
Epoch 64/100 Iteration 78/234: loss=0.043258 lr=0.000020 grad_norm=0.892102
Epoch 64/100 Iteration 79/234: loss=0.046804 lr=0.000020 grad_norm=1.029313
Epoch 64/100 Iteration 80/234: loss=0.046193 lr=0.000020 grad_norm=0.668324
Epoch 64/100 Iteration 81/234: loss=0.043387 lr=0.000020 grad_norm=1.208493
Epoch 64/100 Iteration 82/234: loss=0.043940 lr=0.000020 grad_norm=0.813063
Epoch 64/100 Iteration 83/234: loss=0.040641 lr=0.000020 grad_norm=0.837484
Epoch 64/100 Iteration 84/234: loss=0.044151 lr=0.000020 grad_norm=1.369097
Epoch 64/100 Iteration 85/234: loss=0.040833 lr=0.000020 grad_norm=0.629377
Epoch 64/100 Iteration 86/234: loss=0.045607 lr=0.000020 grad_norm=1.128965
Epoch 64/100 Iteration 87/234: loss=0.040919 lr=0.000020 grad_norm=1.392661
Epoch 64/100 Iteration 88/234: loss=0.045321 lr=0.000020 grad_norm=0.798320
Epoch 64/100 Iteration 89/234: loss=0.050211 lr=0.000020 grad_norm=0.958206
Epoch 64/100 Iteration 90/234: loss=0.041276 lr=0.000020 grad_norm=1.184991
Epoch 64/100 Iteration 91/234: loss=0.049016 lr=0.000020 grad_norm=0.517473
Epoch 64/100 Iteration 92/234: loss=0.042769 lr=0.000020 grad_norm=0.790021
Epoch 64/100 Iteration 93/234: loss=0.040469 lr=0.000020 grad_norm=0.965945
Epoch 64/100 Iteration 94/234: loss=0.044096 lr=0.000020 grad_norm=0.502340
Epoch 64/100 Iteration 95/234: loss=0.046549 lr=0.000020 grad_norm=0.591894
Epoch 64/100 Iteration 96/234: loss=0.046233 lr=0.000020 grad_norm=0.851367
Epoch 64/100 Iteration 97/234: loss=0.047922 lr=0.000020 grad_norm=0.869739
Epoch 64/100 Iteration 98/234: loss=0.043774 lr=0.000020 grad_norm=0.708593
Epoch 64/100 Iteration 99/234: loss=0.044112 lr=0.000020 grad_norm=0.607674
Epoch 64/100 Iteration 100/234: loss=0.053507 lr=0.000020 grad_norm=0.703867
Epoch 64/100 Iteration 101/234: loss=0.043252 lr=0.000020 grad_norm=0.568133
Epoch 64/100 Iteration 102/234: loss=0.042698 lr=0.000020 grad_norm=0.590579
Epoch 64/100 Iteration 103/234: loss=0.044906 lr=0.000020 grad_norm=1.026552
Epoch 64/100 Iteration 104/234: loss=0.050373 lr=0.000020 grad_norm=1.123702
Epoch 64/100 Iteration 105/234: loss=0.048924 lr=0.000020 grad_norm=0.979446
Epoch 64/100 Iteration 106/234: loss=0.046717 lr=0.000020 grad_norm=0.873590
Epoch 64/100 Iteration 107/234: loss=0.041625 lr=0.000020 grad_norm=0.719046
Epoch 64/100 Iteration 108/234: loss=0.045153 lr=0.000020 grad_norm=0.738155
Epoch 64/100 Iteration 109/234: loss=0.039623 lr=0.000020 grad_norm=0.688747
Epoch 64/100 Iteration 110/234: loss=0.045943 lr=0.000020 grad_norm=0.593233
Epoch 64/100 Iteration 111/234: loss=0.044925 lr=0.000020 grad_norm=0.885867
Epoch 64/100 Iteration 112/234: loss=0.042444 lr=0.000020 grad_norm=0.645697
Epoch 64/100 Iteration 113/234: loss=0.046680 lr=0.000020 grad_norm=0.955118
Epoch 64/100 Iteration 114/234: loss=0.047697 lr=0.000020 grad_norm=1.558862
Epoch 64/100 Iteration 115/234: loss=0.044358 lr=0.000020 grad_norm=1.401277
Epoch 64/100 Iteration 116/234: loss=0.045221 lr=0.000020 grad_norm=0.922692
Epoch 64/100 Iteration 117/234: loss=0.039284 lr=0.000020 grad_norm=0.820392
Epoch 64/100 Iteration 118/234: loss=0.047589 lr=0.000020 grad_norm=1.021409
Epoch 64/100 Iteration 119/234: loss=0.039621 lr=0.000020 grad_norm=1.186432
Epoch 64/100 Iteration 120/234: loss=0.047559 lr=0.000020 grad_norm=0.882961
Epoch 64/100 Iteration 121/234: loss=0.043843 lr=0.000020 grad_norm=0.880522
Epoch 64/100 Iteration 122/234: loss=0.036646 lr=0.000020 grad_norm=1.025947
Epoch 64/100 Iteration 123/234: loss=0.043508 lr=0.000020 grad_norm=0.709759
Epoch 64/100 Iteration 124/234: loss=0.047934 lr=0.000020 grad_norm=1.600948
Epoch 64/100 Iteration 125/234: loss=0.034377 lr=0.000020 grad_norm=1.378798
Epoch 64/100 Iteration 126/234: loss=0.045780 lr=0.000020 grad_norm=1.239814
Epoch 64/100 Iteration 127/234: loss=0.044248 lr=0.000020 grad_norm=2.563174
Epoch 64/100 Iteration 128/234: loss=0.047143 lr=0.000020 grad_norm=1.253970
Epoch 64/100 Iteration 129/234: loss=0.045572 lr=0.000020 grad_norm=1.409796
Epoch 64/100 Iteration 130/234: loss=0.039212 lr=0.000020 grad_norm=1.659327
Epoch 64/100 Iteration 131/234: loss=0.043910 lr=0.000020 grad_norm=0.966526
Epoch 64/100 Iteration 132/234: loss=0.045297 lr=0.000020 grad_norm=1.966106
Epoch 64/100 Iteration 133/234: loss=0.038336 lr=0.000020 grad_norm=0.913158
Epoch 64/100 Iteration 134/234: loss=0.048114 lr=0.000020 grad_norm=1.723805
Epoch 64/100 Iteration 135/234: loss=0.042524 lr=0.000020 grad_norm=1.674050
Epoch 64/100 Iteration 136/234: loss=0.045034 lr=0.000020 grad_norm=0.941171
Epoch 64/100 Iteration 137/234: loss=0.047926 lr=0.000020 grad_norm=1.319030
Epoch 64/100 Iteration 138/234: loss=0.045153 lr=0.000020 grad_norm=1.186959
Epoch 64/100 Iteration 139/234: loss=0.045331 lr=0.000020 grad_norm=1.090894
Epoch 64/100 Iteration 140/234: loss=0.039987 lr=0.000020 grad_norm=0.612539
Epoch 64/100 Iteration 141/234: loss=0.044909 lr=0.000020 grad_norm=1.035066
Epoch 64/100 Iteration 142/234: loss=0.046672 lr=0.000020 grad_norm=0.676941
Epoch 64/100 Iteration 143/234: loss=0.047218 lr=0.000020 grad_norm=0.517337
Epoch 64/100 Iteration 144/234: loss=0.040132 lr=0.000020 grad_norm=0.694097
Epoch 64/100 Iteration 145/234: loss=0.042898 lr=0.000020 grad_norm=0.368273
Epoch 64/100 Iteration 146/234: loss=0.044416 lr=0.000020 grad_norm=0.784126
Epoch 64/100 Iteration 147/234: loss=0.045491 lr=0.000020 grad_norm=0.725381
Epoch 64/100 Iteration 148/234: loss=0.040976 lr=0.000020 grad_norm=0.686970
Epoch 64/100 Iteration 149/234: loss=0.045540 lr=0.000020 grad_norm=0.480787
Epoch 64/100 Iteration 150/234: loss=0.039599 lr=0.000020 grad_norm=0.878952
Epoch 64/100 Iteration 151/234: loss=0.046894 lr=0.000020 grad_norm=0.616485
Epoch 64/100 Iteration 152/234: loss=0.043376 lr=0.000020 grad_norm=0.780797
Epoch 64/100 Iteration 153/234: loss=0.042194 lr=0.000020 grad_norm=0.967000
Epoch 64/100 Iteration 154/234: loss=0.045051 lr=0.000020 grad_norm=0.619675
Epoch 64/100 Iteration 155/234: loss=0.043435 lr=0.000020 grad_norm=1.083982
Epoch 64/100 Iteration 156/234: loss=0.044821 lr=0.000020 grad_norm=1.118951
Epoch 64/100 Iteration 157/234: loss=0.044675 lr=0.000020 grad_norm=0.574870
Epoch 64/100 Iteration 158/234: loss=0.044144 lr=0.000020 grad_norm=0.673912
Epoch 64/100 Iteration 159/234: loss=0.046742 lr=0.000020 grad_norm=0.719878
Epoch 64/100 Iteration 160/234: loss=0.043945 lr=0.000020 grad_norm=0.657308
Epoch 64/100 Iteration 161/234: loss=0.040748 lr=0.000020 grad_norm=0.772771
Epoch 64/100 Iteration 162/234: loss=0.046412 lr=0.000020 grad_norm=0.949535
Epoch 64/100 Iteration 163/234: loss=0.045502 lr=0.000020 grad_norm=0.889864
Epoch 64/100 Iteration 164/234: loss=0.042893 lr=0.000020 grad_norm=0.612930
Epoch 64/100 Iteration 165/234: loss=0.046785 lr=0.000020 grad_norm=0.565661
Epoch 64/100 Iteration 166/234: loss=0.045440 lr=0.000020 grad_norm=0.484636
Epoch 64/100 Iteration 167/234: loss=0.046836 lr=0.000020 grad_norm=0.920331
Epoch 64/100 Iteration 168/234: loss=0.041873 lr=0.000020 grad_norm=1.024982
Epoch 64/100 Iteration 169/234: loss=0.043386 lr=0.000020 grad_norm=0.669752
Epoch 64/100 Iteration 170/234: loss=0.045560 lr=0.000020 grad_norm=0.639359
Epoch 64/100 Iteration 171/234: loss=0.041607 lr=0.000020 grad_norm=0.653977
Epoch 64/100 Iteration 172/234: loss=0.044385 lr=0.000020 grad_norm=0.555725
Epoch 64/100 Iteration 173/234: loss=0.048749 lr=0.000020 grad_norm=0.478635
Epoch 64/100 Iteration 174/234: loss=0.043196 lr=0.000020 grad_norm=0.526323
Epoch 64/100 Iteration 175/234: loss=0.041468 lr=0.000020 grad_norm=0.571810
Epoch 64/100 Iteration 176/234: loss=0.044455 lr=0.000020 grad_norm=0.570657
Epoch 64/100 Iteration 177/234: loss=0.045219 lr=0.000020 grad_norm=0.471760
Epoch 64/100 Iteration 178/234: loss=0.037369 lr=0.000020 grad_norm=0.591356
Epoch 64/100 Iteration 179/234: loss=0.046712 lr=0.000020 grad_norm=0.614960
Epoch 64/100 Iteration 180/234: loss=0.044328 lr=0.000020 grad_norm=0.383140
Epoch 64/100 Iteration 181/234: loss=0.041244 lr=0.000020 grad_norm=0.566605
Epoch 64/100 Iteration 182/234: loss=0.044327 lr=0.000020 grad_norm=0.495607
Epoch 64/100 Iteration 183/234: loss=0.045937 lr=0.000020 grad_norm=0.485615
Epoch 64/100 Iteration 184/234: loss=0.052794 lr=0.000020 grad_norm=0.472684
Epoch 64/100 Iteration 185/234: loss=0.048196 lr=0.000020 grad_norm=0.877962
Epoch 64/100 Iteration 186/234: loss=0.044519 lr=0.000020 grad_norm=1.280347
Epoch 64/100 Iteration 187/234: loss=0.046777 lr=0.000020 grad_norm=1.425117
Epoch 64/100 Iteration 188/234: loss=0.041926 lr=0.000020 grad_norm=1.290215
Epoch 64/100 Iteration 189/234: loss=0.044321 lr=0.000020 grad_norm=0.541665
Epoch 64/100 Iteration 190/234: loss=0.045167 lr=0.000020 grad_norm=0.733624
Epoch 64/100 Iteration 191/234: loss=0.042384 lr=0.000020 grad_norm=1.167419
Epoch 64/100 Iteration 192/234: loss=0.044398 lr=0.000020 grad_norm=0.903112
Epoch 64/100 Iteration 193/234: loss=0.046608 lr=0.000020 grad_norm=0.682397
Epoch 64/100 Iteration 194/234: loss=0.042286 lr=0.000020 grad_norm=1.478494
Epoch 64/100 Iteration 195/234: loss=0.037681 lr=0.000020 grad_norm=0.990887
Epoch 64/100 Iteration 196/234: loss=0.046658 lr=0.000020 grad_norm=0.854766
Epoch 64/100 Iteration 197/234: loss=0.045921 lr=0.000020 grad_norm=1.416382
Epoch 64/100 Iteration 198/234: loss=0.043994 lr=0.000020 grad_norm=0.948391
Epoch 64/100 Iteration 199/234: loss=0.046793 lr=0.000020 grad_norm=0.475823
Epoch 64/100 Iteration 200/234: loss=0.045405 lr=0.000020 grad_norm=0.635768
Epoch 64/100 Iteration 201/234: loss=0.040844 lr=0.000020 grad_norm=0.466871
Epoch 64/100 Iteration 202/234: loss=0.047749 lr=0.000020 grad_norm=0.422835
Epoch 64/100 Iteration 203/234: loss=0.045340 lr=0.000020 grad_norm=0.550185
Epoch 64/100 Iteration 204/234: loss=0.050879 lr=0.000020 grad_norm=0.844539
Epoch 64/100 Iteration 205/234: loss=0.048170 lr=0.000020 grad_norm=0.747574
Epoch 64/100 Iteration 206/234: loss=0.045440 lr=0.000020 grad_norm=0.524159
Epoch 64/100 Iteration 207/234: loss=0.045316 lr=0.000020 grad_norm=0.523674
Epoch 64/100 Iteration 208/234: loss=0.041841 lr=0.000020 grad_norm=0.699347
Epoch 64/100 Iteration 209/234: loss=0.045652 lr=0.000020 grad_norm=0.607689
Epoch 64/100 Iteration 210/234: loss=0.041224 lr=0.000020 grad_norm=0.381236
Epoch 64/100 Iteration 211/234: loss=0.049274 lr=0.000020 grad_norm=0.498323
Epoch 64/100 Iteration 212/234: loss=0.046425 lr=0.000020 grad_norm=0.636841
Epoch 64/100 Iteration 213/234: loss=0.047424 lr=0.000020 grad_norm=0.426967
Epoch 64/100 Iteration 214/234: loss=0.042662 lr=0.000020 grad_norm=1.008471
Epoch 64/100 Iteration 215/234: loss=0.041430 lr=0.000020 grad_norm=0.736707
Epoch 64/100 Iteration 216/234: loss=0.047235 lr=0.000020 grad_norm=0.592706
Epoch 64/100 Iteration 217/234: loss=0.045710 lr=0.000020 grad_norm=1.134476
Epoch 64/100 Iteration 218/234: loss=0.044654 lr=0.000020 grad_norm=1.451375
Epoch 64/100 Iteration 219/234: loss=0.049742 lr=0.000020 grad_norm=0.952427
Epoch 64/100 Iteration 220/234: loss=0.042138 lr=0.000020 grad_norm=0.381806
Epoch 64/100 Iteration 221/234: loss=0.045490 lr=0.000020 grad_norm=0.748045
Epoch 64/100 Iteration 222/234: loss=0.047443 lr=0.000020 grad_norm=0.890394
Epoch 64/100 Iteration 223/234: loss=0.039381 lr=0.000020 grad_norm=0.434717
Epoch 64/100 Iteration 224/234: loss=0.041195 lr=0.000020 grad_norm=0.921994
Epoch 64/100 Iteration 225/234: loss=0.043518 lr=0.000020 grad_norm=1.077560
Epoch 64/100 Iteration 226/234: loss=0.039684 lr=0.000020 grad_norm=0.720817
Epoch 64/100 Iteration 227/234: loss=0.039395 lr=0.000020 grad_norm=0.348721
Epoch 64/100 Iteration 228/234: loss=0.045313 lr=0.000020 grad_norm=0.803037
Epoch 64/100 Iteration 229/234: loss=0.044634 lr=0.000020 grad_norm=1.063092
Epoch 64/100 Iteration 230/234: loss=0.046069 lr=0.000020 grad_norm=0.917161
Epoch 64/100 Iteration 231/234: loss=0.043966 lr=0.000020 grad_norm=0.616013
Epoch 64/100 Iteration 232/234: loss=0.037396 lr=0.000020 grad_norm=0.725070
Epoch 64/100 Iteration 233/234: loss=0.045822 lr=0.000020 grad_norm=0.714406
Epoch 64/100 Iteration 234/234: loss=0.041272 lr=0.000020 grad_norm=0.484106
Epoch 64/100 finished. Avg Loss: 0.044194
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 65/100 Iteration 1/234: loss=0.042490 lr=0.000020 grad_norm=0.670502
Epoch 65/100 Iteration 2/234: loss=0.047183 lr=0.000020 grad_norm=0.885816
Epoch 65/100 Iteration 3/234: loss=0.049028 lr=0.000020 grad_norm=1.506432
Epoch 65/100 Iteration 4/234: loss=0.039620 lr=0.000020 grad_norm=1.394575
Epoch 65/100 Iteration 5/234: loss=0.045642 lr=0.000020 grad_norm=0.554120
Epoch 65/100 Iteration 6/234: loss=0.040509 lr=0.000020 grad_norm=1.207448
Epoch 65/100 Iteration 7/234: loss=0.045905 lr=0.000020 grad_norm=1.633915
Epoch 65/100 Iteration 8/234: loss=0.051787 lr=0.000020 grad_norm=1.415680
Epoch 65/100 Iteration 9/234: loss=0.042798 lr=0.000020 grad_norm=0.950715
Epoch 65/100 Iteration 10/234: loss=0.045321 lr=0.000020 grad_norm=0.695094
Epoch 65/100 Iteration 11/234: loss=0.042240 lr=0.000020 grad_norm=1.049128
Epoch 65/100 Iteration 12/234: loss=0.046658 lr=0.000020 grad_norm=0.898183
Epoch 65/100 Iteration 13/234: loss=0.047281 lr=0.000020 grad_norm=0.452699
Epoch 65/100 Iteration 14/234: loss=0.043222 lr=0.000020 grad_norm=0.854967
Epoch 65/100 Iteration 15/234: loss=0.048960 lr=0.000020 grad_norm=1.177902
Epoch 65/100 Iteration 16/234: loss=0.041029 lr=0.000020 grad_norm=0.827222
Epoch 65/100 Iteration 17/234: loss=0.041509 lr=0.000020 grad_norm=0.425444
Epoch 65/100 Iteration 18/234: loss=0.048374 lr=0.000020 grad_norm=1.209243
Epoch 65/100 Iteration 19/234: loss=0.049002 lr=0.000020 grad_norm=2.029144
Epoch 65/100 Iteration 20/234: loss=0.046669 lr=0.000020 grad_norm=1.590625
Epoch 65/100 Iteration 21/234: loss=0.044993 lr=0.000020 grad_norm=0.422154
Epoch 65/100 Iteration 22/234: loss=0.047771 lr=0.000020 grad_norm=1.437538
Epoch 65/100 Iteration 23/234: loss=0.049268 lr=0.000020 grad_norm=1.038381
Epoch 65/100 Iteration 24/234: loss=0.043790 lr=0.000020 grad_norm=0.505424
Epoch 65/100 Iteration 25/234: loss=0.041729 lr=0.000020 grad_norm=1.004910
Epoch 65/100 Iteration 26/234: loss=0.045887 lr=0.000020 grad_norm=1.371676
Epoch 65/100 Iteration 27/234: loss=0.047331 lr=0.000020 grad_norm=1.214569
Epoch 65/100 Iteration 28/234: loss=0.046165 lr=0.000020 grad_norm=0.533318
Epoch 65/100 Iteration 29/234: loss=0.052194 lr=0.000020 grad_norm=1.075176
Epoch 65/100 Iteration 30/234: loss=0.040519 lr=0.000020 grad_norm=1.273533
Epoch 65/100 Iteration 31/234: loss=0.047399 lr=0.000020 grad_norm=0.716660
Epoch 65/100 Iteration 32/234: loss=0.050256 lr=0.000020 grad_norm=1.618045
Epoch 65/100 Iteration 33/234: loss=0.044212 lr=0.000020 grad_norm=1.785085
Epoch 65/100 Iteration 34/234: loss=0.041384 lr=0.000020 grad_norm=0.821710
Epoch 65/100 Iteration 35/234: loss=0.044407 lr=0.000020 grad_norm=0.978435
Epoch 65/100 Iteration 36/234: loss=0.043298 lr=0.000020 grad_norm=0.746611
Epoch 65/100 Iteration 37/234: loss=0.040861 lr=0.000020 grad_norm=0.894461
Epoch 65/100 Iteration 38/234: loss=0.046100 lr=0.000020 grad_norm=1.265324
Epoch 65/100 Iteration 39/234: loss=0.046789 lr=0.000020 grad_norm=0.666651
Epoch 65/100 Iteration 40/234: loss=0.044793 lr=0.000020 grad_norm=0.909876
Epoch 65/100 Iteration 41/234: loss=0.043067 lr=0.000020 grad_norm=0.936942
Epoch 65/100 Iteration 42/234: loss=0.041643 lr=0.000020 grad_norm=0.478209
Epoch 65/100 Iteration 43/234: loss=0.042534 lr=0.000020 grad_norm=0.942472
Epoch 65/100 Iteration 44/234: loss=0.043518 lr=0.000020 grad_norm=0.499358
Epoch 65/100 Iteration 45/234: loss=0.045708 lr=0.000020 grad_norm=0.828890
Epoch 65/100 Iteration 46/234: loss=0.041728 lr=0.000020 grad_norm=1.334016
Epoch 65/100 Iteration 47/234: loss=0.046327 lr=0.000020 grad_norm=0.668308
Epoch 65/100 Iteration 48/234: loss=0.043192 lr=0.000020 grad_norm=0.914742
Epoch 65/100 Iteration 49/234: loss=0.040242 lr=0.000020 grad_norm=1.080277
Epoch 65/100 Iteration 50/234: loss=0.045668 lr=0.000020 grad_norm=0.566361
Epoch 65/100 Iteration 51/234: loss=0.046325 lr=0.000020 grad_norm=1.437326
Epoch 65/100 Iteration 52/234: loss=0.040112 lr=0.000020 grad_norm=1.252362
Epoch 65/100 Iteration 53/234: loss=0.042801 lr=0.000020 grad_norm=0.579109
Epoch 65/100 Iteration 54/234: loss=0.043431 lr=0.000020 grad_norm=0.689140
Epoch 65/100 Iteration 55/234: loss=0.043967 lr=0.000020 grad_norm=0.594726
Epoch 65/100 Iteration 56/234: loss=0.043104 lr=0.000020 grad_norm=0.388320
Epoch 65/100 Iteration 57/234: loss=0.041732 lr=0.000020 grad_norm=0.841546
Epoch 65/100 Iteration 58/234: loss=0.047628 lr=0.000020 grad_norm=0.901318
Epoch 65/100 Iteration 59/234: loss=0.044350 lr=0.000020 grad_norm=0.777152
Epoch 65/100 Iteration 60/234: loss=0.044263 lr=0.000020 grad_norm=0.899138
Epoch 65/100 Iteration 61/234: loss=0.040290 lr=0.000020 grad_norm=0.883118
Epoch 65/100 Iteration 62/234: loss=0.043617 lr=0.000020 grad_norm=0.576077
Epoch 65/100 Iteration 63/234: loss=0.043470 lr=0.000020 grad_norm=0.793383
Epoch 65/100 Iteration 64/234: loss=0.042365 lr=0.000020 grad_norm=0.764792
Epoch 65/100 Iteration 65/234: loss=0.045313 lr=0.000020 grad_norm=0.848366
Epoch 65/100 Iteration 66/234: loss=0.043478 lr=0.000020 grad_norm=1.119706
Epoch 65/100 Iteration 67/234: loss=0.046000 lr=0.000020 grad_norm=0.693789
Epoch 65/100 Iteration 68/234: loss=0.039021 lr=0.000020 grad_norm=1.261191
Epoch 65/100 Iteration 69/234: loss=0.045710 lr=0.000020 grad_norm=0.547253
Epoch 65/100 Iteration 70/234: loss=0.049407 lr=0.000020 grad_norm=1.265812
Epoch 65/100 Iteration 71/234: loss=0.047602 lr=0.000020 grad_norm=1.646702
Epoch 65/100 Iteration 72/234: loss=0.045321 lr=0.000020 grad_norm=0.473204
Epoch 65/100 Iteration 73/234: loss=0.049381 lr=0.000020 grad_norm=1.933449
Epoch 65/100 Iteration 74/234: loss=0.043414 lr=0.000020 grad_norm=2.029475
Epoch 65/100 Iteration 75/234: loss=0.041870 lr=0.000020 grad_norm=0.796416
Epoch 65/100 Iteration 76/234: loss=0.044628 lr=0.000020 grad_norm=1.829489
Epoch 65/100 Iteration 77/234: loss=0.043054 lr=0.000020 grad_norm=0.779589
Epoch 65/100 Iteration 78/234: loss=0.043035 lr=0.000020 grad_norm=1.503466
Epoch 65/100 Iteration 79/234: loss=0.050190 lr=0.000020 grad_norm=1.306515
Epoch 65/100 Iteration 80/234: loss=0.041918 lr=0.000020 grad_norm=0.782124
Epoch 65/100 Iteration 81/234: loss=0.042603 lr=0.000020 grad_norm=1.346740
Epoch 65/100 Iteration 82/234: loss=0.041704 lr=0.000020 grad_norm=0.522355
Epoch 65/100 Iteration 83/234: loss=0.041763 lr=0.000020 grad_norm=1.653871
Epoch 65/100 Iteration 84/234: loss=0.044637 lr=0.000020 grad_norm=1.066582
Epoch 65/100 Iteration 85/234: loss=0.046388 lr=0.000020 grad_norm=0.995464
Epoch 65/100 Iteration 86/234: loss=0.046552 lr=0.000020 grad_norm=1.271912
Epoch 65/100 Iteration 87/234: loss=0.040742 lr=0.000020 grad_norm=0.549140
Epoch 65/100 Iteration 88/234: loss=0.043608 lr=0.000020 grad_norm=1.482195
Epoch 65/100 Iteration 89/234: loss=0.043916 lr=0.000020 grad_norm=0.703341
Epoch 65/100 Iteration 90/234: loss=0.043592 lr=0.000020 grad_norm=1.089673
Epoch 65/100 Iteration 91/234: loss=0.042061 lr=0.000020 grad_norm=1.017303
Epoch 65/100 Iteration 92/234: loss=0.037581 lr=0.000020 grad_norm=0.537663
Epoch 65/100 Iteration 93/234: loss=0.042375 lr=0.000020 grad_norm=0.803365
Epoch 65/100 Iteration 94/234: loss=0.040083 lr=0.000020 grad_norm=0.738563
Epoch 65/100 Iteration 95/234: loss=0.043818 lr=0.000020 grad_norm=0.810819
Epoch 65/100 Iteration 96/234: loss=0.046679 lr=0.000020 grad_norm=0.908415
Epoch 65/100 Iteration 97/234: loss=0.040880 lr=0.000020 grad_norm=0.522641
Epoch 65/100 Iteration 98/234: loss=0.045692 lr=0.000020 grad_norm=0.821687
Epoch 65/100 Iteration 99/234: loss=0.039687 lr=0.000020 grad_norm=0.520969
Epoch 65/100 Iteration 100/234: loss=0.044727 lr=0.000020 grad_norm=0.706527
Epoch 65/100 Iteration 101/234: loss=0.044717 lr=0.000020 grad_norm=0.816548
Epoch 65/100 Iteration 102/234: loss=0.044610 lr=0.000020 grad_norm=0.684095
Epoch 65/100 Iteration 103/234: loss=0.038222 lr=0.000020 grad_norm=0.776756
Epoch 65/100 Iteration 104/234: loss=0.043255 lr=0.000020 grad_norm=0.543986
Epoch 65/100 Iteration 105/234: loss=0.039820 lr=0.000020 grad_norm=0.602993
Epoch 65/100 Iteration 106/234: loss=0.043488 lr=0.000020 grad_norm=0.538620
Epoch 65/100 Iteration 107/234: loss=0.043652 lr=0.000020 grad_norm=0.813221
Epoch 65/100 Iteration 108/234: loss=0.040595 lr=0.000020 grad_norm=0.360851
Epoch 65/100 Iteration 109/234: loss=0.046580 lr=0.000020 grad_norm=1.527264
Epoch 65/100 Iteration 110/234: loss=0.042519 lr=0.000020 grad_norm=2.544413
Epoch 65/100 Iteration 111/234: loss=0.040903 lr=0.000020 grad_norm=1.521908
Epoch 65/100 Iteration 112/234: loss=0.041586 lr=0.000020 grad_norm=1.111014
Epoch 65/100 Iteration 113/234: loss=0.038749 lr=0.000020 grad_norm=1.293750
Epoch 65/100 Iteration 114/234: loss=0.045362 lr=0.000020 grad_norm=1.063703
Epoch 65/100 Iteration 115/234: loss=0.042386 lr=0.000020 grad_norm=1.634418
Epoch 65/100 Iteration 116/234: loss=0.042959 lr=0.000020 grad_norm=0.662459
Epoch 65/100 Iteration 117/234: loss=0.044832 lr=0.000020 grad_norm=1.538817
Epoch 65/100 Iteration 118/234: loss=0.043037 lr=0.000020 grad_norm=0.984114
Epoch 65/100 Iteration 119/234: loss=0.045681 lr=0.000020 grad_norm=1.062437
Epoch 65/100 Iteration 120/234: loss=0.043179 lr=0.000020 grad_norm=1.325697
Epoch 65/100 Iteration 121/234: loss=0.041992 lr=0.000020 grad_norm=0.549496
Epoch 65/100 Iteration 122/234: loss=0.035969 lr=0.000020 grad_norm=1.053306
Epoch 65/100 Iteration 123/234: loss=0.046987 lr=0.000020 grad_norm=0.559904
Epoch 65/100 Iteration 124/234: loss=0.043995 lr=0.000020 grad_norm=1.222514
Epoch 65/100 Iteration 125/234: loss=0.042461 lr=0.000020 grad_norm=1.390855
Epoch 65/100 Iteration 126/234: loss=0.040987 lr=0.000020 grad_norm=0.529028
Epoch 65/100 Iteration 127/234: loss=0.040642 lr=0.000020 grad_norm=0.899340
Epoch 65/100 Iteration 128/234: loss=0.047259 lr=0.000020 grad_norm=0.520211
Epoch 65/100 Iteration 129/234: loss=0.034788 lr=0.000020 grad_norm=0.776318
Epoch 65/100 Iteration 130/234: loss=0.043763 lr=0.000020 grad_norm=0.505637
Epoch 65/100 Iteration 131/234: loss=0.047461 lr=0.000020 grad_norm=0.759213
Epoch 65/100 Iteration 132/234: loss=0.047085 lr=0.000020 grad_norm=1.006871
Epoch 65/100 Iteration 133/234: loss=0.042059 lr=0.000020 grad_norm=0.452201
Epoch 65/100 Iteration 134/234: loss=0.046246 lr=0.000020 grad_norm=0.862181
Epoch 65/100 Iteration 135/234: loss=0.042148 lr=0.000020 grad_norm=0.880853
Epoch 65/100 Iteration 136/234: loss=0.040598 lr=0.000020 grad_norm=0.571032
Epoch 65/100 Iteration 137/234: loss=0.039906 lr=0.000020 grad_norm=0.551361
Epoch 65/100 Iteration 138/234: loss=0.045611 lr=0.000020 grad_norm=0.789150
Epoch 65/100 Iteration 139/234: loss=0.041785 lr=0.000020 grad_norm=0.666268
Epoch 65/100 Iteration 140/234: loss=0.042348 lr=0.000020 grad_norm=0.482270
Epoch 65/100 Iteration 141/234: loss=0.043884 lr=0.000020 grad_norm=0.468965
Epoch 65/100 Iteration 142/234: loss=0.036168 lr=0.000020 grad_norm=0.517006
Epoch 65/100 Iteration 143/234: loss=0.043062 lr=0.000020 grad_norm=0.709331
Epoch 65/100 Iteration 144/234: loss=0.042035 lr=0.000020 grad_norm=0.389352
Epoch 65/100 Iteration 145/234: loss=0.039452 lr=0.000020 grad_norm=0.631606
Epoch 65/100 Iteration 146/234: loss=0.044366 lr=0.000020 grad_norm=0.718476
Epoch 65/100 Iteration 147/234: loss=0.043395 lr=0.000020 grad_norm=0.391499
Epoch 65/100 Iteration 148/234: loss=0.042569 lr=0.000020 grad_norm=1.123942
Epoch 65/100 Iteration 149/234: loss=0.045061 lr=0.000020 grad_norm=1.420713
Epoch 65/100 Iteration 150/234: loss=0.049593 lr=0.000020 grad_norm=0.589690
Epoch 65/100 Iteration 151/234: loss=0.044882 lr=0.000020 grad_norm=1.136425
Epoch 65/100 Iteration 152/234: loss=0.046478 lr=0.000020 grad_norm=1.096152
Epoch 65/100 Iteration 153/234: loss=0.042513 lr=0.000020 grad_norm=0.549524
Epoch 65/100 Iteration 154/234: loss=0.037651 lr=0.000020 grad_norm=0.385847
Epoch 65/100 Iteration 155/234: loss=0.043174 lr=0.000020 grad_norm=0.657461
Epoch 65/100 Iteration 156/234: loss=0.044737 lr=0.000020 grad_norm=0.663657
Epoch 65/100 Iteration 157/234: loss=0.041021 lr=0.000020 grad_norm=0.324911
Epoch 65/100 Iteration 158/234: loss=0.042683 lr=0.000020 grad_norm=0.600072
Epoch 65/100 Iteration 159/234: loss=0.047092 lr=0.000020 grad_norm=0.659895
Epoch 65/100 Iteration 160/234: loss=0.045466 lr=0.000020 grad_norm=0.523839
Epoch 65/100 Iteration 161/234: loss=0.046619 lr=0.000020 grad_norm=0.905360
Epoch 65/100 Iteration 162/234: loss=0.040248 lr=0.000020 grad_norm=0.755384
Epoch 65/100 Iteration 163/234: loss=0.045313 lr=0.000020 grad_norm=0.466166
Epoch 65/100 Iteration 164/234: loss=0.046256 lr=0.000020 grad_norm=1.041202
Epoch 65/100 Iteration 165/234: loss=0.047332 lr=0.000020 grad_norm=0.954967
Epoch 65/100 Iteration 166/234: loss=0.042925 lr=0.000020 grad_norm=0.573169
Epoch 65/100 Iteration 167/234: loss=0.045530 lr=0.000020 grad_norm=0.723714
Epoch 65/100 Iteration 168/234: loss=0.047276 lr=0.000020 grad_norm=0.523612
Epoch 65/100 Iteration 169/234: loss=0.043233 lr=0.000020 grad_norm=0.505278
Epoch 65/100 Iteration 170/234: loss=0.042519 lr=0.000020 grad_norm=0.584311
Epoch 65/100 Iteration 171/234: loss=0.046597 lr=0.000020 grad_norm=0.853588
Epoch 65/100 Iteration 172/234: loss=0.043669 lr=0.000020 grad_norm=0.997869
Epoch 65/100 Iteration 173/234: loss=0.045554 lr=0.000020 grad_norm=0.817985
Epoch 65/100 Iteration 174/234: loss=0.040094 lr=0.000020 grad_norm=0.922037
Epoch 65/100 Iteration 175/234: loss=0.042523 lr=0.000020 grad_norm=0.949883
Epoch 65/100 Iteration 176/234: loss=0.049065 lr=0.000020 grad_norm=1.305956
Epoch 65/100 Iteration 177/234: loss=0.045445 lr=0.000020 grad_norm=3.100593
Epoch 65/100 Iteration 178/234: loss=0.042551 lr=0.000020 grad_norm=2.962071
Epoch 65/100 Iteration 179/234: loss=0.043392 lr=0.000020 grad_norm=0.925490
Epoch 65/100 Iteration 180/234: loss=0.042189 lr=0.000020 grad_norm=1.961522
Epoch 65/100 Iteration 181/234: loss=0.042024 lr=0.000020 grad_norm=1.152298
Epoch 65/100 Iteration 182/234: loss=0.038710 lr=0.000020 grad_norm=1.162178
Epoch 65/100 Iteration 183/234: loss=0.048833 lr=0.000020 grad_norm=0.855856
Epoch 65/100 Iteration 184/234: loss=0.044946 lr=0.000020 grad_norm=1.359185
Epoch 65/100 Iteration 185/234: loss=0.041659 lr=0.000020 grad_norm=1.145992
Epoch 65/100 Iteration 186/234: loss=0.045004 lr=0.000020 grad_norm=0.892053
Epoch 65/100 Iteration 187/234: loss=0.048214 lr=0.000020 grad_norm=1.486900
Epoch 65/100 Iteration 188/234: loss=0.043917 lr=0.000020 grad_norm=0.689067
Epoch 65/100 Iteration 189/234: loss=0.040188 lr=0.000020 grad_norm=1.089047
Epoch 65/100 Iteration 190/234: loss=0.037818 lr=0.000020 grad_norm=0.633611
Epoch 65/100 Iteration 191/234: loss=0.045492 lr=0.000020 grad_norm=1.183188
Epoch 65/100 Iteration 192/234: loss=0.040153 lr=0.000020 grad_norm=1.015508
Epoch 65/100 Iteration 193/234: loss=0.039100 lr=0.000020 grad_norm=1.042205
Epoch 65/100 Iteration 194/234: loss=0.039653 lr=0.000020 grad_norm=0.849904
Epoch 65/100 Iteration 195/234: loss=0.041276 lr=0.000020 grad_norm=1.015105
Epoch 65/100 Iteration 196/234: loss=0.045482 lr=0.000020 grad_norm=1.675878
Epoch 65/100 Iteration 197/234: loss=0.043466 lr=0.000020 grad_norm=1.045663
Epoch 65/100 Iteration 198/234: loss=0.041896 lr=0.000020 grad_norm=0.733586
Epoch 65/100 Iteration 199/234: loss=0.048875 lr=0.000020 grad_norm=1.320747
Epoch 65/100 Iteration 200/234: loss=0.040815 lr=0.000020 grad_norm=0.717734
Epoch 65/100 Iteration 201/234: loss=0.052815 lr=0.000020 grad_norm=0.918195
Epoch 65/100 Iteration 202/234: loss=0.047766 lr=0.000020 grad_norm=1.235454
Epoch 65/100 Iteration 203/234: loss=0.045004 lr=0.000020 grad_norm=0.730917
Epoch 65/100 Iteration 204/234: loss=0.042482 lr=0.000020 grad_norm=1.121887
Epoch 65/100 Iteration 205/234: loss=0.047912 lr=0.000020 grad_norm=1.510706
Epoch 65/100 Iteration 206/234: loss=0.043923 lr=0.000020 grad_norm=1.038793
Epoch 65/100 Iteration 207/234: loss=0.047965 lr=0.000020 grad_norm=0.821377
Epoch 65/100 Iteration 208/234: loss=0.039498 lr=0.000020 grad_norm=1.036462
Epoch 65/100 Iteration 209/234: loss=0.048046 lr=0.000020 grad_norm=0.945848
Epoch 65/100 Iteration 210/234: loss=0.039804 lr=0.000020 grad_norm=1.630500
Epoch 65/100 Iteration 211/234: loss=0.042500 lr=0.000020 grad_norm=0.949550
Epoch 65/100 Iteration 212/234: loss=0.040568 lr=0.000020 grad_norm=1.483120
Epoch 65/100 Iteration 213/234: loss=0.045166 lr=0.000020 grad_norm=0.944418
Epoch 65/100 Iteration 214/234: loss=0.045909 lr=0.000020 grad_norm=1.436345
Epoch 65/100 Iteration 215/234: loss=0.039859 lr=0.000020 grad_norm=1.110400
Epoch 65/100 Iteration 216/234: loss=0.042208 lr=0.000020 grad_norm=0.943759
Epoch 65/100 Iteration 217/234: loss=0.040128 lr=0.000020 grad_norm=1.238651
Epoch 65/100 Iteration 218/234: loss=0.036866 lr=0.000020 grad_norm=0.504810
Epoch 65/100 Iteration 219/234: loss=0.040686 lr=0.000020 grad_norm=1.346330
Epoch 65/100 Iteration 220/234: loss=0.040853 lr=0.000020 grad_norm=0.869675
Epoch 65/100 Iteration 221/234: loss=0.041442 lr=0.000020 grad_norm=1.064166
Epoch 65/100 Iteration 222/234: loss=0.044333 lr=0.000020 grad_norm=1.165463
Epoch 65/100 Iteration 223/234: loss=0.043227 lr=0.000020 grad_norm=0.473519
Epoch 65/100 Iteration 224/234: loss=0.044640 lr=0.000020 grad_norm=1.209664
Epoch 65/100 Iteration 225/234: loss=0.043834 lr=0.000020 grad_norm=0.955538
Epoch 65/100 Iteration 226/234: loss=0.035151 lr=0.000020 grad_norm=0.620277
Epoch 65/100 Iteration 227/234: loss=0.039390 lr=0.000020 grad_norm=0.550513
Epoch 65/100 Iteration 228/234: loss=0.041676 lr=0.000020 grad_norm=0.935046
Epoch 65/100 Iteration 229/234: loss=0.044422 lr=0.000020 grad_norm=1.252014
Epoch 65/100 Iteration 230/234: loss=0.046241 lr=0.000020 grad_norm=0.520829
Epoch 65/100 Iteration 231/234: loss=0.045205 lr=0.000020 grad_norm=1.527378
Epoch 65/100 Iteration 232/234: loss=0.043364 lr=0.000020 grad_norm=1.246392
Epoch 65/100 Iteration 233/234: loss=0.046108 lr=0.000020 grad_norm=0.949822
Epoch 65/100 Iteration 234/234: loss=0.041360 lr=0.000020 grad_norm=0.952167
Epoch 65/100 finished. Avg Loss: 0.043699
Epoch 66/100 Iteration 1/234: loss=0.040910 lr=0.000020 grad_norm=0.737619
Epoch 66/100 Iteration 2/234: loss=0.049686 lr=0.000020 grad_norm=1.219309
Epoch 66/100 Iteration 3/234: loss=0.043215 lr=0.000020 grad_norm=0.770318
Epoch 66/100 Iteration 4/234: loss=0.044285 lr=0.000020 grad_norm=0.822880
Epoch 66/100 Iteration 5/234: loss=0.045706 lr=0.000020 grad_norm=0.822327
Epoch 66/100 Iteration 6/234: loss=0.046274 lr=0.000020 grad_norm=0.548483
Epoch 66/100 Iteration 7/234: loss=0.047398 lr=0.000020 grad_norm=0.780899
Epoch 66/100 Iteration 8/234: loss=0.045499 lr=0.000020 grad_norm=0.700596
Epoch 66/100 Iteration 9/234: loss=0.042308 lr=0.000020 grad_norm=0.548427
Epoch 66/100 Iteration 10/234: loss=0.042149 lr=0.000020 grad_norm=0.583178
Epoch 66/100 Iteration 11/234: loss=0.042773 lr=0.000020 grad_norm=0.950852
Epoch 66/100 Iteration 12/234: loss=0.049368 lr=0.000020 grad_norm=0.813084
Epoch 66/100 Iteration 13/234: loss=0.041878 lr=0.000020 grad_norm=0.711549
Epoch 66/100 Iteration 14/234: loss=0.041558 lr=0.000020 grad_norm=0.669772
Epoch 66/100 Iteration 15/234: loss=0.044034 lr=0.000020 grad_norm=0.639020
Epoch 66/100 Iteration 16/234: loss=0.045504 lr=0.000020 grad_norm=0.789921
Epoch 66/100 Iteration 17/234: loss=0.045037 lr=0.000020 grad_norm=0.903817
Epoch 66/100 Iteration 18/234: loss=0.034914 lr=0.000020 grad_norm=0.400381
Epoch 66/100 Iteration 19/234: loss=0.045573 lr=0.000020 grad_norm=0.916806
Epoch 66/100 Iteration 20/234: loss=0.043911 lr=0.000020 grad_norm=0.866284
Epoch 66/100 Iteration 21/234: loss=0.038743 lr=0.000020 grad_norm=0.516710
Epoch 66/100 Iteration 22/234: loss=0.043885 lr=0.000020 grad_norm=1.114631
Epoch 66/100 Iteration 23/234: loss=0.042203 lr=0.000020 grad_norm=0.459977
Epoch 66/100 Iteration 24/234: loss=0.037864 lr=0.000020 grad_norm=0.881451
Epoch 66/100 Iteration 25/234: loss=0.043306 lr=0.000020 grad_norm=0.635356
Epoch 66/100 Iteration 26/234: loss=0.040832 lr=0.000020 grad_norm=0.510716
Epoch 66/100 Iteration 27/234: loss=0.044436 lr=0.000020 grad_norm=0.631587
Epoch 66/100 Iteration 28/234: loss=0.038274 lr=0.000020 grad_norm=0.476348
Epoch 66/100 Iteration 29/234: loss=0.040983 lr=0.000020 grad_norm=0.402503
Epoch 66/100 Iteration 30/234: loss=0.048347 lr=0.000020 grad_norm=0.577407
Epoch 66/100 Iteration 31/234: loss=0.043643 lr=0.000020 grad_norm=0.641876
Epoch 66/100 Iteration 32/234: loss=0.040160 lr=0.000020 grad_norm=0.548013
Epoch 66/100 Iteration 33/234: loss=0.038696 lr=0.000020 grad_norm=0.530676
Epoch 66/100 Iteration 34/234: loss=0.044513 lr=0.000020 grad_norm=0.556365
Epoch 66/100 Iteration 35/234: loss=0.036256 lr=0.000020 grad_norm=0.595502
Epoch 66/100 Iteration 36/234: loss=0.041144 lr=0.000020 grad_norm=0.646029
Epoch 66/100 Iteration 37/234: loss=0.047116 lr=0.000020 grad_norm=0.989835
Epoch 66/100 Iteration 38/234: loss=0.041355 lr=0.000020 grad_norm=0.493112
Epoch 66/100 Iteration 39/234: loss=0.036421 lr=0.000020 grad_norm=0.694736
Epoch 66/100 Iteration 40/234: loss=0.036812 lr=0.000020 grad_norm=0.861278
Epoch 66/100 Iteration 41/234: loss=0.044651 lr=0.000020 grad_norm=0.556083
Epoch 66/100 Iteration 42/234: loss=0.037006 lr=0.000020 grad_norm=0.590507
Epoch 66/100 Iteration 43/234: loss=0.042340 lr=0.000020 grad_norm=0.556876
Epoch 66/100 Iteration 44/234: loss=0.042498 lr=0.000020 grad_norm=0.582420
Epoch 66/100 Iteration 45/234: loss=0.043530 lr=0.000020 grad_norm=0.686566
Epoch 66/100 Iteration 46/234: loss=0.042802 lr=0.000020 grad_norm=0.576713
Epoch 66/100 Iteration 47/234: loss=0.037067 lr=0.000020 grad_norm=0.674480
Epoch 66/100 Iteration 48/234: loss=0.040172 lr=0.000020 grad_norm=0.818323
Epoch 66/100 Iteration 49/234: loss=0.040592 lr=0.000020 grad_norm=0.691263
Epoch 66/100 Iteration 50/234: loss=0.041286 lr=0.000020 grad_norm=0.528714
Epoch 66/100 Iteration 51/234: loss=0.045534 lr=0.000020 grad_norm=0.712774
Epoch 66/100 Iteration 52/234: loss=0.048659 lr=0.000020 grad_norm=1.070420
Epoch 66/100 Iteration 53/234: loss=0.037163 lr=0.000020 grad_norm=1.542608
Epoch 66/100 Iteration 54/234: loss=0.043639 lr=0.000020 grad_norm=1.006315
Epoch 66/100 Iteration 55/234: loss=0.039761 lr=0.000020 grad_norm=0.784229
Epoch 66/100 Iteration 56/234: loss=0.043136 lr=0.000020 grad_norm=1.427140
Epoch 66/100 Iteration 57/234: loss=0.043709 lr=0.000020 grad_norm=1.033762
Epoch 66/100 Iteration 58/234: loss=0.043162 lr=0.000020 grad_norm=0.567738
Epoch 66/100 Iteration 59/234: loss=0.046394 lr=0.000020 grad_norm=0.974262
Epoch 66/100 Iteration 60/234: loss=0.041759 lr=0.000020 grad_norm=1.016955
Epoch 66/100 Iteration 61/234: loss=0.043168 lr=0.000020 grad_norm=0.710579
Epoch 66/100 Iteration 62/234: loss=0.041477 lr=0.000020 grad_norm=0.566665
Epoch 66/100 Iteration 63/234: loss=0.039188 lr=0.000020 grad_norm=1.022158
Epoch 66/100 Iteration 64/234: loss=0.047079 lr=0.000020 grad_norm=0.841450
Epoch 66/100 Iteration 65/234: loss=0.048151 lr=0.000020 grad_norm=1.263049
Epoch 66/100 Iteration 66/234: loss=0.045874 lr=0.000020 grad_norm=1.572587
Epoch 66/100 Iteration 67/234: loss=0.039010 lr=0.000020 grad_norm=0.875288
Epoch 66/100 Iteration 68/234: loss=0.041930 lr=0.000020 grad_norm=1.060125
Epoch 66/100 Iteration 69/234: loss=0.043346 lr=0.000020 grad_norm=1.765748
Epoch 66/100 Iteration 70/234: loss=0.044552 lr=0.000020 grad_norm=1.620968
Epoch 66/100 Iteration 71/234: loss=0.042696 lr=0.000020 grad_norm=0.617990
Epoch 66/100 Iteration 72/234: loss=0.043219 lr=0.000020 grad_norm=1.879309
Epoch 66/100 Iteration 73/234: loss=0.037492 lr=0.000020 grad_norm=1.566746
Epoch 66/100 Iteration 74/234: loss=0.037966 lr=0.000020 grad_norm=0.863767
Epoch 66/100 Iteration 75/234: loss=0.043997 lr=0.000020 grad_norm=2.532903
Epoch 66/100 Iteration 76/234: loss=0.046828 lr=0.000020 grad_norm=1.252375
Epoch 66/100 Iteration 77/234: loss=0.042358 lr=0.000020 grad_norm=1.171747
Epoch 66/100 Iteration 78/234: loss=0.047899 lr=0.000020 grad_norm=1.524742
Epoch 66/100 Iteration 79/234: loss=0.048004 lr=0.000020 grad_norm=0.553372
Epoch 66/100 Iteration 80/234: loss=0.043545 lr=0.000020 grad_norm=0.909008
Epoch 66/100 Iteration 81/234: loss=0.043031 lr=0.000020 grad_norm=1.006899
Epoch 66/100 Iteration 82/234: loss=0.040168 lr=0.000020 grad_norm=0.432948
Epoch 66/100 Iteration 83/234: loss=0.051665 lr=0.000020 grad_norm=1.371986
Epoch 66/100 Iteration 84/234: loss=0.041535 lr=0.000020 grad_norm=1.392445
Epoch 66/100 Iteration 85/234: loss=0.043716 lr=0.000020 grad_norm=0.628734
Epoch 66/100 Iteration 86/234: loss=0.045508 lr=0.000020 grad_norm=1.736132
Epoch 66/100 Iteration 87/234: loss=0.043233 lr=0.000020 grad_norm=0.697871
Epoch 66/100 Iteration 88/234: loss=0.043444 lr=0.000020 grad_norm=1.215798
Epoch 66/100 Iteration 89/234: loss=0.044226 lr=0.000020 grad_norm=1.092431
Epoch 66/100 Iteration 90/234: loss=0.045124 lr=0.000020 grad_norm=0.956772
Epoch 66/100 Iteration 91/234: loss=0.037931 lr=0.000020 grad_norm=1.101309
Epoch 66/100 Iteration 92/234: loss=0.046137 lr=0.000020 grad_norm=1.329855
Epoch 66/100 Iteration 93/234: loss=0.037265 lr=0.000020 grad_norm=1.702012
Epoch 66/100 Iteration 94/234: loss=0.046056 lr=0.000020 grad_norm=1.205448
Epoch 66/100 Iteration 95/234: loss=0.042326 lr=0.000020 grad_norm=2.593377
Epoch 66/100 Iteration 96/234: loss=0.041880 lr=0.000020 grad_norm=0.520913
Epoch 66/100 Iteration 97/234: loss=0.044885 lr=0.000020 grad_norm=1.869336
Epoch 66/100 Iteration 98/234: loss=0.037307 lr=0.000020 grad_norm=0.525096
Epoch 66/100 Iteration 99/234: loss=0.041695 lr=0.000020 grad_norm=1.797441
Epoch 66/100 Iteration 100/234: loss=0.040782 lr=0.000020 grad_norm=0.563875
Epoch 66/100 Iteration 101/234: loss=0.044438 lr=0.000020 grad_norm=1.732284
Epoch 66/100 Iteration 102/234: loss=0.040877 lr=0.000020 grad_norm=1.032406
Epoch 66/100 Iteration 103/234: loss=0.043171 lr=0.000020 grad_norm=1.306294
Epoch 66/100 Iteration 104/234: loss=0.043800 lr=0.000020 grad_norm=0.803629
Epoch 66/100 Iteration 105/234: loss=0.040808 lr=0.000020 grad_norm=1.278050
Epoch 66/100 Iteration 106/234: loss=0.047457 lr=0.000020 grad_norm=1.053949
Epoch 66/100 Iteration 107/234: loss=0.042105 lr=0.000020 grad_norm=0.798757
Epoch 66/100 Iteration 108/234: loss=0.037271 lr=0.000020 grad_norm=0.771619
Epoch 66/100 Iteration 109/234: loss=0.039312 lr=0.000020 grad_norm=0.606032
Epoch 66/100 Iteration 110/234: loss=0.040247 lr=0.000020 grad_norm=0.632946
Epoch 66/100 Iteration 111/234: loss=0.041064 lr=0.000020 grad_norm=0.616885
Epoch 66/100 Iteration 112/234: loss=0.046380 lr=0.000020 grad_norm=0.712126
Epoch 66/100 Iteration 113/234: loss=0.043704 lr=0.000020 grad_norm=0.431703
Epoch 66/100 Iteration 114/234: loss=0.050050 lr=0.000020 grad_norm=0.581956
Epoch 66/100 Iteration 115/234: loss=0.041908 lr=0.000020 grad_norm=0.406196
Epoch 66/100 Iteration 116/234: loss=0.045162 lr=0.000020 grad_norm=0.539526
Epoch 66/100 Iteration 117/234: loss=0.036358 lr=0.000020 grad_norm=0.541649
Epoch 66/100 Iteration 118/234: loss=0.038736 lr=0.000020 grad_norm=0.701841
Epoch 66/100 Iteration 119/234: loss=0.045450 lr=0.000020 grad_norm=0.921730
Epoch 66/100 Iteration 120/234: loss=0.045796 lr=0.000020 grad_norm=0.485067
Epoch 66/100 Iteration 121/234: loss=0.041225 lr=0.000020 grad_norm=0.726429
Epoch 66/100 Iteration 122/234: loss=0.044186 lr=0.000020 grad_norm=0.900312
Epoch 66/100 Iteration 123/234: loss=0.047249 lr=0.000020 grad_norm=0.515238
Epoch 66/100 Iteration 124/234: loss=0.035222 lr=0.000020 grad_norm=0.501611
Epoch 66/100 Iteration 125/234: loss=0.040582 lr=0.000020 grad_norm=0.568678
Epoch 66/100 Iteration 126/234: loss=0.042629 lr=0.000020 grad_norm=0.697472
Epoch 66/100 Iteration 127/234: loss=0.042960 lr=0.000020 grad_norm=1.087713
Epoch 66/100 Iteration 128/234: loss=0.045901 lr=0.000020 grad_norm=0.678438
Epoch 66/100 Iteration 129/234: loss=0.041448 lr=0.000020 grad_norm=1.017329
Epoch 66/100 Iteration 130/234: loss=0.047053 lr=0.000020 grad_norm=1.276498
Epoch 66/100 Iteration 131/234: loss=0.040084 lr=0.000020 grad_norm=0.913281
Epoch 66/100 Iteration 132/234: loss=0.046572 lr=0.000020 grad_norm=0.600798
Epoch 66/100 Iteration 133/234: loss=0.042016 lr=0.000020 grad_norm=1.189491
Epoch 66/100 Iteration 134/234: loss=0.040319 lr=0.000020 grad_norm=0.683465
Epoch 66/100 Iteration 135/234: loss=0.042858 lr=0.000020 grad_norm=0.561657
Epoch 66/100 Iteration 136/234: loss=0.039725 lr=0.000020 grad_norm=0.759485
Epoch 66/100 Iteration 137/234: loss=0.048863 lr=0.000020 grad_norm=0.616168
Epoch 66/100 Iteration 138/234: loss=0.040998 lr=0.000020 grad_norm=0.705284
Epoch 66/100 Iteration 139/234: loss=0.037998 lr=0.000020 grad_norm=0.389718
Epoch 66/100 Iteration 140/234: loss=0.046583 lr=0.000020 grad_norm=0.846271
Epoch 66/100 Iteration 141/234: loss=0.043254 lr=0.000020 grad_norm=0.504388
Epoch 66/100 Iteration 142/234: loss=0.037150 lr=0.000020 grad_norm=0.417902
Epoch 66/100 Iteration 143/234: loss=0.043601 lr=0.000020 grad_norm=0.619241
Epoch 66/100 Iteration 144/234: loss=0.041408 lr=0.000020 grad_norm=0.566693
Epoch 66/100 Iteration 145/234: loss=0.043662 lr=0.000020 grad_norm=0.925132
Epoch 66/100 Iteration 146/234: loss=0.042723 lr=0.000020 grad_norm=0.665437
Epoch 66/100 Iteration 147/234: loss=0.037382 lr=0.000020 grad_norm=0.942597
Epoch 66/100 Iteration 148/234: loss=0.044077 lr=0.000020 grad_norm=1.057165
Epoch 66/100 Iteration 149/234: loss=0.042268 lr=0.000020 grad_norm=0.656002
Epoch 66/100 Iteration 150/234: loss=0.044336 lr=0.000020 grad_norm=1.063714
Epoch 66/100 Iteration 151/234: loss=0.041021 lr=0.000020 grad_norm=1.609008
Epoch 66/100 Iteration 152/234: loss=0.039048 lr=0.000020 grad_norm=0.917027
Epoch 66/100 Iteration 153/234: loss=0.044688 lr=0.000020 grad_norm=0.694627
Epoch 66/100 Iteration 154/234: loss=0.045076 lr=0.000020 grad_norm=1.011284
Epoch 66/100 Iteration 155/234: loss=0.038739 lr=0.000020 grad_norm=0.319862
Epoch 66/100 Iteration 156/234: loss=0.039789 lr=0.000020 grad_norm=0.445363
Epoch 66/100 Iteration 157/234: loss=0.044255 lr=0.000020 grad_norm=0.378104
Epoch 66/100 Iteration 158/234: loss=0.046819 lr=0.000020 grad_norm=0.558976
Epoch 66/100 Iteration 159/234: loss=0.047154 lr=0.000020 grad_norm=0.556466
Epoch 66/100 Iteration 160/234: loss=0.043884 lr=0.000020 grad_norm=0.468091
Epoch 66/100 Iteration 161/234: loss=0.040091 lr=0.000020 grad_norm=0.455921
Epoch 66/100 Iteration 162/234: loss=0.043707 lr=0.000020 grad_norm=0.475108
Epoch 66/100 Iteration 163/234: loss=0.041599 lr=0.000020 grad_norm=0.504848
Epoch 66/100 Iteration 164/234: loss=0.038622 lr=0.000020 grad_norm=0.363822
Epoch 66/100 Iteration 165/234: loss=0.043959 lr=0.000020 grad_norm=0.495672
Epoch 66/100 Iteration 166/234: loss=0.044447 lr=0.000020 grad_norm=0.587446
Epoch 66/100 Iteration 167/234: loss=0.043901 lr=0.000020 grad_norm=0.634669
Epoch 66/100 Iteration 168/234: loss=0.045963 lr=0.000020 grad_norm=0.588590
Epoch 66/100 Iteration 169/234: loss=0.043171 lr=0.000020 grad_norm=0.315216
Epoch 66/100 Iteration 170/234: loss=0.041231 lr=0.000020 grad_norm=0.459149
Epoch 66/100 Iteration 171/234: loss=0.041219 lr=0.000020 grad_norm=0.489774
Epoch 66/100 Iteration 172/234: loss=0.044285 lr=0.000020 grad_norm=0.585991
Epoch 66/100 Iteration 173/234: loss=0.038081 lr=0.000020 grad_norm=0.813155
Epoch 66/100 Iteration 174/234: loss=0.039650 lr=0.000020 grad_norm=0.390274
Epoch 66/100 Iteration 175/234: loss=0.044608 lr=0.000020 grad_norm=0.814668
Epoch 66/100 Iteration 176/234: loss=0.045132 lr=0.000020 grad_norm=1.305552
Epoch 66/100 Iteration 177/234: loss=0.041479 lr=0.000020 grad_norm=1.060094
Epoch 66/100 Iteration 178/234: loss=0.043705 lr=0.000020 grad_norm=0.484406
Epoch 66/100 Iteration 179/234: loss=0.039737 lr=0.000020 grad_norm=0.919529
Epoch 66/100 Iteration 180/234: loss=0.046406 lr=0.000020 grad_norm=0.580080
Epoch 66/100 Iteration 181/234: loss=0.043770 lr=0.000020 grad_norm=1.099641
Epoch 66/100 Iteration 182/234: loss=0.041716 lr=0.000020 grad_norm=0.553443
Epoch 66/100 Iteration 183/234: loss=0.046541 lr=0.000020 grad_norm=1.168654
Epoch 66/100 Iteration 184/234: loss=0.044066 lr=0.000020 grad_norm=1.710882
Epoch 66/100 Iteration 185/234: loss=0.041633 lr=0.000020 grad_norm=0.810926
Epoch 66/100 Iteration 186/234: loss=0.046440 lr=0.000020 grad_norm=1.453513
Epoch 66/100 Iteration 187/234: loss=0.040904 lr=0.000020 grad_norm=1.399653
Epoch 66/100 Iteration 188/234: loss=0.042541 lr=0.000020 grad_norm=0.809173
Epoch 66/100 Iteration 189/234: loss=0.045135 lr=0.000020 grad_norm=1.576189
Epoch 66/100 Iteration 190/234: loss=0.050022 lr=0.000020 grad_norm=0.825879
Epoch 66/100 Iteration 191/234: loss=0.038300 lr=0.000020 grad_norm=0.730567
Epoch 66/100 Iteration 192/234: loss=0.039416 lr=0.000020 grad_norm=0.833136
Epoch 66/100 Iteration 193/234: loss=0.042210 lr=0.000020 grad_norm=0.575206
Epoch 66/100 Iteration 194/234: loss=0.042017 lr=0.000020 grad_norm=0.863834
Epoch 66/100 Iteration 195/234: loss=0.039537 lr=0.000020 grad_norm=0.490710
Epoch 66/100 Iteration 196/234: loss=0.043265 lr=0.000020 grad_norm=0.629957
Epoch 66/100 Iteration 197/234: loss=0.038196 lr=0.000020 grad_norm=0.660803
Epoch 66/100 Iteration 198/234: loss=0.042525 lr=0.000020 grad_norm=0.570882
Epoch 66/100 Iteration 199/234: loss=0.044809 lr=0.000020 grad_norm=0.888806
Epoch 66/100 Iteration 200/234: loss=0.038506 lr=0.000020 grad_norm=0.775191
Epoch 66/100 Iteration 201/234: loss=0.037412 lr=0.000020 grad_norm=0.398905
Epoch 66/100 Iteration 202/234: loss=0.040723 lr=0.000020 grad_norm=0.651348
Epoch 66/100 Iteration 203/234: loss=0.040017 lr=0.000020 grad_norm=0.403886
Epoch 66/100 Iteration 204/234: loss=0.041546 lr=0.000020 grad_norm=0.638573
Epoch 66/100 Iteration 205/234: loss=0.047096 lr=0.000020 grad_norm=0.586326
Epoch 66/100 Iteration 206/234: loss=0.041041 lr=0.000020 grad_norm=0.364434
Epoch 66/100 Iteration 207/234: loss=0.041060 lr=0.000020 grad_norm=0.635802
Epoch 66/100 Iteration 208/234: loss=0.039499 lr=0.000020 grad_norm=0.581634
Epoch 66/100 Iteration 209/234: loss=0.042022 lr=0.000020 grad_norm=0.497218
Epoch 66/100 Iteration 210/234: loss=0.043455 lr=0.000020 grad_norm=0.567799
Epoch 66/100 Iteration 211/234: loss=0.036493 lr=0.000020 grad_norm=0.483699
Epoch 66/100 Iteration 212/234: loss=0.040700 lr=0.000020 grad_norm=0.508281
Epoch 66/100 Iteration 213/234: loss=0.044512 lr=0.000020 grad_norm=0.512507
Epoch 66/100 Iteration 214/234: loss=0.045270 lr=0.000020 grad_norm=0.481195
Epoch 66/100 Iteration 215/234: loss=0.047835 lr=0.000020 grad_norm=0.437635
Epoch 66/100 Iteration 216/234: loss=0.042200 lr=0.000020 grad_norm=0.770736
Epoch 66/100 Iteration 217/234: loss=0.041445 lr=0.000020 grad_norm=1.120201
Epoch 66/100 Iteration 218/234: loss=0.042171 lr=0.000020 grad_norm=0.471454
Epoch 66/100 Iteration 219/234: loss=0.039503 lr=0.000020 grad_norm=0.747333
Epoch 66/100 Iteration 220/234: loss=0.042043 lr=0.000020 grad_norm=0.547048
Epoch 66/100 Iteration 221/234: loss=0.048831 lr=0.000020 grad_norm=0.895339
Epoch 66/100 Iteration 222/234: loss=0.045716 lr=0.000020 grad_norm=1.179096
Epoch 66/100 Iteration 223/234: loss=0.043029 lr=0.000020 grad_norm=0.556523
Epoch 66/100 Iteration 224/234: loss=0.045193 lr=0.000020 grad_norm=0.844366
Epoch 66/100 Iteration 225/234: loss=0.040609 lr=0.000020 grad_norm=0.897387
Epoch 66/100 Iteration 226/234: loss=0.045174 lr=0.000020 grad_norm=0.541656
Epoch 66/100 Iteration 227/234: loss=0.040299 lr=0.000020 grad_norm=1.112640
Epoch 66/100 Iteration 228/234: loss=0.044283 lr=0.000020 grad_norm=0.725316
Epoch 66/100 Iteration 229/234: loss=0.041157 lr=0.000020 grad_norm=0.591334
Epoch 66/100 Iteration 230/234: loss=0.040076 lr=0.000020 grad_norm=0.624784
Epoch 66/100 Iteration 231/234: loss=0.044589 lr=0.000020 grad_norm=0.569774
Epoch 66/100 Iteration 232/234: loss=0.046095 lr=0.000020 grad_norm=0.871891
Epoch 66/100 Iteration 233/234: loss=0.037509 lr=0.000020 grad_norm=0.482988
Epoch 66/100 Iteration 234/234: loss=0.040850 lr=0.000020 grad_norm=0.657374
Epoch 66/100 finished. Avg Loss: 0.042643
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 67/100 Iteration 1/234: loss=0.040412 lr=0.000020 grad_norm=1.246634
Epoch 67/100 Iteration 2/234: loss=0.045298 lr=0.000020 grad_norm=1.455202
Epoch 67/100 Iteration 3/234: loss=0.042939 lr=0.000020 grad_norm=1.308032
Epoch 67/100 Iteration 4/234: loss=0.039240 lr=0.000020 grad_norm=1.139931
Epoch 67/100 Iteration 5/234: loss=0.042164 lr=0.000020 grad_norm=0.589581
Epoch 67/100 Iteration 6/234: loss=0.044156 lr=0.000020 grad_norm=1.279122
Epoch 67/100 Iteration 7/234: loss=0.041570 lr=0.000020 grad_norm=1.614263
Epoch 67/100 Iteration 8/234: loss=0.039445 lr=0.000020 grad_norm=0.695259
Epoch 67/100 Iteration 9/234: loss=0.045112 lr=0.000020 grad_norm=1.462533
Epoch 67/100 Iteration 10/234: loss=0.041770 lr=0.000020 grad_norm=1.744363
Epoch 67/100 Iteration 11/234: loss=0.042247 lr=0.000020 grad_norm=0.634260
Epoch 67/100 Iteration 12/234: loss=0.046908 lr=0.000020 grad_norm=1.430847
Epoch 67/100 Iteration 13/234: loss=0.045837 lr=0.000020 grad_norm=1.364283
Epoch 67/100 Iteration 14/234: loss=0.040381 lr=0.000020 grad_norm=1.311588
Epoch 67/100 Iteration 15/234: loss=0.043477 lr=0.000020 grad_norm=0.627096
Epoch 67/100 Iteration 16/234: loss=0.040664 lr=0.000020 grad_norm=1.053833
Epoch 67/100 Iteration 17/234: loss=0.045509 lr=0.000020 grad_norm=1.258852
Epoch 67/100 Iteration 18/234: loss=0.038118 lr=0.000020 grad_norm=0.502857
Epoch 67/100 Iteration 19/234: loss=0.040683 lr=0.000020 grad_norm=0.651726
Epoch 67/100 Iteration 20/234: loss=0.041407 lr=0.000020 grad_norm=0.564302
Epoch 67/100 Iteration 21/234: loss=0.037302 lr=0.000020 grad_norm=0.650071
Epoch 67/100 Iteration 22/234: loss=0.044839 lr=0.000020 grad_norm=0.984393
Epoch 67/100 Iteration 23/234: loss=0.036984 lr=0.000020 grad_norm=1.259086
Epoch 67/100 Iteration 24/234: loss=0.042901 lr=0.000020 grad_norm=0.604710
Epoch 67/100 Iteration 25/234: loss=0.042836 lr=0.000020 grad_norm=1.454854
Epoch 67/100 Iteration 26/234: loss=0.042344 lr=0.000020 grad_norm=1.192792
Epoch 67/100 Iteration 27/234: loss=0.042108 lr=0.000020 grad_norm=0.789659
Epoch 67/100 Iteration 28/234: loss=0.043767 lr=0.000020 grad_norm=1.673743
Epoch 67/100 Iteration 29/234: loss=0.041821 lr=0.000020 grad_norm=1.488023
Epoch 67/100 Iteration 30/234: loss=0.043299 lr=0.000020 grad_norm=0.595536
Epoch 67/100 Iteration 31/234: loss=0.042009 lr=0.000020 grad_norm=1.201674
Epoch 67/100 Iteration 32/234: loss=0.039706 lr=0.000020 grad_norm=0.905064
Epoch 67/100 Iteration 33/234: loss=0.047998 lr=0.000020 grad_norm=0.823478
Epoch 67/100 Iteration 34/234: loss=0.047300 lr=0.000020 grad_norm=1.004726
Epoch 67/100 Iteration 35/234: loss=0.041848 lr=0.000020 grad_norm=0.836087
Epoch 67/100 Iteration 36/234: loss=0.043970 lr=0.000020 grad_norm=0.796631
Epoch 67/100 Iteration 37/234: loss=0.040532 lr=0.000020 grad_norm=0.685915
Epoch 67/100 Iteration 38/234: loss=0.044558 lr=0.000020 grad_norm=0.940504
Epoch 67/100 Iteration 39/234: loss=0.044940 lr=0.000020 grad_norm=1.245982
Epoch 67/100 Iteration 40/234: loss=0.045102 lr=0.000020 grad_norm=1.078225
Epoch 67/100 Iteration 41/234: loss=0.042293 lr=0.000020 grad_norm=0.723312
Epoch 67/100 Iteration 42/234: loss=0.050780 lr=0.000020 grad_norm=1.660659
Epoch 67/100 Iteration 43/234: loss=0.041678 lr=0.000020 grad_norm=1.514531
Epoch 67/100 Iteration 44/234: loss=0.041309 lr=0.000020 grad_norm=0.446918
Epoch 67/100 Iteration 45/234: loss=0.042703 lr=0.000020 grad_norm=1.057226
Epoch 67/100 Iteration 46/234: loss=0.042975 lr=0.000020 grad_norm=0.822706
Epoch 67/100 Iteration 47/234: loss=0.042238 lr=0.000020 grad_norm=0.683754
Epoch 67/100 Iteration 48/234: loss=0.046705 lr=0.000020 grad_norm=1.347086
Epoch 67/100 Iteration 49/234: loss=0.039553 lr=0.000020 grad_norm=1.213671
Epoch 67/100 Iteration 50/234: loss=0.043771 lr=0.000020 grad_norm=0.831710
Epoch 67/100 Iteration 51/234: loss=0.042203 lr=0.000020 grad_norm=0.764600
Epoch 67/100 Iteration 52/234: loss=0.041905 lr=0.000020 grad_norm=0.550675
Epoch 67/100 Iteration 53/234: loss=0.042830 lr=0.000020 grad_norm=0.457269
Epoch 67/100 Iteration 54/234: loss=0.040878 lr=0.000020 grad_norm=0.834843
Epoch 67/100 Iteration 55/234: loss=0.042633 lr=0.000020 grad_norm=0.681493
Epoch 67/100 Iteration 56/234: loss=0.042798 lr=0.000020 grad_norm=0.505006
Epoch 67/100 Iteration 57/234: loss=0.040097 lr=0.000020 grad_norm=0.461668
Epoch 67/100 Iteration 58/234: loss=0.042257 lr=0.000020 grad_norm=0.469870
Epoch 67/100 Iteration 59/234: loss=0.040060 lr=0.000020 grad_norm=0.631184
Epoch 67/100 Iteration 60/234: loss=0.044635 lr=0.000020 grad_norm=0.747496
Epoch 67/100 Iteration 61/234: loss=0.045671 lr=0.000020 grad_norm=0.901497
Epoch 67/100 Iteration 62/234: loss=0.033321 lr=0.000020 grad_norm=0.794699
Epoch 67/100 Iteration 63/234: loss=0.041628 lr=0.000020 grad_norm=0.566215
Epoch 67/100 Iteration 64/234: loss=0.041496 lr=0.000020 grad_norm=1.054626
Epoch 67/100 Iteration 65/234: loss=0.042008 lr=0.000020 grad_norm=1.218188
Epoch 67/100 Iteration 66/234: loss=0.042171 lr=0.000020 grad_norm=0.478824
Epoch 67/100 Iteration 67/234: loss=0.038859 lr=0.000020 grad_norm=1.509527
Epoch 67/100 Iteration 68/234: loss=0.045232 lr=0.000020 grad_norm=1.199875
Epoch 67/100 Iteration 69/234: loss=0.039395 lr=0.000020 grad_norm=0.688714
Epoch 67/100 Iteration 70/234: loss=0.044678 lr=0.000020 grad_norm=0.811612
Epoch 67/100 Iteration 71/234: loss=0.044442 lr=0.000020 grad_norm=0.733366
Epoch 67/100 Iteration 72/234: loss=0.037252 lr=0.000020 grad_norm=0.377855
Epoch 67/100 Iteration 73/234: loss=0.044315 lr=0.000020 grad_norm=0.623122
Epoch 67/100 Iteration 74/234: loss=0.036766 lr=0.000020 grad_norm=0.667368
Epoch 67/100 Iteration 75/234: loss=0.039588 lr=0.000020 grad_norm=0.541655
Epoch 67/100 Iteration 76/234: loss=0.042351 lr=0.000020 grad_norm=0.544630
Epoch 67/100 Iteration 77/234: loss=0.043922 lr=0.000020 grad_norm=0.528793
Epoch 67/100 Iteration 78/234: loss=0.043906 lr=0.000020 grad_norm=0.432649
Epoch 67/100 Iteration 79/234: loss=0.047309 lr=0.000020 grad_norm=0.522934
Epoch 67/100 Iteration 80/234: loss=0.043255 lr=0.000020 grad_norm=0.532746
Epoch 67/100 Iteration 81/234: loss=0.038150 lr=0.000020 grad_norm=0.438763
Epoch 67/100 Iteration 82/234: loss=0.040046 lr=0.000020 grad_norm=0.606152
Epoch 67/100 Iteration 83/234: loss=0.039583 lr=0.000020 grad_norm=0.565851
Epoch 67/100 Iteration 84/234: loss=0.038611 lr=0.000020 grad_norm=0.565205
Epoch 67/100 Iteration 85/234: loss=0.044885 lr=0.000020 grad_norm=0.654319
Epoch 67/100 Iteration 86/234: loss=0.039089 lr=0.000020 grad_norm=0.458894
Epoch 67/100 Iteration 87/234: loss=0.038915 lr=0.000020 grad_norm=0.453215
Epoch 67/100 Iteration 88/234: loss=0.045340 lr=0.000020 grad_norm=0.659435
Epoch 67/100 Iteration 89/234: loss=0.038138 lr=0.000020 grad_norm=0.719710
Epoch 67/100 Iteration 90/234: loss=0.040940 lr=0.000020 grad_norm=0.466347
Epoch 67/100 Iteration 91/234: loss=0.041993 lr=0.000020 grad_norm=0.613409
Epoch 67/100 Iteration 92/234: loss=0.044412 lr=0.000020 grad_norm=0.462257
Epoch 67/100 Iteration 93/234: loss=0.044082 lr=0.000020 grad_norm=0.441820
Epoch 67/100 Iteration 94/234: loss=0.045964 lr=0.000020 grad_norm=0.606079
Epoch 67/100 Iteration 95/234: loss=0.043277 lr=0.000020 grad_norm=0.499536
Epoch 67/100 Iteration 96/234: loss=0.042500 lr=0.000020 grad_norm=0.495439
Epoch 67/100 Iteration 97/234: loss=0.042288 lr=0.000020 grad_norm=0.639260
Epoch 67/100 Iteration 98/234: loss=0.042682 lr=0.000020 grad_norm=0.855881
Epoch 67/100 Iteration 99/234: loss=0.042812 lr=0.000020 grad_norm=1.629304
Epoch 67/100 Iteration 100/234: loss=0.046048 lr=0.000020 grad_norm=1.778975
Epoch 67/100 Iteration 101/234: loss=0.042115 lr=0.000020 grad_norm=0.886197
Epoch 67/100 Iteration 102/234: loss=0.043078 lr=0.000020 grad_norm=0.883889
Epoch 67/100 Iteration 103/234: loss=0.038679 lr=0.000020 grad_norm=1.015747
Epoch 67/100 Iteration 104/234: loss=0.045347 lr=0.000020 grad_norm=0.807159
Epoch 67/100 Iteration 105/234: loss=0.037548 lr=0.000020 grad_norm=1.380081
Epoch 67/100 Iteration 106/234: loss=0.040791 lr=0.000020 grad_norm=0.736721
Epoch 67/100 Iteration 107/234: loss=0.044176 lr=0.000020 grad_norm=0.814197
Epoch 67/100 Iteration 108/234: loss=0.039790 lr=0.000020 grad_norm=0.952848
Epoch 67/100 Iteration 109/234: loss=0.041765 lr=0.000020 grad_norm=0.398520
Epoch 67/100 Iteration 110/234: loss=0.049757 lr=0.000020 grad_norm=1.636892
Epoch 67/100 Iteration 111/234: loss=0.044050 lr=0.000020 grad_norm=1.893559
Epoch 67/100 Iteration 112/234: loss=0.048991 lr=0.000020 grad_norm=0.476398
Epoch 67/100 Iteration 113/234: loss=0.042043 lr=0.000020 grad_norm=1.688195
Epoch 67/100 Iteration 114/234: loss=0.040369 lr=0.000020 grad_norm=0.822047
Epoch 67/100 Iteration 115/234: loss=0.042263 lr=0.000020 grad_norm=1.047502
Epoch 67/100 Iteration 116/234: loss=0.045577 lr=0.000020 grad_norm=1.841150
Epoch 67/100 Iteration 117/234: loss=0.044481 lr=0.000020 grad_norm=1.090182
Epoch 67/100 Iteration 118/234: loss=0.043358 lr=0.000020 grad_norm=1.000471
Epoch 67/100 Iteration 119/234: loss=0.044817 lr=0.000020 grad_norm=1.256204
Epoch 67/100 Iteration 120/234: loss=0.039162 lr=0.000020 grad_norm=0.634235
Epoch 67/100 Iteration 121/234: loss=0.046258 lr=0.000020 grad_norm=1.029716
Epoch 67/100 Iteration 122/234: loss=0.039428 lr=0.000020 grad_norm=0.614438
Epoch 67/100 Iteration 123/234: loss=0.044048 lr=0.000020 grad_norm=1.069478
Epoch 67/100 Iteration 124/234: loss=0.042250 lr=0.000020 grad_norm=1.236548
Epoch 67/100 Iteration 125/234: loss=0.039979 lr=0.000020 grad_norm=0.568184
Epoch 67/100 Iteration 126/234: loss=0.040375 lr=0.000020 grad_norm=1.738536
Epoch 67/100 Iteration 127/234: loss=0.052220 lr=0.000020 grad_norm=1.231291
Epoch 67/100 Iteration 128/234: loss=0.039313 lr=0.000020 grad_norm=0.745424
Epoch 67/100 Iteration 129/234: loss=0.045111 lr=0.000020 grad_norm=1.142993
Epoch 67/100 Iteration 130/234: loss=0.044593 lr=0.000020 grad_norm=0.558614
Epoch 67/100 Iteration 131/234: loss=0.040120 lr=0.000020 grad_norm=0.793160
Epoch 67/100 Iteration 132/234: loss=0.038283 lr=0.000020 grad_norm=0.687241
Epoch 67/100 Iteration 133/234: loss=0.041973 lr=0.000020 grad_norm=1.095952
Epoch 67/100 Iteration 134/234: loss=0.042295 lr=0.000020 grad_norm=0.851056
Epoch 67/100 Iteration 135/234: loss=0.044737 lr=0.000020 grad_norm=0.824802
Epoch 67/100 Iteration 136/234: loss=0.040390 lr=0.000020 grad_norm=1.816796
Epoch 67/100 Iteration 137/234: loss=0.042809 lr=0.000020 grad_norm=1.018653
Epoch 67/100 Iteration 138/234: loss=0.040645 lr=0.000020 grad_norm=0.931041
Epoch 67/100 Iteration 139/234: loss=0.044039 lr=0.000020 grad_norm=1.272596
Epoch 67/100 Iteration 140/234: loss=0.045935 lr=0.000020 grad_norm=0.785497
Epoch 67/100 Iteration 141/234: loss=0.043857 lr=0.000020 grad_norm=0.878525
Epoch 67/100 Iteration 142/234: loss=0.043062 lr=0.000020 grad_norm=0.772082
Epoch 67/100 Iteration 143/234: loss=0.044773 lr=0.000020 grad_norm=0.570766
Epoch 67/100 Iteration 144/234: loss=0.040816 lr=0.000020 grad_norm=0.717394
Epoch 67/100 Iteration 145/234: loss=0.041868 lr=0.000020 grad_norm=0.371402
Epoch 67/100 Iteration 146/234: loss=0.041578 lr=0.000020 grad_norm=0.545158
Epoch 67/100 Iteration 147/234: loss=0.041905 lr=0.000020 grad_norm=0.853537
Epoch 67/100 Iteration 148/234: loss=0.042928 lr=0.000020 grad_norm=0.932355
Epoch 67/100 Iteration 149/234: loss=0.041786 lr=0.000020 grad_norm=0.723815
Epoch 67/100 Iteration 150/234: loss=0.046420 lr=0.000020 grad_norm=0.655745
Epoch 67/100 Iteration 151/234: loss=0.046208 lr=0.000020 grad_norm=1.309894
Epoch 67/100 Iteration 152/234: loss=0.040260 lr=0.000020 grad_norm=0.583977
Epoch 67/100 Iteration 153/234: loss=0.046696 lr=0.000020 grad_norm=1.188304
Epoch 67/100 Iteration 154/234: loss=0.041871 lr=0.000020 grad_norm=1.210538
Epoch 67/100 Iteration 155/234: loss=0.042652 lr=0.000020 grad_norm=0.527084
Epoch 67/100 Iteration 156/234: loss=0.037515 lr=0.000020 grad_norm=1.158124
Epoch 67/100 Iteration 157/234: loss=0.040986 lr=0.000020 grad_norm=0.843215
Epoch 67/100 Iteration 158/234: loss=0.040591 lr=0.000020 grad_norm=0.980669
Epoch 67/100 Iteration 159/234: loss=0.046256 lr=0.000020 grad_norm=1.286147
Epoch 67/100 Iteration 160/234: loss=0.040504 lr=0.000020 grad_norm=1.046353
Epoch 67/100 Iteration 161/234: loss=0.043593 lr=0.000020 grad_norm=0.822899
Epoch 67/100 Iteration 162/234: loss=0.044559 lr=0.000020 grad_norm=1.132055
Epoch 67/100 Iteration 163/234: loss=0.041087 lr=0.000020 grad_norm=1.129015
Epoch 67/100 Iteration 164/234: loss=0.043307 lr=0.000020 grad_norm=0.424547
Epoch 67/100 Iteration 165/234: loss=0.046206 lr=0.000020 grad_norm=1.065904
Epoch 67/100 Iteration 166/234: loss=0.044362 lr=0.000020 grad_norm=1.036322
Epoch 67/100 Iteration 167/234: loss=0.043109 lr=0.000020 grad_norm=0.559293
Epoch 67/100 Iteration 168/234: loss=0.041936 lr=0.000020 grad_norm=1.109792
Epoch 67/100 Iteration 169/234: loss=0.039513 lr=0.000020 grad_norm=1.100046
Epoch 67/100 Iteration 170/234: loss=0.038521 lr=0.000020 grad_norm=0.738258
Epoch 67/100 Iteration 171/234: loss=0.045329 lr=0.000020 grad_norm=0.589373
Epoch 67/100 Iteration 172/234: loss=0.040360 lr=0.000020 grad_norm=0.753085
Epoch 67/100 Iteration 173/234: loss=0.041267 lr=0.000020 grad_norm=0.844152
Epoch 67/100 Iteration 174/234: loss=0.040713 lr=0.000020 grad_norm=0.984037
Epoch 67/100 Iteration 175/234: loss=0.042089 lr=0.000020 grad_norm=0.884375
Epoch 67/100 Iteration 176/234: loss=0.048687 lr=0.000020 grad_norm=0.662324
Epoch 67/100 Iteration 177/234: loss=0.041056 lr=0.000020 grad_norm=0.577163
Epoch 67/100 Iteration 178/234: loss=0.041103 lr=0.000020 grad_norm=0.914050
Epoch 67/100 Iteration 179/234: loss=0.036695 lr=0.000020 grad_norm=1.318586
Epoch 67/100 Iteration 180/234: loss=0.042345 lr=0.000020 grad_norm=0.727526
Epoch 67/100 Iteration 181/234: loss=0.040647 lr=0.000020 grad_norm=0.753405
Epoch 67/100 Iteration 182/234: loss=0.042942 lr=0.000020 grad_norm=0.628820
Epoch 67/100 Iteration 183/234: loss=0.041236 lr=0.000020 grad_norm=1.111942
Epoch 67/100 Iteration 184/234: loss=0.045789 lr=0.000020 grad_norm=1.850690
Epoch 67/100 Iteration 185/234: loss=0.044283 lr=0.000020 grad_norm=1.273588
Epoch 67/100 Iteration 186/234: loss=0.043238 lr=0.000020 grad_norm=0.867257
Epoch 67/100 Iteration 187/234: loss=0.040552 lr=0.000020 grad_norm=1.715072
Epoch 67/100 Iteration 188/234: loss=0.043016 lr=0.000020 grad_norm=0.952332
Epoch 67/100 Iteration 189/234: loss=0.045338 lr=0.000020 grad_norm=1.537431
Epoch 67/100 Iteration 190/234: loss=0.045902 lr=0.000020 grad_norm=1.523757
Epoch 67/100 Iteration 191/234: loss=0.039083 lr=0.000020 grad_norm=0.877102
Epoch 67/100 Iteration 192/234: loss=0.044327 lr=0.000020 grad_norm=1.291435
Epoch 67/100 Iteration 193/234: loss=0.040694 lr=0.000020 grad_norm=0.665979
Epoch 67/100 Iteration 194/234: loss=0.043442 lr=0.000020 grad_norm=1.274553
Epoch 67/100 Iteration 195/234: loss=0.042933 lr=0.000020 grad_norm=1.158731
Epoch 67/100 Iteration 196/234: loss=0.041128 lr=0.000020 grad_norm=0.826417
Epoch 67/100 Iteration 197/234: loss=0.041239 lr=0.000020 grad_norm=1.126035
Epoch 67/100 Iteration 198/234: loss=0.045997 lr=0.000020 grad_norm=0.649022
Epoch 67/100 Iteration 199/234: loss=0.041487 lr=0.000020 grad_norm=1.054111
Epoch 67/100 Iteration 200/234: loss=0.038933 lr=0.000020 grad_norm=0.443954
Epoch 67/100 Iteration 201/234: loss=0.043145 lr=0.000020 grad_norm=1.041933
Epoch 67/100 Iteration 202/234: loss=0.041964 lr=0.000020 grad_norm=0.985112
Epoch 67/100 Iteration 203/234: loss=0.039617 lr=0.000020 grad_norm=0.520921
Epoch 67/100 Iteration 204/234: loss=0.046471 lr=0.000020 grad_norm=0.937738
Epoch 67/100 Iteration 205/234: loss=0.045122 lr=0.000020 grad_norm=0.651037
Epoch 67/100 Iteration 206/234: loss=0.049140 lr=0.000020 grad_norm=1.071145
Epoch 67/100 Iteration 207/234: loss=0.039932 lr=0.000020 grad_norm=1.367395
Epoch 67/100 Iteration 208/234: loss=0.038150 lr=0.000020 grad_norm=0.465960
Epoch 67/100 Iteration 209/234: loss=0.044050 lr=0.000020 grad_norm=1.171336
Epoch 67/100 Iteration 210/234: loss=0.038059 lr=0.000020 grad_norm=0.694036
Epoch 67/100 Iteration 211/234: loss=0.039096 lr=0.000020 grad_norm=0.931062
Epoch 67/100 Iteration 212/234: loss=0.037482 lr=0.000020 grad_norm=0.726184
Epoch 67/100 Iteration 213/234: loss=0.038492 lr=0.000020 grad_norm=0.650339
Epoch 67/100 Iteration 214/234: loss=0.045103 lr=0.000020 grad_norm=0.992539
Epoch 67/100 Iteration 215/234: loss=0.046724 lr=0.000020 grad_norm=0.495296
Epoch 67/100 Iteration 216/234: loss=0.042754 lr=0.000020 grad_norm=0.563795
Epoch 67/100 Iteration 217/234: loss=0.038724 lr=0.000020 grad_norm=0.670850
Epoch 67/100 Iteration 218/234: loss=0.037909 lr=0.000020 grad_norm=0.403315
Epoch 67/100 Iteration 219/234: loss=0.045564 lr=0.000020 grad_norm=0.838486
Epoch 67/100 Iteration 220/234: loss=0.045311 lr=0.000020 grad_norm=0.950322
Epoch 67/100 Iteration 221/234: loss=0.039015 lr=0.000020 grad_norm=0.601830
Epoch 67/100 Iteration 222/234: loss=0.036062 lr=0.000020 grad_norm=0.542970
Epoch 67/100 Iteration 223/234: loss=0.041946 lr=0.000020 grad_norm=1.177558
Epoch 67/100 Iteration 224/234: loss=0.035425 lr=0.000020 grad_norm=1.102708
Epoch 67/100 Iteration 225/234: loss=0.043226 lr=0.000020 grad_norm=0.530630
Epoch 67/100 Iteration 226/234: loss=0.046465 lr=0.000020 grad_norm=1.082153
Epoch 67/100 Iteration 227/234: loss=0.043667 lr=0.000020 grad_norm=0.942204
Epoch 67/100 Iteration 228/234: loss=0.046652 lr=0.000020 grad_norm=0.515438
Epoch 67/100 Iteration 229/234: loss=0.037820 lr=0.000020 grad_norm=1.435147
Epoch 67/100 Iteration 230/234: loss=0.039168 lr=0.000020 grad_norm=0.944031
Epoch 67/100 Iteration 231/234: loss=0.043051 lr=0.000020 grad_norm=0.897305
Epoch 67/100 Iteration 232/234: loss=0.046562 lr=0.000020 grad_norm=2.051255
Epoch 67/100 Iteration 233/234: loss=0.041494 lr=0.000020 grad_norm=1.501301
Epoch 67/100 Iteration 234/234: loss=0.045752 lr=0.000020 grad_norm=0.796791
Epoch 67/100 finished. Avg Loss: 0.042426
Epoch 68/100 Iteration 1/234: loss=0.036703 lr=0.000020 grad_norm=1.241642
Epoch 68/100 Iteration 2/234: loss=0.042204 lr=0.000020 grad_norm=0.736304
Epoch 68/100 Iteration 3/234: loss=0.043384 lr=0.000020 grad_norm=1.791645
Epoch 68/100 Iteration 4/234: loss=0.039947 lr=0.000020 grad_norm=1.092246
Epoch 68/100 Iteration 5/234: loss=0.047573 lr=0.000020 grad_norm=1.401951
Epoch 68/100 Iteration 6/234: loss=0.040563 lr=0.000020 grad_norm=0.953102
Epoch 68/100 Iteration 7/234: loss=0.040643 lr=0.000020 grad_norm=1.089388
Epoch 68/100 Iteration 8/234: loss=0.040870 lr=0.000020 grad_norm=0.891646
Epoch 68/100 Iteration 9/234: loss=0.041246 lr=0.000020 grad_norm=0.802557
Epoch 68/100 Iteration 10/234: loss=0.050256 lr=0.000020 grad_norm=1.616129
Epoch 68/100 Iteration 11/234: loss=0.042028 lr=0.000020 grad_norm=0.874853
Epoch 68/100 Iteration 12/234: loss=0.038562 lr=0.000020 grad_norm=0.870166
Epoch 68/100 Iteration 13/234: loss=0.041942 lr=0.000020 grad_norm=1.049537
Epoch 68/100 Iteration 14/234: loss=0.039702 lr=0.000020 grad_norm=0.353578
Epoch 68/100 Iteration 15/234: loss=0.043844 lr=0.000020 grad_norm=0.961068
Epoch 68/100 Iteration 16/234: loss=0.044627 lr=0.000020 grad_norm=0.594280
Epoch 68/100 Iteration 17/234: loss=0.037696 lr=0.000020 grad_norm=0.686560
Epoch 68/100 Iteration 18/234: loss=0.037751 lr=0.000020 grad_norm=0.742287
Epoch 68/100 Iteration 19/234: loss=0.041066 lr=0.000020 grad_norm=0.857029
Epoch 68/100 Iteration 20/234: loss=0.040173 lr=0.000020 grad_norm=0.400217
Epoch 68/100 Iteration 21/234: loss=0.043898 lr=0.000020 grad_norm=0.812681
Epoch 68/100 Iteration 22/234: loss=0.043472 lr=0.000020 grad_norm=0.595872
Epoch 68/100 Iteration 23/234: loss=0.041768 lr=0.000020 grad_norm=0.609728
Epoch 68/100 Iteration 24/234: loss=0.040083 lr=0.000020 grad_norm=0.879590
Epoch 68/100 Iteration 25/234: loss=0.042144 lr=0.000020 grad_norm=0.560718
Epoch 68/100 Iteration 26/234: loss=0.038450 lr=0.000020 grad_norm=0.515726
Epoch 68/100 Iteration 27/234: loss=0.042455 lr=0.000020 grad_norm=0.768687
Epoch 68/100 Iteration 28/234: loss=0.048246 lr=0.000020 grad_norm=0.700563
Epoch 68/100 Iteration 29/234: loss=0.044555 lr=0.000020 grad_norm=0.474181
Epoch 68/100 Iteration 30/234: loss=0.041878 lr=0.000020 grad_norm=0.737632
Epoch 68/100 Iteration 31/234: loss=0.038217 lr=0.000020 grad_norm=0.600432
Epoch 68/100 Iteration 32/234: loss=0.044399 lr=0.000020 grad_norm=0.576269
Epoch 68/100 Iteration 33/234: loss=0.039305 lr=0.000020 grad_norm=0.897328
Epoch 68/100 Iteration 34/234: loss=0.040388 lr=0.000020 grad_norm=0.565860
Epoch 68/100 Iteration 35/234: loss=0.043865 lr=0.000020 grad_norm=0.576822
Epoch 68/100 Iteration 36/234: loss=0.038362 lr=0.000020 grad_norm=0.753651
Epoch 68/100 Iteration 37/234: loss=0.038797 lr=0.000020 grad_norm=0.584104
Epoch 68/100 Iteration 38/234: loss=0.041598 lr=0.000020 grad_norm=0.547202
Epoch 68/100 Iteration 39/234: loss=0.039203 lr=0.000020 grad_norm=0.646332
Epoch 68/100 Iteration 40/234: loss=0.038867 lr=0.000020 grad_norm=0.405378
Epoch 68/100 Iteration 41/234: loss=0.045844 lr=0.000020 grad_norm=0.961450
Epoch 68/100 Iteration 42/234: loss=0.042454 lr=0.000020 grad_norm=0.982521
Epoch 68/100 Iteration 43/234: loss=0.045130 lr=0.000020 grad_norm=0.488885
Epoch 68/100 Iteration 44/234: loss=0.035734 lr=0.000020 grad_norm=0.587864
Epoch 68/100 Iteration 45/234: loss=0.038381 lr=0.000020 grad_norm=0.925355
Epoch 68/100 Iteration 46/234: loss=0.045586 lr=0.000020 grad_norm=0.670120
Epoch 68/100 Iteration 47/234: loss=0.045113 lr=0.000020 grad_norm=0.424233
Epoch 68/100 Iteration 48/234: loss=0.049349 lr=0.000020 grad_norm=0.714056
Epoch 68/100 Iteration 49/234: loss=0.038916 lr=0.000020 grad_norm=0.727245
Epoch 68/100 Iteration 50/234: loss=0.039946 lr=0.000020 grad_norm=0.634859
Epoch 68/100 Iteration 51/234: loss=0.038535 lr=0.000020 grad_norm=0.826219
Epoch 68/100 Iteration 52/234: loss=0.038896 lr=0.000020 grad_norm=0.770091
Epoch 68/100 Iteration 53/234: loss=0.039159 lr=0.000020 grad_norm=0.511829
Epoch 68/100 Iteration 54/234: loss=0.038723 lr=0.000020 grad_norm=0.703645
Epoch 68/100 Iteration 55/234: loss=0.043815 lr=0.000020 grad_norm=0.728935
Epoch 68/100 Iteration 56/234: loss=0.039785 lr=0.000020 grad_norm=0.468040
Epoch 68/100 Iteration 57/234: loss=0.041489 lr=0.000020 grad_norm=0.688110
Epoch 68/100 Iteration 58/234: loss=0.040667 lr=0.000020 grad_norm=0.943535
Epoch 68/100 Iteration 59/234: loss=0.038707 lr=0.000020 grad_norm=0.457633
Epoch 68/100 Iteration 60/234: loss=0.037116 lr=0.000020 grad_norm=0.693196
Epoch 68/100 Iteration 61/234: loss=0.037993 lr=0.000020 grad_norm=0.655001
Epoch 68/100 Iteration 62/234: loss=0.036084 lr=0.000020 grad_norm=0.439032
Epoch 68/100 Iteration 63/234: loss=0.042587 lr=0.000020 grad_norm=0.488136
Epoch 68/100 Iteration 64/234: loss=0.043879 lr=0.000020 grad_norm=0.665086
Epoch 68/100 Iteration 65/234: loss=0.040178 lr=0.000020 grad_norm=0.613290
Epoch 68/100 Iteration 66/234: loss=0.039391 lr=0.000020 grad_norm=0.410418
Epoch 68/100 Iteration 67/234: loss=0.041554 lr=0.000020 grad_norm=1.132099
Epoch 68/100 Iteration 68/234: loss=0.044509 lr=0.000020 grad_norm=1.412495
Epoch 68/100 Iteration 69/234: loss=0.041111 lr=0.000020 grad_norm=0.955500
Epoch 68/100 Iteration 70/234: loss=0.042052 lr=0.000020 grad_norm=0.820134
Epoch 68/100 Iteration 71/234: loss=0.039785 lr=0.000020 grad_norm=0.699157
Epoch 68/100 Iteration 72/234: loss=0.037797 lr=0.000020 grad_norm=0.924657
Epoch 68/100 Iteration 73/234: loss=0.043194 lr=0.000020 grad_norm=1.070869
Epoch 68/100 Iteration 74/234: loss=0.037943 lr=0.000020 grad_norm=0.954729
Epoch 68/100 Iteration 75/234: loss=0.043188 lr=0.000020 grad_norm=0.505472
Epoch 68/100 Iteration 76/234: loss=0.041838 lr=0.000020 grad_norm=0.581858
Epoch 68/100 Iteration 77/234: loss=0.041212 lr=0.000020 grad_norm=0.757521
Epoch 68/100 Iteration 78/234: loss=0.038083 lr=0.000020 grad_norm=0.434197
Epoch 68/100 Iteration 79/234: loss=0.042612 lr=0.000020 grad_norm=0.668500
Epoch 68/100 Iteration 80/234: loss=0.043702 lr=0.000020 grad_norm=0.981202
Epoch 68/100 Iteration 81/234: loss=0.044798 lr=0.000020 grad_norm=0.781349
Epoch 68/100 Iteration 82/234: loss=0.039790 lr=0.000020 grad_norm=0.585872
Epoch 68/100 Iteration 83/234: loss=0.045702 lr=0.000020 grad_norm=1.123956
Epoch 68/100 Iteration 84/234: loss=0.041932 lr=0.000020 grad_norm=1.501176
Epoch 68/100 Iteration 85/234: loss=0.038313 lr=0.000020 grad_norm=0.921040
Epoch 68/100 Iteration 86/234: loss=0.042747 lr=0.000020 grad_norm=0.664814
Epoch 68/100 Iteration 87/234: loss=0.043701 lr=0.000020 grad_norm=1.510877
Epoch 68/100 Iteration 88/234: loss=0.047474 lr=0.000020 grad_norm=2.407167
Epoch 68/100 Iteration 89/234: loss=0.040507 lr=0.000020 grad_norm=1.869198
Epoch 68/100 Iteration 90/234: loss=0.045312 lr=0.000020 grad_norm=0.542055
Epoch 68/100 Iteration 91/234: loss=0.041764 lr=0.000020 grad_norm=2.091053
Epoch 68/100 Iteration 92/234: loss=0.038376 lr=0.000020 grad_norm=1.362733
Epoch 68/100 Iteration 93/234: loss=0.038143 lr=0.000020 grad_norm=1.230243
Epoch 68/100 Iteration 94/234: loss=0.044704 lr=0.000020 grad_norm=1.726656
Epoch 68/100 Iteration 95/234: loss=0.046357 lr=0.000020 grad_norm=0.598414
Epoch 68/100 Iteration 96/234: loss=0.046627 lr=0.000020 grad_norm=1.351079
Epoch 68/100 Iteration 97/234: loss=0.039487 lr=0.000020 grad_norm=0.848868
Epoch 68/100 Iteration 98/234: loss=0.039447 lr=0.000020 grad_norm=1.180024
Epoch 68/100 Iteration 99/234: loss=0.040361 lr=0.000020 grad_norm=1.339409
Epoch 68/100 Iteration 100/234: loss=0.040464 lr=0.000020 grad_norm=0.773272
Epoch 68/100 Iteration 101/234: loss=0.041986 lr=0.000020 grad_norm=0.868483
Epoch 68/100 Iteration 102/234: loss=0.037479 lr=0.000020 grad_norm=0.454330
Epoch 68/100 Iteration 103/234: loss=0.040899 lr=0.000020 grad_norm=0.932009
Epoch 68/100 Iteration 104/234: loss=0.041879 lr=0.000020 grad_norm=0.746520
Epoch 68/100 Iteration 105/234: loss=0.044728 lr=0.000020 grad_norm=0.562119
Epoch 68/100 Iteration 106/234: loss=0.049222 lr=0.000020 grad_norm=1.190253
Epoch 68/100 Iteration 107/234: loss=0.043344 lr=0.000020 grad_norm=0.827889
Epoch 68/100 Iteration 108/234: loss=0.042936 lr=0.000020 grad_norm=0.617826
Epoch 68/100 Iteration 109/234: loss=0.044147 lr=0.000020 grad_norm=0.606982
Epoch 68/100 Iteration 110/234: loss=0.041779 lr=0.000020 grad_norm=0.642461
Epoch 68/100 Iteration 111/234: loss=0.041549 lr=0.000020 grad_norm=0.954961
Epoch 68/100 Iteration 112/234: loss=0.043433 lr=0.000020 grad_norm=0.614567
Epoch 68/100 Iteration 113/234: loss=0.041141 lr=0.000020 grad_norm=0.778813
Epoch 68/100 Iteration 114/234: loss=0.042568 lr=0.000020 grad_norm=0.863398
Epoch 68/100 Iteration 115/234: loss=0.042041 lr=0.000020 grad_norm=0.513364
Epoch 68/100 Iteration 116/234: loss=0.043370 lr=0.000020 grad_norm=0.701659
Epoch 68/100 Iteration 117/234: loss=0.036688 lr=0.000020 grad_norm=0.652749
Epoch 68/100 Iteration 118/234: loss=0.042240 lr=0.000020 grad_norm=0.460337
Epoch 68/100 Iteration 119/234: loss=0.046748 lr=0.000020 grad_norm=0.462235
Epoch 68/100 Iteration 120/234: loss=0.039727 lr=0.000020 grad_norm=0.463800
Epoch 68/100 Iteration 121/234: loss=0.035467 lr=0.000020 grad_norm=0.372187
Epoch 68/100 Iteration 122/234: loss=0.041783 lr=0.000020 grad_norm=0.376979
Epoch 68/100 Iteration 123/234: loss=0.043267 lr=0.000020 grad_norm=0.668536
Epoch 68/100 Iteration 124/234: loss=0.038383 lr=0.000020 grad_norm=0.552154
Epoch 68/100 Iteration 125/234: loss=0.037795 lr=0.000020 grad_norm=0.420210
Epoch 68/100 Iteration 126/234: loss=0.037776 lr=0.000020 grad_norm=0.565811
Epoch 68/100 Iteration 127/234: loss=0.038680 lr=0.000020 grad_norm=0.413538
Epoch 68/100 Iteration 128/234: loss=0.040288 lr=0.000020 grad_norm=0.422999
Epoch 68/100 Iteration 129/234: loss=0.040830 lr=0.000020 grad_norm=0.446256
Epoch 68/100 Iteration 130/234: loss=0.042505 lr=0.000020 grad_norm=0.449186
Epoch 68/100 Iteration 131/234: loss=0.040671 lr=0.000020 grad_norm=0.351356
Epoch 68/100 Iteration 132/234: loss=0.040071 lr=0.000020 grad_norm=0.597199
Epoch 68/100 Iteration 133/234: loss=0.042142 lr=0.000020 grad_norm=0.992619
Epoch 68/100 Iteration 134/234: loss=0.042900 lr=0.000020 grad_norm=1.309732
Epoch 68/100 Iteration 135/234: loss=0.039164 lr=0.000020 grad_norm=0.897685
Epoch 68/100 Iteration 136/234: loss=0.037701 lr=0.000020 grad_norm=0.671480
Epoch 68/100 Iteration 137/234: loss=0.040883 lr=0.000020 grad_norm=1.217963
Epoch 68/100 Iteration 138/234: loss=0.045882 lr=0.000020 grad_norm=0.562429
Epoch 68/100 Iteration 139/234: loss=0.037404 lr=0.000020 grad_norm=0.840787
Epoch 68/100 Iteration 140/234: loss=0.045831 lr=0.000020 grad_norm=0.943679
Epoch 68/100 Iteration 141/234: loss=0.047518 lr=0.000020 grad_norm=0.585906
Epoch 68/100 Iteration 142/234: loss=0.043252 lr=0.000020 grad_norm=0.570873
Epoch 68/100 Iteration 143/234: loss=0.039317 lr=0.000020 grad_norm=0.434714
Epoch 68/100 Iteration 144/234: loss=0.044095 lr=0.000020 grad_norm=0.597999
Epoch 68/100 Iteration 145/234: loss=0.040961 lr=0.000020 grad_norm=0.741945
Epoch 68/100 Iteration 146/234: loss=0.041900 lr=0.000020 grad_norm=0.466060
Epoch 68/100 Iteration 147/234: loss=0.043746 lr=0.000020 grad_norm=0.510460
Epoch 68/100 Iteration 148/234: loss=0.039280 lr=0.000020 grad_norm=0.572518
Epoch 68/100 Iteration 149/234: loss=0.040152 lr=0.000020 grad_norm=0.489809
Epoch 68/100 Iteration 150/234: loss=0.038348 lr=0.000020 grad_norm=0.371484
Epoch 68/100 Iteration 151/234: loss=0.041230 lr=0.000020 grad_norm=0.447479
Epoch 68/100 Iteration 152/234: loss=0.042605 lr=0.000020 grad_norm=0.523641
Epoch 68/100 Iteration 153/234: loss=0.041530 lr=0.000020 grad_norm=0.376729
Epoch 68/100 Iteration 154/234: loss=0.039053 lr=0.000020 grad_norm=0.394851
Epoch 68/100 Iteration 155/234: loss=0.038528 lr=0.000020 grad_norm=0.514287
Epoch 68/100 Iteration 156/234: loss=0.037512 lr=0.000020 grad_norm=0.486849
Epoch 68/100 Iteration 157/234: loss=0.041066 lr=0.000020 grad_norm=0.553665
Epoch 68/100 Iteration 158/234: loss=0.046524 lr=0.000020 grad_norm=0.850094
Epoch 68/100 Iteration 159/234: loss=0.040036 lr=0.000020 grad_norm=0.778526
Epoch 68/100 Iteration 160/234: loss=0.041257 lr=0.000020 grad_norm=0.636071
Epoch 68/100 Iteration 161/234: loss=0.044645 lr=0.000020 grad_norm=0.501328
Epoch 68/100 Iteration 162/234: loss=0.042981 lr=0.000020 grad_norm=0.371230
Epoch 68/100 Iteration 163/234: loss=0.040288 lr=0.000020 grad_norm=0.596010
Epoch 68/100 Iteration 164/234: loss=0.041484 lr=0.000020 grad_norm=0.830556
Epoch 68/100 Iteration 165/234: loss=0.040044 lr=0.000020 grad_norm=1.032196
Epoch 68/100 Iteration 166/234: loss=0.039125 lr=0.000020 grad_norm=0.868951
Epoch 68/100 Iteration 167/234: loss=0.043180 lr=0.000020 grad_norm=0.493963
Epoch 68/100 Iteration 168/234: loss=0.044622 lr=0.000020 grad_norm=1.093178
Epoch 68/100 Iteration 169/234: loss=0.043555 lr=0.000020 grad_norm=1.001693
Epoch 68/100 Iteration 170/234: loss=0.037778 lr=0.000020 grad_norm=0.518650
Epoch 68/100 Iteration 171/234: loss=0.040984 lr=0.000020 grad_norm=0.537115
Epoch 68/100 Iteration 172/234: loss=0.039754 lr=0.000020 grad_norm=0.553435
Epoch 68/100 Iteration 173/234: loss=0.045954 lr=0.000020 grad_norm=0.710063
Epoch 68/100 Iteration 174/234: loss=0.037465 lr=0.000020 grad_norm=0.520409
Epoch 68/100 Iteration 175/234: loss=0.037600 lr=0.000020 grad_norm=0.557346
Epoch 68/100 Iteration 176/234: loss=0.043772 lr=0.000020 grad_norm=0.985308
Epoch 68/100 Iteration 177/234: loss=0.045324 lr=0.000020 grad_norm=1.052179
Epoch 68/100 Iteration 178/234: loss=0.037619 lr=0.000020 grad_norm=0.470095
Epoch 68/100 Iteration 179/234: loss=0.036771 lr=0.000020 grad_norm=0.735974
Epoch 68/100 Iteration 180/234: loss=0.038628 lr=0.000020 grad_norm=1.078383
Epoch 68/100 Iteration 181/234: loss=0.037982 lr=0.000020 grad_norm=0.716493
Epoch 68/100 Iteration 182/234: loss=0.040829 lr=0.000020 grad_norm=0.564622
Epoch 68/100 Iteration 183/234: loss=0.040718 lr=0.000020 grad_norm=1.114016
Epoch 68/100 Iteration 184/234: loss=0.039870 lr=0.000020 grad_norm=1.092867
Epoch 68/100 Iteration 185/234: loss=0.038532 lr=0.000020 grad_norm=0.377679
Epoch 68/100 Iteration 186/234: loss=0.041263 lr=0.000020 grad_norm=1.143586
Epoch 68/100 Iteration 187/234: loss=0.046620 lr=0.000020 grad_norm=1.710885
Epoch 68/100 Iteration 188/234: loss=0.044768 lr=0.000020 grad_norm=1.254737
Epoch 68/100 Iteration 189/234: loss=0.039323 lr=0.000020 grad_norm=0.531290
Epoch 68/100 Iteration 190/234: loss=0.044251 lr=0.000020 grad_norm=1.026071
Epoch 68/100 Iteration 191/234: loss=0.040537 lr=0.000020 grad_norm=0.798868
Epoch 68/100 Iteration 192/234: loss=0.046937 lr=0.000020 grad_norm=0.690310
Epoch 68/100 Iteration 193/234: loss=0.037716 lr=0.000020 grad_norm=0.729761
Epoch 68/100 Iteration 194/234: loss=0.044873 lr=0.000020 grad_norm=0.717091
Epoch 68/100 Iteration 195/234: loss=0.039585 lr=0.000020 grad_norm=0.646405
Epoch 68/100 Iteration 196/234: loss=0.041594 lr=0.000020 grad_norm=0.711057
Epoch 68/100 Iteration 197/234: loss=0.041924 lr=0.000020 grad_norm=0.484284
Epoch 68/100 Iteration 198/234: loss=0.038405 lr=0.000020 grad_norm=0.548826
Epoch 68/100 Iteration 199/234: loss=0.043098 lr=0.000020 grad_norm=0.489315
Epoch 68/100 Iteration 200/234: loss=0.038725 lr=0.000020 grad_norm=0.663386
Epoch 68/100 Iteration 201/234: loss=0.040019 lr=0.000020 grad_norm=0.805735
Epoch 68/100 Iteration 202/234: loss=0.040369 lr=0.000020 grad_norm=0.810083
Epoch 68/100 Iteration 203/234: loss=0.039847 lr=0.000020 grad_norm=0.613712
Epoch 68/100 Iteration 204/234: loss=0.037748 lr=0.000020 grad_norm=0.495066
Epoch 68/100 Iteration 205/234: loss=0.039996 lr=0.000020 grad_norm=0.427185
Epoch 68/100 Iteration 206/234: loss=0.045277 lr=0.000020 grad_norm=0.844208
Epoch 68/100 Iteration 207/234: loss=0.044235 lr=0.000020 grad_norm=1.164784
Epoch 68/100 Iteration 208/234: loss=0.035403 lr=0.000020 grad_norm=0.493070
Epoch 68/100 Iteration 209/234: loss=0.044101 lr=0.000020 grad_norm=1.241647
Epoch 68/100 Iteration 210/234: loss=0.039143 lr=0.000020 grad_norm=1.311594
Epoch 68/100 Iteration 211/234: loss=0.047070 lr=0.000020 grad_norm=0.662963
Epoch 68/100 Iteration 212/234: loss=0.040988 lr=0.000020 grad_norm=2.082944
Epoch 68/100 Iteration 213/234: loss=0.042821 lr=0.000020 grad_norm=1.533899
Epoch 68/100 Iteration 214/234: loss=0.039732 lr=0.000020 grad_norm=1.052148
Epoch 68/100 Iteration 215/234: loss=0.040152 lr=0.000020 grad_norm=1.515314
Epoch 68/100 Iteration 216/234: loss=0.039416 lr=0.000020 grad_norm=0.448844
Epoch 68/100 Iteration 217/234: loss=0.044991 lr=0.000020 grad_norm=1.203261
Epoch 68/100 Iteration 218/234: loss=0.042159 lr=0.000020 grad_norm=0.970611
Epoch 68/100 Iteration 219/234: loss=0.047086 lr=0.000020 grad_norm=0.909415
Epoch 68/100 Iteration 220/234: loss=0.042176 lr=0.000020 grad_norm=1.549962
Epoch 68/100 Iteration 221/234: loss=0.044213 lr=0.000020 grad_norm=1.037866
Epoch 68/100 Iteration 222/234: loss=0.042797 lr=0.000020 grad_norm=0.693652
Epoch 68/100 Iteration 223/234: loss=0.042283 lr=0.000020 grad_norm=0.908982
Epoch 68/100 Iteration 224/234: loss=0.036181 lr=0.000020 grad_norm=0.574147
Epoch 68/100 Iteration 225/234: loss=0.040364 lr=0.000020 grad_norm=0.786066
Epoch 68/100 Iteration 226/234: loss=0.039760 lr=0.000020 grad_norm=0.899791
Epoch 68/100 Iteration 227/234: loss=0.041112 lr=0.000020 grad_norm=0.721662
Epoch 68/100 Iteration 228/234: loss=0.051052 lr=0.000020 grad_norm=0.605287
Epoch 68/100 Iteration 229/234: loss=0.042222 lr=0.000020 grad_norm=1.125561
Epoch 68/100 Iteration 230/234: loss=0.039880 lr=0.000020 grad_norm=0.803790
Epoch 68/100 Iteration 231/234: loss=0.038364 lr=0.000020 grad_norm=0.579936
Epoch 68/100 Iteration 232/234: loss=0.044188 lr=0.000020 grad_norm=0.896383
Epoch 68/100 Iteration 233/234: loss=0.038666 lr=0.000020 grad_norm=0.639436
Epoch 68/100 Iteration 234/234: loss=0.039555 lr=0.000020 grad_norm=0.777532
Epoch 68/100 finished. Avg Loss: 0.041438
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 69/100 Iteration 1/234: loss=0.045933 lr=0.000020 grad_norm=0.766033
Epoch 69/100 Iteration 2/234: loss=0.039560 lr=0.000020 grad_norm=0.825271
Epoch 69/100 Iteration 3/234: loss=0.046137 lr=0.000020 grad_norm=0.876837
Epoch 69/100 Iteration 4/234: loss=0.039576 lr=0.000020 grad_norm=0.465377
Epoch 69/100 Iteration 5/234: loss=0.039167 lr=0.000020 grad_norm=0.865145
Epoch 69/100 Iteration 6/234: loss=0.042785 lr=0.000020 grad_norm=0.942021
Epoch 69/100 Iteration 7/234: loss=0.043598 lr=0.000020 grad_norm=0.491232
Epoch 69/100 Iteration 8/234: loss=0.040568 lr=0.000020 grad_norm=0.937441
Epoch 69/100 Iteration 9/234: loss=0.040768 lr=0.000020 grad_norm=0.919394
Epoch 69/100 Iteration 10/234: loss=0.040691 lr=0.000020 grad_norm=0.577871
Epoch 69/100 Iteration 11/234: loss=0.038766 lr=0.000020 grad_norm=0.546061
Epoch 69/100 Iteration 12/234: loss=0.035636 lr=0.000020 grad_norm=0.665293
Epoch 69/100 Iteration 13/234: loss=0.040232 lr=0.000020 grad_norm=0.663322
Epoch 69/100 Iteration 14/234: loss=0.036088 lr=0.000020 grad_norm=0.326779
Epoch 69/100 Iteration 15/234: loss=0.037045 lr=0.000020 grad_norm=0.529618
Epoch 69/100 Iteration 16/234: loss=0.045466 lr=0.000020 grad_norm=0.617219
Epoch 69/100 Iteration 17/234: loss=0.040828 lr=0.000020 grad_norm=0.679642
Epoch 69/100 Iteration 18/234: loss=0.047019 lr=0.000020 grad_norm=0.689155
Epoch 69/100 Iteration 19/234: loss=0.039673 lr=0.000020 grad_norm=0.421133
Epoch 69/100 Iteration 20/234: loss=0.037655 lr=0.000020 grad_norm=0.690468
Epoch 69/100 Iteration 21/234: loss=0.041031 lr=0.000020 grad_norm=0.681405
Epoch 69/100 Iteration 22/234: loss=0.040465 lr=0.000020 grad_norm=0.970352
Epoch 69/100 Iteration 23/234: loss=0.038679 lr=0.000020 grad_norm=0.831141
Epoch 69/100 Iteration 24/234: loss=0.039452 lr=0.000020 grad_norm=0.398287
Epoch 69/100 Iteration 25/234: loss=0.041403 lr=0.000020 grad_norm=0.709396
Epoch 69/100 Iteration 26/234: loss=0.041461 lr=0.000020 grad_norm=0.488536
Epoch 69/100 Iteration 27/234: loss=0.041023 lr=0.000020 grad_norm=0.635211
Epoch 69/100 Iteration 28/234: loss=0.040365 lr=0.000020 grad_norm=0.649652
Epoch 69/100 Iteration 29/234: loss=0.042710 lr=0.000020 grad_norm=0.413077
Epoch 69/100 Iteration 30/234: loss=0.040236 lr=0.000020 grad_norm=0.486559
Epoch 69/100 Iteration 31/234: loss=0.038579 lr=0.000020 grad_norm=0.492897
Epoch 69/100 Iteration 32/234: loss=0.042779 lr=0.000020 grad_norm=0.600083
Epoch 69/100 Iteration 33/234: loss=0.043614 lr=0.000020 grad_norm=0.642553
Epoch 69/100 Iteration 34/234: loss=0.037173 lr=0.000020 grad_norm=0.426870
Epoch 69/100 Iteration 35/234: loss=0.039812 lr=0.000020 grad_norm=0.687013
Epoch 69/100 Iteration 36/234: loss=0.038138 lr=0.000020 grad_norm=0.971341
Epoch 69/100 Iteration 37/234: loss=0.041889 lr=0.000020 grad_norm=0.477915
Epoch 69/100 Iteration 38/234: loss=0.042509 lr=0.000020 grad_norm=0.853486
Epoch 69/100 Iteration 39/234: loss=0.038647 lr=0.000020 grad_norm=0.612525
Epoch 69/100 Iteration 40/234: loss=0.042743 lr=0.000020 grad_norm=0.926255
Epoch 69/100 Iteration 41/234: loss=0.041456 lr=0.000020 grad_norm=1.607084
Epoch 69/100 Iteration 42/234: loss=0.044832 lr=0.000020 grad_norm=0.548829
Epoch 69/100 Iteration 43/234: loss=0.038197 lr=0.000020 grad_norm=1.214166
Epoch 69/100 Iteration 44/234: loss=0.045345 lr=0.000020 grad_norm=0.584020
Epoch 69/100 Iteration 45/234: loss=0.042919 lr=0.000020 grad_norm=1.269638
Epoch 69/100 Iteration 46/234: loss=0.043351 lr=0.000020 grad_norm=1.528903
Epoch 69/100 Iteration 47/234: loss=0.042004 lr=0.000020 grad_norm=0.684078
Epoch 69/100 Iteration 48/234: loss=0.043235 lr=0.000020 grad_norm=0.695347
Epoch 69/100 Iteration 49/234: loss=0.046477 lr=0.000020 grad_norm=0.861682
Epoch 69/100 Iteration 50/234: loss=0.043840 lr=0.000020 grad_norm=0.411000
Epoch 69/100 Iteration 51/234: loss=0.043652 lr=0.000020 grad_norm=0.831300
Epoch 69/100 Iteration 52/234: loss=0.037732 lr=0.000020 grad_norm=0.972942
Epoch 69/100 Iteration 53/234: loss=0.039424 lr=0.000020 grad_norm=0.384950
Epoch 69/100 Iteration 54/234: loss=0.041662 lr=0.000020 grad_norm=1.152174
Epoch 69/100 Iteration 55/234: loss=0.043509 lr=0.000020 grad_norm=0.878724
Epoch 69/100 Iteration 56/234: loss=0.041817 lr=0.000020 grad_norm=0.639568
Epoch 69/100 Iteration 57/234: loss=0.038498 lr=0.000020 grad_norm=0.854045
Epoch 69/100 Iteration 58/234: loss=0.035329 lr=0.000020 grad_norm=0.352072
Epoch 69/100 Iteration 59/234: loss=0.041347 lr=0.000020 grad_norm=0.754339
Epoch 69/100 Iteration 60/234: loss=0.045327 lr=0.000020 grad_norm=0.982248
Epoch 69/100 Iteration 61/234: loss=0.039149 lr=0.000020 grad_norm=0.558240
Epoch 69/100 Iteration 62/234: loss=0.041015 lr=0.000020 grad_norm=0.634708
Epoch 69/100 Iteration 63/234: loss=0.044782 lr=0.000020 grad_norm=0.735965
Epoch 69/100 Iteration 64/234: loss=0.042545 lr=0.000020 grad_norm=0.940921
Epoch 69/100 Iteration 65/234: loss=0.039233 lr=0.000020 grad_norm=0.738447
Epoch 69/100 Iteration 66/234: loss=0.036367 lr=0.000020 grad_norm=0.431171
Epoch 69/100 Iteration 67/234: loss=0.041412 lr=0.000020 grad_norm=1.031743
Epoch 69/100 Iteration 68/234: loss=0.036624 lr=0.000020 grad_norm=1.107537
Epoch 69/100 Iteration 69/234: loss=0.038575 lr=0.000020 grad_norm=0.479997
Epoch 69/100 Iteration 70/234: loss=0.034615 lr=0.000020 grad_norm=0.530589
Epoch 69/100 Iteration 71/234: loss=0.043162 lr=0.000020 grad_norm=0.530259
Epoch 69/100 Iteration 72/234: loss=0.039520 lr=0.000020 grad_norm=0.428810
Epoch 69/100 Iteration 73/234: loss=0.039303 lr=0.000020 grad_norm=0.713486
Epoch 69/100 Iteration 74/234: loss=0.040948 lr=0.000020 grad_norm=0.628166
Epoch 69/100 Iteration 75/234: loss=0.044882 lr=0.000020 grad_norm=0.434613
Epoch 69/100 Iteration 76/234: loss=0.039397 lr=0.000020 grad_norm=0.535617
Epoch 69/100 Iteration 77/234: loss=0.036071 lr=0.000020 grad_norm=0.367339
Epoch 69/100 Iteration 78/234: loss=0.035959 lr=0.000020 grad_norm=0.617127
Epoch 69/100 Iteration 79/234: loss=0.034196 lr=0.000020 grad_norm=0.659331
Epoch 69/100 Iteration 80/234: loss=0.040730 lr=0.000020 grad_norm=0.427474
Epoch 69/100 Iteration 81/234: loss=0.039843 lr=0.000020 grad_norm=0.645784
Epoch 69/100 Iteration 82/234: loss=0.037749 lr=0.000020 grad_norm=0.672659
Epoch 69/100 Iteration 83/234: loss=0.038456 lr=0.000020 grad_norm=0.781609
Epoch 69/100 Iteration 84/234: loss=0.042514 lr=0.000020 grad_norm=0.957621
Epoch 69/100 Iteration 85/234: loss=0.039157 lr=0.000020 grad_norm=0.881753
Epoch 69/100 Iteration 86/234: loss=0.042273 lr=0.000020 grad_norm=0.532929
Epoch 69/100 Iteration 87/234: loss=0.040841 lr=0.000020 grad_norm=0.837439
Epoch 69/100 Iteration 88/234: loss=0.042937 lr=0.000020 grad_norm=1.003754
Epoch 69/100 Iteration 89/234: loss=0.050124 lr=0.000020 grad_norm=0.775503
Epoch 69/100 Iteration 90/234: loss=0.037034 lr=0.000020 grad_norm=0.523695
Epoch 69/100 Iteration 91/234: loss=0.038776 lr=0.000020 grad_norm=0.907550
Epoch 69/100 Iteration 92/234: loss=0.042520 lr=0.000020 grad_norm=1.076854
Epoch 69/100 Iteration 93/234: loss=0.039042 lr=0.000020 grad_norm=0.697856
Epoch 69/100 Iteration 94/234: loss=0.039973 lr=0.000020 grad_norm=0.699755
Epoch 69/100 Iteration 95/234: loss=0.041557 lr=0.000020 grad_norm=1.239483
Epoch 69/100 Iteration 96/234: loss=0.037506 lr=0.000020 grad_norm=1.248766
Epoch 69/100 Iteration 97/234: loss=0.041290 lr=0.000020 grad_norm=0.538193
Epoch 69/100 Iteration 98/234: loss=0.042702 lr=0.000020 grad_norm=1.741645
Epoch 69/100 Iteration 99/234: loss=0.039652 lr=0.000020 grad_norm=2.088227
Epoch 69/100 Iteration 100/234: loss=0.040953 lr=0.000020 grad_norm=0.720860
Epoch 69/100 Iteration 101/234: loss=0.038959 lr=0.000020 grad_norm=1.735378
Epoch 69/100 Iteration 102/234: loss=0.041099 lr=0.000020 grad_norm=2.019626
Epoch 69/100 Iteration 103/234: loss=0.042214 lr=0.000020 grad_norm=0.649877
Epoch 69/100 Iteration 104/234: loss=0.040494 lr=0.000020 grad_norm=1.559001
Epoch 69/100 Iteration 105/234: loss=0.040996 lr=0.000020 grad_norm=1.886274
Epoch 69/100 Iteration 106/234: loss=0.041960 lr=0.000020 grad_norm=0.901086
Epoch 69/100 Iteration 107/234: loss=0.038800 lr=0.000020 grad_norm=1.661919
Epoch 69/100 Iteration 108/234: loss=0.042069 lr=0.000020 grad_norm=1.451303
Epoch 69/100 Iteration 109/234: loss=0.036421 lr=0.000020 grad_norm=0.816207
Epoch 69/100 Iteration 110/234: loss=0.042722 lr=0.000020 grad_norm=1.216089
Epoch 69/100 Iteration 111/234: loss=0.036398 lr=0.000020 grad_norm=0.815183
Epoch 69/100 Iteration 112/234: loss=0.039132 lr=0.000020 grad_norm=0.743695
Epoch 69/100 Iteration 113/234: loss=0.036559 lr=0.000020 grad_norm=0.784457
Epoch 69/100 Iteration 114/234: loss=0.043188 lr=0.000020 grad_norm=0.912193
Epoch 69/100 Iteration 115/234: loss=0.041398 lr=0.000020 grad_norm=0.664702
Epoch 69/100 Iteration 116/234: loss=0.044297 lr=0.000020 grad_norm=0.412310
Epoch 69/100 Iteration 117/234: loss=0.041368 lr=0.000020 grad_norm=0.695441
Epoch 69/100 Iteration 118/234: loss=0.046452 lr=0.000020 grad_norm=0.767426
Epoch 69/100 Iteration 119/234: loss=0.040812 lr=0.000020 grad_norm=0.477671
Epoch 69/100 Iteration 120/234: loss=0.042572 lr=0.000020 grad_norm=0.802074
Epoch 69/100 Iteration 121/234: loss=0.044452 lr=0.000020 grad_norm=1.018163
Epoch 69/100 Iteration 122/234: loss=0.043823 lr=0.000020 grad_norm=0.483436
Epoch 69/100 Iteration 123/234: loss=0.042815 lr=0.000020 grad_norm=0.924843
Epoch 69/100 Iteration 124/234: loss=0.037894 lr=0.000020 grad_norm=0.424817
Epoch 69/100 Iteration 125/234: loss=0.039181 lr=0.000020 grad_norm=0.667162
Epoch 69/100 Iteration 126/234: loss=0.037505 lr=0.000020 grad_norm=0.554957
Epoch 69/100 Iteration 127/234: loss=0.041814 lr=0.000020 grad_norm=0.986271
Epoch 69/100 Iteration 128/234: loss=0.041678 lr=0.000020 grad_norm=1.111302
Epoch 69/100 Iteration 129/234: loss=0.039540 lr=0.000020 grad_norm=0.413169
Epoch 69/100 Iteration 130/234: loss=0.045637 lr=0.000020 grad_norm=1.219823
Epoch 69/100 Iteration 131/234: loss=0.042337 lr=0.000020 grad_norm=1.646917
Epoch 69/100 Iteration 132/234: loss=0.034872 lr=0.000020 grad_norm=0.536488
Epoch 69/100 Iteration 133/234: loss=0.039553 lr=0.000020 grad_norm=1.202152
Epoch 69/100 Iteration 134/234: loss=0.039408 lr=0.000020 grad_norm=1.040709
Epoch 69/100 Iteration 135/234: loss=0.040321 lr=0.000020 grad_norm=0.786584
Epoch 69/100 Iteration 136/234: loss=0.044292 lr=0.000020 grad_norm=1.623992
Epoch 69/100 Iteration 137/234: loss=0.043541 lr=0.000020 grad_norm=1.292488
Epoch 69/100 Iteration 138/234: loss=0.038750 lr=0.000020 grad_norm=0.578215
Epoch 69/100 Iteration 139/234: loss=0.042135 lr=0.000020 grad_norm=0.527992
Epoch 69/100 Iteration 140/234: loss=0.040836 lr=0.000020 grad_norm=0.681843
Epoch 69/100 Iteration 141/234: loss=0.040342 lr=0.000020 grad_norm=0.868272
Epoch 69/100 Iteration 142/234: loss=0.035902 lr=0.000020 grad_norm=0.670741
Epoch 69/100 Iteration 143/234: loss=0.039134 lr=0.000020 grad_norm=0.738676
Epoch 69/100 Iteration 144/234: loss=0.037639 lr=0.000020 grad_norm=0.702227
Epoch 69/100 Iteration 145/234: loss=0.036991 lr=0.000020 grad_norm=0.439005
Epoch 69/100 Iteration 146/234: loss=0.041589 lr=0.000020 grad_norm=0.887507
Epoch 69/100 Iteration 147/234: loss=0.044079 lr=0.000020 grad_norm=1.049191
Epoch 69/100 Iteration 148/234: loss=0.039900 lr=0.000020 grad_norm=0.854462
Epoch 69/100 Iteration 149/234: loss=0.040900 lr=0.000020 grad_norm=0.554043
Epoch 69/100 Iteration 150/234: loss=0.045126 lr=0.000020 grad_norm=0.548401
Epoch 69/100 Iteration 151/234: loss=0.039258 lr=0.000020 grad_norm=0.315511
Epoch 69/100 Iteration 152/234: loss=0.038726 lr=0.000020 grad_norm=0.481047
Epoch 69/100 Iteration 153/234: loss=0.037224 lr=0.000020 grad_norm=0.521108
Epoch 69/100 Iteration 154/234: loss=0.042637 lr=0.000020 grad_norm=0.551681
Epoch 69/100 Iteration 155/234: loss=0.039687 lr=0.000020 grad_norm=0.642325
Epoch 69/100 Iteration 156/234: loss=0.037772 lr=0.000020 grad_norm=0.716521
Epoch 69/100 Iteration 157/234: loss=0.041085 lr=0.000020 grad_norm=0.509091
Epoch 69/100 Iteration 158/234: loss=0.041130 lr=0.000020 grad_norm=0.445934
Epoch 69/100 Iteration 159/234: loss=0.039340 lr=0.000020 grad_norm=0.607714
Epoch 69/100 Iteration 160/234: loss=0.043777 lr=0.000020 grad_norm=0.582533
Epoch 69/100 Iteration 161/234: loss=0.045533 lr=0.000020 grad_norm=0.416163
Epoch 69/100 Iteration 162/234: loss=0.042991 lr=0.000020 grad_norm=0.707672
Epoch 69/100 Iteration 163/234: loss=0.043427 lr=0.000020 grad_norm=1.125930
Epoch 69/100 Iteration 164/234: loss=0.045323 lr=0.000020 grad_norm=1.118532
Epoch 69/100 Iteration 165/234: loss=0.034832 lr=0.000020 grad_norm=0.689091
Epoch 69/100 Iteration 166/234: loss=0.045718 lr=0.000020 grad_norm=0.641657
Epoch 69/100 Iteration 167/234: loss=0.040647 lr=0.000020 grad_norm=0.481305
Epoch 69/100 Iteration 168/234: loss=0.043269 lr=0.000020 grad_norm=0.465344
Epoch 69/100 Iteration 169/234: loss=0.042757 lr=0.000020 grad_norm=0.480310
Epoch 69/100 Iteration 170/234: loss=0.040537 lr=0.000020 grad_norm=0.545267
Epoch 69/100 Iteration 171/234: loss=0.043811 lr=0.000020 grad_norm=0.640835
Epoch 69/100 Iteration 172/234: loss=0.041509 lr=0.000020 grad_norm=0.774221
Epoch 69/100 Iteration 173/234: loss=0.039980 lr=0.000020 grad_norm=0.772949
Epoch 69/100 Iteration 174/234: loss=0.042436 lr=0.000020 grad_norm=0.467889
Epoch 69/100 Iteration 175/234: loss=0.042277 lr=0.000020 grad_norm=0.934443
Epoch 69/100 Iteration 176/234: loss=0.046417 lr=0.000020 grad_norm=1.286422
Epoch 69/100 Iteration 177/234: loss=0.038710 lr=0.000020 grad_norm=0.932906
Epoch 69/100 Iteration 178/234: loss=0.041997 lr=0.000020 grad_norm=0.816929
Epoch 69/100 Iteration 179/234: loss=0.044670 lr=0.000020 grad_norm=1.862838
Epoch 69/100 Iteration 180/234: loss=0.047309 lr=0.000020 grad_norm=1.861151
Epoch 69/100 Iteration 181/234: loss=0.044368 lr=0.000020 grad_norm=1.242341
Epoch 69/100 Iteration 182/234: loss=0.046787 lr=0.000020 grad_norm=1.328696
Epoch 69/100 Iteration 183/234: loss=0.041248 lr=0.000020 grad_norm=1.241818
Epoch 69/100 Iteration 184/234: loss=0.038270 lr=0.000020 grad_norm=1.070380
Epoch 69/100 Iteration 185/234: loss=0.041053 lr=0.000020 grad_norm=1.359254
Epoch 69/100 Iteration 186/234: loss=0.046015 lr=0.000020 grad_norm=0.977208
Epoch 69/100 Iteration 187/234: loss=0.037344 lr=0.000020 grad_norm=1.000869
Epoch 69/100 Iteration 188/234: loss=0.039825 lr=0.000020 grad_norm=1.087657
Epoch 69/100 Iteration 189/234: loss=0.039495 lr=0.000020 grad_norm=0.705653
Epoch 69/100 Iteration 190/234: loss=0.045041 lr=0.000020 grad_norm=1.091030
Epoch 69/100 Iteration 191/234: loss=0.039762 lr=0.000020 grad_norm=0.785360
Epoch 69/100 Iteration 192/234: loss=0.034146 lr=0.000020 grad_norm=0.561692
Epoch 69/100 Iteration 193/234: loss=0.038909 lr=0.000020 grad_norm=0.934195
Epoch 69/100 Iteration 194/234: loss=0.036147 lr=0.000020 grad_norm=0.430167
Epoch 69/100 Iteration 195/234: loss=0.040525 lr=0.000020 grad_norm=0.760734
Epoch 69/100 Iteration 196/234: loss=0.036629 lr=0.000020 grad_norm=0.700605
Epoch 69/100 Iteration 197/234: loss=0.042366 lr=0.000020 grad_norm=0.602756
Epoch 69/100 Iteration 198/234: loss=0.040224 lr=0.000020 grad_norm=0.705761
Epoch 69/100 Iteration 199/234: loss=0.036774 lr=0.000020 grad_norm=0.655463
Epoch 69/100 Iteration 200/234: loss=0.036174 lr=0.000020 grad_norm=0.725870
Epoch 69/100 Iteration 201/234: loss=0.041818 lr=0.000020 grad_norm=1.060036
Epoch 69/100 Iteration 202/234: loss=0.043478 lr=0.000020 grad_norm=0.535919
Epoch 69/100 Iteration 203/234: loss=0.035110 lr=0.000020 grad_norm=0.673115
Epoch 69/100 Iteration 204/234: loss=0.037994 lr=0.000020 grad_norm=0.797070
Epoch 69/100 Iteration 205/234: loss=0.042447 lr=0.000020 grad_norm=0.525559
Epoch 69/100 Iteration 206/234: loss=0.041553 lr=0.000020 grad_norm=0.673892
Epoch 69/100 Iteration 207/234: loss=0.043583 lr=0.000020 grad_norm=0.481050
Epoch 69/100 Iteration 208/234: loss=0.042251 lr=0.000020 grad_norm=0.835253
Epoch 69/100 Iteration 209/234: loss=0.044030 lr=0.000020 grad_norm=1.460232
Epoch 69/100 Iteration 210/234: loss=0.043134 lr=0.000020 grad_norm=1.248756
Epoch 69/100 Iteration 211/234: loss=0.039622 lr=0.000020 grad_norm=0.429890
Epoch 69/100 Iteration 212/234: loss=0.037188 lr=0.000020 grad_norm=0.875715
Epoch 69/100 Iteration 213/234: loss=0.038011 lr=0.000020 grad_norm=1.056232
Epoch 69/100 Iteration 214/234: loss=0.044752 lr=0.000020 grad_norm=0.510698
Epoch 69/100 Iteration 215/234: loss=0.041841 lr=0.000020 grad_norm=1.209193
Epoch 69/100 Iteration 216/234: loss=0.039445 lr=0.000020 grad_norm=0.814713
Epoch 69/100 Iteration 217/234: loss=0.036505 lr=0.000020 grad_norm=0.546309
Epoch 69/100 Iteration 218/234: loss=0.039729 lr=0.000020 grad_norm=0.895003
Epoch 69/100 Iteration 219/234: loss=0.037348 lr=0.000020 grad_norm=0.379447
Epoch 69/100 Iteration 220/234: loss=0.038371 lr=0.000020 grad_norm=0.845752
Epoch 69/100 Iteration 221/234: loss=0.043965 lr=0.000020 grad_norm=0.664327
Epoch 69/100 Iteration 222/234: loss=0.041585 lr=0.000020 grad_norm=0.690248
Epoch 69/100 Iteration 223/234: loss=0.043543 lr=0.000020 grad_norm=1.175903
Epoch 69/100 Iteration 224/234: loss=0.034820 lr=0.000020 grad_norm=0.712285
Epoch 69/100 Iteration 225/234: loss=0.044277 lr=0.000020 grad_norm=0.833305
Epoch 69/100 Iteration 226/234: loss=0.043133 lr=0.000020 grad_norm=1.191852
Epoch 69/100 Iteration 227/234: loss=0.042200 lr=0.000020 grad_norm=0.657826
Epoch 69/100 Iteration 228/234: loss=0.041587 lr=0.000020 grad_norm=1.293201
Epoch 69/100 Iteration 229/234: loss=0.042672 lr=0.000020 grad_norm=1.115288
Epoch 69/100 Iteration 230/234: loss=0.045912 lr=0.000020 grad_norm=0.627867
Epoch 69/100 Iteration 231/234: loss=0.039414 lr=0.000020 grad_norm=1.540793
Epoch 69/100 Iteration 232/234: loss=0.041603 lr=0.000020 grad_norm=0.872734
Epoch 69/100 Iteration 233/234: loss=0.040546 lr=0.000020 grad_norm=1.030383
Epoch 69/100 Iteration 234/234: loss=0.047125 lr=0.000020 grad_norm=0.924908
Epoch 69/100 finished. Avg Loss: 0.040896
Epoch 70/100 Iteration 1/234: loss=0.039675 lr=0.000020 grad_norm=0.556758
Epoch 70/100 Iteration 2/234: loss=0.037424 lr=0.000020 grad_norm=1.176914
Epoch 70/100 Iteration 3/234: loss=0.042203 lr=0.000020 grad_norm=0.898823
Epoch 70/100 Iteration 4/234: loss=0.038720 lr=0.000020 grad_norm=0.679791
Epoch 70/100 Iteration 5/234: loss=0.040454 lr=0.000020 grad_norm=1.202245
Epoch 70/100 Iteration 6/234: loss=0.042803 lr=0.000020 grad_norm=1.058336
Epoch 70/100 Iteration 7/234: loss=0.041123 lr=0.000020 grad_norm=0.542607
Epoch 70/100 Iteration 8/234: loss=0.044017 lr=0.000020 grad_norm=0.849881
Epoch 70/100 Iteration 9/234: loss=0.037737 lr=0.000020 grad_norm=0.906357
Epoch 70/100 Iteration 10/234: loss=0.040656 lr=0.000020 grad_norm=0.484502
Epoch 70/100 Iteration 11/234: loss=0.040252 lr=0.000020 grad_norm=0.955361
Epoch 70/100 Iteration 12/234: loss=0.045732 lr=0.000020 grad_norm=1.494976
Epoch 70/100 Iteration 13/234: loss=0.039611 lr=0.000020 grad_norm=0.873995
Epoch 70/100 Iteration 14/234: loss=0.042098 lr=0.000020 grad_norm=0.656832
Epoch 70/100 Iteration 15/234: loss=0.042284 lr=0.000020 grad_norm=0.818826
Epoch 70/100 Iteration 16/234: loss=0.036001 lr=0.000020 grad_norm=0.517448
Epoch 70/100 Iteration 17/234: loss=0.039865 lr=0.000020 grad_norm=0.700925
Epoch 70/100 Iteration 18/234: loss=0.042184 lr=0.000020 grad_norm=0.596964
Epoch 70/100 Iteration 19/234: loss=0.042146 lr=0.000020 grad_norm=0.866619
Epoch 70/100 Iteration 20/234: loss=0.037561 lr=0.000020 grad_norm=0.838717
Epoch 70/100 Iteration 21/234: loss=0.042894 lr=0.000020 grad_norm=0.597360
Epoch 70/100 Iteration 22/234: loss=0.040001 lr=0.000020 grad_norm=0.654691
Epoch 70/100 Iteration 23/234: loss=0.040384 lr=0.000020 grad_norm=0.734160
Epoch 70/100 Iteration 24/234: loss=0.036932 lr=0.000020 grad_norm=0.460565
Epoch 70/100 Iteration 25/234: loss=0.035716 lr=0.000020 grad_norm=0.428408
Epoch 70/100 Iteration 26/234: loss=0.040856 lr=0.000020 grad_norm=0.743400
Epoch 70/100 Iteration 27/234: loss=0.040609 lr=0.000020 grad_norm=1.026309
Epoch 70/100 Iteration 28/234: loss=0.041534 lr=0.000020 grad_norm=1.019945
Epoch 70/100 Iteration 29/234: loss=0.037066 lr=0.000020 grad_norm=0.389594
Epoch 70/100 Iteration 30/234: loss=0.039943 lr=0.000020 grad_norm=1.009733
Epoch 70/100 Iteration 31/234: loss=0.041102 lr=0.000020 grad_norm=0.923049
Epoch 70/100 Iteration 32/234: loss=0.040107 lr=0.000020 grad_norm=0.596000
Epoch 70/100 Iteration 33/234: loss=0.040528 lr=0.000020 grad_norm=0.813058
Epoch 70/100 Iteration 34/234: loss=0.038242 lr=0.000020 grad_norm=0.512036
Epoch 70/100 Iteration 35/234: loss=0.039004 lr=0.000020 grad_norm=0.746655
Epoch 70/100 Iteration 36/234: loss=0.041376 lr=0.000020 grad_norm=0.551561
Epoch 70/100 Iteration 37/234: loss=0.038419 lr=0.000020 grad_norm=0.636914
Epoch 70/100 Iteration 38/234: loss=0.043688 lr=0.000020 grad_norm=1.422919
Epoch 70/100 Iteration 39/234: loss=0.035830 lr=0.000020 grad_norm=1.012028
Epoch 70/100 Iteration 40/234: loss=0.039141 lr=0.000020 grad_norm=0.887396
Epoch 70/100 Iteration 41/234: loss=0.037080 lr=0.000020 grad_norm=0.957411
Epoch 70/100 Iteration 42/234: loss=0.045221 lr=0.000020 grad_norm=1.205046
Epoch 70/100 Iteration 43/234: loss=0.044569 lr=0.000020 grad_norm=1.647893
Epoch 70/100 Iteration 44/234: loss=0.046182 lr=0.000020 grad_norm=1.629276
Epoch 70/100 Iteration 45/234: loss=0.038812 lr=0.000020 grad_norm=0.767473
Epoch 70/100 Iteration 46/234: loss=0.041852 lr=0.000020 grad_norm=0.860996
Epoch 70/100 Iteration 47/234: loss=0.048730 lr=0.000020 grad_norm=1.370495
Epoch 70/100 Iteration 48/234: loss=0.040939 lr=0.000020 grad_norm=1.050720
Epoch 70/100 Iteration 49/234: loss=0.043671 lr=0.000020 grad_norm=0.534857
Epoch 70/100 Iteration 50/234: loss=0.041280 lr=0.000020 grad_norm=0.764778
Epoch 70/100 Iteration 51/234: loss=0.042320 lr=0.000020 grad_norm=0.840571
Epoch 70/100 Iteration 52/234: loss=0.038272 lr=0.000020 grad_norm=0.610652
Epoch 70/100 Iteration 53/234: loss=0.042167 lr=0.000020 grad_norm=0.604946
Epoch 70/100 Iteration 54/234: loss=0.043327 lr=0.000020 grad_norm=0.541159
Epoch 70/100 Iteration 55/234: loss=0.038814 lr=0.000020 grad_norm=0.520440
Epoch 70/100 Iteration 56/234: loss=0.041337 lr=0.000020 grad_norm=0.805298
Epoch 70/100 Iteration 57/234: loss=0.038337 lr=0.000020 grad_norm=1.166000
Epoch 70/100 Iteration 58/234: loss=0.038268 lr=0.000020 grad_norm=1.071157
Epoch 70/100 Iteration 59/234: loss=0.039135 lr=0.000020 grad_norm=0.619196
Epoch 70/100 Iteration 60/234: loss=0.038269 lr=0.000020 grad_norm=0.621799
Epoch 70/100 Iteration 61/234: loss=0.038884 lr=0.000020 grad_norm=0.766775
Epoch 70/100 Iteration 62/234: loss=0.044078 lr=0.000020 grad_norm=0.674450
Epoch 70/100 Iteration 63/234: loss=0.041408 lr=0.000020 grad_norm=0.613055
Epoch 70/100 Iteration 64/234: loss=0.035812 lr=0.000020 grad_norm=0.658106
Epoch 70/100 Iteration 65/234: loss=0.040141 lr=0.000020 grad_norm=0.920969
Epoch 70/100 Iteration 66/234: loss=0.043302 lr=0.000020 grad_norm=0.801349
Epoch 70/100 Iteration 67/234: loss=0.037807 lr=0.000020 grad_norm=0.565442
Epoch 70/100 Iteration 68/234: loss=0.040182 lr=0.000020 grad_norm=0.504085
Epoch 70/100 Iteration 69/234: loss=0.042026 lr=0.000020 grad_norm=0.473923
Epoch 70/100 Iteration 70/234: loss=0.041977 lr=0.000020 grad_norm=0.522797
Epoch 70/100 Iteration 71/234: loss=0.043608 lr=0.000020 grad_norm=0.580409
Epoch 70/100 Iteration 72/234: loss=0.044374 lr=0.000020 grad_norm=0.583862
Epoch 70/100 Iteration 73/234: loss=0.037642 lr=0.000020 grad_norm=0.705002
Epoch 70/100 Iteration 74/234: loss=0.039087 lr=0.000020 grad_norm=0.545771
Epoch 70/100 Iteration 75/234: loss=0.038124 lr=0.000020 grad_norm=0.452606
Epoch 70/100 Iteration 76/234: loss=0.038229 lr=0.000020 grad_norm=1.337443
Epoch 70/100 Iteration 77/234: loss=0.044722 lr=0.000020 grad_norm=2.025043
Epoch 70/100 Iteration 78/234: loss=0.042504 lr=0.000020 grad_norm=1.630762
Epoch 70/100 Iteration 79/234: loss=0.033972 lr=0.000020 grad_norm=0.581030
Epoch 70/100 Iteration 80/234: loss=0.040075 lr=0.000020 grad_norm=1.721585
Epoch 70/100 Iteration 81/234: loss=0.041195 lr=0.000020 grad_norm=1.445258
Epoch 70/100 Iteration 82/234: loss=0.042160 lr=0.000020 grad_norm=0.869906
Epoch 70/100 Iteration 83/234: loss=0.041773 lr=0.000020 grad_norm=1.281663
Epoch 70/100 Iteration 84/234: loss=0.040068 lr=0.000020 grad_norm=0.689524
Epoch 70/100 Iteration 85/234: loss=0.040168 lr=0.000020 grad_norm=1.117195
Epoch 70/100 Iteration 86/234: loss=0.041406 lr=0.000020 grad_norm=0.854438
Epoch 70/100 Iteration 87/234: loss=0.038061 lr=0.000020 grad_norm=0.971783
Epoch 70/100 Iteration 88/234: loss=0.041060 lr=0.000020 grad_norm=1.038661
Epoch 70/100 Iteration 89/234: loss=0.040784 lr=0.000020 grad_norm=0.485178
Epoch 70/100 Iteration 90/234: loss=0.039759 lr=0.000020 grad_norm=1.066282
Epoch 70/100 Iteration 91/234: loss=0.042211 lr=0.000020 grad_norm=0.989029
Epoch 70/100 Iteration 92/234: loss=0.035109 lr=0.000020 grad_norm=0.539932
Epoch 70/100 Iteration 93/234: loss=0.037914 lr=0.000020 grad_norm=0.818660
Epoch 70/100 Iteration 94/234: loss=0.041671 lr=0.000020 grad_norm=0.763605
Epoch 70/100 Iteration 95/234: loss=0.042687 lr=0.000020 grad_norm=1.127678
Epoch 70/100 Iteration 96/234: loss=0.042087 lr=0.000020 grad_norm=1.492815
Epoch 70/100 Iteration 97/234: loss=0.042572 lr=0.000020 grad_norm=0.726124
Epoch 70/100 Iteration 98/234: loss=0.034757 lr=0.000020 grad_norm=0.946067
Epoch 70/100 Iteration 99/234: loss=0.039400 lr=0.000020 grad_norm=1.106625
Epoch 70/100 Iteration 100/234: loss=0.041955 lr=0.000020 grad_norm=0.949342
Epoch 70/100 Iteration 101/234: loss=0.038317 lr=0.000020 grad_norm=0.785262
Epoch 70/100 Iteration 102/234: loss=0.037805 lr=0.000020 grad_norm=0.389615
Epoch 70/100 Iteration 103/234: loss=0.038851 lr=0.000020 grad_norm=0.873895
Epoch 70/100 Iteration 104/234: loss=0.039161 lr=0.000020 grad_norm=0.614254
Epoch 70/100 Iteration 105/234: loss=0.036572 lr=0.000020 grad_norm=0.630345
Epoch 70/100 Iteration 106/234: loss=0.043364 lr=0.000020 grad_norm=0.978139
Epoch 70/100 Iteration 107/234: loss=0.034644 lr=0.000020 grad_norm=0.638171
Epoch 70/100 Iteration 108/234: loss=0.042851 lr=0.000020 grad_norm=0.466760
Epoch 70/100 Iteration 109/234: loss=0.038896 lr=0.000020 grad_norm=0.662222
Epoch 70/100 Iteration 110/234: loss=0.038822 lr=0.000020 grad_norm=0.602871
Epoch 70/100 Iteration 111/234: loss=0.039886 lr=0.000020 grad_norm=0.498124
Epoch 70/100 Iteration 112/234: loss=0.044518 lr=0.000020 grad_norm=1.109920
Epoch 70/100 Iteration 113/234: loss=0.039434 lr=0.000020 grad_norm=1.308141
Epoch 70/100 Iteration 114/234: loss=0.040041 lr=0.000020 grad_norm=0.745300
Epoch 70/100 Iteration 115/234: loss=0.037658 lr=0.000020 grad_norm=1.160945
Epoch 70/100 Iteration 116/234: loss=0.042956 lr=0.000020 grad_norm=1.778893
Epoch 70/100 Iteration 117/234: loss=0.040198 lr=0.000020 grad_norm=1.333604
Epoch 70/100 Iteration 118/234: loss=0.042163 lr=0.000020 grad_norm=0.468147
Epoch 70/100 Iteration 119/234: loss=0.042871 lr=0.000020 grad_norm=1.301111
Epoch 70/100 Iteration 120/234: loss=0.040851 lr=0.000020 grad_norm=1.169222
Epoch 70/100 Iteration 121/234: loss=0.042834 lr=0.000020 grad_norm=0.540540
Epoch 70/100 Iteration 122/234: loss=0.042365 lr=0.000020 grad_norm=1.405070
Epoch 70/100 Iteration 123/234: loss=0.038924 lr=0.000020 grad_norm=0.797312
Epoch 70/100 Iteration 124/234: loss=0.037929 lr=0.000020 grad_norm=1.218308
Epoch 70/100 Iteration 125/234: loss=0.044368 lr=0.000020 grad_norm=2.443580
Epoch 70/100 Iteration 126/234: loss=0.039266 lr=0.000020 grad_norm=1.800840
Epoch 70/100 Iteration 127/234: loss=0.039049 lr=0.000020 grad_norm=0.890700
Epoch 70/100 Iteration 128/234: loss=0.041984 lr=0.000020 grad_norm=1.935076
Epoch 70/100 Iteration 129/234: loss=0.043047 lr=0.000020 grad_norm=0.908895
Epoch 70/100 Iteration 130/234: loss=0.041643 lr=0.000020 grad_norm=1.239354
Epoch 70/100 Iteration 131/234: loss=0.041894 lr=0.000020 grad_norm=0.912447
Epoch 70/100 Iteration 132/234: loss=0.038764 lr=0.000020 grad_norm=0.845804
Epoch 70/100 Iteration 133/234: loss=0.042526 lr=0.000020 grad_norm=0.888301
Epoch 70/100 Iteration 134/234: loss=0.036221 lr=0.000020 grad_norm=0.375336
Epoch 70/100 Iteration 135/234: loss=0.039752 lr=0.000020 grad_norm=0.875794
Epoch 70/100 Iteration 136/234: loss=0.037782 lr=0.000020 grad_norm=0.520705
Epoch 70/100 Iteration 137/234: loss=0.040407 lr=0.000020 grad_norm=0.572941
Epoch 70/100 Iteration 138/234: loss=0.045582 lr=0.000020 grad_norm=1.231062
Epoch 70/100 Iteration 139/234: loss=0.041353 lr=0.000020 grad_norm=1.407541
Epoch 70/100 Iteration 140/234: loss=0.044603 lr=0.000020 grad_norm=0.475828
Epoch 70/100 Iteration 141/234: loss=0.037289 lr=0.000020 grad_norm=1.246947
Epoch 70/100 Iteration 142/234: loss=0.037428 lr=0.000020 grad_norm=0.689971
Epoch 70/100 Iteration 143/234: loss=0.042878 lr=0.000020 grad_norm=0.671762
Epoch 70/100 Iteration 144/234: loss=0.040461 lr=0.000020 grad_norm=0.921913
Epoch 70/100 Iteration 145/234: loss=0.040914 lr=0.000020 grad_norm=0.705479
Epoch 70/100 Iteration 146/234: loss=0.038532 lr=0.000020 grad_norm=0.525855
Epoch 70/100 Iteration 147/234: loss=0.034339 lr=0.000020 grad_norm=0.733744
Epoch 70/100 Iteration 148/234: loss=0.037034 lr=0.000020 grad_norm=0.507330
Epoch 70/100 Iteration 149/234: loss=0.038492 lr=0.000020 grad_norm=0.566440
Epoch 70/100 Iteration 150/234: loss=0.040664 lr=0.000020 grad_norm=0.639574
Epoch 70/100 Iteration 151/234: loss=0.039697 lr=0.000020 grad_norm=0.864883
Epoch 70/100 Iteration 152/234: loss=0.040919 lr=0.000020 grad_norm=0.901519
Epoch 70/100 Iteration 153/234: loss=0.032397 lr=0.000020 grad_norm=0.458138
Epoch 70/100 Iteration 154/234: loss=0.039363 lr=0.000020 grad_norm=0.734412
Epoch 70/100 Iteration 155/234: loss=0.038860 lr=0.000020 grad_norm=0.643568
Epoch 70/100 Iteration 156/234: loss=0.033688 lr=0.000020 grad_norm=0.506210
Epoch 70/100 Iteration 157/234: loss=0.037504 lr=0.000020 grad_norm=0.957123
Epoch 70/100 Iteration 158/234: loss=0.041301 lr=0.000020 grad_norm=1.278797
Epoch 70/100 Iteration 159/234: loss=0.043902 lr=0.000020 grad_norm=1.241553
Epoch 70/100 Iteration 160/234: loss=0.040685 lr=0.000020 grad_norm=0.460731
Epoch 70/100 Iteration 161/234: loss=0.037980 lr=0.000020 grad_norm=1.015659
Epoch 70/100 Iteration 162/234: loss=0.036706 lr=0.000020 grad_norm=1.121541
Epoch 70/100 Iteration 163/234: loss=0.040046 lr=0.000020 grad_norm=0.428314
Epoch 70/100 Iteration 164/234: loss=0.044604 lr=0.000020 grad_norm=1.784942
Epoch 70/100 Iteration 165/234: loss=0.035226 lr=0.000020 grad_norm=1.660609
Epoch 70/100 Iteration 166/234: loss=0.041167 lr=0.000020 grad_norm=0.576539
Epoch 70/100 Iteration 167/234: loss=0.044060 lr=0.000020 grad_norm=2.164962
Epoch 70/100 Iteration 168/234: loss=0.041288 lr=0.000020 grad_norm=2.263145
Epoch 70/100 Iteration 169/234: loss=0.042602 lr=0.000020 grad_norm=0.447281
Epoch 70/100 Iteration 170/234: loss=0.041972 lr=0.000020 grad_norm=1.834104
Epoch 70/100 Iteration 171/234: loss=0.037478 lr=0.000020 grad_norm=1.196913
Epoch 70/100 Iteration 172/234: loss=0.032168 lr=0.000020 grad_norm=0.637074
Epoch 70/100 Iteration 173/234: loss=0.040641 lr=0.000020 grad_norm=1.173939
Epoch 70/100 Iteration 174/234: loss=0.040153 lr=0.000020 grad_norm=0.527060
Epoch 70/100 Iteration 175/234: loss=0.043056 lr=0.000020 grad_norm=1.370021
Epoch 70/100 Iteration 176/234: loss=0.042903 lr=0.000020 grad_norm=1.357427
Epoch 70/100 Iteration 177/234: loss=0.041429 lr=0.000020 grad_norm=0.605067
Epoch 70/100 Iteration 178/234: loss=0.040146 lr=0.000020 grad_norm=0.963944
Epoch 70/100 Iteration 179/234: loss=0.038081 lr=0.000020 grad_norm=0.783136
Epoch 70/100 Iteration 180/234: loss=0.039920 lr=0.000020 grad_norm=0.627348
Epoch 70/100 Iteration 181/234: loss=0.044830 lr=0.000020 grad_norm=0.944490
Epoch 70/100 Iteration 182/234: loss=0.038473 lr=0.000020 grad_norm=0.649784
Epoch 70/100 Iteration 183/234: loss=0.039191 lr=0.000020 grad_norm=0.518641
Epoch 70/100 Iteration 184/234: loss=0.039449 lr=0.000020 grad_norm=0.447147
Epoch 70/100 Iteration 185/234: loss=0.036705 lr=0.000020 grad_norm=0.640576
Epoch 70/100 Iteration 186/234: loss=0.035067 lr=0.000020 grad_norm=0.762182
Epoch 70/100 Iteration 187/234: loss=0.039764 lr=0.000020 grad_norm=0.405702
Epoch 70/100 Iteration 188/234: loss=0.039346 lr=0.000020 grad_norm=0.548689
Epoch 70/100 Iteration 189/234: loss=0.038530 lr=0.000020 grad_norm=0.616726
Epoch 70/100 Iteration 190/234: loss=0.040989 lr=0.000020 grad_norm=0.405679
Epoch 70/100 Iteration 191/234: loss=0.041001 lr=0.000020 grad_norm=0.901503
Epoch 70/100 Iteration 192/234: loss=0.037113 lr=0.000020 grad_norm=0.851144
Epoch 70/100 Iteration 193/234: loss=0.039806 lr=0.000020 grad_norm=0.606219
Epoch 70/100 Iteration 194/234: loss=0.037677 lr=0.000020 grad_norm=1.120521
Epoch 70/100 Iteration 195/234: loss=0.037607 lr=0.000020 grad_norm=0.377920
Epoch 70/100 Iteration 196/234: loss=0.036092 lr=0.000020 grad_norm=1.022564
Epoch 70/100 Iteration 197/234: loss=0.044485 lr=0.000020 grad_norm=1.058189
Epoch 70/100 Iteration 198/234: loss=0.039774 lr=0.000020 grad_norm=0.351306
Epoch 70/100 Iteration 199/234: loss=0.039701 lr=0.000020 grad_norm=1.088167
Epoch 70/100 Iteration 200/234: loss=0.041866 lr=0.000020 grad_norm=0.669811
Epoch 70/100 Iteration 201/234: loss=0.040349 lr=0.000020 grad_norm=0.691705
Epoch 70/100 Iteration 202/234: loss=0.041020 lr=0.000020 grad_norm=1.140624
Epoch 70/100 Iteration 203/234: loss=0.038243 lr=0.000020 grad_norm=0.631449
Epoch 70/100 Iteration 204/234: loss=0.038095 lr=0.000020 grad_norm=0.908579
Epoch 70/100 Iteration 205/234: loss=0.045975 lr=0.000020 grad_norm=1.572266
Epoch 70/100 Iteration 206/234: loss=0.040583 lr=0.000020 grad_norm=0.954192
Epoch 70/100 Iteration 207/234: loss=0.039034 lr=0.000020 grad_norm=0.545665
Epoch 70/100 Iteration 208/234: loss=0.041071 lr=0.000020 grad_norm=1.037379
Epoch 70/100 Iteration 209/234: loss=0.038707 lr=0.000020 grad_norm=0.717358
Epoch 70/100 Iteration 210/234: loss=0.040678 lr=0.000020 grad_norm=1.083661
Epoch 70/100 Iteration 211/234: loss=0.035672 lr=0.000020 grad_norm=0.940779
Epoch 70/100 Iteration 212/234: loss=0.040039 lr=0.000020 grad_norm=0.468897
Epoch 70/100 Iteration 213/234: loss=0.045652 lr=0.000020 grad_norm=1.097260
Epoch 70/100 Iteration 214/234: loss=0.040561 lr=0.000020 grad_norm=1.134829
Epoch 70/100 Iteration 215/234: loss=0.038829 lr=0.000020 grad_norm=0.602456
Epoch 70/100 Iteration 216/234: loss=0.044094 lr=0.000020 grad_norm=1.202100
Epoch 70/100 Iteration 217/234: loss=0.037898 lr=0.000020 grad_norm=0.713507
Epoch 70/100 Iteration 218/234: loss=0.043604 lr=0.000020 grad_norm=1.101117
Epoch 70/100 Iteration 219/234: loss=0.038073 lr=0.000020 grad_norm=1.099280
Epoch 70/100 Iteration 220/234: loss=0.037252 lr=0.000020 grad_norm=0.517887
Epoch 70/100 Iteration 221/234: loss=0.038762 lr=0.000020 grad_norm=0.656195
Epoch 70/100 Iteration 222/234: loss=0.037617 lr=0.000020 grad_norm=0.545773
Epoch 70/100 Iteration 223/234: loss=0.042960 lr=0.000020 grad_norm=0.710199
Epoch 70/100 Iteration 224/234: loss=0.042296 lr=0.000020 grad_norm=0.961216
Epoch 70/100 Iteration 225/234: loss=0.044401 lr=0.000020 grad_norm=0.678200
Epoch 70/100 Iteration 226/234: loss=0.041903 lr=0.000020 grad_norm=0.662219
Epoch 70/100 Iteration 227/234: loss=0.043140 lr=0.000020 grad_norm=0.942588
Epoch 70/100 Iteration 228/234: loss=0.042197 lr=0.000020 grad_norm=0.494213
Epoch 70/100 Iteration 229/234: loss=0.042596 lr=0.000020 grad_norm=0.589065
Epoch 70/100 Iteration 230/234: loss=0.039027 lr=0.000020 grad_norm=0.873089
Epoch 70/100 Iteration 231/234: loss=0.041503 lr=0.000020 grad_norm=0.713588
Epoch 70/100 Iteration 232/234: loss=0.035534 lr=0.000020 grad_norm=0.528280
Epoch 70/100 Iteration 233/234: loss=0.039511 lr=0.000020 grad_norm=1.227813
Epoch 70/100 Iteration 234/234: loss=0.037391 lr=0.000020 grad_norm=0.849324
Epoch 70/100 finished. Avg Loss: 0.040210
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 71/100 Iteration 1/234: loss=0.037311 lr=0.000020 grad_norm=0.602799
Epoch 71/100 Iteration 2/234: loss=0.039272 lr=0.000020 grad_norm=1.248731
Epoch 71/100 Iteration 3/234: loss=0.040057 lr=0.000020 grad_norm=0.902401
Epoch 71/100 Iteration 4/234: loss=0.037532 lr=0.000020 grad_norm=0.881913
Epoch 71/100 Iteration 5/234: loss=0.037648 lr=0.000020 grad_norm=1.430531
Epoch 71/100 Iteration 6/234: loss=0.039211 lr=0.000020 grad_norm=1.026004
Epoch 71/100 Iteration 7/234: loss=0.044799 lr=0.000020 grad_norm=1.026736
Epoch 71/100 Iteration 8/234: loss=0.041191 lr=0.000020 grad_norm=2.060446
Epoch 71/100 Iteration 9/234: loss=0.038265 lr=0.000020 grad_norm=1.288564
Epoch 71/100 Iteration 10/234: loss=0.039263 lr=0.000020 grad_norm=1.440778
Epoch 71/100 Iteration 11/234: loss=0.045443 lr=0.000020 grad_norm=2.296353
Epoch 71/100 Iteration 12/234: loss=0.040556 lr=0.000020 grad_norm=0.826228
Epoch 71/100 Iteration 13/234: loss=0.041359 lr=0.000020 grad_norm=1.788681
Epoch 71/100 Iteration 14/234: loss=0.034801 lr=0.000020 grad_norm=0.789077
Epoch 71/100 Iteration 15/234: loss=0.043065 lr=0.000020 grad_norm=1.699250
Epoch 71/100 Iteration 16/234: loss=0.037832 lr=0.000020 grad_norm=1.370267
Epoch 71/100 Iteration 17/234: loss=0.037799 lr=0.000020 grad_norm=1.142909
Epoch 71/100 Iteration 18/234: loss=0.034143 lr=0.000020 grad_norm=1.306177
Epoch 71/100 Iteration 19/234: loss=0.040060 lr=0.000020 grad_norm=1.031425
Epoch 71/100 Iteration 20/234: loss=0.042967 lr=0.000020 grad_norm=1.654772
Epoch 71/100 Iteration 21/234: loss=0.043448 lr=0.000020 grad_norm=0.611220
Epoch 71/100 Iteration 22/234: loss=0.043614 lr=0.000020 grad_norm=1.346898
Epoch 71/100 Iteration 23/234: loss=0.039713 lr=0.000020 grad_norm=0.995353
Epoch 71/100 Iteration 24/234: loss=0.040347 lr=0.000020 grad_norm=1.044715
Epoch 71/100 Iteration 25/234: loss=0.039820 lr=0.000020 grad_norm=0.923252
Epoch 71/100 Iteration 26/234: loss=0.039427 lr=0.000020 grad_norm=0.625028
Epoch 71/100 Iteration 27/234: loss=0.040732 lr=0.000020 grad_norm=0.658531
Epoch 71/100 Iteration 28/234: loss=0.038299 lr=0.000020 grad_norm=0.367667
Epoch 71/100 Iteration 29/234: loss=0.041288 lr=0.000020 grad_norm=0.618067
Epoch 71/100 Iteration 30/234: loss=0.033457 lr=0.000020 grad_norm=0.596832
Epoch 71/100 Iteration 31/234: loss=0.036960 lr=0.000020 grad_norm=0.492944
Epoch 71/100 Iteration 32/234: loss=0.037634 lr=0.000020 grad_norm=0.463047
Epoch 71/100 Iteration 33/234: loss=0.041697 lr=0.000020 grad_norm=0.495399
Epoch 71/100 Iteration 34/234: loss=0.039494 lr=0.000020 grad_norm=0.608562
Epoch 71/100 Iteration 35/234: loss=0.040845 lr=0.000020 grad_norm=0.447052
Epoch 71/100 Iteration 36/234: loss=0.040090 lr=0.000020 grad_norm=0.481056
Epoch 71/100 Iteration 37/234: loss=0.041137 lr=0.000020 grad_norm=0.443480
Epoch 71/100 Iteration 38/234: loss=0.038359 lr=0.000020 grad_norm=0.748761
Epoch 71/100 Iteration 39/234: loss=0.043721 lr=0.000020 grad_norm=0.643766
Epoch 71/100 Iteration 40/234: loss=0.039471 lr=0.000020 grad_norm=0.775162
Epoch 71/100 Iteration 41/234: loss=0.036554 lr=0.000020 grad_norm=0.536653
Epoch 71/100 Iteration 42/234: loss=0.039930 lr=0.000020 grad_norm=0.595071
Epoch 71/100 Iteration 43/234: loss=0.038413 lr=0.000020 grad_norm=0.527944
Epoch 71/100 Iteration 44/234: loss=0.040503 lr=0.000020 grad_norm=0.485841
Epoch 71/100 Iteration 45/234: loss=0.040275 lr=0.000020 grad_norm=0.512389
Epoch 71/100 Iteration 46/234: loss=0.039435 lr=0.000020 grad_norm=0.434384
Epoch 71/100 Iteration 47/234: loss=0.041237 lr=0.000020 grad_norm=0.528302
Epoch 71/100 Iteration 48/234: loss=0.040971 lr=0.000020 grad_norm=0.637543
Epoch 71/100 Iteration 49/234: loss=0.040805 lr=0.000020 grad_norm=0.451891
Epoch 71/100 Iteration 50/234: loss=0.039579 lr=0.000020 grad_norm=0.639332
Epoch 71/100 Iteration 51/234: loss=0.040046 lr=0.000020 grad_norm=0.470997
Epoch 71/100 Iteration 52/234: loss=0.039478 lr=0.000020 grad_norm=0.727628
Epoch 71/100 Iteration 53/234: loss=0.041064 lr=0.000020 grad_norm=0.412539
Epoch 71/100 Iteration 54/234: loss=0.035084 lr=0.000020 grad_norm=0.470986
Epoch 71/100 Iteration 55/234: loss=0.037036 lr=0.000020 grad_norm=0.536054
Epoch 71/100 Iteration 56/234: loss=0.039460 lr=0.000020 grad_norm=0.392508
Epoch 71/100 Iteration 57/234: loss=0.040712 lr=0.000020 grad_norm=0.483567
Epoch 71/100 Iteration 58/234: loss=0.036422 lr=0.000020 grad_norm=0.585296
Epoch 71/100 Iteration 59/234: loss=0.039901 lr=0.000020 grad_norm=0.653265
Epoch 71/100 Iteration 60/234: loss=0.035911 lr=0.000020 grad_norm=0.449738
Epoch 71/100 Iteration 61/234: loss=0.040180 lr=0.000020 grad_norm=0.511918
Epoch 71/100 Iteration 62/234: loss=0.043437 lr=0.000020 grad_norm=0.641325
Epoch 71/100 Iteration 63/234: loss=0.038649 lr=0.000020 grad_norm=0.558542
Epoch 71/100 Iteration 64/234: loss=0.041185 lr=0.000020 grad_norm=0.358954
Epoch 71/100 Iteration 65/234: loss=0.040613 lr=0.000020 grad_norm=0.461764
Epoch 71/100 Iteration 66/234: loss=0.036269 lr=0.000020 grad_norm=0.461149
Epoch 71/100 Iteration 67/234: loss=0.039159 lr=0.000020 grad_norm=0.709563
Epoch 71/100 Iteration 68/234: loss=0.036229 lr=0.000020 grad_norm=0.870475
Epoch 71/100 Iteration 69/234: loss=0.041103 lr=0.000020 grad_norm=0.512500
Epoch 71/100 Iteration 70/234: loss=0.037108 lr=0.000020 grad_norm=0.716981
Epoch 71/100 Iteration 71/234: loss=0.038892 lr=0.000020 grad_norm=0.932740
Epoch 71/100 Iteration 72/234: loss=0.040877 lr=0.000020 grad_norm=0.569626
Epoch 71/100 Iteration 73/234: loss=0.038214 lr=0.000020 grad_norm=0.663092
Epoch 71/100 Iteration 74/234: loss=0.038976 lr=0.000020 grad_norm=0.656963
Epoch 71/100 Iteration 75/234: loss=0.043130 lr=0.000020 grad_norm=0.749672
Epoch 71/100 Iteration 76/234: loss=0.041084 lr=0.000020 grad_norm=1.045204
Epoch 71/100 Iteration 77/234: loss=0.038566 lr=0.000020 grad_norm=0.638837
Epoch 71/100 Iteration 78/234: loss=0.039308 lr=0.000020 grad_norm=0.601182
Epoch 71/100 Iteration 79/234: loss=0.038970 lr=0.000020 grad_norm=0.890131
Epoch 71/100 Iteration 80/234: loss=0.045715 lr=0.000020 grad_norm=0.664632
Epoch 71/100 Iteration 81/234: loss=0.037309 lr=0.000020 grad_norm=0.469919
Epoch 71/100 Iteration 82/234: loss=0.037894 lr=0.000020 grad_norm=0.821395
Epoch 71/100 Iteration 83/234: loss=0.039817 lr=0.000020 grad_norm=0.819947
Epoch 71/100 Iteration 84/234: loss=0.038996 lr=0.000020 grad_norm=0.521876
Epoch 71/100 Iteration 85/234: loss=0.042561 lr=0.000020 grad_norm=0.997674
Epoch 71/100 Iteration 86/234: loss=0.040201 lr=0.000020 grad_norm=0.683949
Epoch 71/100 Iteration 87/234: loss=0.040251 lr=0.000020 grad_norm=1.311894
Epoch 71/100 Iteration 88/234: loss=0.038700 lr=0.000020 grad_norm=1.576948
Epoch 71/100 Iteration 89/234: loss=0.036256 lr=0.000020 grad_norm=0.407788
Epoch 71/100 Iteration 90/234: loss=0.040612 lr=0.000020 grad_norm=1.224516
Epoch 71/100 Iteration 91/234: loss=0.039831 lr=0.000020 grad_norm=1.168433
Epoch 71/100 Iteration 92/234: loss=0.040573 lr=0.000020 grad_norm=0.639427
Epoch 71/100 Iteration 93/234: loss=0.041037 lr=0.000020 grad_norm=0.897847
Epoch 71/100 Iteration 94/234: loss=0.038628 lr=0.000020 grad_norm=0.479650
Epoch 71/100 Iteration 95/234: loss=0.042504 lr=0.000020 grad_norm=0.747923
Epoch 71/100 Iteration 96/234: loss=0.037789 lr=0.000020 grad_norm=0.503557
Epoch 71/100 Iteration 97/234: loss=0.036355 lr=0.000020 grad_norm=0.749426
Epoch 71/100 Iteration 98/234: loss=0.045496 lr=0.000020 grad_norm=1.219413
Epoch 71/100 Iteration 99/234: loss=0.037408 lr=0.000020 grad_norm=0.935623
Epoch 71/100 Iteration 100/234: loss=0.038152 lr=0.000020 grad_norm=0.679118
Epoch 71/100 Iteration 101/234: loss=0.040339 lr=0.000020 grad_norm=0.992949
Epoch 71/100 Iteration 102/234: loss=0.041009 lr=0.000020 grad_norm=0.656871
Epoch 71/100 Iteration 103/234: loss=0.044980 lr=0.000020 grad_norm=1.129498
Epoch 71/100 Iteration 104/234: loss=0.042647 lr=0.000020 grad_norm=1.332496
Epoch 71/100 Iteration 105/234: loss=0.038846 lr=0.000020 grad_norm=0.822378
Epoch 71/100 Iteration 106/234: loss=0.042631 lr=0.000020 grad_norm=0.545960
Epoch 71/100 Iteration 107/234: loss=0.040582 lr=0.000020 grad_norm=1.196734
Epoch 71/100 Iteration 108/234: loss=0.037714 lr=0.000020 grad_norm=1.116984
Epoch 71/100 Iteration 109/234: loss=0.039987 lr=0.000020 grad_norm=0.607630
Epoch 71/100 Iteration 110/234: loss=0.042320 lr=0.000020 grad_norm=0.699426
Epoch 71/100 Iteration 111/234: loss=0.037377 lr=0.000020 grad_norm=0.685066
Epoch 71/100 Iteration 112/234: loss=0.042350 lr=0.000020 grad_norm=0.531249
Epoch 71/100 Iteration 113/234: loss=0.041716 lr=0.000020 grad_norm=0.574470
Epoch 71/100 Iteration 114/234: loss=0.035463 lr=0.000020 grad_norm=0.378205
Epoch 71/100 Iteration 115/234: loss=0.042073 lr=0.000020 grad_norm=0.694239
Epoch 71/100 Iteration 116/234: loss=0.035853 lr=0.000020 grad_norm=0.638558
Epoch 71/100 Iteration 117/234: loss=0.040911 lr=0.000020 grad_norm=0.473638
Epoch 71/100 Iteration 118/234: loss=0.040013 lr=0.000020 grad_norm=0.506093
Epoch 71/100 Iteration 119/234: loss=0.038994 lr=0.000020 grad_norm=0.617398
Epoch 71/100 Iteration 120/234: loss=0.036692 lr=0.000020 grad_norm=0.458682
Epoch 71/100 Iteration 121/234: loss=0.040936 lr=0.000020 grad_norm=0.428576
Epoch 71/100 Iteration 122/234: loss=0.038572 lr=0.000020 grad_norm=0.844828
Epoch 71/100 Iteration 123/234: loss=0.040504 lr=0.000020 grad_norm=0.800811
Epoch 71/100 Iteration 124/234: loss=0.041846 lr=0.000020 grad_norm=0.463113
Epoch 71/100 Iteration 125/234: loss=0.040860 lr=0.000020 grad_norm=0.580678
Epoch 71/100 Iteration 126/234: loss=0.042123 lr=0.000020 grad_norm=0.815981
Epoch 71/100 Iteration 127/234: loss=0.044372 lr=0.000020 grad_norm=0.553708
Epoch 71/100 Iteration 128/234: loss=0.039243 lr=0.000020 grad_norm=0.473420
Epoch 71/100 Iteration 129/234: loss=0.036594 lr=0.000020 grad_norm=0.471763
Epoch 71/100 Iteration 130/234: loss=0.035640 lr=0.000020 grad_norm=0.408993
Epoch 71/100 Iteration 131/234: loss=0.038552 lr=0.000020 grad_norm=0.937445
Epoch 71/100 Iteration 132/234: loss=0.044448 lr=0.000020 grad_norm=1.206032
Epoch 71/100 Iteration 133/234: loss=0.040790 lr=0.000020 grad_norm=0.764970
Epoch 71/100 Iteration 134/234: loss=0.034448 lr=0.000020 grad_norm=0.829430
Epoch 71/100 Iteration 135/234: loss=0.042797 lr=0.000020 grad_norm=1.058055
Epoch 71/100 Iteration 136/234: loss=0.039286 lr=0.000020 grad_norm=0.787988
Epoch 71/100 Iteration 137/234: loss=0.045466 lr=0.000020 grad_norm=1.019907
Epoch 71/100 Iteration 138/234: loss=0.039472 lr=0.000020 grad_norm=1.519070
Epoch 71/100 Iteration 139/234: loss=0.038705 lr=0.000020 grad_norm=0.655252
Epoch 71/100 Iteration 140/234: loss=0.041789 lr=0.000020 grad_norm=1.253688
Epoch 71/100 Iteration 141/234: loss=0.039311 lr=0.000020 grad_norm=1.169075
Epoch 71/100 Iteration 142/234: loss=0.037099 lr=0.000020 grad_norm=0.372816
Epoch 71/100 Iteration 143/234: loss=0.042056 lr=0.000020 grad_norm=1.285117
Epoch 71/100 Iteration 144/234: loss=0.033332 lr=0.000020 grad_norm=0.845892
Epoch 71/100 Iteration 145/234: loss=0.038041 lr=0.000020 grad_norm=0.552537
Epoch 71/100 Iteration 146/234: loss=0.036008 lr=0.000020 grad_norm=0.641512
Epoch 71/100 Iteration 147/234: loss=0.047250 lr=0.000020 grad_norm=0.780345
Epoch 71/100 Iteration 148/234: loss=0.041262 lr=0.000020 grad_norm=1.395679
Epoch 71/100 Iteration 149/234: loss=0.040245 lr=0.000020 grad_norm=1.139205
Epoch 71/100 Iteration 150/234: loss=0.042536 lr=0.000020 grad_norm=0.651508
Epoch 71/100 Iteration 151/234: loss=0.039184 lr=0.000020 grad_norm=1.101955
Epoch 71/100 Iteration 152/234: loss=0.038867 lr=0.000020 grad_norm=0.429469
Epoch 71/100 Iteration 153/234: loss=0.037807 lr=0.000020 grad_norm=1.141701
Epoch 71/100 Iteration 154/234: loss=0.041945 lr=0.000020 grad_norm=1.186280
Epoch 71/100 Iteration 155/234: loss=0.035101 lr=0.000020 grad_norm=0.395312
Epoch 71/100 Iteration 156/234: loss=0.040161 lr=0.000020 grad_norm=0.903689
Epoch 71/100 Iteration 157/234: loss=0.043185 lr=0.000020 grad_norm=0.790765
Epoch 71/100 Iteration 158/234: loss=0.038798 lr=0.000020 grad_norm=0.597394
Epoch 71/100 Iteration 159/234: loss=0.039318 lr=0.000020 grad_norm=0.779058
Epoch 71/100 Iteration 160/234: loss=0.044943 lr=0.000020 grad_norm=0.422471
Epoch 71/100 Iteration 161/234: loss=0.042971 lr=0.000020 grad_norm=0.564363
Epoch 71/100 Iteration 162/234: loss=0.043458 lr=0.000020 grad_norm=0.942317
Epoch 71/100 Iteration 163/234: loss=0.044484 lr=0.000020 grad_norm=1.167265
Epoch 71/100 Iteration 164/234: loss=0.038749 lr=0.000020 grad_norm=0.792997
Epoch 71/100 Iteration 165/234: loss=0.038353 lr=0.000020 grad_norm=0.536233
Epoch 71/100 Iteration 166/234: loss=0.041706 lr=0.000020 grad_norm=0.808598
Epoch 71/100 Iteration 167/234: loss=0.037844 lr=0.000020 grad_norm=0.466966
Epoch 71/100 Iteration 168/234: loss=0.042101 lr=0.000020 grad_norm=0.634853
Epoch 71/100 Iteration 169/234: loss=0.045073 lr=0.000020 grad_norm=1.172190
Epoch 71/100 Iteration 170/234: loss=0.043003 lr=0.000020 grad_norm=1.172135
Epoch 71/100 Iteration 171/234: loss=0.040024 lr=0.000020 grad_norm=0.761092
Epoch 71/100 Iteration 172/234: loss=0.040793 lr=0.000020 grad_norm=0.510005
Epoch 71/100 Iteration 173/234: loss=0.044225 lr=0.000020 grad_norm=0.679807
Epoch 71/100 Iteration 174/234: loss=0.045395 lr=0.000020 grad_norm=1.075371
Epoch 71/100 Iteration 175/234: loss=0.044905 lr=0.000020 grad_norm=1.234652
Epoch 71/100 Iteration 176/234: loss=0.043897 lr=0.000020 grad_norm=1.073231
Epoch 71/100 Iteration 177/234: loss=0.038439 lr=0.000020 grad_norm=0.637068
Epoch 71/100 Iteration 178/234: loss=0.037158 lr=0.000020 grad_norm=0.872036
Epoch 71/100 Iteration 179/234: loss=0.036586 lr=0.000020 grad_norm=1.063336
Epoch 71/100 Iteration 180/234: loss=0.039922 lr=0.000020 grad_norm=1.000206
Epoch 71/100 Iteration 181/234: loss=0.039505 lr=0.000020 grad_norm=0.736778
Epoch 71/100 Iteration 182/234: loss=0.039491 lr=0.000020 grad_norm=0.812179
Epoch 71/100 Iteration 183/234: loss=0.039642 lr=0.000020 grad_norm=0.743865
Epoch 71/100 Iteration 184/234: loss=0.040610 lr=0.000020 grad_norm=0.489564
Epoch 71/100 Iteration 185/234: loss=0.037347 lr=0.000020 grad_norm=0.872336
Epoch 71/100 Iteration 186/234: loss=0.036707 lr=0.000020 grad_norm=0.641671
Epoch 71/100 Iteration 187/234: loss=0.037301 lr=0.000020 grad_norm=0.692707
Epoch 71/100 Iteration 188/234: loss=0.035738 lr=0.000020 grad_norm=0.928991
Epoch 71/100 Iteration 189/234: loss=0.044791 lr=0.000020 grad_norm=0.674552
Epoch 71/100 Iteration 190/234: loss=0.040052 lr=0.000020 grad_norm=0.970933
Epoch 71/100 Iteration 191/234: loss=0.036894 lr=0.000020 grad_norm=1.321900
Epoch 71/100 Iteration 192/234: loss=0.040292 lr=0.000020 grad_norm=0.648755
Epoch 71/100 Iteration 193/234: loss=0.040342 lr=0.000020 grad_norm=0.720663
Epoch 71/100 Iteration 194/234: loss=0.040684 lr=0.000020 grad_norm=1.136428
Epoch 71/100 Iteration 195/234: loss=0.038101 lr=0.000020 grad_norm=1.038882
Epoch 71/100 Iteration 196/234: loss=0.038709 lr=0.000020 grad_norm=0.592980
Epoch 71/100 Iteration 197/234: loss=0.040080 lr=0.000020 grad_norm=0.807611
Epoch 71/100 Iteration 198/234: loss=0.037854 lr=0.000020 grad_norm=1.204802
Epoch 71/100 Iteration 199/234: loss=0.040755 lr=0.000020 grad_norm=1.009836
Epoch 71/100 Iteration 200/234: loss=0.037496 lr=0.000020 grad_norm=0.502365
Epoch 71/100 Iteration 201/234: loss=0.042447 lr=0.000020 grad_norm=0.512486
Epoch 71/100 Iteration 202/234: loss=0.041331 lr=0.000020 grad_norm=0.429516
Epoch 71/100 Iteration 203/234: loss=0.044170 lr=0.000020 grad_norm=0.726429
Epoch 71/100 Iteration 204/234: loss=0.036866 lr=0.000020 grad_norm=0.912428
Epoch 71/100 Iteration 205/234: loss=0.042528 lr=0.000020 grad_norm=0.581406
Epoch 71/100 Iteration 206/234: loss=0.033302 lr=0.000020 grad_norm=0.861675
Epoch 71/100 Iteration 207/234: loss=0.044384 lr=0.000020 grad_norm=0.650953
Epoch 71/100 Iteration 208/234: loss=0.041211 lr=0.000020 grad_norm=1.254142
Epoch 71/100 Iteration 209/234: loss=0.042179 lr=0.000020 grad_norm=1.586433
Epoch 71/100 Iteration 210/234: loss=0.041997 lr=0.000020 grad_norm=0.936966
Epoch 71/100 Iteration 211/234: loss=0.041189 lr=0.000020 grad_norm=1.097581
Epoch 71/100 Iteration 212/234: loss=0.038940 lr=0.000020 grad_norm=1.042829
Epoch 71/100 Iteration 213/234: loss=0.038776 lr=0.000020 grad_norm=0.642051
Epoch 71/100 Iteration 214/234: loss=0.037736 lr=0.000020 grad_norm=1.350517
Epoch 71/100 Iteration 215/234: loss=0.034278 lr=0.000020 grad_norm=0.931771
Epoch 71/100 Iteration 216/234: loss=0.039895 lr=0.000020 grad_norm=0.833672
Epoch 71/100 Iteration 217/234: loss=0.039596 lr=0.000020 grad_norm=1.698195
Epoch 71/100 Iteration 218/234: loss=0.041285 lr=0.000020 grad_norm=0.984853
Epoch 71/100 Iteration 219/234: loss=0.044264 lr=0.000020 grad_norm=0.999596
Epoch 71/100 Iteration 220/234: loss=0.044378 lr=0.000020 grad_norm=1.857326
Epoch 71/100 Iteration 221/234: loss=0.036361 lr=0.000020 grad_norm=0.937515
Epoch 71/100 Iteration 222/234: loss=0.033361 lr=0.000020 grad_norm=0.796614
Epoch 71/100 Iteration 223/234: loss=0.035393 lr=0.000020 grad_norm=0.616935
Epoch 71/100 Iteration 224/234: loss=0.040584 lr=0.000020 grad_norm=0.564454
Epoch 71/100 Iteration 225/234: loss=0.042880 lr=0.000020 grad_norm=1.043933
Epoch 71/100 Iteration 226/234: loss=0.037355 lr=0.000020 grad_norm=0.490781
Epoch 71/100 Iteration 227/234: loss=0.037057 lr=0.000020 grad_norm=0.651202
Epoch 71/100 Iteration 228/234: loss=0.040434 lr=0.000020 grad_norm=0.563375
Epoch 71/100 Iteration 229/234: loss=0.041680 lr=0.000020 grad_norm=0.695260
Epoch 71/100 Iteration 230/234: loss=0.043922 lr=0.000020 grad_norm=0.920878
Epoch 71/100 Iteration 231/234: loss=0.041939 lr=0.000020 grad_norm=0.673912
Epoch 71/100 Iteration 232/234: loss=0.034707 lr=0.000020 grad_norm=0.394503
Epoch 71/100 Iteration 233/234: loss=0.044132 lr=0.000020 grad_norm=0.572485
Epoch 71/100 Iteration 234/234: loss=0.040667 lr=0.000020 grad_norm=0.598853
Epoch 71/100 finished. Avg Loss: 0.039919
Epoch 72/100 Iteration 1/234: loss=0.045492 lr=0.000020 grad_norm=0.922903
Epoch 72/100 Iteration 2/234: loss=0.043909 lr=0.000020 grad_norm=0.994689
Epoch 72/100 Iteration 3/234: loss=0.039233 lr=0.000020 grad_norm=0.436030
Epoch 72/100 Iteration 4/234: loss=0.038007 lr=0.000020 grad_norm=1.138240
Epoch 72/100 Iteration 5/234: loss=0.037697 lr=0.000020 grad_norm=0.979298
Epoch 72/100 Iteration 6/234: loss=0.041617 lr=0.000020 grad_norm=0.880062
Epoch 72/100 Iteration 7/234: loss=0.038167 lr=0.000020 grad_norm=1.524472
Epoch 72/100 Iteration 8/234: loss=0.039761 lr=0.000020 grad_norm=0.788007
Epoch 72/100 Iteration 9/234: loss=0.042355 lr=0.000020 grad_norm=1.404289
Epoch 72/100 Iteration 10/234: loss=0.036459 lr=0.000020 grad_norm=0.862576
Epoch 72/100 Iteration 11/234: loss=0.033629 lr=0.000020 grad_norm=0.617178
Epoch 72/100 Iteration 12/234: loss=0.039646 lr=0.000020 grad_norm=1.071959
Epoch 72/100 Iteration 13/234: loss=0.042962 lr=0.000020 grad_norm=0.431327
Epoch 72/100 Iteration 14/234: loss=0.037725 lr=0.000020 grad_norm=0.964444
Epoch 72/100 Iteration 15/234: loss=0.039604 lr=0.000020 grad_norm=0.675714
Epoch 72/100 Iteration 16/234: loss=0.045611 lr=0.000020 grad_norm=0.694283
Epoch 72/100 Iteration 17/234: loss=0.038073 lr=0.000020 grad_norm=0.846234
Epoch 72/100 Iteration 18/234: loss=0.037309 lr=0.000020 grad_norm=0.484751
Epoch 72/100 Iteration 19/234: loss=0.041980 lr=0.000020 grad_norm=1.017815
Epoch 72/100 Iteration 20/234: loss=0.041495 lr=0.000020 grad_norm=1.255066
Epoch 72/100 Iteration 21/234: loss=0.038728 lr=0.000020 grad_norm=1.518887
Epoch 72/100 Iteration 22/234: loss=0.037843 lr=0.000020 grad_norm=1.403592
Epoch 72/100 Iteration 23/234: loss=0.041358 lr=0.000020 grad_norm=0.420556
Epoch 72/100 Iteration 24/234: loss=0.040604 lr=0.000020 grad_norm=0.983550
Epoch 72/100 Iteration 25/234: loss=0.041712 lr=0.000020 grad_norm=0.515027
Epoch 72/100 Iteration 26/234: loss=0.039532 lr=0.000020 grad_norm=1.295337
Epoch 72/100 Iteration 27/234: loss=0.043528 lr=0.000020 grad_norm=1.253770
Epoch 72/100 Iteration 28/234: loss=0.042116 lr=0.000020 grad_norm=1.183714
Epoch 72/100 Iteration 29/234: loss=0.038985 lr=0.000020 grad_norm=1.123331
Epoch 72/100 Iteration 30/234: loss=0.040751 lr=0.000020 grad_norm=0.593689
Epoch 72/100 Iteration 31/234: loss=0.039713 lr=0.000020 grad_norm=0.950238
Epoch 72/100 Iteration 32/234: loss=0.037246 lr=0.000020 grad_norm=0.880294
Epoch 72/100 Iteration 33/234: loss=0.041597 lr=0.000020 grad_norm=0.926938
Epoch 72/100 Iteration 34/234: loss=0.034507 lr=0.000020 grad_norm=1.076236
Epoch 72/100 Iteration 35/234: loss=0.039136 lr=0.000020 grad_norm=0.501501
Epoch 72/100 Iteration 36/234: loss=0.043235 lr=0.000020 grad_norm=1.645378
Epoch 72/100 Iteration 37/234: loss=0.037480 lr=0.000020 grad_norm=1.149118
Epoch 72/100 Iteration 38/234: loss=0.042656 lr=0.000020 grad_norm=0.818817
Epoch 72/100 Iteration 39/234: loss=0.038931 lr=0.000020 grad_norm=0.998573
Epoch 72/100 Iteration 40/234: loss=0.042512 lr=0.000020 grad_norm=0.644433
Epoch 72/100 Iteration 41/234: loss=0.039180 lr=0.000020 grad_norm=0.675260
Epoch 72/100 Iteration 42/234: loss=0.038454 lr=0.000020 grad_norm=0.683608
Epoch 72/100 Iteration 43/234: loss=0.037271 lr=0.000020 grad_norm=0.568038
Epoch 72/100 Iteration 44/234: loss=0.039483 lr=0.000020 grad_norm=0.574050
Epoch 72/100 Iteration 45/234: loss=0.042252 lr=0.000020 grad_norm=0.506563
Epoch 72/100 Iteration 46/234: loss=0.039897 lr=0.000020 grad_norm=0.510545
Epoch 72/100 Iteration 47/234: loss=0.038553 lr=0.000020 grad_norm=0.526002
Epoch 72/100 Iteration 48/234: loss=0.038414 lr=0.000020 grad_norm=0.431484
Epoch 72/100 Iteration 49/234: loss=0.041704 lr=0.000020 grad_norm=0.481520
Epoch 72/100 Iteration 50/234: loss=0.039768 lr=0.000020 grad_norm=0.564240
Epoch 72/100 Iteration 51/234: loss=0.036988 lr=0.000020 grad_norm=0.465407
Epoch 72/100 Iteration 52/234: loss=0.038213 lr=0.000020 grad_norm=0.464695
Epoch 72/100 Iteration 53/234: loss=0.036176 lr=0.000020 grad_norm=0.485109
Epoch 72/100 Iteration 54/234: loss=0.036369 lr=0.000020 grad_norm=0.629980
Epoch 72/100 Iteration 55/234: loss=0.036949 lr=0.000020 grad_norm=0.500718
Epoch 72/100 Iteration 56/234: loss=0.037034 lr=0.000020 grad_norm=0.378710
Epoch 72/100 Iteration 57/234: loss=0.044509 lr=0.000020 grad_norm=0.503811
Epoch 72/100 Iteration 58/234: loss=0.038951 lr=0.000020 grad_norm=0.399643
Epoch 72/100 Iteration 59/234: loss=0.040930 lr=0.000020 grad_norm=0.522499
Epoch 72/100 Iteration 60/234: loss=0.040672 lr=0.000020 grad_norm=0.570561
Epoch 72/100 Iteration 61/234: loss=0.035101 lr=0.000020 grad_norm=0.618014
Epoch 72/100 Iteration 62/234: loss=0.038929 lr=0.000020 grad_norm=0.788855
Epoch 72/100 Iteration 63/234: loss=0.033235 lr=0.000020 grad_norm=0.494535
Epoch 72/100 Iteration 64/234: loss=0.045447 lr=0.000020 grad_norm=0.882321
Epoch 72/100 Iteration 65/234: loss=0.037414 lr=0.000020 grad_norm=1.174838
Epoch 72/100 Iteration 66/234: loss=0.037077 lr=0.000020 grad_norm=0.708717
Epoch 72/100 Iteration 67/234: loss=0.036078 lr=0.000020 grad_norm=0.890087
Epoch 72/100 Iteration 68/234: loss=0.040892 lr=0.000020 grad_norm=1.013678
Epoch 72/100 Iteration 69/234: loss=0.036845 lr=0.000020 grad_norm=0.604514
Epoch 72/100 Iteration 70/234: loss=0.040394 lr=0.000020 grad_norm=1.012830
Epoch 72/100 Iteration 71/234: loss=0.038152 lr=0.000020 grad_norm=1.195069
Epoch 72/100 Iteration 72/234: loss=0.037365 lr=0.000020 grad_norm=0.550660
Epoch 72/100 Iteration 73/234: loss=0.039790 lr=0.000020 grad_norm=0.740211
Epoch 72/100 Iteration 74/234: loss=0.041729 lr=0.000020 grad_norm=1.123820
Epoch 72/100 Iteration 75/234: loss=0.039090 lr=0.000020 grad_norm=0.652602
Epoch 72/100 Iteration 76/234: loss=0.039692 lr=0.000020 grad_norm=0.922231
Epoch 72/100 Iteration 77/234: loss=0.044051 lr=0.000020 grad_norm=0.860036
Epoch 72/100 Iteration 78/234: loss=0.040931 lr=0.000020 grad_norm=0.674151
Epoch 72/100 Iteration 79/234: loss=0.039326 lr=0.000020 grad_norm=0.649387
Epoch 72/100 Iteration 80/234: loss=0.042633 lr=0.000020 grad_norm=0.786348
Epoch 72/100 Iteration 81/234: loss=0.040035 lr=0.000020 grad_norm=0.877440
Epoch 72/100 Iteration 82/234: loss=0.042026 lr=0.000020 grad_norm=0.486164
Epoch 72/100 Iteration 83/234: loss=0.037087 lr=0.000020 grad_norm=0.808558
Epoch 72/100 Iteration 84/234: loss=0.042487 lr=0.000020 grad_norm=1.016785
Epoch 72/100 Iteration 85/234: loss=0.035296 lr=0.000020 grad_norm=0.821460
Epoch 72/100 Iteration 86/234: loss=0.035049 lr=0.000020 grad_norm=0.592163
Epoch 72/100 Iteration 87/234: loss=0.045095 lr=0.000020 grad_norm=1.020082
Epoch 72/100 Iteration 88/234: loss=0.038944 lr=0.000020 grad_norm=1.051303
Epoch 72/100 Iteration 89/234: loss=0.039790 lr=0.000020 grad_norm=0.704587
Epoch 72/100 Iteration 90/234: loss=0.040657 lr=0.000020 grad_norm=0.774866
Epoch 72/100 Iteration 91/234: loss=0.039493 lr=0.000020 grad_norm=0.721500
Epoch 72/100 Iteration 92/234: loss=0.036705 lr=0.000020 grad_norm=0.736474
Epoch 72/100 Iteration 93/234: loss=0.042137 lr=0.000020 grad_norm=0.437562
Epoch 72/100 Iteration 94/234: loss=0.038772 lr=0.000020 grad_norm=0.625436
Epoch 72/100 Iteration 95/234: loss=0.042962 lr=0.000020 grad_norm=0.771589
Epoch 72/100 Iteration 96/234: loss=0.040478 lr=0.000020 grad_norm=0.643732
Epoch 72/100 Iteration 97/234: loss=0.036259 lr=0.000020 grad_norm=0.311262
Epoch 72/100 Iteration 98/234: loss=0.043436 lr=0.000020 grad_norm=0.589484
Epoch 72/100 Iteration 99/234: loss=0.041349 lr=0.000020 grad_norm=0.536357
Epoch 72/100 Iteration 100/234: loss=0.041265 lr=0.000020 grad_norm=0.573063
Epoch 72/100 Iteration 101/234: loss=0.039421 lr=0.000020 grad_norm=0.625577
Epoch 72/100 Iteration 102/234: loss=0.042669 lr=0.000020 grad_norm=0.699676
Epoch 72/100 Iteration 103/234: loss=0.038806 lr=0.000020 grad_norm=0.555295
Epoch 72/100 Iteration 104/234: loss=0.035175 lr=0.000020 grad_norm=0.632363
Epoch 72/100 Iteration 105/234: loss=0.041217 lr=0.000020 grad_norm=0.961290
Epoch 72/100 Iteration 106/234: loss=0.037151 lr=0.000020 grad_norm=0.853980
Epoch 72/100 Iteration 107/234: loss=0.038048 lr=0.000020 grad_norm=0.604623
Epoch 72/100 Iteration 108/234: loss=0.042947 lr=0.000020 grad_norm=1.579184
Epoch 72/100 Iteration 109/234: loss=0.039211 lr=0.000020 grad_norm=1.319523
Epoch 72/100 Iteration 110/234: loss=0.041674 lr=0.000020 grad_norm=0.574693
Epoch 72/100 Iteration 111/234: loss=0.042297 lr=0.000020 grad_norm=0.779998
Epoch 72/100 Iteration 112/234: loss=0.039505 lr=0.000020 grad_norm=1.109317
Epoch 72/100 Iteration 113/234: loss=0.045830 lr=0.000020 grad_norm=0.693542
Epoch 72/100 Iteration 114/234: loss=0.039517 lr=0.000020 grad_norm=0.602225
Epoch 72/100 Iteration 115/234: loss=0.040477 lr=0.000020 grad_norm=0.865076
Epoch 72/100 Iteration 116/234: loss=0.041027 lr=0.000020 grad_norm=1.191760
Epoch 72/100 Iteration 117/234: loss=0.039821 lr=0.000020 grad_norm=0.979287
Epoch 72/100 Iteration 118/234: loss=0.040322 lr=0.000020 grad_norm=0.512243
Epoch 72/100 Iteration 119/234: loss=0.040022 lr=0.000020 grad_norm=0.512821
Epoch 72/100 Iteration 120/234: loss=0.038159 lr=0.000020 grad_norm=0.885095
Epoch 72/100 Iteration 121/234: loss=0.043323 lr=0.000020 grad_norm=0.973251
Epoch 72/100 Iteration 122/234: loss=0.035879 lr=0.000020 grad_norm=0.807795
Epoch 72/100 Iteration 123/234: loss=0.041310 lr=0.000020 grad_norm=0.491859
Epoch 72/100 Iteration 124/234: loss=0.035366 lr=0.000020 grad_norm=0.457841
Epoch 72/100 Iteration 125/234: loss=0.041702 lr=0.000020 grad_norm=0.794644
Epoch 72/100 Iteration 126/234: loss=0.041833 lr=0.000020 grad_norm=0.775617
Epoch 72/100 Iteration 127/234: loss=0.039570 lr=0.000020 grad_norm=0.504658
Epoch 72/100 Iteration 128/234: loss=0.039691 lr=0.000020 grad_norm=0.484613
Epoch 72/100 Iteration 129/234: loss=0.041562 lr=0.000020 grad_norm=0.626100
Epoch 72/100 Iteration 130/234: loss=0.035850 lr=0.000020 grad_norm=0.479836
Epoch 72/100 Iteration 131/234: loss=0.039368 lr=0.000020 grad_norm=0.746323
Epoch 72/100 Iteration 132/234: loss=0.041524 lr=0.000020 grad_norm=1.184209
Epoch 72/100 Iteration 133/234: loss=0.041748 lr=0.000020 grad_norm=0.990993
Epoch 72/100 Iteration 134/234: loss=0.037400 lr=0.000020 grad_norm=0.620062
Epoch 72/100 Iteration 135/234: loss=0.036196 lr=0.000020 grad_norm=1.523136
Epoch 72/100 Iteration 136/234: loss=0.041131 lr=0.000020 grad_norm=1.218221
Epoch 72/100 Iteration 137/234: loss=0.037894 lr=0.000020 grad_norm=0.657799
Epoch 72/100 Iteration 138/234: loss=0.044858 lr=0.000020 grad_norm=0.880849
Epoch 72/100 Iteration 139/234: loss=0.039359 lr=0.000020 grad_norm=0.880692
Epoch 72/100 Iteration 140/234: loss=0.034904 lr=0.000020 grad_norm=1.014307
Epoch 72/100 Iteration 141/234: loss=0.042432 lr=0.000020 grad_norm=0.581366
Epoch 72/100 Iteration 142/234: loss=0.038503 lr=0.000020 grad_norm=0.684827
Epoch 72/100 Iteration 143/234: loss=0.039947 lr=0.000020 grad_norm=0.791551
Epoch 72/100 Iteration 144/234: loss=0.043307 lr=0.000020 grad_norm=0.597208
Epoch 72/100 Iteration 145/234: loss=0.037644 lr=0.000020 grad_norm=0.769684
Epoch 72/100 Iteration 146/234: loss=0.042214 lr=0.000020 grad_norm=1.136075
Epoch 72/100 Iteration 147/234: loss=0.036865 lr=0.000020 grad_norm=0.896974
Epoch 72/100 Iteration 148/234: loss=0.034484 lr=0.000020 grad_norm=0.406718
Epoch 72/100 Iteration 149/234: loss=0.035773 lr=0.000020 grad_norm=0.679887
Epoch 72/100 Iteration 150/234: loss=0.038410 lr=0.000020 grad_norm=0.479075
Epoch 72/100 Iteration 151/234: loss=0.041613 lr=0.000020 grad_norm=0.779390
Epoch 72/100 Iteration 152/234: loss=0.040476 lr=0.000020 grad_norm=1.387586
Epoch 72/100 Iteration 153/234: loss=0.040803 lr=0.000020 grad_norm=1.629248
Epoch 72/100 Iteration 154/234: loss=0.039321 lr=0.000020 grad_norm=1.210638
Epoch 72/100 Iteration 155/234: loss=0.034003 lr=0.000020 grad_norm=0.534534
Epoch 72/100 Iteration 156/234: loss=0.038789 lr=0.000020 grad_norm=0.811685
Epoch 72/100 Iteration 157/234: loss=0.040335 lr=0.000020 grad_norm=0.666980
Epoch 72/100 Iteration 158/234: loss=0.037320 lr=0.000020 grad_norm=0.606035
Epoch 72/100 Iteration 159/234: loss=0.041383 lr=0.000020 grad_norm=1.109403
Epoch 72/100 Iteration 160/234: loss=0.038129 lr=0.000020 grad_norm=1.200773
Epoch 72/100 Iteration 161/234: loss=0.038078 lr=0.000020 grad_norm=0.787017
Epoch 72/100 Iteration 162/234: loss=0.041429 lr=0.000020 grad_norm=0.764155
Epoch 72/100 Iteration 163/234: loss=0.040197 lr=0.000020 grad_norm=0.818855
Epoch 72/100 Iteration 164/234: loss=0.037744 lr=0.000020 grad_norm=0.467176
Epoch 72/100 Iteration 165/234: loss=0.040673 lr=0.000020 grad_norm=1.061032
Epoch 72/100 Iteration 166/234: loss=0.043530 lr=0.000020 grad_norm=0.979417
Epoch 72/100 Iteration 167/234: loss=0.044051 lr=0.000020 grad_norm=0.659810
Epoch 72/100 Iteration 168/234: loss=0.039663 lr=0.000020 grad_norm=0.767355
Epoch 72/100 Iteration 169/234: loss=0.044733 lr=0.000020 grad_norm=0.753968
Epoch 72/100 Iteration 170/234: loss=0.043979 lr=0.000020 grad_norm=0.611330
Epoch 72/100 Iteration 171/234: loss=0.038218 lr=0.000020 grad_norm=0.874253
Epoch 72/100 Iteration 172/234: loss=0.040288 lr=0.000020 grad_norm=0.589424
Epoch 72/100 Iteration 173/234: loss=0.038743 lr=0.000020 grad_norm=0.671764
Epoch 72/100 Iteration 174/234: loss=0.036791 lr=0.000020 grad_norm=0.690234
Epoch 72/100 Iteration 175/234: loss=0.041839 lr=0.000020 grad_norm=0.575865
Epoch 72/100 Iteration 176/234: loss=0.036886 lr=0.000020 grad_norm=0.851231
Epoch 72/100 Iteration 177/234: loss=0.040364 lr=0.000020 grad_norm=0.741499
Epoch 72/100 Iteration 178/234: loss=0.039094 lr=0.000020 grad_norm=0.527561
Epoch 72/100 Iteration 179/234: loss=0.038419 lr=0.000020 grad_norm=1.040884
Epoch 72/100 Iteration 180/234: loss=0.038696 lr=0.000020 grad_norm=0.958513
Epoch 72/100 Iteration 181/234: loss=0.042710 lr=0.000020 grad_norm=0.490688
Epoch 72/100 Iteration 182/234: loss=0.034744 lr=0.000020 grad_norm=0.910736
Epoch 72/100 Iteration 183/234: loss=0.038185 lr=0.000020 grad_norm=0.852480
Epoch 72/100 Iteration 184/234: loss=0.038637 lr=0.000020 grad_norm=0.500993
Epoch 72/100 Iteration 185/234: loss=0.039424 lr=0.000020 grad_norm=0.758152
Epoch 72/100 Iteration 186/234: loss=0.043830 lr=0.000020 grad_norm=0.870966
Epoch 72/100 Iteration 187/234: loss=0.037725 lr=0.000020 grad_norm=1.079509
Epoch 72/100 Iteration 188/234: loss=0.038197 lr=0.000020 grad_norm=0.742192
Epoch 72/100 Iteration 189/234: loss=0.043845 lr=0.000020 grad_norm=0.598956
Epoch 72/100 Iteration 190/234: loss=0.042538 lr=0.000020 grad_norm=0.881741
Epoch 72/100 Iteration 191/234: loss=0.039688 lr=0.000020 grad_norm=1.069103
Epoch 72/100 Iteration 192/234: loss=0.045402 lr=0.000020 grad_norm=1.114970
Epoch 72/100 Iteration 193/234: loss=0.037543 lr=0.000020 grad_norm=0.789514
Epoch 72/100 Iteration 194/234: loss=0.034569 lr=0.000020 grad_norm=0.608891
Epoch 72/100 Iteration 195/234: loss=0.035441 lr=0.000020 grad_norm=0.593248
Epoch 72/100 Iteration 196/234: loss=0.036285 lr=0.000020 grad_norm=0.556850
Epoch 72/100 Iteration 197/234: loss=0.042197 lr=0.000020 grad_norm=0.550251
Epoch 72/100 Iteration 198/234: loss=0.042797 lr=0.000020 grad_norm=0.573315
Epoch 72/100 Iteration 199/234: loss=0.038936 lr=0.000020 grad_norm=0.761911
Epoch 72/100 Iteration 200/234: loss=0.037039 lr=0.000020 grad_norm=0.433071
Epoch 72/100 Iteration 201/234: loss=0.043950 lr=0.000020 grad_norm=0.868713
Epoch 72/100 Iteration 202/234: loss=0.037313 lr=0.000020 grad_norm=0.991622
Epoch 72/100 Iteration 203/234: loss=0.043479 lr=0.000020 grad_norm=0.761675
Epoch 72/100 Iteration 204/234: loss=0.037456 lr=0.000020 grad_norm=1.552014
Epoch 72/100 Iteration 205/234: loss=0.043038 lr=0.000020 grad_norm=0.727698
Epoch 72/100 Iteration 206/234: loss=0.041778 lr=0.000020 grad_norm=1.800741
Epoch 72/100 Iteration 207/234: loss=0.043027 lr=0.000020 grad_norm=2.094941
Epoch 72/100 Iteration 208/234: loss=0.043258 lr=0.000020 grad_norm=1.164365
Epoch 72/100 Iteration 209/234: loss=0.041292 lr=0.000020 grad_norm=1.114783
Epoch 72/100 Iteration 210/234: loss=0.037641 lr=0.000020 grad_norm=1.040768
Epoch 72/100 Iteration 211/234: loss=0.036524 lr=0.000020 grad_norm=0.618716
Epoch 72/100 Iteration 212/234: loss=0.040383 lr=0.000020 grad_norm=0.983790
Epoch 72/100 Iteration 213/234: loss=0.039489 lr=0.000020 grad_norm=0.473004
Epoch 72/100 Iteration 214/234: loss=0.040968 lr=0.000020 grad_norm=0.921958
Epoch 72/100 Iteration 215/234: loss=0.037791 lr=0.000020 grad_norm=0.729028
Epoch 72/100 Iteration 216/234: loss=0.036821 lr=0.000020 grad_norm=0.766657
Epoch 72/100 Iteration 217/234: loss=0.039249 lr=0.000020 grad_norm=1.032416
Epoch 72/100 Iteration 218/234: loss=0.037819 lr=0.000020 grad_norm=0.734913
Epoch 72/100 Iteration 219/234: loss=0.040643 lr=0.000020 grad_norm=0.656581
Epoch 72/100 Iteration 220/234: loss=0.035552 lr=0.000020 grad_norm=0.530520
Epoch 72/100 Iteration 221/234: loss=0.043746 lr=0.000020 grad_norm=0.723571
Epoch 72/100 Iteration 222/234: loss=0.034238 lr=0.000020 grad_norm=0.667079
Epoch 72/100 Iteration 223/234: loss=0.044345 lr=0.000020 grad_norm=0.519438
Epoch 72/100 Iteration 224/234: loss=0.044444 lr=0.000020 grad_norm=0.975186
Epoch 72/100 Iteration 225/234: loss=0.038855 lr=0.000020 grad_norm=0.881921
Epoch 72/100 Iteration 226/234: loss=0.042399 lr=0.000020 grad_norm=0.697476
Epoch 72/100 Iteration 227/234: loss=0.038488 lr=0.000020 grad_norm=0.640346
Epoch 72/100 Iteration 228/234: loss=0.039298 lr=0.000020 grad_norm=0.718467
Epoch 72/100 Iteration 229/234: loss=0.044881 lr=0.000020 grad_norm=1.014295
Epoch 72/100 Iteration 230/234: loss=0.041964 lr=0.000020 grad_norm=1.017605
Epoch 72/100 Iteration 231/234: loss=0.039412 lr=0.000020 grad_norm=0.463268
Epoch 72/100 Iteration 232/234: loss=0.041556 lr=0.000020 grad_norm=0.706850
Epoch 72/100 Iteration 233/234: loss=0.038307 lr=0.000020 grad_norm=0.429541
Epoch 72/100 Iteration 234/234: loss=0.040638 lr=0.000020 grad_norm=0.566085
Epoch 72/100 finished. Avg Loss: 0.039773
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 73/100 Iteration 1/234: loss=0.035373 lr=0.000020 grad_norm=0.666329
Epoch 73/100 Iteration 2/234: loss=0.037691 lr=0.000020 grad_norm=0.477575
Epoch 73/100 Iteration 3/234: loss=0.038992 lr=0.000020 grad_norm=0.682387
Epoch 73/100 Iteration 4/234: loss=0.039253 lr=0.000020 grad_norm=0.752898
Epoch 73/100 Iteration 5/234: loss=0.039065 lr=0.000020 grad_norm=0.643181
Epoch 73/100 Iteration 6/234: loss=0.040050 lr=0.000020 grad_norm=0.376947
Epoch 73/100 Iteration 7/234: loss=0.041100 lr=0.000020 grad_norm=0.583240
Epoch 73/100 Iteration 8/234: loss=0.036703 lr=0.000020 grad_norm=0.654487
Epoch 73/100 Iteration 9/234: loss=0.043802 lr=0.000020 grad_norm=0.972966
Epoch 73/100 Iteration 10/234: loss=0.036584 lr=0.000020 grad_norm=1.137375
Epoch 73/100 Iteration 11/234: loss=0.040155 lr=0.000020 grad_norm=0.992343
Epoch 73/100 Iteration 12/234: loss=0.039376 lr=0.000020 grad_norm=0.430073
Epoch 73/100 Iteration 13/234: loss=0.039490 lr=0.000020 grad_norm=0.996200
Epoch 73/100 Iteration 14/234: loss=0.041576 lr=0.000020 grad_norm=1.010148
Epoch 73/100 Iteration 15/234: loss=0.039545 lr=0.000020 grad_norm=0.508616
Epoch 73/100 Iteration 16/234: loss=0.040826 lr=0.000020 grad_norm=0.910513
Epoch 73/100 Iteration 17/234: loss=0.040519 lr=0.000020 grad_norm=1.304100
Epoch 73/100 Iteration 18/234: loss=0.042457 lr=0.000020 grad_norm=1.160043
Epoch 73/100 Iteration 19/234: loss=0.038257 lr=0.000020 grad_norm=0.545177
Epoch 73/100 Iteration 20/234: loss=0.038107 lr=0.000020 grad_norm=0.615826
Epoch 73/100 Iteration 21/234: loss=0.036661 lr=0.000020 grad_norm=0.650400
Epoch 73/100 Iteration 22/234: loss=0.039911 lr=0.000020 grad_norm=0.638961
Epoch 73/100 Iteration 23/234: loss=0.043270 lr=0.000020 grad_norm=0.514082
Epoch 73/100 Iteration 24/234: loss=0.041357 lr=0.000020 grad_norm=0.812858
Epoch 73/100 Iteration 25/234: loss=0.039220 lr=0.000020 grad_norm=0.845048
Epoch 73/100 Iteration 26/234: loss=0.038168 lr=0.000020 grad_norm=0.807300
Epoch 73/100 Iteration 27/234: loss=0.035533 lr=0.000020 grad_norm=0.857025
Epoch 73/100 Iteration 28/234: loss=0.039462 lr=0.000020 grad_norm=0.879400
Epoch 73/100 Iteration 29/234: loss=0.040467 lr=0.000020 grad_norm=0.718993
Epoch 73/100 Iteration 30/234: loss=0.044539 lr=0.000020 grad_norm=0.892494
Epoch 73/100 Iteration 31/234: loss=0.037076 lr=0.000020 grad_norm=2.107076
Epoch 73/100 Iteration 32/234: loss=0.038434 lr=0.000020 grad_norm=2.296305
Epoch 73/100 Iteration 33/234: loss=0.040899 lr=0.000020 grad_norm=1.238902
Epoch 73/100 Iteration 34/234: loss=0.040318 lr=0.000020 grad_norm=0.920074
Epoch 73/100 Iteration 35/234: loss=0.036120 lr=0.000020 grad_norm=1.382825
Epoch 73/100 Iteration 36/234: loss=0.037357 lr=0.000020 grad_norm=0.912959
Epoch 73/100 Iteration 37/234: loss=0.036776 lr=0.000020 grad_norm=0.834971
Epoch 73/100 Iteration 38/234: loss=0.040265 lr=0.000020 grad_norm=0.698340
Epoch 73/100 Iteration 39/234: loss=0.041643 lr=0.000020 grad_norm=0.885527
Epoch 73/100 Iteration 40/234: loss=0.042227 lr=0.000020 grad_norm=0.806322
Epoch 73/100 Iteration 41/234: loss=0.037010 lr=0.000020 grad_norm=0.505785
Epoch 73/100 Iteration 42/234: loss=0.043198 lr=0.000020 grad_norm=0.920846
Epoch 73/100 Iteration 43/234: loss=0.036825 lr=0.000020 grad_norm=0.903932
Epoch 73/100 Iteration 44/234: loss=0.034774 lr=0.000020 grad_norm=0.411978
Epoch 73/100 Iteration 45/234: loss=0.040566 lr=0.000020 grad_norm=1.124553
Epoch 73/100 Iteration 46/234: loss=0.039045 lr=0.000020 grad_norm=1.251311
Epoch 73/100 Iteration 47/234: loss=0.040664 lr=0.000020 grad_norm=0.606152
Epoch 73/100 Iteration 48/234: loss=0.043500 lr=0.000020 grad_norm=1.324815
Epoch 73/100 Iteration 49/234: loss=0.045742 lr=0.000020 grad_norm=1.412458
Epoch 73/100 Iteration 50/234: loss=0.043376 lr=0.000020 grad_norm=0.721942
Epoch 73/100 Iteration 51/234: loss=0.037885 lr=0.000020 grad_norm=1.096188
Epoch 73/100 Iteration 52/234: loss=0.042828 lr=0.000020 grad_norm=1.315911
Epoch 73/100 Iteration 53/234: loss=0.042187 lr=0.000020 grad_norm=1.867872
Epoch 73/100 Iteration 54/234: loss=0.041311 lr=0.000020 grad_norm=1.529478
Epoch 73/100 Iteration 55/234: loss=0.037791 lr=0.000020 grad_norm=0.669435
Epoch 73/100 Iteration 56/234: loss=0.043697 lr=0.000020 grad_norm=1.447175
Epoch 73/100 Iteration 57/234: loss=0.034937 lr=0.000020 grad_norm=1.217051
Epoch 73/100 Iteration 58/234: loss=0.039983 lr=0.000020 grad_norm=0.721455
Epoch 73/100 Iteration 59/234: loss=0.037979 lr=0.000020 grad_norm=1.449439
Epoch 73/100 Iteration 60/234: loss=0.040701 lr=0.000020 grad_norm=0.739022
Epoch 73/100 Iteration 61/234: loss=0.040150 lr=0.000020 grad_norm=1.200501
Epoch 73/100 Iteration 62/234: loss=0.038417 lr=0.000020 grad_norm=1.415835
Epoch 73/100 Iteration 63/234: loss=0.037572 lr=0.000020 grad_norm=0.473148
Epoch 73/100 Iteration 64/234: loss=0.041903 lr=0.000020 grad_norm=1.444991
Epoch 73/100 Iteration 65/234: loss=0.039545 lr=0.000020 grad_norm=0.945364
Epoch 73/100 Iteration 66/234: loss=0.036774 lr=0.000020 grad_norm=1.021254
Epoch 73/100 Iteration 67/234: loss=0.040861 lr=0.000020 grad_norm=1.079336
Epoch 73/100 Iteration 68/234: loss=0.044358 lr=0.000020 grad_norm=0.683367
Epoch 73/100 Iteration 69/234: loss=0.045104 lr=0.000020 grad_norm=1.471034
Epoch 73/100 Iteration 70/234: loss=0.036987 lr=0.000020 grad_norm=0.655927
Epoch 73/100 Iteration 71/234: loss=0.042047 lr=0.000020 grad_norm=1.449198
Epoch 73/100 Iteration 72/234: loss=0.038482 lr=0.000020 grad_norm=1.005521
Epoch 73/100 Iteration 73/234: loss=0.040123 lr=0.000020 grad_norm=1.311268
Epoch 73/100 Iteration 74/234: loss=0.037700 lr=0.000020 grad_norm=1.135007
Epoch 73/100 Iteration 75/234: loss=0.039572 lr=0.000020 grad_norm=0.750411
Epoch 73/100 Iteration 76/234: loss=0.039061 lr=0.000020 grad_norm=1.018113
Epoch 73/100 Iteration 77/234: loss=0.038539 lr=0.000020 grad_norm=0.823020
Epoch 73/100 Iteration 78/234: loss=0.039328 lr=0.000020 grad_norm=1.386799
Epoch 73/100 Iteration 79/234: loss=0.042608 lr=0.000020 grad_norm=0.499353
Epoch 73/100 Iteration 80/234: loss=0.033335 lr=0.000020 grad_norm=1.126260
Epoch 73/100 Iteration 81/234: loss=0.035872 lr=0.000020 grad_norm=0.494438
Epoch 73/100 Iteration 82/234: loss=0.036132 lr=0.000020 grad_norm=1.184023
Epoch 73/100 Iteration 83/234: loss=0.037930 lr=0.000020 grad_norm=0.656548
Epoch 73/100 Iteration 84/234: loss=0.037941 lr=0.000020 grad_norm=0.934498
Epoch 73/100 Iteration 85/234: loss=0.033835 lr=0.000020 grad_norm=0.593701
Epoch 73/100 Iteration 86/234: loss=0.044836 lr=0.000020 grad_norm=1.275525
Epoch 73/100 Iteration 87/234: loss=0.039332 lr=0.000020 grad_norm=1.635902
Epoch 73/100 Iteration 88/234: loss=0.037401 lr=0.000020 grad_norm=0.548063
Epoch 73/100 Iteration 89/234: loss=0.040612 lr=0.000020 grad_norm=1.414070
Epoch 73/100 Iteration 90/234: loss=0.037220 lr=0.000020 grad_norm=0.834250
Epoch 73/100 Iteration 91/234: loss=0.041100 lr=0.000020 grad_norm=0.956797
Epoch 73/100 Iteration 92/234: loss=0.034074 lr=0.000020 grad_norm=0.565082
Epoch 73/100 Iteration 93/234: loss=0.042782 lr=0.000020 grad_norm=1.116790
Epoch 73/100 Iteration 94/234: loss=0.036716 lr=0.000020 grad_norm=1.233384
Epoch 73/100 Iteration 95/234: loss=0.039391 lr=0.000020 grad_norm=0.425111
Epoch 73/100 Iteration 96/234: loss=0.034722 lr=0.000020 grad_norm=0.909826
Epoch 73/100 Iteration 97/234: loss=0.039039 lr=0.000020 grad_norm=0.735590
Epoch 73/100 Iteration 98/234: loss=0.041034 lr=0.000020 grad_norm=0.779615
Epoch 73/100 Iteration 99/234: loss=0.038095 lr=0.000020 grad_norm=0.613253
Epoch 73/100 Iteration 100/234: loss=0.041599 lr=0.000020 grad_norm=0.693840
Epoch 73/100 Iteration 101/234: loss=0.042904 lr=0.000020 grad_norm=1.334559
Epoch 73/100 Iteration 102/234: loss=0.041488 lr=0.000020 grad_norm=0.823574
Epoch 73/100 Iteration 103/234: loss=0.042443 lr=0.000020 grad_norm=1.109593
Epoch 73/100 Iteration 104/234: loss=0.036180 lr=0.000020 grad_norm=0.726437
Epoch 73/100 Iteration 105/234: loss=0.036349 lr=0.000020 grad_norm=0.637072
Epoch 73/100 Iteration 106/234: loss=0.038012 lr=0.000020 grad_norm=0.857012
Epoch 73/100 Iteration 107/234: loss=0.038892 lr=0.000020 grad_norm=0.618020
Epoch 73/100 Iteration 108/234: loss=0.036527 lr=0.000020 grad_norm=1.088477
Epoch 73/100 Iteration 109/234: loss=0.042755 lr=0.000020 grad_norm=0.459677
Epoch 73/100 Iteration 110/234: loss=0.041292 lr=0.000020 grad_norm=0.891245
Epoch 73/100 Iteration 111/234: loss=0.043025 lr=0.000020 grad_norm=0.537783
Epoch 73/100 Iteration 112/234: loss=0.041328 lr=0.000020 grad_norm=0.546601
Epoch 73/100 Iteration 113/234: loss=0.038789 lr=0.000020 grad_norm=0.624428
Epoch 73/100 Iteration 114/234: loss=0.039319 lr=0.000020 grad_norm=0.669655
Epoch 73/100 Iteration 115/234: loss=0.040658 lr=0.000020 grad_norm=0.602639
Epoch 73/100 Iteration 116/234: loss=0.038437 lr=0.000020 grad_norm=0.353750
Epoch 73/100 Iteration 117/234: loss=0.041237 lr=0.000020 grad_norm=0.670416
Epoch 73/100 Iteration 118/234: loss=0.038105 lr=0.000020 grad_norm=0.739966
Epoch 73/100 Iteration 119/234: loss=0.038126 lr=0.000020 grad_norm=0.436853
Epoch 73/100 Iteration 120/234: loss=0.036111 lr=0.000020 grad_norm=0.554527
Epoch 73/100 Iteration 121/234: loss=0.037746 lr=0.000020 grad_norm=0.936617
Epoch 73/100 Iteration 122/234: loss=0.037697 lr=0.000020 grad_norm=0.737048
Epoch 73/100 Iteration 123/234: loss=0.041687 lr=0.000020 grad_norm=0.396319
Epoch 73/100 Iteration 124/234: loss=0.040001 lr=0.000020 grad_norm=1.100815
Epoch 73/100 Iteration 125/234: loss=0.041117 lr=0.000020 grad_norm=1.169816
Epoch 73/100 Iteration 126/234: loss=0.042322 lr=0.000020 grad_norm=0.875129
Epoch 73/100 Iteration 127/234: loss=0.041126 lr=0.000020 grad_norm=0.496378
Epoch 73/100 Iteration 128/234: loss=0.040856 lr=0.000020 grad_norm=0.571088
Epoch 73/100 Iteration 129/234: loss=0.040137 lr=0.000020 grad_norm=1.002599
Epoch 73/100 Iteration 130/234: loss=0.036352 lr=0.000020 grad_norm=0.975183
Epoch 73/100 Iteration 131/234: loss=0.038196 lr=0.000020 grad_norm=0.689539
Epoch 73/100 Iteration 132/234: loss=0.036104 lr=0.000020 grad_norm=0.534428
Epoch 73/100 Iteration 133/234: loss=0.040959 lr=0.000020 grad_norm=0.584923
Epoch 73/100 Iteration 134/234: loss=0.039560 lr=0.000020 grad_norm=0.456132
Epoch 73/100 Iteration 135/234: loss=0.033243 lr=0.000020 grad_norm=0.401783
Epoch 73/100 Iteration 136/234: loss=0.038915 lr=0.000020 grad_norm=0.470211
Epoch 73/100 Iteration 137/234: loss=0.036588 lr=0.000020 grad_norm=0.648586
Epoch 73/100 Iteration 138/234: loss=0.039687 lr=0.000020 grad_norm=0.654995
Epoch 73/100 Iteration 139/234: loss=0.037790 lr=0.000020 grad_norm=0.461297
Epoch 73/100 Iteration 140/234: loss=0.042163 lr=0.000020 grad_norm=0.583556
Epoch 73/100 Iteration 141/234: loss=0.041985 lr=0.000020 grad_norm=0.609717
Epoch 73/100 Iteration 142/234: loss=0.034874 lr=0.000020 grad_norm=0.388704
Epoch 73/100 Iteration 143/234: loss=0.041523 lr=0.000020 grad_norm=0.604225
Epoch 73/100 Iteration 144/234: loss=0.038269 lr=0.000020 grad_norm=0.629367
Epoch 73/100 Iteration 145/234: loss=0.040499 lr=0.000020 grad_norm=0.619565
Epoch 73/100 Iteration 146/234: loss=0.042420 lr=0.000020 grad_norm=0.469684
Epoch 73/100 Iteration 147/234: loss=0.040391 lr=0.000020 grad_norm=0.764160
Epoch 73/100 Iteration 148/234: loss=0.040855 lr=0.000020 grad_norm=0.569724
Epoch 73/100 Iteration 149/234: loss=0.040725 lr=0.000020 grad_norm=0.723808
Epoch 73/100 Iteration 150/234: loss=0.037931 lr=0.000020 grad_norm=0.827282
Epoch 73/100 Iteration 151/234: loss=0.037007 lr=0.000020 grad_norm=0.920559
Epoch 73/100 Iteration 152/234: loss=0.039040 lr=0.000020 grad_norm=0.795216
Epoch 73/100 Iteration 153/234: loss=0.040960 lr=0.000020 grad_norm=0.488606
Epoch 73/100 Iteration 154/234: loss=0.038400 lr=0.000020 grad_norm=0.447007
Epoch 73/100 Iteration 155/234: loss=0.040559 lr=0.000020 grad_norm=0.966213
Epoch 73/100 Iteration 156/234: loss=0.035247 lr=0.000020 grad_norm=1.274196
Epoch 73/100 Iteration 157/234: loss=0.039877 lr=0.000020 grad_norm=0.785255
Epoch 73/100 Iteration 158/234: loss=0.034833 lr=0.000020 grad_norm=0.475736
Epoch 73/100 Iteration 159/234: loss=0.039733 lr=0.000020 grad_norm=0.996130
Epoch 73/100 Iteration 160/234: loss=0.033183 lr=0.000020 grad_norm=0.795362
Epoch 73/100 Iteration 161/234: loss=0.043920 lr=0.000020 grad_norm=0.441885
Epoch 73/100 Iteration 162/234: loss=0.039375 lr=0.000020 grad_norm=0.805288
Epoch 73/100 Iteration 163/234: loss=0.039382 lr=0.000020 grad_norm=0.622974
Epoch 73/100 Iteration 164/234: loss=0.038945 lr=0.000020 grad_norm=0.366815
Epoch 73/100 Iteration 165/234: loss=0.044190 lr=0.000020 grad_norm=0.757773
Epoch 73/100 Iteration 166/234: loss=0.040014 lr=0.000020 grad_norm=0.964276
Epoch 73/100 Iteration 167/234: loss=0.039875 lr=0.000020 grad_norm=0.735418
Epoch 73/100 Iteration 168/234: loss=0.038468 lr=0.000020 grad_norm=0.413293
Epoch 73/100 Iteration 169/234: loss=0.037386 lr=0.000020 grad_norm=0.666377
Epoch 73/100 Iteration 170/234: loss=0.041107 lr=0.000020 grad_norm=1.170306
Epoch 73/100 Iteration 171/234: loss=0.044269 lr=0.000020 grad_norm=1.451812
Epoch 73/100 Iteration 172/234: loss=0.045140 lr=0.000020 grad_norm=1.699067
Epoch 73/100 Iteration 173/234: loss=0.045057 lr=0.000020 grad_norm=1.539630
Epoch 73/100 Iteration 174/234: loss=0.042924 lr=0.000020 grad_norm=1.266979
Epoch 73/100 Iteration 175/234: loss=0.043131 lr=0.000020 grad_norm=0.866999
Epoch 73/100 Iteration 176/234: loss=0.041103 lr=0.000020 grad_norm=0.563217
Epoch 73/100 Iteration 177/234: loss=0.039472 lr=0.000020 grad_norm=0.638099
Epoch 73/100 Iteration 178/234: loss=0.041505 lr=0.000020 grad_norm=1.045785
Epoch 73/100 Iteration 179/234: loss=0.040314 lr=0.000020 grad_norm=1.291506
Epoch 73/100 Iteration 180/234: loss=0.039979 lr=0.000020 grad_norm=0.749712
Epoch 73/100 Iteration 181/234: loss=0.032066 lr=0.000020 grad_norm=0.698157
Epoch 73/100 Iteration 182/234: loss=0.035636 lr=0.000020 grad_norm=0.872241
Epoch 73/100 Iteration 183/234: loss=0.035953 lr=0.000020 grad_norm=0.336610
Epoch 73/100 Iteration 184/234: loss=0.040199 lr=0.000020 grad_norm=0.939031
Epoch 73/100 Iteration 185/234: loss=0.040208 lr=0.000020 grad_norm=1.241389
Epoch 73/100 Iteration 186/234: loss=0.040758 lr=0.000020 grad_norm=0.661606
Epoch 73/100 Iteration 187/234: loss=0.040581 lr=0.000020 grad_norm=0.615260
Epoch 73/100 Iteration 188/234: loss=0.038644 lr=0.000020 grad_norm=0.852846
Epoch 73/100 Iteration 189/234: loss=0.039288 lr=0.000020 grad_norm=0.596318
Epoch 73/100 Iteration 190/234: loss=0.040555 lr=0.000020 grad_norm=1.016138
Epoch 73/100 Iteration 191/234: loss=0.039221 lr=0.000020 grad_norm=0.754313
Epoch 73/100 Iteration 192/234: loss=0.034986 lr=0.000020 grad_norm=0.688875
Epoch 73/100 Iteration 193/234: loss=0.040513 lr=0.000020 grad_norm=1.179841
Epoch 73/100 Iteration 194/234: loss=0.046308 lr=0.000020 grad_norm=1.722759
Epoch 73/100 Iteration 195/234: loss=0.041846 lr=0.000020 grad_norm=1.787977
Epoch 73/100 Iteration 196/234: loss=0.040487 lr=0.000020 grad_norm=0.794150
Epoch 73/100 Iteration 197/234: loss=0.037613 lr=0.000020 grad_norm=0.856528
Epoch 73/100 Iteration 198/234: loss=0.044292 lr=0.000020 grad_norm=1.580280
Epoch 73/100 Iteration 199/234: loss=0.042202 lr=0.000020 grad_norm=1.543707
Epoch 73/100 Iteration 200/234: loss=0.041846 lr=0.000020 grad_norm=0.725115
Epoch 73/100 Iteration 201/234: loss=0.036799 lr=0.000020 grad_norm=0.900768
Epoch 73/100 Iteration 202/234: loss=0.037406 lr=0.000020 grad_norm=1.061888
Epoch 73/100 Iteration 203/234: loss=0.042346 lr=0.000020 grad_norm=0.923461
Epoch 73/100 Iteration 204/234: loss=0.039524 lr=0.000020 grad_norm=0.715024
Epoch 73/100 Iteration 205/234: loss=0.045333 lr=0.000020 grad_norm=0.636192
Epoch 73/100 Iteration 206/234: loss=0.040292 lr=0.000020 grad_norm=0.503582
Epoch 73/100 Iteration 207/234: loss=0.037727 lr=0.000020 grad_norm=0.536692
Epoch 73/100 Iteration 208/234: loss=0.037664 lr=0.000020 grad_norm=0.855018
Epoch 73/100 Iteration 209/234: loss=0.045670 lr=0.000020 grad_norm=1.129955
Epoch 73/100 Iteration 210/234: loss=0.039455 lr=0.000020 grad_norm=0.984238
Epoch 73/100 Iteration 211/234: loss=0.038136 lr=0.000020 grad_norm=0.520278
Epoch 73/100 Iteration 212/234: loss=0.042570 lr=0.000020 grad_norm=1.404500
Epoch 73/100 Iteration 213/234: loss=0.036254 lr=0.000020 grad_norm=1.185387
Epoch 73/100 Iteration 214/234: loss=0.043561 lr=0.000020 grad_norm=1.248673
Epoch 73/100 Iteration 215/234: loss=0.044179 lr=0.000020 grad_norm=2.654528
Epoch 73/100 Iteration 216/234: loss=0.041252 lr=0.000020 grad_norm=1.878684
Epoch 73/100 Iteration 217/234: loss=0.043701 lr=0.000020 grad_norm=1.076902
Epoch 73/100 Iteration 218/234: loss=0.040130 lr=0.000020 grad_norm=2.331627
Epoch 73/100 Iteration 219/234: loss=0.045896 lr=0.000020 grad_norm=1.107664
Epoch 73/100 Iteration 220/234: loss=0.036732 lr=0.000020 grad_norm=1.615987
Epoch 73/100 Iteration 221/234: loss=0.039262 lr=0.000020 grad_norm=1.421864
Epoch 73/100 Iteration 222/234: loss=0.035753 lr=0.000020 grad_norm=1.255594
Epoch 73/100 Iteration 223/234: loss=0.038169 lr=0.000020 grad_norm=0.522200
Epoch 73/100 Iteration 224/234: loss=0.035622 lr=0.000020 grad_norm=1.154275
Epoch 73/100 Iteration 225/234: loss=0.039315 lr=0.000020 grad_norm=0.726478
Epoch 73/100 Iteration 226/234: loss=0.034507 lr=0.000020 grad_norm=0.711087
Epoch 73/100 Iteration 227/234: loss=0.039598 lr=0.000020 grad_norm=1.135299
Epoch 73/100 Iteration 228/234: loss=0.033898 lr=0.000020 grad_norm=0.471913
Epoch 73/100 Iteration 229/234: loss=0.037218 lr=0.000020 grad_norm=0.954932
Epoch 73/100 Iteration 230/234: loss=0.040711 lr=0.000020 grad_norm=0.901186
Epoch 73/100 Iteration 231/234: loss=0.037794 lr=0.000020 grad_norm=0.369401
Epoch 73/100 Iteration 232/234: loss=0.037145 lr=0.000020 grad_norm=0.901489
Epoch 73/100 Iteration 233/234: loss=0.037881 lr=0.000020 grad_norm=0.667137
Epoch 73/100 Iteration 234/234: loss=0.035027 lr=0.000020 grad_norm=0.639402
Epoch 73/100 finished. Avg Loss: 0.039541
Epoch 74/100 Iteration 1/234: loss=0.037605 lr=0.000020 grad_norm=0.568458
Epoch 74/100 Iteration 2/234: loss=0.038725 lr=0.000020 grad_norm=0.785475
Epoch 74/100 Iteration 3/234: loss=0.038696 lr=0.000020 grad_norm=0.940090
Epoch 74/100 Iteration 4/234: loss=0.040126 lr=0.000020 grad_norm=0.655288
Epoch 74/100 Iteration 5/234: loss=0.036018 lr=0.000020 grad_norm=0.739183
Epoch 74/100 Iteration 6/234: loss=0.039691 lr=0.000020 grad_norm=0.625734
Epoch 74/100 Iteration 7/234: loss=0.035606 lr=0.000020 grad_norm=0.477423
Epoch 74/100 Iteration 8/234: loss=0.043326 lr=0.000020 grad_norm=0.528745
Epoch 74/100 Iteration 9/234: loss=0.038509 lr=0.000020 grad_norm=0.812383
Epoch 74/100 Iteration 10/234: loss=0.038739 lr=0.000020 grad_norm=0.696115
Epoch 74/100 Iteration 11/234: loss=0.040177 lr=0.000020 grad_norm=0.772996
Epoch 74/100 Iteration 12/234: loss=0.036770 lr=0.000020 grad_norm=0.898490
Epoch 74/100 Iteration 13/234: loss=0.042605 lr=0.000020 grad_norm=0.611022
Epoch 74/100 Iteration 14/234: loss=0.038122 lr=0.000020 grad_norm=0.974608
Epoch 74/100 Iteration 15/234: loss=0.038621 lr=0.000020 grad_norm=1.157816
Epoch 74/100 Iteration 16/234: loss=0.040751 lr=0.000020 grad_norm=0.545579
Epoch 74/100 Iteration 17/234: loss=0.040790 lr=0.000020 grad_norm=1.387780
Epoch 74/100 Iteration 18/234: loss=0.039594 lr=0.000020 grad_norm=1.386345
Epoch 74/100 Iteration 19/234: loss=0.037405 lr=0.000020 grad_norm=0.705634
Epoch 74/100 Iteration 20/234: loss=0.041203 lr=0.000020 grad_norm=1.041828
Epoch 74/100 Iteration 21/234: loss=0.044424 lr=0.000020 grad_norm=1.478444
Epoch 74/100 Iteration 22/234: loss=0.040930 lr=0.000020 grad_norm=1.660919
Epoch 74/100 Iteration 23/234: loss=0.036658 lr=0.000020 grad_norm=0.806868
Epoch 74/100 Iteration 24/234: loss=0.046723 lr=0.000020 grad_norm=1.015615
Epoch 74/100 Iteration 25/234: loss=0.036220 lr=0.000020 grad_norm=2.051183
Epoch 74/100 Iteration 26/234: loss=0.038556 lr=0.000020 grad_norm=1.676158
Epoch 74/100 Iteration 27/234: loss=0.037760 lr=0.000020 grad_norm=0.524280
Epoch 74/100 Iteration 28/234: loss=0.040897 lr=0.000020 grad_norm=1.690394
Epoch 74/100 Iteration 29/234: loss=0.034923 lr=0.000020 grad_norm=0.917861
Epoch 74/100 Iteration 30/234: loss=0.042740 lr=0.000020 grad_norm=1.009702
Epoch 74/100 Iteration 31/234: loss=0.041369 lr=0.000020 grad_norm=1.797469
Epoch 74/100 Iteration 32/234: loss=0.043965 lr=0.000020 grad_norm=1.085167
Epoch 74/100 Iteration 33/234: loss=0.036855 lr=0.000020 grad_norm=0.760608
Epoch 74/100 Iteration 34/234: loss=0.041268 lr=0.000020 grad_norm=1.403776
Epoch 74/100 Iteration 35/234: loss=0.040726 lr=0.000020 grad_norm=0.933120
Epoch 74/100 Iteration 36/234: loss=0.039931 lr=0.000020 grad_norm=0.973433
Epoch 74/100 Iteration 37/234: loss=0.041291 lr=0.000020 grad_norm=1.637136
Epoch 74/100 Iteration 38/234: loss=0.037464 lr=0.000020 grad_norm=0.796583
Epoch 74/100 Iteration 39/234: loss=0.040126 lr=0.000020 grad_norm=1.166370
Epoch 74/100 Iteration 40/234: loss=0.037749 lr=0.000020 grad_norm=1.320396
Epoch 74/100 Iteration 41/234: loss=0.034775 lr=0.000020 grad_norm=0.714734
Epoch 74/100 Iteration 42/234: loss=0.038877 lr=0.000020 grad_norm=0.910571
Epoch 74/100 Iteration 43/234: loss=0.038908 lr=0.000020 grad_norm=0.736904
Epoch 74/100 Iteration 44/234: loss=0.043539 lr=0.000020 grad_norm=0.567090
Epoch 74/100 Iteration 45/234: loss=0.040142 lr=0.000020 grad_norm=0.905444
Epoch 74/100 Iteration 46/234: loss=0.033849 lr=0.000020 grad_norm=0.549661
Epoch 74/100 Iteration 47/234: loss=0.041473 lr=0.000020 grad_norm=0.574133
Epoch 74/100 Iteration 48/234: loss=0.037791 lr=0.000020 grad_norm=0.736466
Epoch 74/100 Iteration 49/234: loss=0.041095 lr=0.000020 grad_norm=0.554007
Epoch 74/100 Iteration 50/234: loss=0.038973 lr=0.000020 grad_norm=0.755680
Epoch 74/100 Iteration 51/234: loss=0.036539 lr=0.000020 grad_norm=0.505761
Epoch 74/100 Iteration 52/234: loss=0.041908 lr=0.000020 grad_norm=0.772102
Epoch 74/100 Iteration 53/234: loss=0.043297 lr=0.000020 grad_norm=1.198664
Epoch 74/100 Iteration 54/234: loss=0.035305 lr=0.000020 grad_norm=0.640144
Epoch 74/100 Iteration 55/234: loss=0.040568 lr=0.000020 grad_norm=0.813485
Epoch 74/100 Iteration 56/234: loss=0.038448 lr=0.000020 grad_norm=0.874654
Epoch 74/100 Iteration 57/234: loss=0.037550 lr=0.000020 grad_norm=0.465228
Epoch 74/100 Iteration 58/234: loss=0.036909 lr=0.000020 grad_norm=1.031641
Epoch 74/100 Iteration 59/234: loss=0.039628 lr=0.000020 grad_norm=0.546745
Epoch 74/100 Iteration 60/234: loss=0.039013 lr=0.000020 grad_norm=0.855832
Epoch 74/100 Iteration 61/234: loss=0.039657 lr=0.000020 grad_norm=0.933865
Epoch 74/100 Iteration 62/234: loss=0.037991 lr=0.000020 grad_norm=0.414220
Epoch 74/100 Iteration 63/234: loss=0.045939 lr=0.000020 grad_norm=0.698817
Epoch 74/100 Iteration 64/234: loss=0.041288 lr=0.000020 grad_norm=0.584909
Epoch 74/100 Iteration 65/234: loss=0.040619 lr=0.000020 grad_norm=0.947555
Epoch 74/100 Iteration 66/234: loss=0.037339 lr=0.000020 grad_norm=0.619996
Epoch 74/100 Iteration 67/234: loss=0.037892 lr=0.000020 grad_norm=0.857327
Epoch 74/100 Iteration 68/234: loss=0.037262 lr=0.000020 grad_norm=1.298001
Epoch 74/100 Iteration 69/234: loss=0.038979 lr=0.000020 grad_norm=1.155098
Epoch 74/100 Iteration 70/234: loss=0.037531 lr=0.000020 grad_norm=0.492273
Epoch 74/100 Iteration 71/234: loss=0.040767 lr=0.000020 grad_norm=1.452794
Epoch 74/100 Iteration 72/234: loss=0.038135 lr=0.000020 grad_norm=1.074148
Epoch 74/100 Iteration 73/234: loss=0.040406 lr=0.000020 grad_norm=0.840415
Epoch 74/100 Iteration 74/234: loss=0.044536 lr=0.000020 grad_norm=1.525549
Epoch 74/100 Iteration 75/234: loss=0.036227 lr=0.000020 grad_norm=1.476634
Epoch 74/100 Iteration 76/234: loss=0.034708 lr=0.000020 grad_norm=0.677124
Epoch 74/100 Iteration 77/234: loss=0.038554 lr=0.000020 grad_norm=0.974629
Epoch 74/100 Iteration 78/234: loss=0.034787 lr=0.000020 grad_norm=0.830853
Epoch 74/100 Iteration 79/234: loss=0.037870 lr=0.000020 grad_norm=0.703357
Epoch 74/100 Iteration 80/234: loss=0.037857 lr=0.000020 grad_norm=1.040405
Epoch 74/100 Iteration 81/234: loss=0.040767 lr=0.000020 grad_norm=0.572026
Epoch 74/100 Iteration 82/234: loss=0.039438 lr=0.000020 grad_norm=1.129640
Epoch 74/100 Iteration 83/234: loss=0.036540 lr=0.000020 grad_norm=0.943486
Epoch 74/100 Iteration 84/234: loss=0.036403 lr=0.000020 grad_norm=0.536273
Epoch 74/100 Iteration 85/234: loss=0.039030 lr=0.000020 grad_norm=0.847967
Epoch 74/100 Iteration 86/234: loss=0.041095 lr=0.000020 grad_norm=1.067293
Epoch 74/100 Iteration 87/234: loss=0.036404 lr=0.000020 grad_norm=0.718105
Epoch 74/100 Iteration 88/234: loss=0.036431 lr=0.000020 grad_norm=0.559184
Epoch 74/100 Iteration 89/234: loss=0.033031 lr=0.000020 grad_norm=0.889444
Epoch 74/100 Iteration 90/234: loss=0.041321 lr=0.000020 grad_norm=0.583106
Epoch 74/100 Iteration 91/234: loss=0.037370 lr=0.000020 grad_norm=1.361529
Epoch 74/100 Iteration 92/234: loss=0.036390 lr=0.000020 grad_norm=1.058600
Epoch 74/100 Iteration 93/234: loss=0.036069 lr=0.000020 grad_norm=0.566941
Epoch 74/100 Iteration 94/234: loss=0.038897 lr=0.000020 grad_norm=0.943262
Epoch 74/100 Iteration 95/234: loss=0.042513 lr=0.000020 grad_norm=1.112179
Epoch 74/100 Iteration 96/234: loss=0.035850 lr=0.000020 grad_norm=0.476675
Epoch 74/100 Iteration 97/234: loss=0.036046 lr=0.000020 grad_norm=0.874406
Epoch 74/100 Iteration 98/234: loss=0.041321 lr=0.000020 grad_norm=0.794817
Epoch 74/100 Iteration 99/234: loss=0.036380 lr=0.000020 grad_norm=0.696619
Epoch 74/100 Iteration 100/234: loss=0.037143 lr=0.000020 grad_norm=1.299759
Epoch 74/100 Iteration 101/234: loss=0.036576 lr=0.000020 grad_norm=1.243328
Epoch 74/100 Iteration 102/234: loss=0.043253 lr=0.000020 grad_norm=0.588665
Epoch 74/100 Iteration 103/234: loss=0.043021 lr=0.000020 grad_norm=1.120326
Epoch 74/100 Iteration 104/234: loss=0.041551 lr=0.000020 grad_norm=0.900486
Epoch 74/100 Iteration 105/234: loss=0.042941 lr=0.000020 grad_norm=0.434970
Epoch 74/100 Iteration 106/234: loss=0.035502 lr=0.000020 grad_norm=1.023527
Epoch 74/100 Iteration 107/234: loss=0.034251 lr=0.000020 grad_norm=1.071240
Epoch 74/100 Iteration 108/234: loss=0.038078 lr=0.000020 grad_norm=0.511622
Epoch 74/100 Iteration 109/234: loss=0.040683 lr=0.000020 grad_norm=0.515088
Epoch 74/100 Iteration 110/234: loss=0.040330 lr=0.000020 grad_norm=0.720184
Epoch 74/100 Iteration 111/234: loss=0.036487 lr=0.000020 grad_norm=0.389940
Epoch 74/100 Iteration 112/234: loss=0.036966 lr=0.000020 grad_norm=0.548675
Epoch 74/100 Iteration 113/234: loss=0.042112 lr=0.000020 grad_norm=0.619107
Epoch 74/100 Iteration 114/234: loss=0.036680 lr=0.000020 grad_norm=0.539106
Epoch 74/100 Iteration 115/234: loss=0.039769 lr=0.000020 grad_norm=0.447909
Epoch 74/100 Iteration 116/234: loss=0.036341 lr=0.000020 grad_norm=0.628398
Epoch 74/100 Iteration 117/234: loss=0.041554 lr=0.000020 grad_norm=0.476700
Epoch 74/100 Iteration 118/234: loss=0.042624 lr=0.000020 grad_norm=0.400931
Epoch 74/100 Iteration 119/234: loss=0.039214 lr=0.000020 grad_norm=0.603426
Epoch 74/100 Iteration 120/234: loss=0.043075 lr=0.000020 grad_norm=0.710153
Epoch 74/100 Iteration 121/234: loss=0.037752 lr=0.000020 grad_norm=0.582917
Epoch 74/100 Iteration 122/234: loss=0.040072 lr=0.000020 grad_norm=0.401910
Epoch 74/100 Iteration 123/234: loss=0.039286 lr=0.000020 grad_norm=0.797834
Epoch 74/100 Iteration 124/234: loss=0.039760 lr=0.000020 grad_norm=0.467403
Epoch 74/100 Iteration 125/234: loss=0.039466 lr=0.000020 grad_norm=0.737676
Epoch 74/100 Iteration 126/234: loss=0.038726 lr=0.000020 grad_norm=0.858015
Epoch 74/100 Iteration 127/234: loss=0.040305 lr=0.000020 grad_norm=0.592973
Epoch 74/100 Iteration 128/234: loss=0.040247 lr=0.000020 grad_norm=0.502875
Epoch 74/100 Iteration 129/234: loss=0.041649 lr=0.000020 grad_norm=0.622871
Epoch 74/100 Iteration 130/234: loss=0.037323 lr=0.000020 grad_norm=0.479543
Epoch 74/100 Iteration 131/234: loss=0.037762 lr=0.000020 grad_norm=0.465845
Epoch 74/100 Iteration 132/234: loss=0.039443 lr=0.000020 grad_norm=0.500594
Epoch 74/100 Iteration 133/234: loss=0.035710 lr=0.000020 grad_norm=0.598549
Epoch 74/100 Iteration 134/234: loss=0.036421 lr=0.000020 grad_norm=0.688455
Epoch 74/100 Iteration 135/234: loss=0.042786 lr=0.000020 grad_norm=0.484887
Epoch 74/100 Iteration 136/234: loss=0.036586 lr=0.000020 grad_norm=0.834236
Epoch 74/100 Iteration 137/234: loss=0.038363 lr=0.000020 grad_norm=0.958160
Epoch 74/100 Iteration 138/234: loss=0.037879 lr=0.000020 grad_norm=0.481897
Epoch 74/100 Iteration 139/234: loss=0.039525 lr=0.000020 grad_norm=0.739941
Epoch 74/100 Iteration 140/234: loss=0.035838 lr=0.000020 grad_norm=0.865181
Epoch 74/100 Iteration 141/234: loss=0.038957 lr=0.000020 grad_norm=0.554644
Epoch 74/100 Iteration 142/234: loss=0.043736 lr=0.000020 grad_norm=0.981302
Epoch 74/100 Iteration 143/234: loss=0.035327 lr=0.000020 grad_norm=1.498063
Epoch 74/100 Iteration 144/234: loss=0.038575 lr=0.000020 grad_norm=1.069973
Epoch 74/100 Iteration 145/234: loss=0.040339 lr=0.000020 grad_norm=0.632607
Epoch 74/100 Iteration 146/234: loss=0.042830 lr=0.000020 grad_norm=1.059935
Epoch 74/100 Iteration 147/234: loss=0.038273 lr=0.000020 grad_norm=0.851106
Epoch 74/100 Iteration 148/234: loss=0.035715 lr=0.000020 grad_norm=0.725431
Epoch 74/100 Iteration 149/234: loss=0.038346 lr=0.000020 grad_norm=1.183777
Epoch 74/100 Iteration 150/234: loss=0.038969 lr=0.000020 grad_norm=1.134341
Epoch 74/100 Iteration 151/234: loss=0.041957 lr=0.000020 grad_norm=1.142677
Epoch 74/100 Iteration 152/234: loss=0.038786 lr=0.000020 grad_norm=0.501541
Epoch 74/100 Iteration 153/234: loss=0.038726 lr=0.000020 grad_norm=0.954012
Epoch 74/100 Iteration 154/234: loss=0.038343 lr=0.000020 grad_norm=0.845080
Epoch 74/100 Iteration 155/234: loss=0.040338 lr=0.000020 grad_norm=0.434183
Epoch 74/100 Iteration 156/234: loss=0.037347 lr=0.000020 grad_norm=0.823546
Epoch 74/100 Iteration 157/234: loss=0.038220 lr=0.000020 grad_norm=0.681933
Epoch 74/100 Iteration 158/234: loss=0.044261 lr=0.000020 grad_norm=0.600014
Epoch 74/100 Iteration 159/234: loss=0.039613 lr=0.000020 grad_norm=0.896418
Epoch 74/100 Iteration 160/234: loss=0.043333 lr=0.000020 grad_norm=0.734217
Epoch 74/100 Iteration 161/234: loss=0.034658 lr=0.000020 grad_norm=0.405516
Epoch 74/100 Iteration 162/234: loss=0.040005 lr=0.000020 grad_norm=0.817796
Epoch 74/100 Iteration 163/234: loss=0.041317 lr=0.000020 grad_norm=0.750339
Epoch 74/100 Iteration 164/234: loss=0.037888 lr=0.000020 grad_norm=0.476659
Epoch 74/100 Iteration 165/234: loss=0.038860 lr=0.000020 grad_norm=0.994157
Epoch 74/100 Iteration 166/234: loss=0.038977 lr=0.000020 grad_norm=1.261201
Epoch 74/100 Iteration 167/234: loss=0.036584 lr=0.000020 grad_norm=0.951991
Epoch 74/100 Iteration 168/234: loss=0.039652 lr=0.000020 grad_norm=1.028066
Epoch 74/100 Iteration 169/234: loss=0.037174 lr=0.000020 grad_norm=1.512740
Epoch 74/100 Iteration 170/234: loss=0.034345 lr=0.000020 grad_norm=0.826047
Epoch 74/100 Iteration 171/234: loss=0.043559 lr=0.000020 grad_norm=1.446452
Epoch 74/100 Iteration 172/234: loss=0.041409 lr=0.000020 grad_norm=2.644998
Epoch 74/100 Iteration 173/234: loss=0.040829 lr=0.000020 grad_norm=1.715199
Epoch 74/100 Iteration 174/234: loss=0.037129 lr=0.000020 grad_norm=0.832284
Epoch 74/100 Iteration 175/234: loss=0.035886 lr=0.000020 grad_norm=1.980801
Epoch 74/100 Iteration 176/234: loss=0.037355 lr=0.000020 grad_norm=1.258517
Epoch 74/100 Iteration 177/234: loss=0.044224 lr=0.000020 grad_norm=1.102879
Epoch 74/100 Iteration 178/234: loss=0.041285 lr=0.000020 grad_norm=0.712116
Epoch 74/100 Iteration 179/234: loss=0.042381 lr=0.000020 grad_norm=1.066463
Epoch 74/100 Iteration 180/234: loss=0.041575 lr=0.000020 grad_norm=1.665354
Epoch 74/100 Iteration 181/234: loss=0.042219 lr=0.000020 grad_norm=0.887479
Epoch 74/100 Iteration 182/234: loss=0.037886 lr=0.000020 grad_norm=0.768412
Epoch 74/100 Iteration 183/234: loss=0.043401 lr=0.000020 grad_norm=1.196938
Epoch 74/100 Iteration 184/234: loss=0.040306 lr=0.000020 grad_norm=0.885431
Epoch 74/100 Iteration 185/234: loss=0.040840 lr=0.000020 grad_norm=0.831940
Epoch 74/100 Iteration 186/234: loss=0.042540 lr=0.000020 grad_norm=0.870407
Epoch 74/100 Iteration 187/234: loss=0.041416 lr=0.000020 grad_norm=0.857187
Epoch 74/100 Iteration 188/234: loss=0.036784 lr=0.000020 grad_norm=1.259783
Epoch 74/100 Iteration 189/234: loss=0.032753 lr=0.000020 grad_norm=0.569394
Epoch 74/100 Iteration 190/234: loss=0.040699 lr=0.000020 grad_norm=1.770909
Epoch 74/100 Iteration 191/234: loss=0.034075 lr=0.000020 grad_norm=1.468337
Epoch 74/100 Iteration 192/234: loss=0.038290 lr=0.000020 grad_norm=0.803004
Epoch 74/100 Iteration 193/234: loss=0.039658 lr=0.000020 grad_norm=1.769280
Epoch 74/100 Iteration 194/234: loss=0.035671 lr=0.000020 grad_norm=0.872450
Epoch 74/100 Iteration 195/234: loss=0.039357 lr=0.000020 grad_norm=1.273743
Epoch 74/100 Iteration 196/234: loss=0.032744 lr=0.000020 grad_norm=1.075280
Epoch 74/100 Iteration 197/234: loss=0.035568 lr=0.000020 grad_norm=0.938924
Epoch 74/100 Iteration 198/234: loss=0.040525 lr=0.000020 grad_norm=1.491103
Epoch 74/100 Iteration 199/234: loss=0.037935 lr=0.000020 grad_norm=0.815450
Epoch 74/100 Iteration 200/234: loss=0.034525 lr=0.000020 grad_norm=0.852430
Epoch 74/100 Iteration 201/234: loss=0.036133 lr=0.000020 grad_norm=0.514685
Epoch 74/100 Iteration 202/234: loss=0.040264 lr=0.000020 grad_norm=1.375974
Epoch 74/100 Iteration 203/234: loss=0.037108 lr=0.000020 grad_norm=0.856852
Epoch 74/100 Iteration 204/234: loss=0.041883 lr=0.000020 grad_norm=1.038021
Epoch 74/100 Iteration 205/234: loss=0.040453 lr=0.000020 grad_norm=1.469018
Epoch 74/100 Iteration 206/234: loss=0.037268 lr=0.000020 grad_norm=0.720235
Epoch 74/100 Iteration 207/234: loss=0.037813 lr=0.000020 grad_norm=1.081408
Epoch 74/100 Iteration 208/234: loss=0.044073 lr=0.000020 grad_norm=1.566665
Epoch 74/100 Iteration 209/234: loss=0.038642 lr=0.000020 grad_norm=0.510623
Epoch 74/100 Iteration 210/234: loss=0.039263 lr=0.000020 grad_norm=0.934280
Epoch 74/100 Iteration 211/234: loss=0.038226 lr=0.000020 grad_norm=0.642951
Epoch 74/100 Iteration 212/234: loss=0.036143 lr=0.000020 grad_norm=0.502366
Epoch 74/100 Iteration 213/234: loss=0.038981 lr=0.000020 grad_norm=0.519607
Epoch 74/100 Iteration 214/234: loss=0.036678 lr=0.000020 grad_norm=0.664019
Epoch 74/100 Iteration 215/234: loss=0.036543 lr=0.000020 grad_norm=0.420470
Epoch 74/100 Iteration 216/234: loss=0.041548 lr=0.000020 grad_norm=1.140848
Epoch 74/100 Iteration 217/234: loss=0.038086 lr=0.000020 grad_norm=0.978248
Epoch 74/100 Iteration 218/234: loss=0.035152 lr=0.000020 grad_norm=0.691594
Epoch 74/100 Iteration 219/234: loss=0.035700 lr=0.000020 grad_norm=1.138092
Epoch 74/100 Iteration 220/234: loss=0.041972 lr=0.000020 grad_norm=0.526993
Epoch 74/100 Iteration 221/234: loss=0.035653 lr=0.000020 grad_norm=1.076208
Epoch 74/100 Iteration 222/234: loss=0.039024 lr=0.000020 grad_norm=0.540937
Epoch 74/100 Iteration 223/234: loss=0.040706 lr=0.000020 grad_norm=1.002175
Epoch 74/100 Iteration 224/234: loss=0.036891 lr=0.000020 grad_norm=1.092229
Epoch 74/100 Iteration 225/234: loss=0.039472 lr=0.000020 grad_norm=0.644057
Epoch 74/100 Iteration 226/234: loss=0.042985 lr=0.000020 grad_norm=1.599277
Epoch 74/100 Iteration 227/234: loss=0.042525 lr=0.000020 grad_norm=0.862623
Epoch 74/100 Iteration 228/234: loss=0.035370 lr=0.000020 grad_norm=1.016864
Epoch 74/100 Iteration 229/234: loss=0.036064 lr=0.000020 grad_norm=1.265835
Epoch 74/100 Iteration 230/234: loss=0.036631 lr=0.000020 grad_norm=0.657205
Epoch 74/100 Iteration 231/234: loss=0.039307 lr=0.000020 grad_norm=1.072651
Epoch 74/100 Iteration 232/234: loss=0.042867 lr=0.000020 grad_norm=0.533129
Epoch 74/100 Iteration 233/234: loss=0.039754 lr=0.000020 grad_norm=1.124125
Epoch 74/100 Iteration 234/234: loss=0.041440 lr=0.000020 grad_norm=0.856599
Epoch 74/100 finished. Avg Loss: 0.039016
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 75/100 Iteration 1/234: loss=0.036284 lr=0.000020 grad_norm=0.739340
Epoch 75/100 Iteration 2/234: loss=0.044146 lr=0.000020 grad_norm=1.040933
Epoch 75/100 Iteration 3/234: loss=0.038767 lr=0.000020 grad_norm=0.355150
Epoch 75/100 Iteration 4/234: loss=0.042139 lr=0.000020 grad_norm=1.116258
Epoch 75/100 Iteration 5/234: loss=0.040680 lr=0.000020 grad_norm=0.590704
Epoch 75/100 Iteration 6/234: loss=0.042395 lr=0.000020 grad_norm=1.053461
Epoch 75/100 Iteration 7/234: loss=0.036533 lr=0.000020 grad_norm=1.071946
Epoch 75/100 Iteration 8/234: loss=0.036717 lr=0.000020 grad_norm=0.449045
Epoch 75/100 Iteration 9/234: loss=0.036422 lr=0.000020 grad_norm=0.943577
Epoch 75/100 Iteration 10/234: loss=0.032479 lr=0.000020 grad_norm=0.453943
Epoch 75/100 Iteration 11/234: loss=0.043894 lr=0.000020 grad_norm=1.482274
Epoch 75/100 Iteration 12/234: loss=0.038622 lr=0.000020 grad_norm=1.032317
Epoch 75/100 Iteration 13/234: loss=0.040483 lr=0.000020 grad_norm=1.072112
Epoch 75/100 Iteration 14/234: loss=0.039496 lr=0.000020 grad_norm=1.751577
Epoch 75/100 Iteration 15/234: loss=0.037712 lr=0.000020 grad_norm=0.588374
Epoch 75/100 Iteration 16/234: loss=0.038926 lr=0.000020 grad_norm=1.360649
Epoch 75/100 Iteration 17/234: loss=0.037713 lr=0.000020 grad_norm=0.535551
Epoch 75/100 Iteration 18/234: loss=0.038656 lr=0.000020 grad_norm=1.540355
Epoch 75/100 Iteration 19/234: loss=0.041016 lr=0.000020 grad_norm=0.700706
Epoch 75/100 Iteration 20/234: loss=0.037303 lr=0.000020 grad_norm=1.166838
Epoch 75/100 Iteration 21/234: loss=0.037930 lr=0.000020 grad_norm=0.751234
Epoch 75/100 Iteration 22/234: loss=0.037254 lr=0.000020 grad_norm=0.904230
Epoch 75/100 Iteration 23/234: loss=0.035899 lr=0.000020 grad_norm=0.891600
Epoch 75/100 Iteration 24/234: loss=0.035137 lr=0.000020 grad_norm=0.707362
Epoch 75/100 Iteration 25/234: loss=0.038454 lr=0.000020 grad_norm=0.928244
Epoch 75/100 Iteration 26/234: loss=0.035561 lr=0.000020 grad_norm=0.589482
Epoch 75/100 Iteration 27/234: loss=0.037875 lr=0.000020 grad_norm=0.853948
Epoch 75/100 Iteration 28/234: loss=0.040494 lr=0.000020 grad_norm=0.393946
Epoch 75/100 Iteration 29/234: loss=0.042175 lr=0.000020 grad_norm=0.638887
Epoch 75/100 Iteration 30/234: loss=0.038903 lr=0.000020 grad_norm=0.556011
Epoch 75/100 Iteration 31/234: loss=0.035440 lr=0.000020 grad_norm=0.655475
Epoch 75/100 Iteration 32/234: loss=0.041221 lr=0.000020 grad_norm=0.616997
Epoch 75/100 Iteration 33/234: loss=0.035869 lr=0.000020 grad_norm=0.807021
Epoch 75/100 Iteration 34/234: loss=0.040023 lr=0.000020 grad_norm=0.587866
Epoch 75/100 Iteration 35/234: loss=0.042840 lr=0.000020 grad_norm=0.723899
Epoch 75/100 Iteration 36/234: loss=0.038534 lr=0.000020 grad_norm=1.241003
Epoch 75/100 Iteration 37/234: loss=0.040410 lr=0.000020 grad_norm=1.090388
Epoch 75/100 Iteration 38/234: loss=0.042442 lr=0.000020 grad_norm=0.611760
Epoch 75/100 Iteration 39/234: loss=0.037302 lr=0.000020 grad_norm=0.661816
Epoch 75/100 Iteration 40/234: loss=0.033341 lr=0.000020 grad_norm=0.445709
Epoch 75/100 Iteration 41/234: loss=0.036710 lr=0.000020 grad_norm=0.619495
Epoch 75/100 Iteration 42/234: loss=0.033809 lr=0.000020 grad_norm=0.574022
Epoch 75/100 Iteration 43/234: loss=0.042670 lr=0.000020 grad_norm=1.123881
Epoch 75/100 Iteration 44/234: loss=0.036809 lr=0.000020 grad_norm=0.632832
Epoch 75/100 Iteration 45/234: loss=0.040157 lr=0.000020 grad_norm=1.201617
Epoch 75/100 Iteration 46/234: loss=0.038108 lr=0.000020 grad_norm=1.791959
Epoch 75/100 Iteration 47/234: loss=0.039591 lr=0.000020 grad_norm=0.995534
Epoch 75/100 Iteration 48/234: loss=0.042716 lr=0.000020 grad_norm=1.110162
Epoch 75/100 Iteration 49/234: loss=0.040448 lr=0.000020 grad_norm=0.835082
Epoch 75/100 Iteration 50/234: loss=0.040637 lr=0.000020 grad_norm=0.948332
Epoch 75/100 Iteration 51/234: loss=0.042672 lr=0.000020 grad_norm=1.115410
Epoch 75/100 Iteration 52/234: loss=0.037512 lr=0.000020 grad_norm=0.421412
Epoch 75/100 Iteration 53/234: loss=0.036194 lr=0.000020 grad_norm=0.963819
Epoch 75/100 Iteration 54/234: loss=0.041320 lr=0.000020 grad_norm=0.695364
Epoch 75/100 Iteration 55/234: loss=0.036650 lr=0.000020 grad_norm=0.934173
Epoch 75/100 Iteration 56/234: loss=0.032518 lr=0.000020 grad_norm=0.658426
Epoch 75/100 Iteration 57/234: loss=0.038215 lr=0.000020 grad_norm=0.745832
Epoch 75/100 Iteration 58/234: loss=0.038728 lr=0.000020 grad_norm=0.563897
Epoch 75/100 Iteration 59/234: loss=0.037721 lr=0.000020 grad_norm=1.142086
Epoch 75/100 Iteration 60/234: loss=0.035894 lr=0.000020 grad_norm=1.210062
Epoch 75/100 Iteration 61/234: loss=0.036407 lr=0.000020 grad_norm=0.687092
Epoch 75/100 Iteration 62/234: loss=0.043488 lr=0.000020 grad_norm=1.498640
Epoch 75/100 Iteration 63/234: loss=0.036633 lr=0.000020 grad_norm=0.990781
Epoch 75/100 Iteration 64/234: loss=0.038652 lr=0.000020 grad_norm=0.646879
Epoch 75/100 Iteration 65/234: loss=0.037106 lr=0.000020 grad_norm=1.134314
Epoch 75/100 Iteration 66/234: loss=0.039672 lr=0.000020 grad_norm=0.636215
Epoch 75/100 Iteration 67/234: loss=0.044073 lr=0.000020 grad_norm=0.850318
Epoch 75/100 Iteration 68/234: loss=0.036761 lr=0.000020 grad_norm=1.151941
Epoch 75/100 Iteration 69/234: loss=0.039282 lr=0.000020 grad_norm=0.677726
Epoch 75/100 Iteration 70/234: loss=0.037095 lr=0.000020 grad_norm=0.682358
Epoch 75/100 Iteration 71/234: loss=0.044319 lr=0.000020 grad_norm=0.524752
Epoch 75/100 Iteration 72/234: loss=0.039584 lr=0.000020 grad_norm=0.971376
Epoch 75/100 Iteration 73/234: loss=0.034905 lr=0.000020 grad_norm=0.987146
Epoch 75/100 Iteration 74/234: loss=0.037016 lr=0.000020 grad_norm=0.580255
Epoch 75/100 Iteration 75/234: loss=0.037683 lr=0.000020 grad_norm=1.036299
Epoch 75/100 Iteration 76/234: loss=0.039358 lr=0.000020 grad_norm=0.449348
Epoch 75/100 Iteration 77/234: loss=0.035998 lr=0.000020 grad_norm=0.659431
Epoch 75/100 Iteration 78/234: loss=0.034983 lr=0.000020 grad_norm=0.448996
Epoch 75/100 Iteration 79/234: loss=0.037791 lr=0.000020 grad_norm=0.481527
Epoch 75/100 Iteration 80/234: loss=0.038179 lr=0.000020 grad_norm=0.748730
Epoch 75/100 Iteration 81/234: loss=0.036702 lr=0.000020 grad_norm=0.581360
Epoch 75/100 Iteration 82/234: loss=0.040042 lr=0.000020 grad_norm=0.449938
Epoch 75/100 Iteration 83/234: loss=0.039430 lr=0.000020 grad_norm=0.685617
Epoch 75/100 Iteration 84/234: loss=0.039927 lr=0.000020 grad_norm=0.691226
Epoch 75/100 Iteration 85/234: loss=0.039233 lr=0.000020 grad_norm=0.628386
Epoch 75/100 Iteration 86/234: loss=0.041163 lr=0.000020 grad_norm=0.745009
Epoch 75/100 Iteration 87/234: loss=0.036857 lr=0.000020 grad_norm=0.489063
Epoch 75/100 Iteration 88/234: loss=0.040977 lr=0.000020 grad_norm=0.451107
Epoch 75/100 Iteration 89/234: loss=0.038489 lr=0.000020 grad_norm=0.518635
Epoch 75/100 Iteration 90/234: loss=0.037534 lr=0.000020 grad_norm=0.436146
Epoch 75/100 Iteration 91/234: loss=0.038719 lr=0.000020 grad_norm=0.448154
Epoch 75/100 Iteration 92/234: loss=0.033758 lr=0.000020 grad_norm=0.491327
Epoch 75/100 Iteration 93/234: loss=0.041240 lr=0.000020 grad_norm=0.590081
Epoch 75/100 Iteration 94/234: loss=0.034210 lr=0.000020 grad_norm=0.444984
Epoch 75/100 Iteration 95/234: loss=0.039302 lr=0.000020 grad_norm=1.177722
Epoch 75/100 Iteration 96/234: loss=0.039169 lr=0.000020 grad_norm=1.223941
Epoch 75/100 Iteration 97/234: loss=0.033766 lr=0.000020 grad_norm=0.540619
Epoch 75/100 Iteration 98/234: loss=0.037533 lr=0.000020 grad_norm=0.724127
Epoch 75/100 Iteration 99/234: loss=0.038663 lr=0.000020 grad_norm=0.900143
Epoch 75/100 Iteration 100/234: loss=0.035349 lr=0.000020 grad_norm=0.502618
Epoch 75/100 Iteration 101/234: loss=0.035378 lr=0.000020 grad_norm=0.432424
Epoch 75/100 Iteration 102/234: loss=0.033859 lr=0.000020 grad_norm=0.662639
Epoch 75/100 Iteration 103/234: loss=0.038072 lr=0.000020 grad_norm=0.547114
Epoch 75/100 Iteration 104/234: loss=0.035581 lr=0.000020 grad_norm=0.778151
Epoch 75/100 Iteration 105/234: loss=0.035164 lr=0.000020 grad_norm=0.757604
Epoch 75/100 Iteration 106/234: loss=0.035249 lr=0.000020 grad_norm=0.493587
Epoch 75/100 Iteration 107/234: loss=0.035253 lr=0.000020 grad_norm=0.825021
Epoch 75/100 Iteration 108/234: loss=0.038964 lr=0.000020 grad_norm=0.927310
Epoch 75/100 Iteration 109/234: loss=0.036762 lr=0.000020 grad_norm=0.697358
Epoch 75/100 Iteration 110/234: loss=0.042360 lr=0.000020 grad_norm=0.542788
Epoch 75/100 Iteration 111/234: loss=0.037317 lr=0.000020 grad_norm=0.915290
Epoch 75/100 Iteration 112/234: loss=0.039372 lr=0.000020 grad_norm=0.716114
Epoch 75/100 Iteration 113/234: loss=0.038754 lr=0.000020 grad_norm=0.891370
Epoch 75/100 Iteration 114/234: loss=0.043006 lr=0.000020 grad_norm=1.093891
Epoch 75/100 Iteration 115/234: loss=0.034962 lr=0.000020 grad_norm=0.752578
Epoch 75/100 Iteration 116/234: loss=0.037788 lr=0.000020 grad_norm=0.638109
Epoch 75/100 Iteration 117/234: loss=0.040738 lr=0.000020 grad_norm=1.018399
Epoch 75/100 Iteration 118/234: loss=0.034950 lr=0.000020 grad_norm=0.612972
Epoch 75/100 Iteration 119/234: loss=0.038043 lr=0.000020 grad_norm=0.800721
Epoch 75/100 Iteration 120/234: loss=0.032450 lr=0.000020 grad_norm=0.755435
Epoch 75/100 Iteration 121/234: loss=0.036596 lr=0.000020 grad_norm=0.689896
Epoch 75/100 Iteration 122/234: loss=0.034785 lr=0.000020 grad_norm=1.091926
Epoch 75/100 Iteration 123/234: loss=0.038898 lr=0.000020 grad_norm=0.684744
Epoch 75/100 Iteration 124/234: loss=0.036103 lr=0.000020 grad_norm=0.498766
Epoch 75/100 Iteration 125/234: loss=0.034684 lr=0.000020 grad_norm=0.362766
Epoch 75/100 Iteration 126/234: loss=0.037974 lr=0.000020 grad_norm=0.569258
Epoch 75/100 Iteration 127/234: loss=0.036509 lr=0.000020 grad_norm=0.487422
Epoch 75/100 Iteration 128/234: loss=0.034137 lr=0.000020 grad_norm=0.548210
Epoch 75/100 Iteration 129/234: loss=0.037233 lr=0.000020 grad_norm=0.522811
Epoch 75/100 Iteration 130/234: loss=0.039773 lr=0.000020 grad_norm=0.679086
Epoch 75/100 Iteration 131/234: loss=0.035829 lr=0.000020 grad_norm=0.714869
Epoch 75/100 Iteration 132/234: loss=0.038890 lr=0.000020 grad_norm=0.585254
Epoch 75/100 Iteration 133/234: loss=0.038010 lr=0.000020 grad_norm=1.379309
Epoch 75/100 Iteration 134/234: loss=0.040123 lr=0.000020 grad_norm=1.402262
Epoch 75/100 Iteration 135/234: loss=0.037411 lr=0.000020 grad_norm=0.597794
Epoch 75/100 Iteration 136/234: loss=0.038435 lr=0.000020 grad_norm=1.045427
Epoch 75/100 Iteration 137/234: loss=0.035297 lr=0.000020 grad_norm=1.316708
Epoch 75/100 Iteration 138/234: loss=0.033605 lr=0.000020 grad_norm=0.650549
Epoch 75/100 Iteration 139/234: loss=0.037903 lr=0.000020 grad_norm=1.155247
Epoch 75/100 Iteration 140/234: loss=0.037982 lr=0.000020 grad_norm=1.220424
Epoch 75/100 Iteration 141/234: loss=0.037341 lr=0.000020 grad_norm=0.946390
Epoch 75/100 Iteration 142/234: loss=0.042214 lr=0.000020 grad_norm=1.550986
Epoch 75/100 Iteration 143/234: loss=0.035318 lr=0.000020 grad_norm=0.817661
Epoch 75/100 Iteration 144/234: loss=0.038912 lr=0.000020 grad_norm=1.178069
Epoch 75/100 Iteration 145/234: loss=0.037993 lr=0.000020 grad_norm=1.155572
Epoch 75/100 Iteration 146/234: loss=0.038257 lr=0.000020 grad_norm=0.680963
Epoch 75/100 Iteration 147/234: loss=0.039229 lr=0.000020 grad_norm=1.740915
Epoch 75/100 Iteration 148/234: loss=0.041433 lr=0.000020 grad_norm=1.716258
Epoch 75/100 Iteration 149/234: loss=0.037105 lr=0.000020 grad_norm=0.934650
Epoch 75/100 Iteration 150/234: loss=0.036939 lr=0.000020 grad_norm=0.841345
Epoch 75/100 Iteration 151/234: loss=0.041720 lr=0.000020 grad_norm=0.668380
Epoch 75/100 Iteration 152/234: loss=0.041116 lr=0.000020 grad_norm=1.147281
Epoch 75/100 Iteration 153/234: loss=0.034473 lr=0.000020 grad_norm=0.940284
Epoch 75/100 Iteration 154/234: loss=0.040381 lr=0.000020 grad_norm=0.815405
Epoch 75/100 Iteration 155/234: loss=0.037699 lr=0.000020 grad_norm=0.865039
Epoch 75/100 Iteration 156/234: loss=0.036095 lr=0.000020 grad_norm=0.691571
Epoch 75/100 Iteration 157/234: loss=0.039667 lr=0.000020 grad_norm=0.609621
Epoch 75/100 Iteration 158/234: loss=0.037697 lr=0.000020 grad_norm=0.628309
Epoch 75/100 Iteration 159/234: loss=0.040848 lr=0.000020 grad_norm=1.060185
Epoch 75/100 Iteration 160/234: loss=0.038318 lr=0.000020 grad_norm=0.703150
Epoch 75/100 Iteration 161/234: loss=0.037086 lr=0.000020 grad_norm=0.678575
Epoch 75/100 Iteration 162/234: loss=0.038929 lr=0.000020 grad_norm=1.109582
Epoch 75/100 Iteration 163/234: loss=0.037658 lr=0.000020 grad_norm=0.456462
Epoch 75/100 Iteration 164/234: loss=0.039252 lr=0.000020 grad_norm=1.403261
Epoch 75/100 Iteration 165/234: loss=0.037269 lr=0.000020 grad_norm=0.882962
Epoch 75/100 Iteration 166/234: loss=0.038278 lr=0.000020 grad_norm=0.878119
Epoch 75/100 Iteration 167/234: loss=0.039241 lr=0.000020 grad_norm=0.921607
Epoch 75/100 Iteration 168/234: loss=0.039721 lr=0.000020 grad_norm=0.779857
Epoch 75/100 Iteration 169/234: loss=0.039397 lr=0.000020 grad_norm=1.292126
Epoch 75/100 Iteration 170/234: loss=0.036646 lr=0.000020 grad_norm=0.575381
Epoch 75/100 Iteration 171/234: loss=0.036416 lr=0.000020 grad_norm=1.377346
Epoch 75/100 Iteration 172/234: loss=0.043323 lr=0.000020 grad_norm=0.857643
Epoch 75/100 Iteration 173/234: loss=0.039187 lr=0.000020 grad_norm=1.222684
Epoch 75/100 Iteration 174/234: loss=0.033978 lr=0.000020 grad_norm=1.487925
Epoch 75/100 Iteration 175/234: loss=0.040541 lr=0.000020 grad_norm=0.725628
Epoch 75/100 Iteration 176/234: loss=0.039272 lr=0.000020 grad_norm=1.533340
Epoch 75/100 Iteration 177/234: loss=0.037087 lr=0.000020 grad_norm=1.114779
Epoch 75/100 Iteration 178/234: loss=0.039623 lr=0.000020 grad_norm=0.918976
Epoch 75/100 Iteration 179/234: loss=0.037900 lr=0.000020 grad_norm=0.905814
Epoch 75/100 Iteration 180/234: loss=0.037528 lr=0.000020 grad_norm=0.654679
Epoch 75/100 Iteration 181/234: loss=0.034659 lr=0.000020 grad_norm=1.022655
Epoch 75/100 Iteration 182/234: loss=0.036412 lr=0.000020 grad_norm=0.439253
Epoch 75/100 Iteration 183/234: loss=0.038286 lr=0.000020 grad_norm=0.726239
Epoch 75/100 Iteration 184/234: loss=0.034591 lr=0.000020 grad_norm=0.556427
Epoch 75/100 Iteration 185/234: loss=0.041285 lr=0.000020 grad_norm=0.687901
Epoch 75/100 Iteration 186/234: loss=0.034379 lr=0.000020 grad_norm=0.595763
Epoch 75/100 Iteration 187/234: loss=0.038188 lr=0.000020 grad_norm=0.842441
Epoch 75/100 Iteration 188/234: loss=0.037079 lr=0.000020 grad_norm=0.812957
Epoch 75/100 Iteration 189/234: loss=0.038560 lr=0.000020 grad_norm=0.714010
Epoch 75/100 Iteration 190/234: loss=0.040861 lr=0.000020 grad_norm=1.302411
Epoch 75/100 Iteration 191/234: loss=0.036852 lr=0.000020 grad_norm=0.622166
Epoch 75/100 Iteration 192/234: loss=0.036091 lr=0.000020 grad_norm=0.889974
Epoch 75/100 Iteration 193/234: loss=0.043981 lr=0.000020 grad_norm=1.171834
Epoch 75/100 Iteration 194/234: loss=0.041398 lr=0.000020 grad_norm=0.906331
Epoch 75/100 Iteration 195/234: loss=0.036330 lr=0.000020 grad_norm=0.688871
Epoch 75/100 Iteration 196/234: loss=0.038661 lr=0.000020 grad_norm=0.684532
Epoch 75/100 Iteration 197/234: loss=0.041157 lr=0.000020 grad_norm=0.937608
Epoch 75/100 Iteration 198/234: loss=0.039372 lr=0.000020 grad_norm=0.481352
Epoch 75/100 Iteration 199/234: loss=0.041001 lr=0.000020 grad_norm=0.965511
Epoch 75/100 Iteration 200/234: loss=0.038424 lr=0.000020 grad_norm=0.789398
Epoch 75/100 Iteration 201/234: loss=0.037001 lr=0.000020 grad_norm=0.465131
Epoch 75/100 Iteration 202/234: loss=0.040169 lr=0.000020 grad_norm=0.637612
Epoch 75/100 Iteration 203/234: loss=0.044563 lr=0.000020 grad_norm=0.747610
Epoch 75/100 Iteration 204/234: loss=0.039960 lr=0.000020 grad_norm=0.698497
Epoch 75/100 Iteration 205/234: loss=0.038119 lr=0.000020 grad_norm=0.366582
Epoch 75/100 Iteration 206/234: loss=0.040013 lr=0.000020 grad_norm=0.636493
Epoch 75/100 Iteration 207/234: loss=0.039629 lr=0.000020 grad_norm=0.663297
Epoch 75/100 Iteration 208/234: loss=0.036407 lr=0.000020 grad_norm=0.386754
Epoch 75/100 Iteration 209/234: loss=0.039039 lr=0.000020 grad_norm=0.804141
Epoch 75/100 Iteration 210/234: loss=0.034802 lr=0.000020 grad_norm=0.906226
Epoch 75/100 Iteration 211/234: loss=0.038789 lr=0.000020 grad_norm=0.425923
Epoch 75/100 Iteration 212/234: loss=0.040104 lr=0.000020 grad_norm=1.262912
Epoch 75/100 Iteration 213/234: loss=0.037463 lr=0.000020 grad_norm=1.471574
Epoch 75/100 Iteration 214/234: loss=0.038453 lr=0.000020 grad_norm=0.782240
Epoch 75/100 Iteration 215/234: loss=0.036548 lr=0.000020 grad_norm=0.848055
Epoch 75/100 Iteration 216/234: loss=0.040970 lr=0.000020 grad_norm=0.625771
Epoch 75/100 Iteration 217/234: loss=0.036662 lr=0.000020 grad_norm=0.529700
Epoch 75/100 Iteration 218/234: loss=0.035359 lr=0.000020 grad_norm=0.537391
Epoch 75/100 Iteration 219/234: loss=0.040496 lr=0.000020 grad_norm=0.516916
Epoch 75/100 Iteration 220/234: loss=0.039787 lr=0.000020 grad_norm=1.173790
Epoch 75/100 Iteration 221/234: loss=0.041141 lr=0.000020 grad_norm=1.239812
Epoch 75/100 Iteration 222/234: loss=0.037071 lr=0.000020 grad_norm=0.828513
Epoch 75/100 Iteration 223/234: loss=0.041089 lr=0.000020 grad_norm=0.658036
Epoch 75/100 Iteration 224/234: loss=0.036985 lr=0.000020 grad_norm=1.015640
Epoch 75/100 Iteration 225/234: loss=0.037638 lr=0.000020 grad_norm=0.761041
Epoch 75/100 Iteration 226/234: loss=0.038748 lr=0.000020 grad_norm=0.765293
Epoch 75/100 Iteration 227/234: loss=0.041936 lr=0.000020 grad_norm=0.864847
Epoch 75/100 Iteration 228/234: loss=0.033782 lr=0.000020 grad_norm=0.927883
Epoch 75/100 Iteration 229/234: loss=0.035027 lr=0.000020 grad_norm=0.631954
Epoch 75/100 Iteration 230/234: loss=0.032396 lr=0.000020 grad_norm=0.421093
Epoch 75/100 Iteration 231/234: loss=0.039860 lr=0.000020 grad_norm=0.930031
Epoch 75/100 Iteration 232/234: loss=0.039717 lr=0.000020 grad_norm=0.456283
Epoch 75/100 Iteration 233/234: loss=0.044050 lr=0.000020 grad_norm=0.866589
Epoch 75/100 Iteration 234/234: loss=0.036385 lr=0.000020 grad_norm=1.310131
Epoch 75/100 finished. Avg Loss: 0.038251
Epoch 76/100 Iteration 1/234: loss=0.035844 lr=0.000020 grad_norm=0.714741
Epoch 76/100 Iteration 2/234: loss=0.035278 lr=0.000020 grad_norm=0.777504
Epoch 76/100 Iteration 3/234: loss=0.035669 lr=0.000020 grad_norm=0.806451
Epoch 76/100 Iteration 4/234: loss=0.040037 lr=0.000020 grad_norm=0.366972
Epoch 76/100 Iteration 5/234: loss=0.041554 lr=0.000020 grad_norm=0.888715
Epoch 76/100 Iteration 6/234: loss=0.038415 lr=0.000020 grad_norm=0.640940
Epoch 76/100 Iteration 7/234: loss=0.039833 lr=0.000020 grad_norm=0.991895
Epoch 76/100 Iteration 8/234: loss=0.036279 lr=0.000020 grad_norm=0.996610
Epoch 76/100 Iteration 9/234: loss=0.040472 lr=0.000020 grad_norm=0.465155
Epoch 76/100 Iteration 10/234: loss=0.037173 lr=0.000020 grad_norm=1.080740
Epoch 76/100 Iteration 11/234: loss=0.032035 lr=0.000020 grad_norm=0.716279
Epoch 76/100 Iteration 12/234: loss=0.039511 lr=0.000020 grad_norm=0.650627
Epoch 76/100 Iteration 13/234: loss=0.036092 lr=0.000020 grad_norm=1.311602
Epoch 76/100 Iteration 14/234: loss=0.038262 lr=0.000020 grad_norm=0.824674
Epoch 76/100 Iteration 15/234: loss=0.038476 lr=0.000020 grad_norm=0.605899
Epoch 76/100 Iteration 16/234: loss=0.034055 lr=0.000020 grad_norm=0.677398
Epoch 76/100 Iteration 17/234: loss=0.036745 lr=0.000020 grad_norm=0.464352
Epoch 76/100 Iteration 18/234: loss=0.037603 lr=0.000020 grad_norm=0.760356
Epoch 76/100 Iteration 19/234: loss=0.033339 lr=0.000020 grad_norm=0.695117
Epoch 76/100 Iteration 20/234: loss=0.040595 lr=0.000020 grad_norm=0.522053
Epoch 76/100 Iteration 21/234: loss=0.036979 lr=0.000020 grad_norm=0.463052
Epoch 76/100 Iteration 22/234: loss=0.041328 lr=0.000020 grad_norm=0.795646
Epoch 76/100 Iteration 23/234: loss=0.043070 lr=0.000020 grad_norm=1.356326
Epoch 76/100 Iteration 24/234: loss=0.036944 lr=0.000020 grad_norm=1.065785
Epoch 76/100 Iteration 25/234: loss=0.041751 lr=0.000020 grad_norm=0.621687
Epoch 76/100 Iteration 26/234: loss=0.038549 lr=0.000020 grad_norm=1.370665
Epoch 76/100 Iteration 27/234: loss=0.035406 lr=0.000020 grad_norm=1.391246
Epoch 76/100 Iteration 28/234: loss=0.041849 lr=0.000020 grad_norm=1.083102
Epoch 76/100 Iteration 29/234: loss=0.040874 lr=0.000020 grad_norm=1.338954
Epoch 76/100 Iteration 30/234: loss=0.037217 lr=0.000020 grad_norm=1.069420
Epoch 76/100 Iteration 31/234: loss=0.038796 lr=0.000020 grad_norm=1.041636
Epoch 76/100 Iteration 32/234: loss=0.039359 lr=0.000020 grad_norm=0.757300
Epoch 76/100 Iteration 33/234: loss=0.039020 lr=0.000020 grad_norm=0.937327
Epoch 76/100 Iteration 34/234: loss=0.035836 lr=0.000020 grad_norm=1.009127
Epoch 76/100 Iteration 35/234: loss=0.040952 lr=0.000020 grad_norm=0.908938
Epoch 76/100 Iteration 36/234: loss=0.031653 lr=0.000020 grad_norm=0.897088
Epoch 76/100 Iteration 37/234: loss=0.038660 lr=0.000020 grad_norm=0.554921
Epoch 76/100 Iteration 38/234: loss=0.038031 lr=0.000020 grad_norm=1.176958
Epoch 76/100 Iteration 39/234: loss=0.037907 lr=0.000020 grad_norm=1.405563
Epoch 76/100 Iteration 40/234: loss=0.037062 lr=0.000020 grad_norm=0.638928
Epoch 76/100 Iteration 41/234: loss=0.042229 lr=0.000020 grad_norm=1.012075
Epoch 76/100 Iteration 42/234: loss=0.036087 lr=0.000020 grad_norm=1.650660
Epoch 76/100 Iteration 43/234: loss=0.031949 lr=0.000020 grad_norm=0.830615
Epoch 76/100 Iteration 44/234: loss=0.039018 lr=0.000020 grad_norm=1.123944
Epoch 76/100 Iteration 45/234: loss=0.043900 lr=0.000020 grad_norm=1.892881
Epoch 76/100 Iteration 46/234: loss=0.045215 lr=0.000020 grad_norm=1.221204
Epoch 76/100 Iteration 47/234: loss=0.041902 lr=0.000020 grad_norm=0.492183
Epoch 76/100 Iteration 48/234: loss=0.037120 lr=0.000020 grad_norm=0.767061
Epoch 76/100 Iteration 49/234: loss=0.033218 lr=0.000020 grad_norm=0.699977
Epoch 76/100 Iteration 50/234: loss=0.035513 lr=0.000020 grad_norm=0.405101
Epoch 76/100 Iteration 51/234: loss=0.040615 lr=0.000020 grad_norm=0.968127
Epoch 76/100 Iteration 52/234: loss=0.036293 lr=0.000020 grad_norm=0.787637
Epoch 76/100 Iteration 53/234: loss=0.039909 lr=0.000020 grad_norm=0.456463
Epoch 76/100 Iteration 54/234: loss=0.037702 lr=0.000020 grad_norm=0.781458
Epoch 76/100 Iteration 55/234: loss=0.039894 lr=0.000020 grad_norm=0.834889
Epoch 76/100 Iteration 56/234: loss=0.041770 lr=0.000020 grad_norm=0.786246
Epoch 76/100 Iteration 57/234: loss=0.037284 lr=0.000020 grad_norm=0.549284
Epoch 76/100 Iteration 58/234: loss=0.037463 lr=0.000020 grad_norm=0.947553
Epoch 76/100 Iteration 59/234: loss=0.039550 lr=0.000020 grad_norm=0.820274
Epoch 76/100 Iteration 60/234: loss=0.037269 lr=0.000020 grad_norm=0.623271
Epoch 76/100 Iteration 61/234: loss=0.041912 lr=0.000020 grad_norm=1.099565
Epoch 76/100 Iteration 62/234: loss=0.042027 lr=0.000020 grad_norm=0.857726
Epoch 76/100 Iteration 63/234: loss=0.041388 lr=0.000020 grad_norm=0.674572
Epoch 76/100 Iteration 64/234: loss=0.039972 lr=0.000020 grad_norm=1.355405
Epoch 76/100 Iteration 65/234: loss=0.036432 lr=0.000020 grad_norm=1.228955
Epoch 76/100 Iteration 66/234: loss=0.039553 lr=0.000020 grad_norm=0.453991
Epoch 76/100 Iteration 67/234: loss=0.032613 lr=0.000020 grad_norm=1.395127
Epoch 76/100 Iteration 68/234: loss=0.035328 lr=0.000020 grad_norm=1.085006
Epoch 76/100 Iteration 69/234: loss=0.038236 lr=0.000020 grad_norm=0.507034
Epoch 76/100 Iteration 70/234: loss=0.033202 lr=0.000020 grad_norm=0.920355
Epoch 76/100 Iteration 71/234: loss=0.040615 lr=0.000020 grad_norm=0.551154
Epoch 76/100 Iteration 72/234: loss=0.039138 lr=0.000020 grad_norm=1.200427
Epoch 76/100 Iteration 73/234: loss=0.041053 lr=0.000020 grad_norm=1.587132
Epoch 76/100 Iteration 74/234: loss=0.038872 lr=0.000020 grad_norm=1.109408
Epoch 76/100 Iteration 75/234: loss=0.035934 lr=0.000020 grad_norm=0.402325
Epoch 76/100 Iteration 76/234: loss=0.040964 lr=0.000020 grad_norm=1.189089
Epoch 76/100 Iteration 77/234: loss=0.038026 lr=0.000020 grad_norm=0.774519
Epoch 76/100 Iteration 78/234: loss=0.038258 lr=0.000020 grad_norm=0.939048
Epoch 76/100 Iteration 79/234: loss=0.038321 lr=0.000020 grad_norm=1.230121
Epoch 76/100 Iteration 80/234: loss=0.036175 lr=0.000020 grad_norm=0.659442
Epoch 76/100 Iteration 81/234: loss=0.039646 lr=0.000020 grad_norm=1.237527
Epoch 76/100 Iteration 82/234: loss=0.034375 lr=0.000020 grad_norm=1.112700
Epoch 76/100 Iteration 83/234: loss=0.034432 lr=0.000020 grad_norm=0.939703
Epoch 76/100 Iteration 84/234: loss=0.037352 lr=0.000020 grad_norm=1.198544
Epoch 76/100 Iteration 85/234: loss=0.039152 lr=0.000020 grad_norm=0.786532
Epoch 76/100 Iteration 86/234: loss=0.036591 lr=0.000020 grad_norm=1.710444
Epoch 76/100 Iteration 87/234: loss=0.039838 lr=0.000020 grad_norm=0.604887
Epoch 76/100 Iteration 88/234: loss=0.041251 lr=0.000020 grad_norm=1.593040
Epoch 76/100 Iteration 89/234: loss=0.036080 lr=0.000020 grad_norm=0.686579
Epoch 76/100 Iteration 90/234: loss=0.037161 lr=0.000020 grad_norm=1.067535
Epoch 76/100 Iteration 91/234: loss=0.038944 lr=0.000020 grad_norm=0.677074
Epoch 76/100 Iteration 92/234: loss=0.037919 lr=0.000020 grad_norm=0.737578
Epoch 76/100 Iteration 93/234: loss=0.038660 lr=0.000020 grad_norm=0.497494
Epoch 76/100 Iteration 94/234: loss=0.039452 lr=0.000020 grad_norm=0.859796
Epoch 76/100 Iteration 95/234: loss=0.035926 lr=0.000020 grad_norm=0.746473
Epoch 76/100 Iteration 96/234: loss=0.037096 lr=0.000020 grad_norm=0.535169
Epoch 76/100 Iteration 97/234: loss=0.034435 lr=0.000020 grad_norm=0.649167
Epoch 76/100 Iteration 98/234: loss=0.033598 lr=0.000020 grad_norm=0.417192
Epoch 76/100 Iteration 99/234: loss=0.037749 lr=0.000020 grad_norm=0.532362
Epoch 76/100 Iteration 100/234: loss=0.035643 lr=0.000020 grad_norm=0.448799
Epoch 76/100 Iteration 101/234: loss=0.037376 lr=0.000020 grad_norm=0.523286
Epoch 76/100 Iteration 102/234: loss=0.038615 lr=0.000020 grad_norm=0.573663
Epoch 76/100 Iteration 103/234: loss=0.037184 lr=0.000020 grad_norm=0.451115
Epoch 76/100 Iteration 104/234: loss=0.041413 lr=0.000020 grad_norm=0.444480
Epoch 76/100 Iteration 105/234: loss=0.037336 lr=0.000020 grad_norm=0.376858
Epoch 76/100 Iteration 106/234: loss=0.035785 lr=0.000020 grad_norm=0.386823
Epoch 76/100 Iteration 107/234: loss=0.037642 lr=0.000020 grad_norm=0.455916
Epoch 76/100 Iteration 108/234: loss=0.037128 lr=0.000020 grad_norm=0.286480
Epoch 76/100 Iteration 109/234: loss=0.039897 lr=0.000020 grad_norm=0.501514
Epoch 76/100 Iteration 110/234: loss=0.038281 lr=0.000020 grad_norm=0.408498
Epoch 76/100 Iteration 111/234: loss=0.038923 lr=0.000020 grad_norm=0.445729
Epoch 76/100 Iteration 112/234: loss=0.035416 lr=0.000020 grad_norm=0.407836
Epoch 76/100 Iteration 113/234: loss=0.037573 lr=0.000020 grad_norm=0.458662
Epoch 76/100 Iteration 114/234: loss=0.038958 lr=0.000020 grad_norm=0.825279
Epoch 76/100 Iteration 115/234: loss=0.041825 lr=0.000020 grad_norm=0.664682
Epoch 76/100 Iteration 116/234: loss=0.039663 lr=0.000020 grad_norm=0.703089
Epoch 76/100 Iteration 117/234: loss=0.038208 lr=0.000020 grad_norm=0.607171
Epoch 76/100 Iteration 118/234: loss=0.039802 lr=0.000020 grad_norm=0.907443
Epoch 76/100 Iteration 119/234: loss=0.035109 lr=0.000020 grad_norm=0.749725
Epoch 76/100 Iteration 120/234: loss=0.039128 lr=0.000020 grad_norm=0.425994
Epoch 76/100 Iteration 121/234: loss=0.037822 lr=0.000020 grad_norm=0.818962
Epoch 76/100 Iteration 122/234: loss=0.037591 lr=0.000020 grad_norm=0.896569
Epoch 76/100 Iteration 123/234: loss=0.040854 lr=0.000020 grad_norm=0.508038
Epoch 76/100 Iteration 124/234: loss=0.036832 lr=0.000020 grad_norm=0.456143
Epoch 76/100 Iteration 125/234: loss=0.039098 lr=0.000020 grad_norm=0.351380
Epoch 76/100 Iteration 126/234: loss=0.037323 lr=0.000020 grad_norm=0.446868
Epoch 76/100 Iteration 127/234: loss=0.038063 lr=0.000020 grad_norm=0.360482
Epoch 76/100 Iteration 128/234: loss=0.035444 lr=0.000020 grad_norm=0.415593
Epoch 76/100 Iteration 129/234: loss=0.039403 lr=0.000020 grad_norm=0.454175
Epoch 76/100 Iteration 130/234: loss=0.036075 lr=0.000020 grad_norm=0.670387
Epoch 76/100 Iteration 131/234: loss=0.037235 lr=0.000020 grad_norm=0.551965
Epoch 76/100 Iteration 132/234: loss=0.042736 lr=0.000020 grad_norm=0.713821
Epoch 76/100 Iteration 133/234: loss=0.033273 lr=0.000020 grad_norm=1.162746
Epoch 76/100 Iteration 134/234: loss=0.038776 lr=0.000020 grad_norm=0.784970
Epoch 76/100 Iteration 135/234: loss=0.041416 lr=0.000020 grad_norm=0.916277
Epoch 76/100 Iteration 136/234: loss=0.034435 lr=0.000020 grad_norm=0.715920
Epoch 76/100 Iteration 137/234: loss=0.035138 lr=0.000020 grad_norm=0.554064
Epoch 76/100 Iteration 138/234: loss=0.037106 lr=0.000020 grad_norm=1.038977
Epoch 76/100 Iteration 139/234: loss=0.040480 lr=0.000020 grad_norm=0.605010
Epoch 76/100 Iteration 140/234: loss=0.040861 lr=0.000020 grad_norm=1.086380
Epoch 76/100 Iteration 141/234: loss=0.041047 lr=0.000020 grad_norm=1.431800
Epoch 76/100 Iteration 142/234: loss=0.040341 lr=0.000020 grad_norm=0.648279
Epoch 76/100 Iteration 143/234: loss=0.038141 lr=0.000020 grad_norm=1.363208
Epoch 76/100 Iteration 144/234: loss=0.035558 lr=0.000020 grad_norm=1.386929
Epoch 76/100 Iteration 145/234: loss=0.034627 lr=0.000020 grad_norm=0.647092
Epoch 76/100 Iteration 146/234: loss=0.033618 lr=0.000020 grad_norm=1.071735
Epoch 76/100 Iteration 147/234: loss=0.036191 lr=0.000020 grad_norm=0.729250
Epoch 76/100 Iteration 148/234: loss=0.035600 lr=0.000020 grad_norm=1.121456
Epoch 76/100 Iteration 149/234: loss=0.036664 lr=0.000020 grad_norm=0.605102
Epoch 76/100 Iteration 150/234: loss=0.039786 lr=0.000020 grad_norm=1.025118
Epoch 76/100 Iteration 151/234: loss=0.039113 lr=0.000020 grad_norm=0.872073
Epoch 76/100 Iteration 152/234: loss=0.037355 lr=0.000020 grad_norm=0.608694
Epoch 76/100 Iteration 153/234: loss=0.042999 lr=0.000020 grad_norm=1.105258
Epoch 76/100 Iteration 154/234: loss=0.035973 lr=0.000020 grad_norm=0.751886
Epoch 76/100 Iteration 155/234: loss=0.033725 lr=0.000020 grad_norm=0.645547
Epoch 76/100 Iteration 156/234: loss=0.035725 lr=0.000020 grad_norm=1.171795
Epoch 76/100 Iteration 157/234: loss=0.038974 lr=0.000020 grad_norm=0.648332
Epoch 76/100 Iteration 158/234: loss=0.036850 lr=0.000020 grad_norm=0.610955
Epoch 76/100 Iteration 159/234: loss=0.037165 lr=0.000020 grad_norm=0.880141
Epoch 76/100 Iteration 160/234: loss=0.036558 lr=0.000020 grad_norm=0.575544
Epoch 76/100 Iteration 161/234: loss=0.038930 lr=0.000020 grad_norm=0.835652
Epoch 76/100 Iteration 162/234: loss=0.036888 lr=0.000020 grad_norm=0.627464
Epoch 76/100 Iteration 163/234: loss=0.042212 lr=0.000020 grad_norm=0.541101
Epoch 76/100 Iteration 164/234: loss=0.033862 lr=0.000020 grad_norm=0.771793
Epoch 76/100 Iteration 165/234: loss=0.036162 lr=0.000020 grad_norm=0.570238
Epoch 76/100 Iteration 166/234: loss=0.040078 lr=0.000020 grad_norm=0.718449
Epoch 76/100 Iteration 167/234: loss=0.040140 lr=0.000020 grad_norm=0.760071
Epoch 76/100 Iteration 168/234: loss=0.032222 lr=0.000020 grad_norm=0.426301
Epoch 76/100 Iteration 169/234: loss=0.038931 lr=0.000020 grad_norm=0.686270
Epoch 76/100 Iteration 170/234: loss=0.039390 lr=0.000020 grad_norm=0.795830
Epoch 76/100 Iteration 171/234: loss=0.039433 lr=0.000020 grad_norm=0.491249
Epoch 76/100 Iteration 172/234: loss=0.037229 lr=0.000020 grad_norm=0.863590
Epoch 76/100 Iteration 173/234: loss=0.038839 lr=0.000020 grad_norm=1.191767
Epoch 76/100 Iteration 174/234: loss=0.038236 lr=0.000020 grad_norm=0.753047
Epoch 76/100 Iteration 175/234: loss=0.037078 lr=0.000020 grad_norm=0.657339
Epoch 76/100 Iteration 176/234: loss=0.035561 lr=0.000020 grad_norm=1.066079
Epoch 76/100 Iteration 177/234: loss=0.037848 lr=0.000020 grad_norm=0.654464
Epoch 76/100 Iteration 178/234: loss=0.040114 lr=0.000020 grad_norm=1.152881
Epoch 76/100 Iteration 179/234: loss=0.037495 lr=0.000020 grad_norm=1.419275
Epoch 76/100 Iteration 180/234: loss=0.041172 lr=0.000020 grad_norm=0.678407
Epoch 76/100 Iteration 181/234: loss=0.041334 lr=0.000020 grad_norm=1.089705
Epoch 76/100 Iteration 182/234: loss=0.040105 lr=0.000020 grad_norm=1.146727
Epoch 76/100 Iteration 183/234: loss=0.036646 lr=0.000020 grad_norm=0.710600
Epoch 76/100 Iteration 184/234: loss=0.040816 lr=0.000020 grad_norm=0.673708
Epoch 76/100 Iteration 185/234: loss=0.036926 lr=0.000020 grad_norm=1.070471
Epoch 76/100 Iteration 186/234: loss=0.032343 lr=0.000020 grad_norm=0.506923
Epoch 76/100 Iteration 187/234: loss=0.040162 lr=0.000020 grad_norm=0.965399
Epoch 76/100 Iteration 188/234: loss=0.037699 lr=0.000020 grad_norm=1.312500
Epoch 76/100 Iteration 189/234: loss=0.033857 lr=0.000020 grad_norm=0.981826
Epoch 76/100 Iteration 190/234: loss=0.035497 lr=0.000020 grad_norm=0.423304
Epoch 76/100 Iteration 191/234: loss=0.034276 lr=0.000020 grad_norm=0.825673
Epoch 76/100 Iteration 192/234: loss=0.034293 lr=0.000020 grad_norm=0.643517
Epoch 76/100 Iteration 193/234: loss=0.034244 lr=0.000020 grad_norm=0.554615
Epoch 76/100 Iteration 194/234: loss=0.037091 lr=0.000020 grad_norm=0.915782
Epoch 76/100 Iteration 195/234: loss=0.037362 lr=0.000020 grad_norm=0.618256
Epoch 76/100 Iteration 196/234: loss=0.031974 lr=0.000020 grad_norm=0.522367
Epoch 76/100 Iteration 197/234: loss=0.037234 lr=0.000020 grad_norm=0.800140
Epoch 76/100 Iteration 198/234: loss=0.039512 lr=0.000020 grad_norm=1.023909
Epoch 76/100 Iteration 199/234: loss=0.041654 lr=0.000020 grad_norm=1.138378
Epoch 76/100 Iteration 200/234: loss=0.035542 lr=0.000020 grad_norm=0.550215
Epoch 76/100 Iteration 201/234: loss=0.037577 lr=0.000020 grad_norm=0.856752
Epoch 76/100 Iteration 202/234: loss=0.034845 lr=0.000020 grad_norm=0.833204
Epoch 76/100 Iteration 203/234: loss=0.035618 lr=0.000020 grad_norm=0.632081
Epoch 76/100 Iteration 204/234: loss=0.037655 lr=0.000020 grad_norm=0.879944
Epoch 76/100 Iteration 205/234: loss=0.036424 lr=0.000020 grad_norm=0.525162
Epoch 76/100 Iteration 206/234: loss=0.040145 lr=0.000020 grad_norm=0.523123
Epoch 76/100 Iteration 207/234: loss=0.037622 lr=0.000020 grad_norm=0.629586
Epoch 76/100 Iteration 208/234: loss=0.037981 lr=0.000020 grad_norm=0.617337
Epoch 76/100 Iteration 209/234: loss=0.038834 lr=0.000020 grad_norm=0.610439
Epoch 76/100 Iteration 210/234: loss=0.035251 lr=0.000020 grad_norm=0.541706
Epoch 76/100 Iteration 211/234: loss=0.038238 lr=0.000020 grad_norm=0.495560
Epoch 76/100 Iteration 212/234: loss=0.036216 lr=0.000020 grad_norm=0.547695
Epoch 76/100 Iteration 213/234: loss=0.035501 lr=0.000020 grad_norm=0.571793
Epoch 76/100 Iteration 214/234: loss=0.036833 lr=0.000020 grad_norm=0.425805
Epoch 76/100 Iteration 215/234: loss=0.036577 lr=0.000020 grad_norm=0.884856
Epoch 76/100 Iteration 216/234: loss=0.040290 lr=0.000020 grad_norm=1.012706
Epoch 76/100 Iteration 217/234: loss=0.042306 lr=0.000020 grad_norm=0.719716
Epoch 76/100 Iteration 218/234: loss=0.035306 lr=0.000020 grad_norm=0.722887
Epoch 76/100 Iteration 219/234: loss=0.035503 lr=0.000020 grad_norm=0.390207
Epoch 76/100 Iteration 220/234: loss=0.032567 lr=0.000020 grad_norm=0.750274
Epoch 76/100 Iteration 221/234: loss=0.034264 lr=0.000020 grad_norm=0.514245
Epoch 76/100 Iteration 222/234: loss=0.040456 lr=0.000020 grad_norm=1.252865
Epoch 76/100 Iteration 223/234: loss=0.038309 lr=0.000020 grad_norm=1.531958
Epoch 76/100 Iteration 224/234: loss=0.038073 lr=0.000020 grad_norm=0.507676
Epoch 76/100 Iteration 225/234: loss=0.037288 lr=0.000020 grad_norm=1.340614
Epoch 76/100 Iteration 226/234: loss=0.033569 lr=0.000020 grad_norm=1.211627
Epoch 76/100 Iteration 227/234: loss=0.034526 lr=0.000020 grad_norm=0.706212
Epoch 76/100 Iteration 228/234: loss=0.035022 lr=0.000020 grad_norm=1.316457
Epoch 76/100 Iteration 229/234: loss=0.038346 lr=0.000020 grad_norm=0.517369
Epoch 76/100 Iteration 230/234: loss=0.035193 lr=0.000020 grad_norm=1.300499
Epoch 76/100 Iteration 231/234: loss=0.036237 lr=0.000020 grad_norm=0.478710
Epoch 76/100 Iteration 232/234: loss=0.033564 lr=0.000020 grad_norm=1.337384
Epoch 76/100 Iteration 233/234: loss=0.036927 lr=0.000020 grad_norm=1.059952
Epoch 76/100 Iteration 234/234: loss=0.034952 lr=0.000020 grad_norm=1.014994
Epoch 76/100 finished. Avg Loss: 0.037715
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 77/100 Iteration 1/234: loss=0.035782 lr=0.000020 grad_norm=1.415532
Epoch 77/100 Iteration 2/234: loss=0.035200 lr=0.000020 grad_norm=0.429874
Epoch 77/100 Iteration 3/234: loss=0.040836 lr=0.000020 grad_norm=0.843692
Epoch 77/100 Iteration 4/234: loss=0.036705 lr=0.000020 grad_norm=0.904537
Epoch 77/100 Iteration 5/234: loss=0.034905 lr=0.000020 grad_norm=0.713786
Epoch 77/100 Iteration 6/234: loss=0.033652 lr=0.000020 grad_norm=0.446844
Epoch 77/100 Iteration 7/234: loss=0.036521 lr=0.000020 grad_norm=0.525024
Epoch 77/100 Iteration 8/234: loss=0.036310 lr=0.000020 grad_norm=0.689857
Epoch 77/100 Iteration 9/234: loss=0.037320 lr=0.000020 grad_norm=0.568781
Epoch 77/100 Iteration 10/234: loss=0.036682 lr=0.000020 grad_norm=0.424334
Epoch 77/100 Iteration 11/234: loss=0.036557 lr=0.000020 grad_norm=0.321353
Epoch 77/100 Iteration 12/234: loss=0.038631 lr=0.000020 grad_norm=0.399922
Epoch 77/100 Iteration 13/234: loss=0.036778 lr=0.000020 grad_norm=0.530632
Epoch 77/100 Iteration 14/234: loss=0.040741 lr=0.000020 grad_norm=0.614914
Epoch 77/100 Iteration 15/234: loss=0.036257 lr=0.000020 grad_norm=0.612956
Epoch 77/100 Iteration 16/234: loss=0.034259 lr=0.000020 grad_norm=0.889416
Epoch 77/100 Iteration 17/234: loss=0.035113 lr=0.000020 grad_norm=0.866541
Epoch 77/100 Iteration 18/234: loss=0.040997 lr=0.000020 grad_norm=0.609611
Epoch 77/100 Iteration 19/234: loss=0.034197 lr=0.000020 grad_norm=0.468405
Epoch 77/100 Iteration 20/234: loss=0.037999 lr=0.000020 grad_norm=0.636704
Epoch 77/100 Iteration 21/234: loss=0.036703 lr=0.000020 grad_norm=1.004418
Epoch 77/100 Iteration 22/234: loss=0.038004 lr=0.000020 grad_norm=0.624834
Epoch 77/100 Iteration 23/234: loss=0.034298 lr=0.000020 grad_norm=0.843301
Epoch 77/100 Iteration 24/234: loss=0.039555 lr=0.000020 grad_norm=1.150602
Epoch 77/100 Iteration 25/234: loss=0.037805 lr=0.000020 grad_norm=0.557600
Epoch 77/100 Iteration 26/234: loss=0.038981 lr=0.000020 grad_norm=1.482425
Epoch 77/100 Iteration 27/234: loss=0.037432 lr=0.000020 grad_norm=1.633458
Epoch 77/100 Iteration 28/234: loss=0.038487 lr=0.000020 grad_norm=0.456633
Epoch 77/100 Iteration 29/234: loss=0.036473 lr=0.000020 grad_norm=1.243118
Epoch 77/100 Iteration 30/234: loss=0.040644 lr=0.000020 grad_norm=0.685004
Epoch 77/100 Iteration 31/234: loss=0.038290 lr=0.000020 grad_norm=1.296367
Epoch 77/100 Iteration 32/234: loss=0.038749 lr=0.000020 grad_norm=1.498136
Epoch 77/100 Iteration 33/234: loss=0.035968 lr=0.000020 grad_norm=0.830820
Epoch 77/100 Iteration 34/234: loss=0.034448 lr=0.000020 grad_norm=0.765368
Epoch 77/100 Iteration 35/234: loss=0.036404 lr=0.000020 grad_norm=0.592490
Epoch 77/100 Iteration 36/234: loss=0.035702 lr=0.000020 grad_norm=0.792985
Epoch 77/100 Iteration 37/234: loss=0.038314 lr=0.000020 grad_norm=0.472858
Epoch 77/100 Iteration 38/234: loss=0.039705 lr=0.000020 grad_norm=0.778803
Epoch 77/100 Iteration 39/234: loss=0.038807 lr=0.000020 grad_norm=0.730940
Epoch 77/100 Iteration 40/234: loss=0.038844 lr=0.000020 grad_norm=0.614181
Epoch 77/100 Iteration 41/234: loss=0.040128 lr=0.000020 grad_norm=1.250435
Epoch 77/100 Iteration 42/234: loss=0.038985 lr=0.000020 grad_norm=1.143721
Epoch 77/100 Iteration 43/234: loss=0.037332 lr=0.000020 grad_norm=0.462938
Epoch 77/100 Iteration 44/234: loss=0.037377 lr=0.000020 grad_norm=0.752370
Epoch 77/100 Iteration 45/234: loss=0.037216 lr=0.000020 grad_norm=0.806471
Epoch 77/100 Iteration 46/234: loss=0.041346 lr=0.000020 grad_norm=0.605121
Epoch 77/100 Iteration 47/234: loss=0.034896 lr=0.000020 grad_norm=0.956391
Epoch 77/100 Iteration 48/234: loss=0.037844 lr=0.000020 grad_norm=0.687411
Epoch 77/100 Iteration 49/234: loss=0.037379 lr=0.000020 grad_norm=0.689695
Epoch 77/100 Iteration 50/234: loss=0.038196 lr=0.000020 grad_norm=0.707315
Epoch 77/100 Iteration 51/234: loss=0.034928 lr=0.000020 grad_norm=0.811536
Epoch 77/100 Iteration 52/234: loss=0.039119 lr=0.000020 grad_norm=0.754951
Epoch 77/100 Iteration 53/234: loss=0.038144 lr=0.000020 grad_norm=0.616565
Epoch 77/100 Iteration 54/234: loss=0.040504 lr=0.000020 grad_norm=1.266771
Epoch 77/100 Iteration 55/234: loss=0.037126 lr=0.000020 grad_norm=1.131813
Epoch 77/100 Iteration 56/234: loss=0.035641 lr=0.000020 grad_norm=0.696643
Epoch 77/100 Iteration 57/234: loss=0.039893 lr=0.000020 grad_norm=0.698550
Epoch 77/100 Iteration 58/234: loss=0.039852 lr=0.000020 grad_norm=0.583995
Epoch 77/100 Iteration 59/234: loss=0.036045 lr=0.000020 grad_norm=0.834860
Epoch 77/100 Iteration 60/234: loss=0.040003 lr=0.000020 grad_norm=0.619414
Epoch 77/100 Iteration 61/234: loss=0.039103 lr=0.000020 grad_norm=1.111075
Epoch 77/100 Iteration 62/234: loss=0.034126 lr=0.000020 grad_norm=1.549200
Epoch 77/100 Iteration 63/234: loss=0.037931 lr=0.000020 grad_norm=0.491443
Epoch 77/100 Iteration 64/234: loss=0.037930 lr=0.000020 grad_norm=1.347705
Epoch 77/100 Iteration 65/234: loss=0.035954 lr=0.000020 grad_norm=1.155890
Epoch 77/100 Iteration 66/234: loss=0.039859 lr=0.000020 grad_norm=0.792100
Epoch 77/100 Iteration 67/234: loss=0.036783 lr=0.000020 grad_norm=1.054879
Epoch 77/100 Iteration 68/234: loss=0.039920 lr=0.000020 grad_norm=0.691846
Epoch 77/100 Iteration 69/234: loss=0.039933 lr=0.000020 grad_norm=0.969671
Epoch 77/100 Iteration 70/234: loss=0.037902 lr=0.000020 grad_norm=0.603395
Epoch 77/100 Iteration 71/234: loss=0.036017 lr=0.000020 grad_norm=0.741052
Epoch 77/100 Iteration 72/234: loss=0.039668 lr=0.000020 grad_norm=0.715163
Epoch 77/100 Iteration 73/234: loss=0.035602 lr=0.000020 grad_norm=0.442243
Epoch 77/100 Iteration 74/234: loss=0.029889 lr=0.000020 grad_norm=0.627287
Epoch 77/100 Iteration 75/234: loss=0.038576 lr=0.000020 grad_norm=0.498940
Epoch 77/100 Iteration 76/234: loss=0.034653 lr=0.000020 grad_norm=0.832573
Epoch 77/100 Iteration 77/234: loss=0.037135 lr=0.000020 grad_norm=0.760138
Epoch 77/100 Iteration 78/234: loss=0.037068 lr=0.000020 grad_norm=0.445082
Epoch 77/100 Iteration 79/234: loss=0.040993 lr=0.000020 grad_norm=0.786649
Epoch 77/100 Iteration 80/234: loss=0.036872 lr=0.000020 grad_norm=0.579701
Epoch 77/100 Iteration 81/234: loss=0.040595 lr=0.000020 grad_norm=0.554732
Epoch 77/100 Iteration 82/234: loss=0.037395 lr=0.000020 grad_norm=0.483245
Epoch 77/100 Iteration 83/234: loss=0.034932 lr=0.000020 grad_norm=0.397854
Epoch 77/100 Iteration 84/234: loss=0.039066 lr=0.000020 grad_norm=0.498101
Epoch 77/100 Iteration 85/234: loss=0.032598 lr=0.000020 grad_norm=0.422295
Epoch 77/100 Iteration 86/234: loss=0.037829 lr=0.000020 grad_norm=0.529799
Epoch 77/100 Iteration 87/234: loss=0.036859 lr=0.000020 grad_norm=0.611783
Epoch 77/100 Iteration 88/234: loss=0.032079 lr=0.000020 grad_norm=0.534768
Epoch 77/100 Iteration 89/234: loss=0.037878 lr=0.000020 grad_norm=0.516402
Epoch 77/100 Iteration 90/234: loss=0.041234 lr=0.000020 grad_norm=0.588113
Epoch 77/100 Iteration 91/234: loss=0.038470 lr=0.000020 grad_norm=0.614713
Epoch 77/100 Iteration 92/234: loss=0.034708 lr=0.000020 grad_norm=0.501068
Epoch 77/100 Iteration 93/234: loss=0.045306 lr=0.000020 grad_norm=0.565039
Epoch 77/100 Iteration 94/234: loss=0.033133 lr=0.000020 grad_norm=0.795516
Epoch 77/100 Iteration 95/234: loss=0.035910 lr=0.000020 grad_norm=0.609000
Epoch 77/100 Iteration 96/234: loss=0.039488 lr=0.000020 grad_norm=0.420686
Epoch 77/100 Iteration 97/234: loss=0.037578 lr=0.000020 grad_norm=0.599586
Epoch 77/100 Iteration 98/234: loss=0.038888 lr=0.000020 grad_norm=0.753941
Epoch 77/100 Iteration 99/234: loss=0.031682 lr=0.000020 grad_norm=1.101094
Epoch 77/100 Iteration 100/234: loss=0.041924 lr=0.000020 grad_norm=0.812695
Epoch 77/100 Iteration 101/234: loss=0.041102 lr=0.000020 grad_norm=2.010816
Epoch 77/100 Iteration 102/234: loss=0.038177 lr=0.000020 grad_norm=1.247856
Epoch 77/100 Iteration 103/234: loss=0.037005 lr=0.000020 grad_norm=0.975933
Epoch 77/100 Iteration 104/234: loss=0.036363 lr=0.000020 grad_norm=1.753221
Epoch 77/100 Iteration 105/234: loss=0.033344 lr=0.000020 grad_norm=0.446551
Epoch 77/100 Iteration 106/234: loss=0.039398 lr=0.000020 grad_norm=1.460607
Epoch 77/100 Iteration 107/234: loss=0.033124 lr=0.000020 grad_norm=1.169402
Epoch 77/100 Iteration 108/234: loss=0.039033 lr=0.000020 grad_norm=1.209475
Epoch 77/100 Iteration 109/234: loss=0.036459 lr=0.000020 grad_norm=1.335141
Epoch 77/100 Iteration 110/234: loss=0.034654 lr=0.000020 grad_norm=0.666452
Epoch 77/100 Iteration 111/234: loss=0.036706 lr=0.000020 grad_norm=1.874134
Epoch 77/100 Iteration 112/234: loss=0.038022 lr=0.000020 grad_norm=1.556600
Epoch 77/100 Iteration 113/234: loss=0.040916 lr=0.000020 grad_norm=1.043109
Epoch 77/100 Iteration 114/234: loss=0.040837 lr=0.000020 grad_norm=2.045271
Epoch 77/100 Iteration 115/234: loss=0.034722 lr=0.000020 grad_norm=1.137656
Epoch 77/100 Iteration 116/234: loss=0.034293 lr=0.000020 grad_norm=1.362191
Epoch 77/100 Iteration 117/234: loss=0.038258 lr=0.000020 grad_norm=0.786039
Epoch 77/100 Iteration 118/234: loss=0.037730 lr=0.000020 grad_norm=1.154061
Epoch 77/100 Iteration 119/234: loss=0.040227 lr=0.000020 grad_norm=0.938462
Epoch 77/100 Iteration 120/234: loss=0.035919 lr=0.000020 grad_norm=1.108056
Epoch 77/100 Iteration 121/234: loss=0.034537 lr=0.000020 grad_norm=1.386187
Epoch 77/100 Iteration 122/234: loss=0.039075 lr=0.000020 grad_norm=0.665252
Epoch 77/100 Iteration 123/234: loss=0.040871 lr=0.000020 grad_norm=1.561403
Epoch 77/100 Iteration 124/234: loss=0.040934 lr=0.000020 grad_norm=0.519512
Epoch 77/100 Iteration 125/234: loss=0.038322 lr=0.000020 grad_norm=1.170505
Epoch 77/100 Iteration 126/234: loss=0.040512 lr=0.000020 grad_norm=0.461716
Epoch 77/100 Iteration 127/234: loss=0.041676 lr=0.000020 grad_norm=1.057289
Epoch 77/100 Iteration 128/234: loss=0.039384 lr=0.000020 grad_norm=0.677848
Epoch 77/100 Iteration 129/234: loss=0.036632 lr=0.000020 grad_norm=0.967211
Epoch 77/100 Iteration 130/234: loss=0.039383 lr=0.000020 grad_norm=0.838988
Epoch 77/100 Iteration 131/234: loss=0.038725 lr=0.000020 grad_norm=1.282101
Epoch 77/100 Iteration 132/234: loss=0.039421 lr=0.000020 grad_norm=1.870146
Epoch 77/100 Iteration 133/234: loss=0.037624 lr=0.000020 grad_norm=0.619180
Epoch 77/100 Iteration 134/234: loss=0.034421 lr=0.000020 grad_norm=0.974778
Epoch 77/100 Iteration 135/234: loss=0.042480 lr=0.000020 grad_norm=0.629604
Epoch 77/100 Iteration 136/234: loss=0.035078 lr=0.000020 grad_norm=1.472191
Epoch 77/100 Iteration 137/234: loss=0.039192 lr=0.000020 grad_norm=0.576825
Epoch 77/100 Iteration 138/234: loss=0.037001 lr=0.000020 grad_norm=1.147582
Epoch 77/100 Iteration 139/234: loss=0.034382 lr=0.000020 grad_norm=0.563678
Epoch 77/100 Iteration 140/234: loss=0.038288 lr=0.000020 grad_norm=1.253504
Epoch 77/100 Iteration 141/234: loss=0.039766 lr=0.000020 grad_norm=0.435420
Epoch 77/100 Iteration 142/234: loss=0.037930 lr=0.000020 grad_norm=1.432245
Epoch 77/100 Iteration 143/234: loss=0.033244 lr=0.000020 grad_norm=0.545832
Epoch 77/100 Iteration 144/234: loss=0.039840 lr=0.000020 grad_norm=1.327630
Epoch 77/100 Iteration 145/234: loss=0.041904 lr=0.000020 grad_norm=1.097441
Epoch 77/100 Iteration 146/234: loss=0.042184 lr=0.000020 grad_norm=1.356551
Epoch 77/100 Iteration 147/234: loss=0.037406 lr=0.000020 grad_norm=1.243913
Epoch 77/100 Iteration 148/234: loss=0.037761 lr=0.000020 grad_norm=0.409450
Epoch 77/100 Iteration 149/234: loss=0.039409 lr=0.000020 grad_norm=1.164583
Epoch 77/100 Iteration 150/234: loss=0.035812 lr=0.000020 grad_norm=0.787174
Epoch 77/100 Iteration 151/234: loss=0.042168 lr=0.000020 grad_norm=1.156047
Epoch 77/100 Iteration 152/234: loss=0.037777 lr=0.000020 grad_norm=0.654817
Epoch 77/100 Iteration 153/234: loss=0.037382 lr=0.000020 grad_norm=0.931095
Epoch 77/100 Iteration 154/234: loss=0.037853 lr=0.000020 grad_norm=0.780115
Epoch 77/100 Iteration 155/234: loss=0.034494 lr=0.000020 grad_norm=0.671217
Epoch 77/100 Iteration 156/234: loss=0.035505 lr=0.000020 grad_norm=0.885243
Epoch 77/100 Iteration 157/234: loss=0.037045 lr=0.000020 grad_norm=0.583550
Epoch 77/100 Iteration 158/234: loss=0.033957 lr=0.000020 grad_norm=0.879291
Epoch 77/100 Iteration 159/234: loss=0.035290 lr=0.000020 grad_norm=1.091201
Epoch 77/100 Iteration 160/234: loss=0.037893 lr=0.000020 grad_norm=0.546091
Epoch 77/100 Iteration 161/234: loss=0.042065 lr=0.000020 grad_norm=0.880066
Epoch 77/100 Iteration 162/234: loss=0.043542 lr=0.000020 grad_norm=1.406031
Epoch 77/100 Iteration 163/234: loss=0.037843 lr=0.000020 grad_norm=1.396257
Epoch 77/100 Iteration 164/234: loss=0.041117 lr=0.000020 grad_norm=1.056827
Epoch 77/100 Iteration 165/234: loss=0.038585 lr=0.000020 grad_norm=0.579720
Epoch 77/100 Iteration 166/234: loss=0.039551 lr=0.000020 grad_norm=1.104288
Epoch 77/100 Iteration 167/234: loss=0.037517 lr=0.000020 grad_norm=1.181370
Epoch 77/100 Iteration 168/234: loss=0.040304 lr=0.000020 grad_norm=0.612887
Epoch 77/100 Iteration 169/234: loss=0.031549 lr=0.000020 grad_norm=0.942019
Epoch 77/100 Iteration 170/234: loss=0.038724 lr=0.000020 grad_norm=0.691090
Epoch 77/100 Iteration 171/234: loss=0.036528 lr=0.000020 grad_norm=0.603496
Epoch 77/100 Iteration 172/234: loss=0.042019 lr=0.000020 grad_norm=1.024876
Epoch 77/100 Iteration 173/234: loss=0.038428 lr=0.000020 grad_norm=0.970584
Epoch 77/100 Iteration 174/234: loss=0.036366 lr=0.000020 grad_norm=0.590035
Epoch 77/100 Iteration 175/234: loss=0.036693 lr=0.000020 grad_norm=0.659750
Epoch 77/100 Iteration 176/234: loss=0.033555 lr=0.000020 grad_norm=1.020847
Epoch 77/100 Iteration 177/234: loss=0.038982 lr=0.000020 grad_norm=0.578626
Epoch 77/100 Iteration 178/234: loss=0.037507 lr=0.000020 grad_norm=1.491684
Epoch 77/100 Iteration 179/234: loss=0.033484 lr=0.000020 grad_norm=0.775444
Epoch 77/100 Iteration 180/234: loss=0.042430 lr=0.000020 grad_norm=1.732933
Epoch 77/100 Iteration 181/234: loss=0.037112 lr=0.000020 grad_norm=2.953620
Epoch 77/100 Iteration 182/234: loss=0.034496 lr=0.000020 grad_norm=1.227897
Epoch 77/100 Iteration 183/234: loss=0.039061 lr=0.000020 grad_norm=1.868284
Epoch 77/100 Iteration 184/234: loss=0.038612 lr=0.000020 grad_norm=1.786030
Epoch 77/100 Iteration 185/234: loss=0.042168 lr=0.000020 grad_norm=1.513871
Epoch 77/100 Iteration 186/234: loss=0.039467 lr=0.000020 grad_norm=1.285728
Epoch 77/100 Iteration 187/234: loss=0.039147 lr=0.000020 grad_norm=1.620694
Epoch 77/100 Iteration 188/234: loss=0.042893 lr=0.000020 grad_norm=0.962588
Epoch 77/100 Iteration 189/234: loss=0.037859 lr=0.000020 grad_norm=1.168163
Epoch 77/100 Iteration 190/234: loss=0.036883 lr=0.000020 grad_norm=1.164436
Epoch 77/100 Iteration 191/234: loss=0.031981 lr=0.000020 grad_norm=0.598094
Epoch 77/100 Iteration 192/234: loss=0.040261 lr=0.000020 grad_norm=1.135253
Epoch 77/100 Iteration 193/234: loss=0.041464 lr=0.000020 grad_norm=0.381940
Epoch 77/100 Iteration 194/234: loss=0.039596 lr=0.000020 grad_norm=1.088703
Epoch 77/100 Iteration 195/234: loss=0.038777 lr=0.000020 grad_norm=0.596475
Epoch 77/100 Iteration 196/234: loss=0.035194 lr=0.000020 grad_norm=1.267087
Epoch 77/100 Iteration 197/234: loss=0.037936 lr=0.000020 grad_norm=0.531146
Epoch 77/100 Iteration 198/234: loss=0.038070 lr=0.000020 grad_norm=0.970996
Epoch 77/100 Iteration 199/234: loss=0.036371 lr=0.000020 grad_norm=0.591543
Epoch 77/100 Iteration 200/234: loss=0.040082 lr=0.000020 grad_norm=0.678278
Epoch 77/100 Iteration 201/234: loss=0.033266 lr=0.000020 grad_norm=0.439128
Epoch 77/100 Iteration 202/234: loss=0.038477 lr=0.000020 grad_norm=1.096509
Epoch 77/100 Iteration 203/234: loss=0.035238 lr=0.000020 grad_norm=1.035873
Epoch 77/100 Iteration 204/234: loss=0.035229 lr=0.000020 grad_norm=0.549441
Epoch 77/100 Iteration 205/234: loss=0.038357 lr=0.000020 grad_norm=0.903020
Epoch 77/100 Iteration 206/234: loss=0.038424 lr=0.000020 grad_norm=0.422327
Epoch 77/100 Iteration 207/234: loss=0.038550 lr=0.000020 grad_norm=1.189069
Epoch 77/100 Iteration 208/234: loss=0.036131 lr=0.000020 grad_norm=0.743521
Epoch 77/100 Iteration 209/234: loss=0.037617 lr=0.000020 grad_norm=0.817324
Epoch 77/100 Iteration 210/234: loss=0.037125 lr=0.000020 grad_norm=0.669453
Epoch 77/100 Iteration 211/234: loss=0.038803 lr=0.000020 grad_norm=0.595007
Epoch 77/100 Iteration 212/234: loss=0.033660 lr=0.000020 grad_norm=0.451049
Epoch 77/100 Iteration 213/234: loss=0.038872 lr=0.000020 grad_norm=0.609872
Epoch 77/100 Iteration 214/234: loss=0.032818 lr=0.000020 grad_norm=0.433689
Epoch 77/100 Iteration 215/234: loss=0.035827 lr=0.000020 grad_norm=0.513009
Epoch 77/100 Iteration 216/234: loss=0.033770 lr=0.000020 grad_norm=0.423050
Epoch 77/100 Iteration 217/234: loss=0.038933 lr=0.000020 grad_norm=0.612094
Epoch 77/100 Iteration 218/234: loss=0.038628 lr=0.000020 grad_norm=0.630411
Epoch 77/100 Iteration 219/234: loss=0.031257 lr=0.000020 grad_norm=0.421630
Epoch 77/100 Iteration 220/234: loss=0.036424 lr=0.000020 grad_norm=0.506429
Epoch 77/100 Iteration 221/234: loss=0.035357 lr=0.000020 grad_norm=0.578147
Epoch 77/100 Iteration 222/234: loss=0.034362 lr=0.000020 grad_norm=0.505052
Epoch 77/100 Iteration 223/234: loss=0.040072 lr=0.000020 grad_norm=0.486886
Epoch 77/100 Iteration 224/234: loss=0.038350 lr=0.000020 grad_norm=0.748017
Epoch 77/100 Iteration 225/234: loss=0.035112 lr=0.000020 grad_norm=0.559789
Epoch 77/100 Iteration 226/234: loss=0.035809 lr=0.000020 grad_norm=0.782202
Epoch 77/100 Iteration 227/234: loss=0.036522 lr=0.000020 grad_norm=0.749675
Epoch 77/100 Iteration 228/234: loss=0.038254 lr=0.000020 grad_norm=0.544366
Epoch 77/100 Iteration 229/234: loss=0.036388 lr=0.000020 grad_norm=0.444509
Epoch 77/100 Iteration 230/234: loss=0.037132 lr=0.000020 grad_norm=0.664170
Epoch 77/100 Iteration 231/234: loss=0.034859 lr=0.000020 grad_norm=0.633107
Epoch 77/100 Iteration 232/234: loss=0.040244 lr=0.000020 grad_norm=0.567313
Epoch 77/100 Iteration 233/234: loss=0.037991 lr=0.000020 grad_norm=0.933378
Epoch 77/100 Iteration 234/234: loss=0.038665 lr=0.000020 grad_norm=0.691090
Epoch 77/100 finished. Avg Loss: 0.037581
Epoch 78/100 Iteration 1/234: loss=0.037201 lr=0.000020 grad_norm=0.698685
Epoch 78/100 Iteration 2/234: loss=0.036999 lr=0.000020 grad_norm=0.510307
Epoch 78/100 Iteration 3/234: loss=0.035046 lr=0.000020 grad_norm=0.832866
Epoch 78/100 Iteration 4/234: loss=0.033557 lr=0.000020 grad_norm=0.513842
Epoch 78/100 Iteration 5/234: loss=0.039079 lr=0.000020 grad_norm=0.919071
Epoch 78/100 Iteration 6/234: loss=0.035370 lr=0.000020 grad_norm=0.485019
Epoch 78/100 Iteration 7/234: loss=0.035167 lr=0.000020 grad_norm=0.420499
Epoch 78/100 Iteration 8/234: loss=0.035639 lr=0.000020 grad_norm=0.454695
Epoch 78/100 Iteration 9/234: loss=0.037383 lr=0.000020 grad_norm=0.526426
Epoch 78/100 Iteration 10/234: loss=0.034748 lr=0.000020 grad_norm=0.513158
Epoch 78/100 Iteration 11/234: loss=0.038733 lr=0.000020 grad_norm=0.390976
Epoch 78/100 Iteration 12/234: loss=0.037656 lr=0.000020 grad_norm=0.412076
Epoch 78/100 Iteration 13/234: loss=0.043346 lr=0.000020 grad_norm=0.731212
Epoch 78/100 Iteration 14/234: loss=0.038093 lr=0.000020 grad_norm=0.844919
Epoch 78/100 Iteration 15/234: loss=0.038466 lr=0.000020 grad_norm=0.618360
Epoch 78/100 Iteration 16/234: loss=0.037653 lr=0.000020 grad_norm=0.706810
Epoch 78/100 Iteration 17/234: loss=0.037121 lr=0.000020 grad_norm=0.528764
Epoch 78/100 Iteration 18/234: loss=0.040116 lr=0.000020 grad_norm=0.853379
Epoch 78/100 Iteration 19/234: loss=0.037592 lr=0.000020 grad_norm=1.205725
Epoch 78/100 Iteration 20/234: loss=0.036874 lr=0.000020 grad_norm=0.490117
Epoch 78/100 Iteration 21/234: loss=0.039246 lr=0.000020 grad_norm=1.087466
Epoch 78/100 Iteration 22/234: loss=0.033771 lr=0.000020 grad_norm=1.044466
Epoch 78/100 Iteration 23/234: loss=0.036343 lr=0.000020 grad_norm=0.724105
Epoch 78/100 Iteration 24/234: loss=0.036204 lr=0.000020 grad_norm=0.824270
Epoch 78/100 Iteration 25/234: loss=0.037716 lr=0.000020 grad_norm=0.865944
Epoch 78/100 Iteration 26/234: loss=0.036419 lr=0.000020 grad_norm=0.802178
Epoch 78/100 Iteration 27/234: loss=0.039439 lr=0.000020 grad_norm=0.520545
Epoch 78/100 Iteration 28/234: loss=0.038651 lr=0.000020 grad_norm=0.545355
Epoch 78/100 Iteration 29/234: loss=0.036045 lr=0.000020 grad_norm=1.010827
Epoch 78/100 Iteration 30/234: loss=0.033273 lr=0.000020 grad_norm=0.821240
Epoch 78/100 Iteration 31/234: loss=0.037912 lr=0.000020 grad_norm=0.522493
Epoch 78/100 Iteration 32/234: loss=0.039120 lr=0.000020 grad_norm=1.341488
Epoch 78/100 Iteration 33/234: loss=0.036993 lr=0.000020 grad_norm=1.249122
Epoch 78/100 Iteration 34/234: loss=0.039687 lr=0.000020 grad_norm=0.455807
Epoch 78/100 Iteration 35/234: loss=0.034837 lr=0.000020 grad_norm=1.448385
Epoch 78/100 Iteration 36/234: loss=0.038672 lr=0.000020 grad_norm=1.425598
Epoch 78/100 Iteration 37/234: loss=0.041967 lr=0.000020 grad_norm=0.663128
Epoch 78/100 Iteration 38/234: loss=0.041025 lr=0.000020 grad_norm=0.873580
Epoch 78/100 Iteration 39/234: loss=0.036715 lr=0.000020 grad_norm=0.841170
Epoch 78/100 Iteration 40/234: loss=0.036115 lr=0.000020 grad_norm=0.490871
Epoch 78/100 Iteration 41/234: loss=0.040382 lr=0.000020 grad_norm=0.635718
Epoch 78/100 Iteration 42/234: loss=0.036658 lr=0.000020 grad_norm=0.999117
Epoch 78/100 Iteration 43/234: loss=0.034920 lr=0.000020 grad_norm=0.709825
Epoch 78/100 Iteration 44/234: loss=0.038541 lr=0.000020 grad_norm=1.384867
Epoch 78/100 Iteration 45/234: loss=0.033963 lr=0.000020 grad_norm=1.409117
Epoch 78/100 Iteration 46/234: loss=0.035906 lr=0.000020 grad_norm=0.386330
Epoch 78/100 Iteration 47/234: loss=0.037548 lr=0.000020 grad_norm=1.798877
Epoch 78/100 Iteration 48/234: loss=0.037167 lr=0.000020 grad_norm=1.921846
Epoch 78/100 Iteration 49/234: loss=0.037648 lr=0.000020 grad_norm=0.625291
Epoch 78/100 Iteration 50/234: loss=0.039308 lr=0.000020 grad_norm=1.550326
Epoch 78/100 Iteration 51/234: loss=0.035395 lr=0.000020 grad_norm=1.389998
Epoch 78/100 Iteration 52/234: loss=0.035669 lr=0.000020 grad_norm=0.876740
Epoch 78/100 Iteration 53/234: loss=0.043232 lr=0.000020 grad_norm=1.616021
Epoch 78/100 Iteration 54/234: loss=0.034508 lr=0.000020 grad_norm=1.167530
Epoch 78/100 Iteration 55/234: loss=0.035646 lr=0.000020 grad_norm=0.847098
Epoch 78/100 Iteration 56/234: loss=0.038228 lr=0.000020 grad_norm=1.405498
Epoch 78/100 Iteration 57/234: loss=0.036120 lr=0.000020 grad_norm=0.941121
Epoch 78/100 Iteration 58/234: loss=0.035569 lr=0.000020 grad_norm=0.727726
Epoch 78/100 Iteration 59/234: loss=0.039752 lr=0.000020 grad_norm=0.889682
Epoch 78/100 Iteration 60/234: loss=0.036117 lr=0.000020 grad_norm=0.929880
Epoch 78/100 Iteration 61/234: loss=0.038220 lr=0.000020 grad_norm=0.526865
Epoch 78/100 Iteration 62/234: loss=0.039172 lr=0.000020 grad_norm=0.810820
Epoch 78/100 Iteration 63/234: loss=0.038257 lr=0.000020 grad_norm=0.916347
Epoch 78/100 Iteration 64/234: loss=0.040595 lr=0.000020 grad_norm=0.615825
Epoch 78/100 Iteration 65/234: loss=0.035361 lr=0.000020 grad_norm=0.870124
Epoch 78/100 Iteration 66/234: loss=0.031674 lr=0.000020 grad_norm=0.575509
Epoch 78/100 Iteration 67/234: loss=0.038251 lr=0.000020 grad_norm=0.788818
Epoch 78/100 Iteration 68/234: loss=0.035492 lr=0.000020 grad_norm=0.636776
Epoch 78/100 Iteration 69/234: loss=0.037901 lr=0.000020 grad_norm=0.628783
Epoch 78/100 Iteration 70/234: loss=0.036676 lr=0.000020 grad_norm=0.465299
Epoch 78/100 Iteration 71/234: loss=0.039004 lr=0.000020 grad_norm=0.678220
Epoch 78/100 Iteration 72/234: loss=0.038643 lr=0.000020 grad_norm=0.558523
Epoch 78/100 Iteration 73/234: loss=0.034839 lr=0.000020 grad_norm=0.372272
Epoch 78/100 Iteration 74/234: loss=0.039184 lr=0.000020 grad_norm=0.574489
Epoch 78/100 Iteration 75/234: loss=0.038339 lr=0.000020 grad_norm=0.463803
Epoch 78/100 Iteration 76/234: loss=0.043844 lr=0.000020 grad_norm=0.482257
Epoch 78/100 Iteration 77/234: loss=0.030199 lr=0.000020 grad_norm=0.485511
Epoch 78/100 Iteration 78/234: loss=0.035176 lr=0.000020 grad_norm=0.538098
Epoch 78/100 Iteration 79/234: loss=0.040121 lr=0.000020 grad_norm=0.449082
Epoch 78/100 Iteration 80/234: loss=0.035951 lr=0.000020 grad_norm=0.464597
Epoch 78/100 Iteration 81/234: loss=0.038840 lr=0.000020 grad_norm=0.646636
Epoch 78/100 Iteration 82/234: loss=0.037133 lr=0.000020 grad_norm=0.609594
Epoch 78/100 Iteration 83/234: loss=0.035816 lr=0.000020 grad_norm=0.428078
Epoch 78/100 Iteration 84/234: loss=0.037724 lr=0.000020 grad_norm=0.642214
Epoch 78/100 Iteration 85/234: loss=0.035183 lr=0.000020 grad_norm=0.719511
Epoch 78/100 Iteration 86/234: loss=0.037847 lr=0.000020 grad_norm=0.440995
Epoch 78/100 Iteration 87/234: loss=0.040504 lr=0.000020 grad_norm=0.528607
Epoch 78/100 Iteration 88/234: loss=0.037157 lr=0.000020 grad_norm=0.644382
Epoch 78/100 Iteration 89/234: loss=0.035265 lr=0.000020 grad_norm=0.741751
Epoch 78/100 Iteration 90/234: loss=0.038510 lr=0.000020 grad_norm=0.605396
Epoch 78/100 Iteration 91/234: loss=0.037087 lr=0.000020 grad_norm=1.071685
Epoch 78/100 Iteration 92/234: loss=0.035797 lr=0.000020 grad_norm=0.936008
Epoch 78/100 Iteration 93/234: loss=0.036600 lr=0.000020 grad_norm=0.704877
Epoch 78/100 Iteration 94/234: loss=0.033511 lr=0.000020 grad_norm=0.685622
Epoch 78/100 Iteration 95/234: loss=0.036921 lr=0.000020 grad_norm=0.798183
Epoch 78/100 Iteration 96/234: loss=0.038535 lr=0.000020 grad_norm=1.063855
Epoch 78/100 Iteration 97/234: loss=0.037201 lr=0.000020 grad_norm=0.952089
Epoch 78/100 Iteration 98/234: loss=0.037749 lr=0.000020 grad_norm=0.479907
Epoch 78/100 Iteration 99/234: loss=0.037067 lr=0.000020 grad_norm=1.178223
Epoch 78/100 Iteration 100/234: loss=0.035820 lr=0.000020 grad_norm=1.054465
Epoch 78/100 Iteration 101/234: loss=0.035317 lr=0.000020 grad_norm=0.452521
Epoch 78/100 Iteration 102/234: loss=0.040786 lr=0.000020 grad_norm=1.669203
Epoch 78/100 Iteration 103/234: loss=0.035214 lr=0.000020 grad_norm=1.447607
Epoch 78/100 Iteration 104/234: loss=0.037347 lr=0.000020 grad_norm=0.933838
Epoch 78/100 Iteration 105/234: loss=0.034215 lr=0.000020 grad_norm=1.579434
Epoch 78/100 Iteration 106/234: loss=0.033265 lr=0.000020 grad_norm=0.492696
Epoch 78/100 Iteration 107/234: loss=0.035706 lr=0.000020 grad_norm=1.428114
Epoch 78/100 Iteration 108/234: loss=0.039612 lr=0.000020 grad_norm=0.953858
Epoch 78/100 Iteration 109/234: loss=0.035420 lr=0.000020 grad_norm=1.003328
Epoch 78/100 Iteration 110/234: loss=0.039021 lr=0.000020 grad_norm=1.186481
Epoch 78/100 Iteration 111/234: loss=0.034832 lr=0.000020 grad_norm=0.694542
Epoch 78/100 Iteration 112/234: loss=0.034086 lr=0.000020 grad_norm=0.576941
Epoch 78/100 Iteration 113/234: loss=0.034945 lr=0.000020 grad_norm=0.567857
Epoch 78/100 Iteration 114/234: loss=0.038985 lr=0.000020 grad_norm=0.767488
Epoch 78/100 Iteration 115/234: loss=0.036304 lr=0.000020 grad_norm=0.512453
Epoch 78/100 Iteration 116/234: loss=0.034373 lr=0.000020 grad_norm=0.608195
Epoch 78/100 Iteration 117/234: loss=0.039787 lr=0.000020 grad_norm=0.548107
Epoch 78/100 Iteration 118/234: loss=0.040206 lr=0.000020 grad_norm=0.498542
Epoch 78/100 Iteration 119/234: loss=0.040112 lr=0.000020 grad_norm=1.063593
Epoch 78/100 Iteration 120/234: loss=0.037060 lr=0.000020 grad_norm=0.754934
Epoch 78/100 Iteration 121/234: loss=0.035300 lr=0.000020 grad_norm=0.748487
Epoch 78/100 Iteration 122/234: loss=0.040504 lr=0.000020 grad_norm=1.304106
Epoch 78/100 Iteration 123/234: loss=0.038010 lr=0.000020 grad_norm=0.718188
Epoch 78/100 Iteration 124/234: loss=0.038838 lr=0.000020 grad_norm=0.646214
Epoch 78/100 Iteration 125/234: loss=0.041120 lr=0.000020 grad_norm=0.801004
Epoch 78/100 Iteration 126/234: loss=0.032197 lr=0.000020 grad_norm=0.365155
Epoch 78/100 Iteration 127/234: loss=0.033517 lr=0.000020 grad_norm=0.644089
Epoch 78/100 Iteration 128/234: loss=0.035530 lr=0.000020 grad_norm=0.597121
Epoch 78/100 Iteration 129/234: loss=0.033335 lr=0.000020 grad_norm=0.456809
Epoch 78/100 Iteration 130/234: loss=0.034241 lr=0.000020 grad_norm=0.618719
Epoch 78/100 Iteration 131/234: loss=0.040075 lr=0.000020 grad_norm=0.438886
Epoch 78/100 Iteration 132/234: loss=0.036926 lr=0.000020 grad_norm=0.573108
Epoch 78/100 Iteration 133/234: loss=0.033173 lr=0.000020 grad_norm=0.466525
Epoch 78/100 Iteration 134/234: loss=0.036972 lr=0.000020 grad_norm=0.399259
Epoch 78/100 Iteration 135/234: loss=0.033453 lr=0.000020 grad_norm=0.456407
Epoch 78/100 Iteration 136/234: loss=0.038665 lr=0.000020 grad_norm=0.507323
Epoch 78/100 Iteration 137/234: loss=0.039591 lr=0.000020 grad_norm=0.494215
Epoch 78/100 Iteration 138/234: loss=0.037230 lr=0.000020 grad_norm=0.615395
Epoch 78/100 Iteration 139/234: loss=0.037316 lr=0.000020 grad_norm=0.672246
Epoch 78/100 Iteration 140/234: loss=0.033889 lr=0.000020 grad_norm=0.356550
Epoch 78/100 Iteration 141/234: loss=0.038867 lr=0.000020 grad_norm=0.651720
Epoch 78/100 Iteration 142/234: loss=0.036773 lr=0.000020 grad_norm=0.708014
Epoch 78/100 Iteration 143/234: loss=0.039332 lr=0.000020 grad_norm=0.451688
Epoch 78/100 Iteration 144/234: loss=0.032662 lr=0.000020 grad_norm=0.627787
Epoch 78/100 Iteration 145/234: loss=0.036239 lr=0.000020 grad_norm=0.847227
Epoch 78/100 Iteration 146/234: loss=0.039579 lr=0.000020 grad_norm=0.545109
Epoch 78/100 Iteration 147/234: loss=0.037228 lr=0.000020 grad_norm=0.622795
Epoch 78/100 Iteration 148/234: loss=0.035310 lr=0.000020 grad_norm=0.621802
Epoch 78/100 Iteration 149/234: loss=0.034992 lr=0.000020 grad_norm=0.458356
Epoch 78/100 Iteration 150/234: loss=0.038942 lr=0.000020 grad_norm=0.724913
Epoch 78/100 Iteration 151/234: loss=0.032061 lr=0.000020 grad_norm=1.008735
Epoch 78/100 Iteration 152/234: loss=0.030606 lr=0.000020 grad_norm=0.348252
Epoch 78/100 Iteration 153/234: loss=0.035200 lr=0.000020 grad_norm=0.972011
Epoch 78/100 Iteration 154/234: loss=0.033866 lr=0.000020 grad_norm=0.781839
Epoch 78/100 Iteration 155/234: loss=0.038851 lr=0.000020 grad_norm=0.858965
Epoch 78/100 Iteration 156/234: loss=0.040328 lr=0.000020 grad_norm=1.721835
Epoch 78/100 Iteration 157/234: loss=0.036842 lr=0.000020 grad_norm=1.487584
Epoch 78/100 Iteration 158/234: loss=0.034136 lr=0.000020 grad_norm=0.896979
Epoch 78/100 Iteration 159/234: loss=0.036213 lr=0.000020 grad_norm=1.191271
Epoch 78/100 Iteration 160/234: loss=0.033288 lr=0.000020 grad_norm=0.860531
Epoch 78/100 Iteration 161/234: loss=0.034791 lr=0.000020 grad_norm=1.076937
Epoch 78/100 Iteration 162/234: loss=0.033774 lr=0.000020 grad_norm=0.712555
Epoch 78/100 Iteration 163/234: loss=0.032921 lr=0.000020 grad_norm=0.857212
Epoch 78/100 Iteration 164/234: loss=0.037898 lr=0.000020 grad_norm=1.013857
Epoch 78/100 Iteration 165/234: loss=0.040813 lr=0.000020 grad_norm=1.039296
Epoch 78/100 Iteration 166/234: loss=0.035866 lr=0.000020 grad_norm=0.971499
Epoch 78/100 Iteration 167/234: loss=0.041066 lr=0.000020 grad_norm=0.504532
Epoch 78/100 Iteration 168/234: loss=0.037212 lr=0.000020 grad_norm=0.637349
Epoch 78/100 Iteration 169/234: loss=0.035104 lr=0.000020 grad_norm=0.731301
Epoch 78/100 Iteration 170/234: loss=0.035348 lr=0.000020 grad_norm=0.518454
Epoch 78/100 Iteration 171/234: loss=0.035967 lr=0.000020 grad_norm=0.805127
Epoch 78/100 Iteration 172/234: loss=0.030701 lr=0.000020 grad_norm=0.463076
Epoch 78/100 Iteration 173/234: loss=0.039840 lr=0.000020 grad_norm=0.705487
Epoch 78/100 Iteration 174/234: loss=0.036601 lr=0.000020 grad_norm=0.964725
Epoch 78/100 Iteration 175/234: loss=0.039493 lr=0.000020 grad_norm=0.716476
Epoch 78/100 Iteration 176/234: loss=0.036269 lr=0.000020 grad_norm=0.646729
Epoch 78/100 Iteration 177/234: loss=0.039048 lr=0.000020 grad_norm=0.782427
Epoch 78/100 Iteration 178/234: loss=0.033025 lr=0.000020 grad_norm=0.830833
Epoch 78/100 Iteration 179/234: loss=0.035889 lr=0.000020 grad_norm=0.554354
Epoch 78/100 Iteration 180/234: loss=0.039491 lr=0.000020 grad_norm=1.033987
Epoch 78/100 Iteration 181/234: loss=0.041076 lr=0.000020 grad_norm=0.772452
Epoch 78/100 Iteration 182/234: loss=0.041282 lr=0.000020 grad_norm=0.794241
Epoch 78/100 Iteration 183/234: loss=0.035971 lr=0.000020 grad_norm=0.962954
Epoch 78/100 Iteration 184/234: loss=0.037967 lr=0.000020 grad_norm=0.669863
Epoch 78/100 Iteration 185/234: loss=0.039167 lr=0.000020 grad_norm=0.736949
Epoch 78/100 Iteration 186/234: loss=0.035024 lr=0.000020 grad_norm=0.913279
Epoch 78/100 Iteration 187/234: loss=0.035453 lr=0.000020 grad_norm=0.703575
Epoch 78/100 Iteration 188/234: loss=0.037021 lr=0.000020 grad_norm=0.518699
Epoch 78/100 Iteration 189/234: loss=0.037696 lr=0.000020 grad_norm=0.707464
Epoch 78/100 Iteration 190/234: loss=0.033926 lr=0.000020 grad_norm=0.820900
Epoch 78/100 Iteration 191/234: loss=0.041250 lr=0.000020 grad_norm=0.583946
Epoch 78/100 Iteration 192/234: loss=0.034835 lr=0.000020 grad_norm=0.686046
Epoch 78/100 Iteration 193/234: loss=0.040524 lr=0.000020 grad_norm=0.653584
Epoch 78/100 Iteration 194/234: loss=0.038102 lr=0.000020 grad_norm=0.476226
Epoch 78/100 Iteration 195/234: loss=0.035672 lr=0.000020 grad_norm=0.780564
Epoch 78/100 Iteration 196/234: loss=0.040796 lr=0.000020 grad_norm=0.752296
Epoch 78/100 Iteration 197/234: loss=0.036843 lr=0.000020 grad_norm=0.674948
Epoch 78/100 Iteration 198/234: loss=0.039392 lr=0.000020 grad_norm=0.945566
Epoch 78/100 Iteration 199/234: loss=0.037701 lr=0.000020 grad_norm=1.060037
Epoch 78/100 Iteration 200/234: loss=0.037125 lr=0.000020 grad_norm=0.720551
Epoch 78/100 Iteration 201/234: loss=0.038100 lr=0.000020 grad_norm=0.548054
Epoch 78/100 Iteration 202/234: loss=0.037066 lr=0.000020 grad_norm=0.937300
Epoch 78/100 Iteration 203/234: loss=0.036639 lr=0.000020 grad_norm=0.587881
Epoch 78/100 Iteration 204/234: loss=0.033139 lr=0.000020 grad_norm=0.415630
Epoch 78/100 Iteration 205/234: loss=0.040992 lr=0.000020 grad_norm=0.513472
Epoch 78/100 Iteration 206/234: loss=0.038621 lr=0.000020 grad_norm=0.748414
Epoch 78/100 Iteration 207/234: loss=0.037703 lr=0.000020 grad_norm=0.844911
Epoch 78/100 Iteration 208/234: loss=0.040045 lr=0.000020 grad_norm=0.744150
Epoch 78/100 Iteration 209/234: loss=0.037804 lr=0.000020 grad_norm=0.552605
Epoch 78/100 Iteration 210/234: loss=0.032561 lr=0.000020 grad_norm=0.560485
Epoch 78/100 Iteration 211/234: loss=0.037457 lr=0.000020 grad_norm=0.730946
Epoch 78/100 Iteration 212/234: loss=0.037869 lr=0.000020 grad_norm=0.391918
Epoch 78/100 Iteration 213/234: loss=0.036085 lr=0.000020 grad_norm=0.880905
Epoch 78/100 Iteration 214/234: loss=0.041918 lr=0.000020 grad_norm=0.954257
Epoch 78/100 Iteration 215/234: loss=0.033346 lr=0.000020 grad_norm=0.502180
Epoch 78/100 Iteration 216/234: loss=0.037221 lr=0.000020 grad_norm=0.729489
Epoch 78/100 Iteration 217/234: loss=0.038802 lr=0.000020 grad_norm=0.523732
Epoch 78/100 Iteration 218/234: loss=0.039256 lr=0.000020 grad_norm=0.634166
Epoch 78/100 Iteration 219/234: loss=0.035128 lr=0.000020 grad_norm=0.791326
Epoch 78/100 Iteration 220/234: loss=0.038592 lr=0.000020 grad_norm=0.804200
Epoch 78/100 Iteration 221/234: loss=0.036460 lr=0.000020 grad_norm=0.581313
Epoch 78/100 Iteration 222/234: loss=0.034296 lr=0.000020 grad_norm=0.436744
Epoch 78/100 Iteration 223/234: loss=0.040231 lr=0.000020 grad_norm=0.543454
Epoch 78/100 Iteration 224/234: loss=0.036652 lr=0.000020 grad_norm=0.572344
Epoch 78/100 Iteration 225/234: loss=0.040521 lr=0.000020 grad_norm=0.594016
Epoch 78/100 Iteration 226/234: loss=0.035744 lr=0.000020 grad_norm=0.456474
Epoch 78/100 Iteration 227/234: loss=0.036842 lr=0.000020 grad_norm=0.484173
Epoch 78/100 Iteration 228/234: loss=0.038357 lr=0.000020 grad_norm=0.506965
Epoch 78/100 Iteration 229/234: loss=0.038602 lr=0.000020 grad_norm=0.557137
Epoch 78/100 Iteration 230/234: loss=0.036242 lr=0.000020 grad_norm=0.910822
Epoch 78/100 Iteration 231/234: loss=0.042939 lr=0.000020 grad_norm=1.145302
Epoch 78/100 Iteration 232/234: loss=0.033224 lr=0.000020 grad_norm=0.524434
Epoch 78/100 Iteration 233/234: loss=0.041634 lr=0.000020 grad_norm=0.961790
Epoch 78/100 Iteration 234/234: loss=0.031987 lr=0.000020 grad_norm=1.263694
Epoch 78/100 finished. Avg Loss: 0.037065
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 79/100 Iteration 1/234: loss=0.035385 lr=0.000020 grad_norm=0.708577
Epoch 79/100 Iteration 2/234: loss=0.033602 lr=0.000020 grad_norm=1.118906
Epoch 79/100 Iteration 3/234: loss=0.037504 lr=0.000020 grad_norm=1.361876
Epoch 79/100 Iteration 4/234: loss=0.037987 lr=0.000020 grad_norm=1.267712
Epoch 79/100 Iteration 5/234: loss=0.040949 lr=0.000020 grad_norm=0.900658
Epoch 79/100 Iteration 6/234: loss=0.035596 lr=0.000020 grad_norm=0.622746
Epoch 79/100 Iteration 7/234: loss=0.035058 lr=0.000020 grad_norm=0.836052
Epoch 79/100 Iteration 8/234: loss=0.035910 lr=0.000020 grad_norm=0.766790
Epoch 79/100 Iteration 9/234: loss=0.036238 lr=0.000020 grad_norm=0.558632
Epoch 79/100 Iteration 10/234: loss=0.039293 lr=0.000020 grad_norm=1.205619
Epoch 79/100 Iteration 11/234: loss=0.041491 lr=0.000020 grad_norm=0.950667
Epoch 79/100 Iteration 12/234: loss=0.036747 lr=0.000020 grad_norm=0.475914
Epoch 79/100 Iteration 13/234: loss=0.033715 lr=0.000020 grad_norm=0.666454
Epoch 79/100 Iteration 14/234: loss=0.032884 lr=0.000020 grad_norm=0.479577
Epoch 79/100 Iteration 15/234: loss=0.036494 lr=0.000020 grad_norm=0.426932
Epoch 79/100 Iteration 16/234: loss=0.035756 lr=0.000020 grad_norm=0.617530
Epoch 79/100 Iteration 17/234: loss=0.037802 lr=0.000020 grad_norm=0.432275
Epoch 79/100 Iteration 18/234: loss=0.035542 lr=0.000020 grad_norm=0.413046
Epoch 79/100 Iteration 19/234: loss=0.032627 lr=0.000020 grad_norm=0.322934
Epoch 79/100 Iteration 20/234: loss=0.039547 lr=0.000020 grad_norm=0.526001
Epoch 79/100 Iteration 21/234: loss=0.036208 lr=0.000020 grad_norm=0.661236
Epoch 79/100 Iteration 22/234: loss=0.033542 lr=0.000020 grad_norm=0.681229
Epoch 79/100 Iteration 23/234: loss=0.032641 lr=0.000020 grad_norm=0.575864
Epoch 79/100 Iteration 24/234: loss=0.034530 lr=0.000020 grad_norm=0.525566
Epoch 79/100 Iteration 25/234: loss=0.041987 lr=0.000020 grad_norm=0.618599
Epoch 79/100 Iteration 26/234: loss=0.036184 lr=0.000020 grad_norm=0.915085
Epoch 79/100 Iteration 27/234: loss=0.036590 lr=0.000020 grad_norm=0.559950
Epoch 79/100 Iteration 28/234: loss=0.036287 lr=0.000020 grad_norm=0.460470
Epoch 79/100 Iteration 29/234: loss=0.037616 lr=0.000020 grad_norm=0.673494
Epoch 79/100 Iteration 30/234: loss=0.039841 lr=0.000020 grad_norm=0.475852
Epoch 79/100 Iteration 31/234: loss=0.041571 lr=0.000020 grad_norm=1.011104
Epoch 79/100 Iteration 32/234: loss=0.034680 lr=0.000020 grad_norm=1.087517
Epoch 79/100 Iteration 33/234: loss=0.036721 lr=0.000020 grad_norm=0.465334
Epoch 79/100 Iteration 34/234: loss=0.038164 lr=0.000020 grad_norm=0.810592
Epoch 79/100 Iteration 35/234: loss=0.036245 lr=0.000020 grad_norm=0.490325
Epoch 79/100 Iteration 36/234: loss=0.035786 lr=0.000020 grad_norm=0.660809
Epoch 79/100 Iteration 37/234: loss=0.038436 lr=0.000020 grad_norm=0.993786
Epoch 79/100 Iteration 38/234: loss=0.035119 lr=0.000020 grad_norm=0.561425
Epoch 79/100 Iteration 39/234: loss=0.038731 lr=0.000020 grad_norm=1.068888
Epoch 79/100 Iteration 40/234: loss=0.035885 lr=0.000020 grad_norm=0.874739
Epoch 79/100 Iteration 41/234: loss=0.038101 lr=0.000020 grad_norm=0.691442
Epoch 79/100 Iteration 42/234: loss=0.039893 lr=0.000020 grad_norm=1.581207
Epoch 79/100 Iteration 43/234: loss=0.034144 lr=0.000020 grad_norm=1.187565
Epoch 79/100 Iteration 44/234: loss=0.035295 lr=0.000020 grad_norm=0.621329
Epoch 79/100 Iteration 45/234: loss=0.042933 lr=0.000020 grad_norm=1.151355
Epoch 79/100 Iteration 46/234: loss=0.035021 lr=0.000020 grad_norm=0.609814
Epoch 79/100 Iteration 47/234: loss=0.041699 lr=0.000020 grad_norm=0.892203
Epoch 79/100 Iteration 48/234: loss=0.037942 lr=0.000020 grad_norm=0.979224
Epoch 79/100 Iteration 49/234: loss=0.036704 lr=0.000020 grad_norm=0.466822
Epoch 79/100 Iteration 50/234: loss=0.038168 lr=0.000020 grad_norm=0.816602
Epoch 79/100 Iteration 51/234: loss=0.038155 lr=0.000020 grad_norm=0.824956
Epoch 79/100 Iteration 52/234: loss=0.039845 lr=0.000020 grad_norm=0.595374
Epoch 79/100 Iteration 53/234: loss=0.036102 lr=0.000020 grad_norm=0.604682
Epoch 79/100 Iteration 54/234: loss=0.039151 lr=0.000020 grad_norm=0.527259
Epoch 79/100 Iteration 55/234: loss=0.041114 lr=0.000020 grad_norm=0.570430
Epoch 79/100 Iteration 56/234: loss=0.032269 lr=0.000020 grad_norm=0.720716
Epoch 79/100 Iteration 57/234: loss=0.036184 lr=0.000020 grad_norm=0.621842
Epoch 79/100 Iteration 58/234: loss=0.039050 lr=0.000020 grad_norm=0.447000
Epoch 79/100 Iteration 59/234: loss=0.035762 lr=0.000020 grad_norm=0.638812
Epoch 79/100 Iteration 60/234: loss=0.038120 lr=0.000020 grad_norm=0.686149
Epoch 79/100 Iteration 61/234: loss=0.035295 lr=0.000020 grad_norm=0.524566
Epoch 79/100 Iteration 62/234: loss=0.043487 lr=0.000020 grad_norm=0.682925
Epoch 79/100 Iteration 63/234: loss=0.037917 lr=0.000020 grad_norm=0.798626
Epoch 79/100 Iteration 64/234: loss=0.038904 lr=0.000020 grad_norm=0.780336
Epoch 79/100 Iteration 65/234: loss=0.039503 lr=0.000020 grad_norm=0.408841
Epoch 79/100 Iteration 66/234: loss=0.037873 lr=0.000020 grad_norm=0.851315
Epoch 79/100 Iteration 67/234: loss=0.039229 lr=0.000020 grad_norm=1.095301
Epoch 79/100 Iteration 68/234: loss=0.038637 lr=0.000020 grad_norm=0.681065
Epoch 79/100 Iteration 69/234: loss=0.033607 lr=0.000020 grad_norm=0.451407
Epoch 79/100 Iteration 70/234: loss=0.033208 lr=0.000020 grad_norm=0.524780
Epoch 79/100 Iteration 71/234: loss=0.038243 lr=0.000020 grad_norm=0.605318
Epoch 79/100 Iteration 72/234: loss=0.035037 lr=0.000020 grad_norm=0.648068
Epoch 79/100 Iteration 73/234: loss=0.036676 lr=0.000020 grad_norm=0.475610
Epoch 79/100 Iteration 74/234: loss=0.032648 lr=0.000020 grad_norm=0.940742
Epoch 79/100 Iteration 75/234: loss=0.040188 lr=0.000020 grad_norm=1.387095
Epoch 79/100 Iteration 76/234: loss=0.039947 lr=0.000020 grad_norm=1.309291
Epoch 79/100 Iteration 77/234: loss=0.039238 lr=0.000020 grad_norm=0.992738
Epoch 79/100 Iteration 78/234: loss=0.036086 lr=0.000020 grad_norm=1.055874
Epoch 79/100 Iteration 79/234: loss=0.034090 lr=0.000020 grad_norm=1.558955
Epoch 79/100 Iteration 80/234: loss=0.042068 lr=0.000020 grad_norm=1.669282
Epoch 79/100 Iteration 81/234: loss=0.036969 lr=0.000020 grad_norm=1.105417
Epoch 79/100 Iteration 82/234: loss=0.039252 lr=0.000020 grad_norm=0.720055
Epoch 79/100 Iteration 83/234: loss=0.037871 lr=0.000020 grad_norm=1.559259
Epoch 79/100 Iteration 84/234: loss=0.036255 lr=0.000020 grad_norm=1.361745
Epoch 79/100 Iteration 85/234: loss=0.037204 lr=0.000020 grad_norm=0.384243
Epoch 79/100 Iteration 86/234: loss=0.034396 lr=0.000020 grad_norm=1.564777
Epoch 79/100 Iteration 87/234: loss=0.038124 lr=0.000020 grad_norm=1.303872
Epoch 79/100 Iteration 88/234: loss=0.043481 lr=0.000020 grad_norm=0.505552
Epoch 79/100 Iteration 89/234: loss=0.040431 lr=0.000020 grad_norm=1.224276
Epoch 79/100 Iteration 90/234: loss=0.038211 lr=0.000020 grad_norm=1.651211
Epoch 79/100 Iteration 91/234: loss=0.037455 lr=0.000020 grad_norm=1.186193
Epoch 79/100 Iteration 92/234: loss=0.034494 lr=0.000020 grad_norm=0.437651
Epoch 79/100 Iteration 93/234: loss=0.036625 lr=0.000020 grad_norm=1.186971
Epoch 79/100 Iteration 94/234: loss=0.034050 lr=0.000020 grad_norm=1.110502
Epoch 79/100 Iteration 95/234: loss=0.036122 lr=0.000020 grad_norm=0.545533
Epoch 79/100 Iteration 96/234: loss=0.036534 lr=0.000020 grad_norm=1.668416
Epoch 79/100 Iteration 97/234: loss=0.041136 lr=0.000020 grad_norm=1.181953
Epoch 79/100 Iteration 98/234: loss=0.034666 lr=0.000020 grad_norm=0.819497
Epoch 79/100 Iteration 99/234: loss=0.037562 lr=0.000020 grad_norm=1.846093
Epoch 79/100 Iteration 100/234: loss=0.043113 lr=0.000020 grad_norm=1.479673
Epoch 79/100 Iteration 101/234: loss=0.034340 lr=0.000020 grad_norm=0.630502
Epoch 79/100 Iteration 102/234: loss=0.032485 lr=0.000020 grad_norm=1.229727
Epoch 79/100 Iteration 103/234: loss=0.036515 lr=0.000020 grad_norm=0.617425
Epoch 79/100 Iteration 104/234: loss=0.039635 lr=0.000020 grad_norm=1.190899
Epoch 79/100 Iteration 105/234: loss=0.033930 lr=0.000020 grad_norm=0.982983
Epoch 79/100 Iteration 106/234: loss=0.034014 lr=0.000020 grad_norm=0.921841
Epoch 79/100 Iteration 107/234: loss=0.038661 lr=0.000020 grad_norm=0.759866
Epoch 79/100 Iteration 108/234: loss=0.037630 lr=0.000020 grad_norm=0.919626
Epoch 79/100 Iteration 109/234: loss=0.040102 lr=0.000020 grad_norm=0.986870
Epoch 79/100 Iteration 110/234: loss=0.037095 lr=0.000020 grad_norm=0.513344
Epoch 79/100 Iteration 111/234: loss=0.037366 lr=0.000020 grad_norm=0.674860
Epoch 79/100 Iteration 112/234: loss=0.034968 lr=0.000020 grad_norm=0.552270
Epoch 79/100 Iteration 113/234: loss=0.039994 lr=0.000020 grad_norm=0.675743
Epoch 79/100 Iteration 114/234: loss=0.041534 lr=0.000020 grad_norm=1.019668
Epoch 79/100 Iteration 115/234: loss=0.035748 lr=0.000020 grad_norm=0.806990
Epoch 79/100 Iteration 116/234: loss=0.041744 lr=0.000020 grad_norm=0.891304
Epoch 79/100 Iteration 117/234: loss=0.043000 lr=0.000020 grad_norm=1.287543
Epoch 79/100 Iteration 118/234: loss=0.041487 lr=0.000020 grad_norm=1.044312
Epoch 79/100 Iteration 119/234: loss=0.036979 lr=0.000020 grad_norm=1.113708
Epoch 79/100 Iteration 120/234: loss=0.037813 lr=0.000020 grad_norm=1.102062
Epoch 79/100 Iteration 121/234: loss=0.038778 lr=0.000020 grad_norm=0.451484
Epoch 79/100 Iteration 122/234: loss=0.038364 lr=0.000020 grad_norm=0.747900
Epoch 79/100 Iteration 123/234: loss=0.035900 lr=0.000020 grad_norm=0.735337
Epoch 79/100 Iteration 124/234: loss=0.036150 lr=0.000020 grad_norm=0.772448
Epoch 79/100 Iteration 125/234: loss=0.042156 lr=0.000020 grad_norm=1.080486
Epoch 79/100 Iteration 126/234: loss=0.035329 lr=0.000020 grad_norm=0.542883
Epoch 79/100 Iteration 127/234: loss=0.040020 lr=0.000020 grad_norm=0.860870
Epoch 79/100 Iteration 128/234: loss=0.038549 lr=0.000020 grad_norm=0.939297
Epoch 79/100 Iteration 129/234: loss=0.040959 lr=0.000020 grad_norm=0.877775
Epoch 79/100 Iteration 130/234: loss=0.034897 lr=0.000020 grad_norm=0.844744
Epoch 79/100 Iteration 131/234: loss=0.035354 lr=0.000020 grad_norm=0.486661
Epoch 79/100 Iteration 132/234: loss=0.034698 lr=0.000020 grad_norm=0.625883
Epoch 79/100 Iteration 133/234: loss=0.038585 lr=0.000020 grad_norm=0.656934
Epoch 79/100 Iteration 134/234: loss=0.033538 lr=0.000020 grad_norm=1.166461
Epoch 79/100 Iteration 135/234: loss=0.037219 lr=0.000020 grad_norm=0.603912
Epoch 79/100 Iteration 136/234: loss=0.033521 lr=0.000020 grad_norm=1.073617
Epoch 79/100 Iteration 137/234: loss=0.037123 lr=0.000020 grad_norm=1.034571
Epoch 79/100 Iteration 138/234: loss=0.037278 lr=0.000020 grad_norm=0.720498
Epoch 79/100 Iteration 139/234: loss=0.038219 lr=0.000020 grad_norm=0.639367
Epoch 79/100 Iteration 140/234: loss=0.038567 lr=0.000020 grad_norm=0.781089
Epoch 79/100 Iteration 141/234: loss=0.033370 lr=0.000020 grad_norm=0.553784
Epoch 79/100 Iteration 142/234: loss=0.039319 lr=0.000020 grad_norm=0.730162
Epoch 79/100 Iteration 143/234: loss=0.037442 lr=0.000020 grad_norm=0.478668
Epoch 79/100 Iteration 144/234: loss=0.036663 lr=0.000020 grad_norm=0.529622
Epoch 79/100 Iteration 145/234: loss=0.039386 lr=0.000020 grad_norm=0.553126
Epoch 79/100 Iteration 146/234: loss=0.033923 lr=0.000020 grad_norm=0.580409
Epoch 79/100 Iteration 147/234: loss=0.038348 lr=0.000020 grad_norm=0.579380
Epoch 79/100 Iteration 148/234: loss=0.036338 lr=0.000020 grad_norm=0.430709
Epoch 79/100 Iteration 149/234: loss=0.036695 lr=0.000020 grad_norm=0.493478
Epoch 79/100 Iteration 150/234: loss=0.035938 lr=0.000020 grad_norm=0.463705
Epoch 79/100 Iteration 151/234: loss=0.038299 lr=0.000020 grad_norm=0.596705
Epoch 79/100 Iteration 152/234: loss=0.037056 lr=0.000020 grad_norm=0.420280
Epoch 79/100 Iteration 153/234: loss=0.034305 lr=0.000020 grad_norm=0.573217
Epoch 79/100 Iteration 154/234: loss=0.039897 lr=0.000020 grad_norm=0.629551
Epoch 79/100 Iteration 155/234: loss=0.036666 lr=0.000020 grad_norm=0.622104
Epoch 79/100 Iteration 156/234: loss=0.034700 lr=0.000020 grad_norm=0.931091
Epoch 79/100 Iteration 157/234: loss=0.036905 lr=0.000020 grad_norm=0.505420
Epoch 79/100 Iteration 158/234: loss=0.034003 lr=0.000020 grad_norm=1.357170
Epoch 79/100 Iteration 159/234: loss=0.035906 lr=0.000020 grad_norm=0.998000
Epoch 79/100 Iteration 160/234: loss=0.034475 lr=0.000020 grad_norm=0.695454
Epoch 79/100 Iteration 161/234: loss=0.038291 lr=0.000020 grad_norm=1.337602
Epoch 79/100 Iteration 162/234: loss=0.036219 lr=0.000020 grad_norm=0.824142
Epoch 79/100 Iteration 163/234: loss=0.037512 lr=0.000020 grad_norm=0.797460
Epoch 79/100 Iteration 164/234: loss=0.037891 lr=0.000020 grad_norm=1.043617
Epoch 79/100 Iteration 165/234: loss=0.039155 lr=0.000020 grad_norm=1.109857
Epoch 79/100 Iteration 166/234: loss=0.037903 lr=0.000020 grad_norm=1.057173
Epoch 79/100 Iteration 167/234: loss=0.040512 lr=0.000020 grad_norm=0.901593
Epoch 79/100 Iteration 168/234: loss=0.036225 lr=0.000020 grad_norm=0.656598
Epoch 79/100 Iteration 169/234: loss=0.034037 lr=0.000020 grad_norm=0.727406
Epoch 79/100 Iteration 170/234: loss=0.035353 lr=0.000020 grad_norm=0.584813
Epoch 79/100 Iteration 171/234: loss=0.035343 lr=0.000020 grad_norm=0.637893
Epoch 79/100 Iteration 172/234: loss=0.035590 lr=0.000020 grad_norm=0.799190
Epoch 79/100 Iteration 173/234: loss=0.034285 lr=0.000020 grad_norm=0.565708
Epoch 79/100 Iteration 174/234: loss=0.029675 lr=0.000020 grad_norm=0.525575
Epoch 79/100 Iteration 175/234: loss=0.041405 lr=0.000020 grad_norm=0.849683
Epoch 79/100 Iteration 176/234: loss=0.038195 lr=0.000020 grad_norm=0.683693
Epoch 79/100 Iteration 177/234: loss=0.037691 lr=0.000020 grad_norm=0.778131
Epoch 79/100 Iteration 178/234: loss=0.039639 lr=0.000020 grad_norm=1.193713
Epoch 79/100 Iteration 179/234: loss=0.037612 lr=0.000020 grad_norm=1.123039
Epoch 79/100 Iteration 180/234: loss=0.038494 lr=0.000020 grad_norm=0.585484
Epoch 79/100 Iteration 181/234: loss=0.037996 lr=0.000020 grad_norm=1.000259
Epoch 79/100 Iteration 182/234: loss=0.040945 lr=0.000020 grad_norm=0.993858
Epoch 79/100 Iteration 183/234: loss=0.038696 lr=0.000020 grad_norm=0.793432
Epoch 79/100 Iteration 184/234: loss=0.038409 lr=0.000020 grad_norm=0.581490
Epoch 79/100 Iteration 185/234: loss=0.036427 lr=0.000020 grad_norm=0.828637
Epoch 79/100 Iteration 186/234: loss=0.033215 lr=0.000020 grad_norm=1.086879
Epoch 79/100 Iteration 187/234: loss=0.036640 lr=0.000020 grad_norm=0.900024
Epoch 79/100 Iteration 188/234: loss=0.034518 lr=0.000020 grad_norm=0.685164
Epoch 79/100 Iteration 189/234: loss=0.032396 lr=0.000020 grad_norm=0.636656
Epoch 79/100 Iteration 190/234: loss=0.034300 lr=0.000020 grad_norm=0.893415
Epoch 79/100 Iteration 191/234: loss=0.038526 lr=0.000020 grad_norm=1.329682
Epoch 79/100 Iteration 192/234: loss=0.041839 lr=0.000020 grad_norm=1.033740
Epoch 79/100 Iteration 193/234: loss=0.040248 lr=0.000020 grad_norm=0.683447
Epoch 79/100 Iteration 194/234: loss=0.034382 lr=0.000020 grad_norm=0.777750
Epoch 79/100 Iteration 195/234: loss=0.037492 lr=0.000020 grad_norm=0.795534
Epoch 79/100 Iteration 196/234: loss=0.040966 lr=0.000020 grad_norm=0.577259
Epoch 79/100 Iteration 197/234: loss=0.036782 lr=0.000020 grad_norm=0.557616
Epoch 79/100 Iteration 198/234: loss=0.040003 lr=0.000020 grad_norm=0.586454
Epoch 79/100 Iteration 199/234: loss=0.036961 lr=0.000020 grad_norm=0.608806
Epoch 79/100 Iteration 200/234: loss=0.036955 lr=0.000020 grad_norm=0.586411
Epoch 79/100 Iteration 201/234: loss=0.032503 lr=0.000020 grad_norm=0.567781
Epoch 79/100 Iteration 202/234: loss=0.041832 lr=0.000020 grad_norm=0.501555
Epoch 79/100 Iteration 203/234: loss=0.031717 lr=0.000020 grad_norm=0.556511
Epoch 79/100 Iteration 204/234: loss=0.038823 lr=0.000020 grad_norm=0.571574
Epoch 79/100 Iteration 205/234: loss=0.042648 lr=0.000020 grad_norm=0.637923
Epoch 79/100 Iteration 206/234: loss=0.038352 lr=0.000020 grad_norm=0.549782
Epoch 79/100 Iteration 207/234: loss=0.037664 lr=0.000020 grad_norm=0.522318
Epoch 79/100 Iteration 208/234: loss=0.036597 lr=0.000020 grad_norm=0.452598
Epoch 79/100 Iteration 209/234: loss=0.034584 lr=0.000020 grad_norm=0.573597
Epoch 79/100 Iteration 210/234: loss=0.031554 lr=0.000020 grad_norm=0.676884
Epoch 79/100 Iteration 211/234: loss=0.038781 lr=0.000020 grad_norm=1.119383
Epoch 79/100 Iteration 212/234: loss=0.037274 lr=0.000020 grad_norm=1.000877
Epoch 79/100 Iteration 213/234: loss=0.034902 lr=0.000020 grad_norm=0.459950
Epoch 79/100 Iteration 214/234: loss=0.036031 lr=0.000020 grad_norm=0.711134
Epoch 79/100 Iteration 215/234: loss=0.035863 lr=0.000020 grad_norm=0.534099
Epoch 79/100 Iteration 216/234: loss=0.034314 lr=0.000020 grad_norm=0.725478
Epoch 79/100 Iteration 217/234: loss=0.036725 lr=0.000020 grad_norm=0.458652
Epoch 79/100 Iteration 218/234: loss=0.044306 lr=0.000020 grad_norm=0.803030
Epoch 79/100 Iteration 219/234: loss=0.037000 lr=0.000020 grad_norm=0.955233
Epoch 79/100 Iteration 220/234: loss=0.036206 lr=0.000020 grad_norm=0.537929
Epoch 79/100 Iteration 221/234: loss=0.036737 lr=0.000020 grad_norm=0.633526
Epoch 79/100 Iteration 222/234: loss=0.035582 lr=0.000020 grad_norm=0.710452
Epoch 79/100 Iteration 223/234: loss=0.036132 lr=0.000020 grad_norm=0.542740
Epoch 79/100 Iteration 224/234: loss=0.039341 lr=0.000020 grad_norm=0.495831
Epoch 79/100 Iteration 225/234: loss=0.034337 lr=0.000020 grad_norm=0.855703
Epoch 79/100 Iteration 226/234: loss=0.037095 lr=0.000020 grad_norm=0.963805
Epoch 79/100 Iteration 227/234: loss=0.035174 lr=0.000020 grad_norm=0.853522
Epoch 79/100 Iteration 228/234: loss=0.036644 lr=0.000020 grad_norm=0.919211
Epoch 79/100 Iteration 229/234: loss=0.042908 lr=0.000020 grad_norm=1.106117
Epoch 79/100 Iteration 230/234: loss=0.033877 lr=0.000020 grad_norm=1.337785
Epoch 79/100 Iteration 231/234: loss=0.034673 lr=0.000020 grad_norm=0.816735
Epoch 79/100 Iteration 232/234: loss=0.033819 lr=0.000020 grad_norm=0.505467
Epoch 79/100 Iteration 233/234: loss=0.037278 lr=0.000020 grad_norm=0.940374
Epoch 79/100 Iteration 234/234: loss=0.035439 lr=0.000020 grad_norm=0.654547
Epoch 79/100 finished. Avg Loss: 0.037152
Epoch 80/100 Iteration 1/234: loss=0.034673 lr=0.000020 grad_norm=0.641732
Epoch 80/100 Iteration 2/234: loss=0.038035 lr=0.000020 grad_norm=1.278805
Epoch 80/100 Iteration 3/234: loss=0.038354 lr=0.000020 grad_norm=0.846460
Epoch 80/100 Iteration 4/234: loss=0.038776 lr=0.000020 grad_norm=0.515248
Epoch 80/100 Iteration 5/234: loss=0.035191 lr=0.000020 grad_norm=0.809316
Epoch 80/100 Iteration 6/234: loss=0.034741 lr=0.000020 grad_norm=0.467353
Epoch 80/100 Iteration 7/234: loss=0.036831 lr=0.000020 grad_norm=0.588495
Epoch 80/100 Iteration 8/234: loss=0.034580 lr=0.000020 grad_norm=0.602645
Epoch 80/100 Iteration 9/234: loss=0.039072 lr=0.000020 grad_norm=0.562120
Epoch 80/100 Iteration 10/234: loss=0.034281 lr=0.000020 grad_norm=0.494749
Epoch 80/100 Iteration 11/234: loss=0.039054 lr=0.000020 grad_norm=0.562252
Epoch 80/100 Iteration 12/234: loss=0.036732 lr=0.000020 grad_norm=0.770944
Epoch 80/100 Iteration 13/234: loss=0.034506 lr=0.000020 grad_norm=0.818040
Epoch 80/100 Iteration 14/234: loss=0.038061 lr=0.000020 grad_norm=0.556163
Epoch 80/100 Iteration 15/234: loss=0.040208 lr=0.000020 grad_norm=1.017299
Epoch 80/100 Iteration 16/234: loss=0.036704 lr=0.000020 grad_norm=1.529580
Epoch 80/100 Iteration 17/234: loss=0.038895 lr=0.000020 grad_norm=0.966183
Epoch 80/100 Iteration 18/234: loss=0.035143 lr=0.000020 grad_norm=0.338493
Epoch 80/100 Iteration 19/234: loss=0.035681 lr=0.000020 grad_norm=0.663880
Epoch 80/100 Iteration 20/234: loss=0.036328 lr=0.000020 grad_norm=0.447006
Epoch 80/100 Iteration 21/234: loss=0.039460 lr=0.000020 grad_norm=1.108580
Epoch 80/100 Iteration 22/234: loss=0.037178 lr=0.000020 grad_norm=1.774017
Epoch 80/100 Iteration 23/234: loss=0.034550 lr=0.000020 grad_norm=1.044422
Epoch 80/100 Iteration 24/234: loss=0.038106 lr=0.000020 grad_norm=0.614005
Epoch 80/100 Iteration 25/234: loss=0.035006 lr=0.000020 grad_norm=1.034907
Epoch 80/100 Iteration 26/234: loss=0.035567 lr=0.000020 grad_norm=0.692082
Epoch 80/100 Iteration 27/234: loss=0.039375 lr=0.000020 grad_norm=0.576420
Epoch 80/100 Iteration 28/234: loss=0.036883 lr=0.000020 grad_norm=0.653596
Epoch 80/100 Iteration 29/234: loss=0.035422 lr=0.000020 grad_norm=0.555565
Epoch 80/100 Iteration 30/234: loss=0.038346 lr=0.000020 grad_norm=0.624927
Epoch 80/100 Iteration 31/234: loss=0.033785 lr=0.000020 grad_norm=0.843685
Epoch 80/100 Iteration 32/234: loss=0.032863 lr=0.000020 grad_norm=0.479772
Epoch 80/100 Iteration 33/234: loss=0.038039 lr=0.000020 grad_norm=0.779012
Epoch 80/100 Iteration 34/234: loss=0.034973 lr=0.000020 grad_norm=1.023348
Epoch 80/100 Iteration 35/234: loss=0.036735 lr=0.000020 grad_norm=0.580806
Epoch 80/100 Iteration 36/234: loss=0.035868 lr=0.000020 grad_norm=0.651685
Epoch 80/100 Iteration 37/234: loss=0.034638 lr=0.000020 grad_norm=0.696347
Epoch 80/100 Iteration 38/234: loss=0.035581 lr=0.000020 grad_norm=0.413761
Epoch 80/100 Iteration 39/234: loss=0.035572 lr=0.000020 grad_norm=0.721681
Epoch 80/100 Iteration 40/234: loss=0.035345 lr=0.000020 grad_norm=0.522882
Epoch 80/100 Iteration 41/234: loss=0.033167 lr=0.000020 grad_norm=0.514848
Epoch 80/100 Iteration 42/234: loss=0.032535 lr=0.000020 grad_norm=0.563992
Epoch 80/100 Iteration 43/234: loss=0.038139 lr=0.000020 grad_norm=0.487432
Epoch 80/100 Iteration 44/234: loss=0.037709 lr=0.000020 grad_norm=0.420595
Epoch 80/100 Iteration 45/234: loss=0.033407 lr=0.000020 grad_norm=0.578252
Epoch 80/100 Iteration 46/234: loss=0.038559 lr=0.000020 grad_norm=0.727682
Epoch 80/100 Iteration 47/234: loss=0.039758 lr=0.000020 grad_norm=0.480062
Epoch 80/100 Iteration 48/234: loss=0.036187 lr=0.000020 grad_norm=0.421894
Epoch 80/100 Iteration 49/234: loss=0.033619 lr=0.000020 grad_norm=0.391777
Epoch 80/100 Iteration 50/234: loss=0.033357 lr=0.000020 grad_norm=0.474539
Epoch 80/100 Iteration 51/234: loss=0.039103 lr=0.000020 grad_norm=0.532588
Epoch 80/100 Iteration 52/234: loss=0.033087 lr=0.000020 grad_norm=0.454310
Epoch 80/100 Iteration 53/234: loss=0.033667 lr=0.000020 grad_norm=0.339314
Epoch 80/100 Iteration 54/234: loss=0.034402 lr=0.000020 grad_norm=0.601053
Epoch 80/100 Iteration 55/234: loss=0.037793 lr=0.000020 grad_norm=0.694779
Epoch 80/100 Iteration 56/234: loss=0.033453 lr=0.000020 grad_norm=1.353708
Epoch 80/100 Iteration 57/234: loss=0.036280 lr=0.000020 grad_norm=1.734035
Epoch 80/100 Iteration 58/234: loss=0.041493 lr=0.000020 grad_norm=1.055153
Epoch 80/100 Iteration 59/234: loss=0.037572 lr=0.000020 grad_norm=0.600912
Epoch 80/100 Iteration 60/234: loss=0.035084 lr=0.000020 grad_norm=1.034491
Epoch 80/100 Iteration 61/234: loss=0.036716 lr=0.000020 grad_norm=0.576071
Epoch 80/100 Iteration 62/234: loss=0.038235 lr=0.000020 grad_norm=0.733361
Epoch 80/100 Iteration 63/234: loss=0.036757 lr=0.000020 grad_norm=1.110923
Epoch 80/100 Iteration 64/234: loss=0.031459 lr=0.000020 grad_norm=0.564572
Epoch 80/100 Iteration 65/234: loss=0.037869 lr=0.000020 grad_norm=1.203406
Epoch 80/100 Iteration 66/234: loss=0.038939 lr=0.000020 grad_norm=1.946488
Epoch 80/100 Iteration 67/234: loss=0.039796 lr=0.000020 grad_norm=1.348354
Epoch 80/100 Iteration 68/234: loss=0.040717 lr=0.000020 grad_norm=0.928622
Epoch 80/100 Iteration 69/234: loss=0.036536 lr=0.000020 grad_norm=1.147718
Epoch 80/100 Iteration 70/234: loss=0.040053 lr=0.000020 grad_norm=0.762343
Epoch 80/100 Iteration 71/234: loss=0.031782 lr=0.000020 grad_norm=0.891846
Epoch 80/100 Iteration 72/234: loss=0.035212 lr=0.000020 grad_norm=0.546378
Epoch 80/100 Iteration 73/234: loss=0.036525 lr=0.000020 grad_norm=0.744194
Epoch 80/100 Iteration 74/234: loss=0.039124 lr=0.000020 grad_norm=0.830754
Epoch 80/100 Iteration 75/234: loss=0.033645 lr=0.000020 grad_norm=0.509183
Epoch 80/100 Iteration 76/234: loss=0.036922 lr=0.000020 grad_norm=0.680497
Epoch 80/100 Iteration 77/234: loss=0.034187 lr=0.000020 grad_norm=0.800448
Epoch 80/100 Iteration 78/234: loss=0.039130 lr=0.000020 grad_norm=0.684283
Epoch 80/100 Iteration 79/234: loss=0.038279 lr=0.000020 grad_norm=0.716010
Epoch 80/100 Iteration 80/234: loss=0.036960 lr=0.000020 grad_norm=0.966906
Epoch 80/100 Iteration 81/234: loss=0.040272 lr=0.000020 grad_norm=0.596761
Epoch 80/100 Iteration 82/234: loss=0.036854 lr=0.000020 grad_norm=1.041092
Epoch 80/100 Iteration 83/234: loss=0.037101 lr=0.000020 grad_norm=0.816597
Epoch 80/100 Iteration 84/234: loss=0.040100 lr=0.000020 grad_norm=0.812026
Epoch 80/100 Iteration 85/234: loss=0.038298 lr=0.000020 grad_norm=1.023665
Epoch 80/100 Iteration 86/234: loss=0.035925 lr=0.000020 grad_norm=0.574164
Epoch 80/100 Iteration 87/234: loss=0.034710 lr=0.000020 grad_norm=0.480677
Epoch 80/100 Iteration 88/234: loss=0.035883 lr=0.000020 grad_norm=0.677825
Epoch 80/100 Iteration 89/234: loss=0.038459 lr=0.000020 grad_norm=0.639517
Epoch 80/100 Iteration 90/234: loss=0.044362 lr=0.000020 grad_norm=0.444703
Epoch 80/100 Iteration 91/234: loss=0.040665 lr=0.000020 grad_norm=0.861432
Epoch 80/100 Iteration 92/234: loss=0.032435 lr=0.000020 grad_norm=0.761572
Epoch 80/100 Iteration 93/234: loss=0.033669 lr=0.000020 grad_norm=0.692516
Epoch 80/100 Iteration 94/234: loss=0.033290 lr=0.000020 grad_norm=0.741231
Epoch 80/100 Iteration 95/234: loss=0.038826 lr=0.000020 grad_norm=0.494710
Epoch 80/100 Iteration 96/234: loss=0.038879 lr=0.000020 grad_norm=0.602456
Epoch 80/100 Iteration 97/234: loss=0.036509 lr=0.000020 grad_norm=0.630651
Epoch 80/100 Iteration 98/234: loss=0.037403 lr=0.000020 grad_norm=0.577301
Epoch 80/100 Iteration 99/234: loss=0.033394 lr=0.000020 grad_norm=0.472465
Epoch 80/100 Iteration 100/234: loss=0.038303 lr=0.000020 grad_norm=0.620035
Epoch 80/100 Iteration 101/234: loss=0.032847 lr=0.000020 grad_norm=0.629125
Epoch 80/100 Iteration 102/234: loss=0.036548 lr=0.000020 grad_norm=0.554698
Epoch 80/100 Iteration 103/234: loss=0.034959 lr=0.000020 grad_norm=0.474388
Epoch 80/100 Iteration 104/234: loss=0.036724 lr=0.000020 grad_norm=0.575818
Epoch 80/100 Iteration 105/234: loss=0.033933 lr=0.000020 grad_norm=0.672327
Epoch 80/100 Iteration 106/234: loss=0.034829 lr=0.000020 grad_norm=0.597086
Epoch 80/100 Iteration 107/234: loss=0.038181 lr=0.000020 grad_norm=0.790473
Epoch 80/100 Iteration 108/234: loss=0.038606 lr=0.000020 grad_norm=1.121568
Epoch 80/100 Iteration 109/234: loss=0.033506 lr=0.000020 grad_norm=1.034001
Epoch 80/100 Iteration 110/234: loss=0.036111 lr=0.000020 grad_norm=0.513417
Epoch 80/100 Iteration 111/234: loss=0.039521 lr=0.000020 grad_norm=0.776176
Epoch 80/100 Iteration 112/234: loss=0.033801 lr=0.000020 grad_norm=0.760689
Epoch 80/100 Iteration 113/234: loss=0.037724 lr=0.000020 grad_norm=0.454740
Epoch 80/100 Iteration 114/234: loss=0.036017 lr=0.000020 grad_norm=0.993959
Epoch 80/100 Iteration 115/234: loss=0.038192 lr=0.000020 grad_norm=1.471743
Epoch 80/100 Iteration 116/234: loss=0.037020 lr=0.000020 grad_norm=1.242177
Epoch 80/100 Iteration 117/234: loss=0.042033 lr=0.000020 grad_norm=0.901514
Epoch 80/100 Iteration 118/234: loss=0.035559 lr=0.000020 grad_norm=0.964851
Epoch 80/100 Iteration 119/234: loss=0.037813 lr=0.000020 grad_norm=0.595409
Epoch 80/100 Iteration 120/234: loss=0.037722 lr=0.000020 grad_norm=0.681659
Epoch 80/100 Iteration 121/234: loss=0.037034 lr=0.000020 grad_norm=0.508843
Epoch 80/100 Iteration 122/234: loss=0.039812 lr=0.000020 grad_norm=0.460435
Epoch 80/100 Iteration 123/234: loss=0.038927 lr=0.000020 grad_norm=0.731671
Epoch 80/100 Iteration 124/234: loss=0.038062 lr=0.000020 grad_norm=0.663674
Epoch 80/100 Iteration 125/234: loss=0.034195 lr=0.000020 grad_norm=0.359082
Epoch 80/100 Iteration 126/234: loss=0.038904 lr=0.000020 grad_norm=0.763861
Epoch 80/100 Iteration 127/234: loss=0.034481 lr=0.000020 grad_norm=0.498482
Epoch 80/100 Iteration 128/234: loss=0.036220 lr=0.000020 grad_norm=0.608334
Epoch 80/100 Iteration 129/234: loss=0.036681 lr=0.000020 grad_norm=0.934917
Epoch 80/100 Iteration 130/234: loss=0.038121 lr=0.000020 grad_norm=1.009850
Epoch 80/100 Iteration 131/234: loss=0.037010 lr=0.000020 grad_norm=0.887363
Epoch 80/100 Iteration 132/234: loss=0.035186 lr=0.000020 grad_norm=0.681587
Epoch 80/100 Iteration 133/234: loss=0.036667 lr=0.000020 grad_norm=0.522392
Epoch 80/100 Iteration 134/234: loss=0.036059 lr=0.000020 grad_norm=0.893143
Epoch 80/100 Iteration 135/234: loss=0.037700 lr=0.000020 grad_norm=1.076691
Epoch 80/100 Iteration 136/234: loss=0.038919 lr=0.000020 grad_norm=0.687077
Epoch 80/100 Iteration 137/234: loss=0.033410 lr=0.000020 grad_norm=0.402282
Epoch 80/100 Iteration 138/234: loss=0.034890 lr=0.000020 grad_norm=0.765069
Epoch 80/100 Iteration 139/234: loss=0.037919 lr=0.000020 grad_norm=1.110679
Epoch 80/100 Iteration 140/234: loss=0.038758 lr=0.000020 grad_norm=0.982178
Epoch 80/100 Iteration 141/234: loss=0.031900 lr=0.000020 grad_norm=0.371168
Epoch 80/100 Iteration 142/234: loss=0.040315 lr=0.000020 grad_norm=0.539324
Epoch 80/100 Iteration 143/234: loss=0.036424 lr=0.000020 grad_norm=0.785825
Epoch 80/100 Iteration 144/234: loss=0.036290 lr=0.000020 grad_norm=0.537577
Epoch 80/100 Iteration 145/234: loss=0.035934 lr=0.000020 grad_norm=0.489307
Epoch 80/100 Iteration 146/234: loss=0.037954 lr=0.000020 grad_norm=0.635781
Epoch 80/100 Iteration 147/234: loss=0.036793 lr=0.000020 grad_norm=0.429618
Epoch 80/100 Iteration 148/234: loss=0.037364 lr=0.000020 grad_norm=0.443904
Epoch 80/100 Iteration 149/234: loss=0.037747 lr=0.000020 grad_norm=0.635036
Epoch 80/100 Iteration 150/234: loss=0.036788 lr=0.000020 grad_norm=0.710537
Epoch 80/100 Iteration 151/234: loss=0.037047 lr=0.000020 grad_norm=0.432581
Epoch 80/100 Iteration 152/234: loss=0.040900 lr=0.000020 grad_norm=0.862144
Epoch 80/100 Iteration 153/234: loss=0.040206 lr=0.000020 grad_norm=1.119928
Epoch 80/100 Iteration 154/234: loss=0.037986 lr=0.000020 grad_norm=0.769157
Epoch 80/100 Iteration 155/234: loss=0.040230 lr=0.000020 grad_norm=0.680602
Epoch 80/100 Iteration 156/234: loss=0.038291 lr=0.000020 grad_norm=0.674280
Epoch 80/100 Iteration 157/234: loss=0.034602 lr=0.000020 grad_norm=0.541379
Epoch 80/100 Iteration 158/234: loss=0.035848 lr=0.000020 grad_norm=0.492558
Epoch 80/100 Iteration 159/234: loss=0.036407 lr=0.000020 grad_norm=0.774600
Epoch 80/100 Iteration 160/234: loss=0.035327 lr=0.000020 grad_norm=0.796164
Epoch 80/100 Iteration 161/234: loss=0.038043 lr=0.000020 grad_norm=0.563545
Epoch 80/100 Iteration 162/234: loss=0.037130 lr=0.000020 grad_norm=0.583583
Epoch 80/100 Iteration 163/234: loss=0.035757 lr=0.000020 grad_norm=0.891874
Epoch 80/100 Iteration 164/234: loss=0.034463 lr=0.000020 grad_norm=0.558672
Epoch 80/100 Iteration 165/234: loss=0.032968 lr=0.000020 grad_norm=0.527434
Epoch 80/100 Iteration 166/234: loss=0.039684 lr=0.000020 grad_norm=0.965270
Epoch 80/100 Iteration 167/234: loss=0.037491 lr=0.000020 grad_norm=1.204678
Epoch 80/100 Iteration 168/234: loss=0.034896 lr=0.000020 grad_norm=0.591407
Epoch 80/100 Iteration 169/234: loss=0.038222 lr=0.000020 grad_norm=0.885213
Epoch 80/100 Iteration 170/234: loss=0.039672 lr=0.000020 grad_norm=1.389659
Epoch 80/100 Iteration 171/234: loss=0.038299 lr=0.000020 grad_norm=1.302669
Epoch 80/100 Iteration 172/234: loss=0.037139 lr=0.000020 grad_norm=0.595635
Epoch 80/100 Iteration 173/234: loss=0.035860 lr=0.000020 grad_norm=1.251879
Epoch 80/100 Iteration 174/234: loss=0.037992 lr=0.000020 grad_norm=1.202127
Epoch 80/100 Iteration 175/234: loss=0.037556 lr=0.000020 grad_norm=0.948011
Epoch 80/100 Iteration 176/234: loss=0.035325 lr=0.000020 grad_norm=1.903025
Epoch 80/100 Iteration 177/234: loss=0.034983 lr=0.000020 grad_norm=1.777527
Epoch 80/100 Iteration 178/234: loss=0.035691 lr=0.000020 grad_norm=0.983665
Epoch 80/100 Iteration 179/234: loss=0.036446 lr=0.000020 grad_norm=1.691030
Epoch 80/100 Iteration 180/234: loss=0.035256 lr=0.000020 grad_norm=1.646208
Epoch 80/100 Iteration 181/234: loss=0.042945 lr=0.000020 grad_norm=0.969145
Epoch 80/100 Iteration 182/234: loss=0.043344 lr=0.000020 grad_norm=1.838498
Epoch 80/100 Iteration 183/234: loss=0.041141 lr=0.000020 grad_norm=1.929347
Epoch 80/100 Iteration 184/234: loss=0.034517 lr=0.000020 grad_norm=1.475081
Epoch 80/100 Iteration 185/234: loss=0.034332 lr=0.000020 grad_norm=1.027599
Epoch 80/100 Iteration 186/234: loss=0.039360 lr=0.000020 grad_norm=1.440369
Epoch 80/100 Iteration 187/234: loss=0.038312 lr=0.000020 grad_norm=1.224694
Epoch 80/100 Iteration 188/234: loss=0.034131 lr=0.000020 grad_norm=0.613551
Epoch 80/100 Iteration 189/234: loss=0.034357 lr=0.000020 grad_norm=1.083483
Epoch 80/100 Iteration 190/234: loss=0.036833 lr=0.000020 grad_norm=0.680558
Epoch 80/100 Iteration 191/234: loss=0.035988 lr=0.000020 grad_norm=1.004905
Epoch 80/100 Iteration 192/234: loss=0.035904 lr=0.000020 grad_norm=0.787582
Epoch 80/100 Iteration 193/234: loss=0.041233 lr=0.000020 grad_norm=1.227593
Epoch 80/100 Iteration 194/234: loss=0.043711 lr=0.000020 grad_norm=2.128702
Epoch 80/100 Iteration 195/234: loss=0.036601 lr=0.000020 grad_norm=1.044581
Epoch 80/100 Iteration 196/234: loss=0.033623 lr=0.000020 grad_norm=0.998022
Epoch 80/100 Iteration 197/234: loss=0.036840 lr=0.000020 grad_norm=0.936984
Epoch 80/100 Iteration 198/234: loss=0.035094 lr=0.000020 grad_norm=0.850182
Epoch 80/100 Iteration 199/234: loss=0.035890 lr=0.000020 grad_norm=1.232343
Epoch 80/100 Iteration 200/234: loss=0.035313 lr=0.000020 grad_norm=0.525795
Epoch 80/100 Iteration 201/234: loss=0.036245 lr=0.000020 grad_norm=1.141611
Epoch 80/100 Iteration 202/234: loss=0.037418 lr=0.000020 grad_norm=0.485714
Epoch 80/100 Iteration 203/234: loss=0.036362 lr=0.000020 grad_norm=0.830374
Epoch 80/100 Iteration 204/234: loss=0.033458 lr=0.000020 grad_norm=0.367917
Epoch 80/100 Iteration 205/234: loss=0.037174 lr=0.000020 grad_norm=1.094872
Epoch 80/100 Iteration 206/234: loss=0.035464 lr=0.000020 grad_norm=1.222022
Epoch 80/100 Iteration 207/234: loss=0.037901 lr=0.000020 grad_norm=0.713084
Epoch 80/100 Iteration 208/234: loss=0.038913 lr=0.000020 grad_norm=0.943553
Epoch 80/100 Iteration 209/234: loss=0.039733 lr=0.000020 grad_norm=0.484122
Epoch 80/100 Iteration 210/234: loss=0.038663 lr=0.000020 grad_norm=1.149689
Epoch 80/100 Iteration 211/234: loss=0.033031 lr=0.000020 grad_norm=0.862083
Epoch 80/100 Iteration 212/234: loss=0.033577 lr=0.000020 grad_norm=0.492693
Epoch 80/100 Iteration 213/234: loss=0.038084 lr=0.000020 grad_norm=0.933159
Epoch 80/100 Iteration 214/234: loss=0.036012 lr=0.000020 grad_norm=0.576661
Epoch 80/100 Iteration 215/234: loss=0.031956 lr=0.000020 grad_norm=0.584691
Epoch 80/100 Iteration 216/234: loss=0.035930 lr=0.000020 grad_norm=0.654468
Epoch 80/100 Iteration 217/234: loss=0.038132 lr=0.000020 grad_norm=0.527320
Epoch 80/100 Iteration 218/234: loss=0.037018 lr=0.000020 grad_norm=0.847154
Epoch 80/100 Iteration 219/234: loss=0.040020 lr=0.000020 grad_norm=0.623111
Epoch 80/100 Iteration 220/234: loss=0.038275 lr=0.000020 grad_norm=0.426207
Epoch 80/100 Iteration 221/234: loss=0.033859 lr=0.000020 grad_norm=0.714683
Epoch 80/100 Iteration 222/234: loss=0.038065 lr=0.000020 grad_norm=0.494154
Epoch 80/100 Iteration 223/234: loss=0.032413 lr=0.000020 grad_norm=0.399488
Epoch 80/100 Iteration 224/234: loss=0.035359 lr=0.000020 grad_norm=0.479887
Epoch 80/100 Iteration 225/234: loss=0.036483 lr=0.000020 grad_norm=0.551238
Epoch 80/100 Iteration 226/234: loss=0.037742 lr=0.000020 grad_norm=0.534972
Epoch 80/100 Iteration 227/234: loss=0.039418 lr=0.000020 grad_norm=0.535122
Epoch 80/100 Iteration 228/234: loss=0.037095 lr=0.000020 grad_norm=0.657280
Epoch 80/100 Iteration 229/234: loss=0.035294 lr=0.000020 grad_norm=0.419407
Epoch 80/100 Iteration 230/234: loss=0.035296 lr=0.000020 grad_norm=0.570737
Epoch 80/100 Iteration 231/234: loss=0.034731 lr=0.000020 grad_norm=1.006421
Epoch 80/100 Iteration 232/234: loss=0.040221 lr=0.000020 grad_norm=1.010349
Epoch 80/100 Iteration 233/234: loss=0.033167 lr=0.000020 grad_norm=0.709373
Epoch 80/100 Iteration 234/234: loss=0.031998 lr=0.000020 grad_norm=0.496001
Epoch 80/100 finished. Avg Loss: 0.036724
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 81/100 Iteration 1/234: loss=0.037252 lr=0.000020 grad_norm=0.800173
Epoch 81/100 Iteration 2/234: loss=0.038477 lr=0.000020 grad_norm=1.014762
Epoch 81/100 Iteration 3/234: loss=0.038920 lr=0.000020 grad_norm=1.020655
Epoch 81/100 Iteration 4/234: loss=0.038253 lr=0.000020 grad_norm=1.274261
Epoch 81/100 Iteration 5/234: loss=0.036138 lr=0.000020 grad_norm=1.184628
Epoch 81/100 Iteration 6/234: loss=0.036842 lr=0.000020 grad_norm=0.541741
Epoch 81/100 Iteration 7/234: loss=0.037289 lr=0.000020 grad_norm=0.852319
Epoch 81/100 Iteration 8/234: loss=0.040011 lr=0.000020 grad_norm=1.169704
Epoch 81/100 Iteration 9/234: loss=0.035271 lr=0.000020 grad_norm=0.832035
Epoch 81/100 Iteration 10/234: loss=0.031145 lr=0.000020 grad_norm=0.470085
Epoch 81/100 Iteration 11/234: loss=0.038245 lr=0.000020 grad_norm=0.796127
Epoch 81/100 Iteration 12/234: loss=0.036044 lr=0.000020 grad_norm=0.577225
Epoch 81/100 Iteration 13/234: loss=0.042665 lr=0.000020 grad_norm=0.726869
Epoch 81/100 Iteration 14/234: loss=0.039350 lr=0.000020 grad_norm=1.085409
Epoch 81/100 Iteration 15/234: loss=0.031686 lr=0.000020 grad_norm=0.701863
Epoch 81/100 Iteration 16/234: loss=0.038337 lr=0.000020 grad_norm=0.823950
Epoch 81/100 Iteration 17/234: loss=0.033430 lr=0.000020 grad_norm=1.355104
Epoch 81/100 Iteration 18/234: loss=0.038529 lr=0.000020 grad_norm=0.622172
Epoch 81/100 Iteration 19/234: loss=0.036855 lr=0.000020 grad_norm=1.133898
Epoch 81/100 Iteration 20/234: loss=0.038917 lr=0.000020 grad_norm=1.630268
Epoch 81/100 Iteration 21/234: loss=0.038362 lr=0.000020 grad_norm=1.159558
Epoch 81/100 Iteration 22/234: loss=0.036096 lr=0.000020 grad_norm=0.643008
Epoch 81/100 Iteration 23/234: loss=0.039053 lr=0.000020 grad_norm=1.382985
Epoch 81/100 Iteration 24/234: loss=0.033831 lr=0.000020 grad_norm=1.077777
Epoch 81/100 Iteration 25/234: loss=0.038783 lr=0.000020 grad_norm=0.774417
Epoch 81/100 Iteration 26/234: loss=0.036785 lr=0.000020 grad_norm=1.723296
Epoch 81/100 Iteration 27/234: loss=0.038129 lr=0.000020 grad_norm=0.978918
Epoch 81/100 Iteration 28/234: loss=0.038147 lr=0.000020 grad_norm=1.095339
Epoch 81/100 Iteration 29/234: loss=0.037512 lr=0.000020 grad_norm=1.631339
Epoch 81/100 Iteration 30/234: loss=0.035903 lr=0.000020 grad_norm=0.736937
Epoch 81/100 Iteration 31/234: loss=0.034664 lr=0.000020 grad_norm=1.286295
Epoch 81/100 Iteration 32/234: loss=0.036052 lr=0.000020 grad_norm=1.098407
Epoch 81/100 Iteration 33/234: loss=0.033716 lr=0.000020 grad_norm=0.686545
Epoch 81/100 Iteration 34/234: loss=0.036594 lr=0.000020 grad_norm=1.102150
Epoch 81/100 Iteration 35/234: loss=0.038771 lr=0.000020 grad_norm=1.071004
Epoch 81/100 Iteration 36/234: loss=0.039955 lr=0.000020 grad_norm=0.826583
Epoch 81/100 Iteration 37/234: loss=0.038809 lr=0.000020 grad_norm=1.210356
Epoch 81/100 Iteration 38/234: loss=0.039392 lr=0.000020 grad_norm=0.809681
Epoch 81/100 Iteration 39/234: loss=0.037831 lr=0.000020 grad_norm=0.762184
Epoch 81/100 Iteration 40/234: loss=0.040510 lr=0.000020 grad_norm=0.880773
Epoch 81/100 Iteration 41/234: loss=0.038644 lr=0.000020 grad_norm=0.884261
Epoch 81/100 Iteration 42/234: loss=0.038686 lr=0.000020 grad_norm=0.697501
Epoch 81/100 Iteration 43/234: loss=0.033992 lr=0.000020 grad_norm=0.573319
Epoch 81/100 Iteration 44/234: loss=0.035344 lr=0.000020 grad_norm=0.651078
Epoch 81/100 Iteration 45/234: loss=0.036321 lr=0.000020 grad_norm=0.854182
Epoch 81/100 Iteration 46/234: loss=0.037284 lr=0.000020 grad_norm=1.086597
Epoch 81/100 Iteration 47/234: loss=0.033433 lr=0.000020 grad_norm=0.363757
Epoch 81/100 Iteration 48/234: loss=0.038963 lr=0.000020 grad_norm=1.196547
Epoch 81/100 Iteration 49/234: loss=0.032325 lr=0.000020 grad_norm=0.850460
Epoch 81/100 Iteration 50/234: loss=0.035769 lr=0.000020 grad_norm=1.225085
Epoch 81/100 Iteration 51/234: loss=0.037734 lr=0.000020 grad_norm=1.337709
Epoch 81/100 Iteration 52/234: loss=0.040146 lr=0.000020 grad_norm=0.533827
Epoch 81/100 Iteration 53/234: loss=0.035394 lr=0.000020 grad_norm=1.227215
Epoch 81/100 Iteration 54/234: loss=0.038924 lr=0.000020 grad_norm=0.535952
Epoch 81/100 Iteration 55/234: loss=0.036490 lr=0.000020 grad_norm=0.837862
Epoch 81/100 Iteration 56/234: loss=0.039596 lr=0.000020 grad_norm=0.697914
Epoch 81/100 Iteration 57/234: loss=0.039022 lr=0.000020 grad_norm=0.621925
Epoch 81/100 Iteration 58/234: loss=0.035406 lr=0.000020 grad_norm=0.605358
Epoch 81/100 Iteration 59/234: loss=0.039042 lr=0.000020 grad_norm=0.627895
Epoch 81/100 Iteration 60/234: loss=0.033971 lr=0.000020 grad_norm=0.761160
Epoch 81/100 Iteration 61/234: loss=0.036433 lr=0.000020 grad_norm=0.413074
Epoch 81/100 Iteration 62/234: loss=0.037162 lr=0.000020 grad_norm=0.998175
Epoch 81/100 Iteration 63/234: loss=0.038613 lr=0.000020 grad_norm=1.295917
Epoch 81/100 Iteration 64/234: loss=0.034480 lr=0.000020 grad_norm=0.663859
Epoch 81/100 Iteration 65/234: loss=0.033696 lr=0.000020 grad_norm=0.493914
Epoch 81/100 Iteration 66/234: loss=0.037629 lr=0.000020 grad_norm=0.661717
Epoch 81/100 Iteration 67/234: loss=0.036410 lr=0.000020 grad_norm=0.408908
Epoch 81/100 Iteration 68/234: loss=0.038463 lr=0.000020 grad_norm=0.944370
Epoch 81/100 Iteration 69/234: loss=0.033937 lr=0.000020 grad_norm=0.704808
Epoch 81/100 Iteration 70/234: loss=0.035239 lr=0.000020 grad_norm=0.568576
Epoch 81/100 Iteration 71/234: loss=0.035151 lr=0.000020 grad_norm=0.702441
Epoch 81/100 Iteration 72/234: loss=0.032052 lr=0.000020 grad_norm=0.542875
Epoch 81/100 Iteration 73/234: loss=0.036753 lr=0.000020 grad_norm=0.657071
Epoch 81/100 Iteration 74/234: loss=0.034111 lr=0.000020 grad_norm=0.779008
Epoch 81/100 Iteration 75/234: loss=0.039129 lr=0.000020 grad_norm=0.959231
Epoch 81/100 Iteration 76/234: loss=0.034384 lr=0.000020 grad_norm=0.508647
Epoch 81/100 Iteration 77/234: loss=0.036801 lr=0.000020 grad_norm=0.841897
Epoch 81/100 Iteration 78/234: loss=0.037662 lr=0.000020 grad_norm=0.744222
Epoch 81/100 Iteration 79/234: loss=0.032472 lr=0.000020 grad_norm=0.670533
Epoch 81/100 Iteration 80/234: loss=0.037297 lr=0.000020 grad_norm=0.807525
Epoch 81/100 Iteration 81/234: loss=0.035298 lr=0.000020 grad_norm=0.441457
Epoch 81/100 Iteration 82/234: loss=0.037232 lr=0.000020 grad_norm=0.790725
Epoch 81/100 Iteration 83/234: loss=0.038103 lr=0.000020 grad_norm=0.594484
Epoch 81/100 Iteration 84/234: loss=0.039314 lr=0.000020 grad_norm=0.630213
Epoch 81/100 Iteration 85/234: loss=0.040790 lr=0.000020 grad_norm=0.947161
Epoch 81/100 Iteration 86/234: loss=0.035228 lr=0.000020 grad_norm=0.597565
Epoch 81/100 Iteration 87/234: loss=0.033127 lr=0.000020 grad_norm=0.636324
Epoch 81/100 Iteration 88/234: loss=0.036695 lr=0.000020 grad_norm=0.665465
Epoch 81/100 Iteration 89/234: loss=0.038521 lr=0.000020 grad_norm=0.552358
Epoch 81/100 Iteration 90/234: loss=0.034048 lr=0.000020 grad_norm=0.666995
Epoch 81/100 Iteration 91/234: loss=0.037427 lr=0.000020 grad_norm=0.780720
Epoch 81/100 Iteration 92/234: loss=0.039970 lr=0.000020 grad_norm=0.908928
Epoch 81/100 Iteration 93/234: loss=0.037155 lr=0.000020 grad_norm=0.616637
Epoch 81/100 Iteration 94/234: loss=0.034238 lr=0.000020 grad_norm=0.481445
Epoch 81/100 Iteration 95/234: loss=0.037373 lr=0.000020 grad_norm=0.931679
Epoch 81/100 Iteration 96/234: loss=0.034087 lr=0.000020 grad_norm=1.396022
Epoch 81/100 Iteration 97/234: loss=0.033918 lr=0.000020 grad_norm=0.714057
Epoch 81/100 Iteration 98/234: loss=0.037606 lr=0.000020 grad_norm=0.839133
Epoch 81/100 Iteration 99/234: loss=0.033099 lr=0.000020 grad_norm=1.056160
Epoch 81/100 Iteration 100/234: loss=0.036814 lr=0.000020 grad_norm=0.598442
Epoch 81/100 Iteration 101/234: loss=0.030478 lr=0.000020 grad_norm=0.984210
Epoch 81/100 Iteration 102/234: loss=0.035324 lr=0.000020 grad_norm=0.949179
Epoch 81/100 Iteration 103/234: loss=0.038379 lr=0.000020 grad_norm=1.105763
Epoch 81/100 Iteration 104/234: loss=0.036524 lr=0.000020 grad_norm=1.007777
Epoch 81/100 Iteration 105/234: loss=0.035883 lr=0.000020 grad_norm=0.643812
Epoch 81/100 Iteration 106/234: loss=0.039146 lr=0.000020 grad_norm=1.387997
Epoch 81/100 Iteration 107/234: loss=0.038818 lr=0.000020 grad_norm=1.257913
Epoch 81/100 Iteration 108/234: loss=0.036281 lr=0.000020 grad_norm=0.832538
Epoch 81/100 Iteration 109/234: loss=0.040020 lr=0.000020 grad_norm=0.956652
Epoch 81/100 Iteration 110/234: loss=0.038756 lr=0.000020 grad_norm=0.851026
Epoch 81/100 Iteration 111/234: loss=0.036191 lr=0.000020 grad_norm=0.556988
Epoch 81/100 Iteration 112/234: loss=0.036930 lr=0.000020 grad_norm=0.870384
Epoch 81/100 Iteration 113/234: loss=0.037014 lr=0.000020 grad_norm=0.766743
Epoch 81/100 Iteration 114/234: loss=0.039704 lr=0.000020 grad_norm=0.689976
Epoch 81/100 Iteration 115/234: loss=0.033785 lr=0.000020 grad_norm=0.900533
Epoch 81/100 Iteration 116/234: loss=0.031189 lr=0.000020 grad_norm=0.497955
Epoch 81/100 Iteration 117/234: loss=0.037312 lr=0.000020 grad_norm=0.723332
Epoch 81/100 Iteration 118/234: loss=0.038749 lr=0.000020 grad_norm=0.963473
Epoch 81/100 Iteration 119/234: loss=0.038752 lr=0.000020 grad_norm=0.528408
Epoch 81/100 Iteration 120/234: loss=0.035016 lr=0.000020 grad_norm=0.695180
Epoch 81/100 Iteration 121/234: loss=0.031190 lr=0.000020 grad_norm=0.475976
Epoch 81/100 Iteration 122/234: loss=0.037346 lr=0.000020 grad_norm=0.650964
Epoch 81/100 Iteration 123/234: loss=0.038527 lr=0.000020 grad_norm=0.608381
Epoch 81/100 Iteration 124/234: loss=0.036409 lr=0.000020 grad_norm=0.542493
Epoch 81/100 Iteration 125/234: loss=0.038149 lr=0.000020 grad_norm=0.524352
Epoch 81/100 Iteration 126/234: loss=0.035960 lr=0.000020 grad_norm=0.485695
Epoch 81/100 Iteration 127/234: loss=0.032252 lr=0.000020 grad_norm=0.328611
Epoch 81/100 Iteration 128/234: loss=0.037185 lr=0.000020 grad_norm=0.507662
Epoch 81/100 Iteration 129/234: loss=0.036758 lr=0.000020 grad_norm=0.379687
Epoch 81/100 Iteration 130/234: loss=0.035480 lr=0.000020 grad_norm=0.586187
Epoch 81/100 Iteration 131/234: loss=0.030421 lr=0.000020 grad_norm=0.801828
Epoch 81/100 Iteration 132/234: loss=0.039384 lr=0.000020 grad_norm=0.542180
Epoch 81/100 Iteration 133/234: loss=0.035838 lr=0.000020 grad_norm=0.565574
Epoch 81/100 Iteration 134/234: loss=0.036719 lr=0.000020 grad_norm=0.867132
Epoch 81/100 Iteration 135/234: loss=0.033995 lr=0.000020 grad_norm=0.741237
Epoch 81/100 Iteration 136/234: loss=0.039106 lr=0.000020 grad_norm=0.476004
Epoch 81/100 Iteration 137/234: loss=0.038389 lr=0.000020 grad_norm=1.143623
Epoch 81/100 Iteration 138/234: loss=0.034100 lr=0.000020 grad_norm=1.065587
Epoch 81/100 Iteration 139/234: loss=0.036478 lr=0.000020 grad_norm=0.529877
Epoch 81/100 Iteration 140/234: loss=0.034785 lr=0.000020 grad_norm=0.975096
Epoch 81/100 Iteration 141/234: loss=0.035289 lr=0.000020 grad_norm=1.221457
Epoch 81/100 Iteration 142/234: loss=0.039353 lr=0.000020 grad_norm=0.832029
Epoch 81/100 Iteration 143/234: loss=0.038442 lr=0.000020 grad_norm=0.657689
Epoch 81/100 Iteration 144/234: loss=0.035531 lr=0.000020 grad_norm=0.809391
Epoch 81/100 Iteration 145/234: loss=0.038264 lr=0.000020 grad_norm=0.591662
Epoch 81/100 Iteration 146/234: loss=0.038198 lr=0.000020 grad_norm=0.629394
Epoch 81/100 Iteration 147/234: loss=0.039386 lr=0.000020 grad_norm=1.497318
Epoch 81/100 Iteration 148/234: loss=0.038343 lr=0.000020 grad_norm=1.813253
Epoch 81/100 Iteration 149/234: loss=0.041472 lr=0.000020 grad_norm=1.241603
Epoch 81/100 Iteration 150/234: loss=0.036471 lr=0.000020 grad_norm=0.622217
Epoch 81/100 Iteration 151/234: loss=0.033877 lr=0.000020 grad_norm=0.745759
Epoch 81/100 Iteration 152/234: loss=0.033984 lr=0.000020 grad_norm=0.641645
Epoch 81/100 Iteration 153/234: loss=0.036914 lr=0.000020 grad_norm=0.477812
Epoch 81/100 Iteration 154/234: loss=0.042703 lr=0.000020 grad_norm=0.655701
Epoch 81/100 Iteration 155/234: loss=0.039715 lr=0.000020 grad_norm=0.618106
Epoch 81/100 Iteration 156/234: loss=0.032017 lr=0.000020 grad_norm=0.612168
Epoch 81/100 Iteration 157/234: loss=0.033652 lr=0.000020 grad_norm=0.596282
Epoch 81/100 Iteration 158/234: loss=0.039972 lr=0.000020 grad_norm=0.680342
Epoch 81/100 Iteration 159/234: loss=0.035297 lr=0.000020 grad_norm=1.015412
Epoch 81/100 Iteration 160/234: loss=0.038681 lr=0.000020 grad_norm=0.903949
Epoch 81/100 Iteration 161/234: loss=0.038344 lr=0.000020 grad_norm=0.630367
Epoch 81/100 Iteration 162/234: loss=0.039688 lr=0.000020 grad_norm=0.862947
Epoch 81/100 Iteration 163/234: loss=0.035319 lr=0.000020 grad_norm=1.172869
Epoch 81/100 Iteration 164/234: loss=0.035709 lr=0.000020 grad_norm=1.348649
Epoch 81/100 Iteration 165/234: loss=0.039423 lr=0.000020 grad_norm=1.074543
Epoch 81/100 Iteration 166/234: loss=0.039493 lr=0.000020 grad_norm=0.637346
Epoch 81/100 Iteration 167/234: loss=0.031592 lr=0.000020 grad_norm=0.685007
Epoch 81/100 Iteration 168/234: loss=0.039873 lr=0.000020 grad_norm=1.101617
Epoch 81/100 Iteration 169/234: loss=0.036722 lr=0.000020 grad_norm=1.067626
Epoch 81/100 Iteration 170/234: loss=0.038454 lr=0.000020 grad_norm=0.431990
Epoch 81/100 Iteration 171/234: loss=0.039718 lr=0.000020 grad_norm=1.136095
Epoch 81/100 Iteration 172/234: loss=0.033707 lr=0.000020 grad_norm=1.088458
Epoch 81/100 Iteration 173/234: loss=0.037293 lr=0.000020 grad_norm=0.650675
Epoch 81/100 Iteration 174/234: loss=0.033722 lr=0.000020 grad_norm=1.077484
Epoch 81/100 Iteration 175/234: loss=0.038903 lr=0.000020 grad_norm=1.505440
Epoch 81/100 Iteration 176/234: loss=0.034509 lr=0.000020 grad_norm=1.277158
Epoch 81/100 Iteration 177/234: loss=0.036914 lr=0.000020 grad_norm=0.752679
Epoch 81/100 Iteration 178/234: loss=0.031988 lr=0.000020 grad_norm=0.594271
Epoch 81/100 Iteration 179/234: loss=0.034521 lr=0.000020 grad_norm=0.744731
Epoch 81/100 Iteration 180/234: loss=0.036738 lr=0.000020 grad_norm=0.518749
Epoch 81/100 Iteration 181/234: loss=0.040046 lr=0.000020 grad_norm=0.607437
Epoch 81/100 Iteration 182/234: loss=0.039314 lr=0.000020 grad_norm=0.898707
Epoch 81/100 Iteration 183/234: loss=0.035093 lr=0.000020 grad_norm=0.758832
Epoch 81/100 Iteration 184/234: loss=0.032948 lr=0.000020 grad_norm=0.539164
Epoch 81/100 Iteration 185/234: loss=0.035924 lr=0.000020 grad_norm=0.991105
Epoch 81/100 Iteration 186/234: loss=0.038137 lr=0.000020 grad_norm=1.140909
Epoch 81/100 Iteration 187/234: loss=0.034795 lr=0.000020 grad_norm=0.954039
Epoch 81/100 Iteration 188/234: loss=0.034598 lr=0.000020 grad_norm=0.746368
Epoch 81/100 Iteration 189/234: loss=0.039046 lr=0.000020 grad_norm=0.606850
Epoch 81/100 Iteration 190/234: loss=0.035514 lr=0.000020 grad_norm=0.648378
Epoch 81/100 Iteration 191/234: loss=0.034672 lr=0.000020 grad_norm=0.577105
Epoch 81/100 Iteration 192/234: loss=0.040900 lr=0.000020 grad_norm=0.489531
Epoch 81/100 Iteration 193/234: loss=0.034314 lr=0.000020 grad_norm=0.530890
Epoch 81/100 Iteration 194/234: loss=0.039477 lr=0.000020 grad_norm=0.615651
Epoch 81/100 Iteration 195/234: loss=0.035677 lr=0.000020 grad_norm=0.788118
Epoch 81/100 Iteration 196/234: loss=0.033474 lr=0.000020 grad_norm=0.801219
Epoch 81/100 Iteration 197/234: loss=0.034634 lr=0.000020 grad_norm=0.382858
Epoch 81/100 Iteration 198/234: loss=0.036618 lr=0.000020 grad_norm=0.808261
Epoch 81/100 Iteration 199/234: loss=0.039934 lr=0.000020 grad_norm=1.004184
Epoch 81/100 Iteration 200/234: loss=0.040276 lr=0.000020 grad_norm=0.470327
Epoch 81/100 Iteration 201/234: loss=0.038855 lr=0.000020 grad_norm=0.722197
Epoch 81/100 Iteration 202/234: loss=0.035948 lr=0.000020 grad_norm=1.235937
Epoch 81/100 Iteration 203/234: loss=0.033637 lr=0.000020 grad_norm=0.898161
Epoch 81/100 Iteration 204/234: loss=0.035203 lr=0.000020 grad_norm=0.692366
Epoch 81/100 Iteration 205/234: loss=0.037247 lr=0.000020 grad_norm=1.612500
Epoch 81/100 Iteration 206/234: loss=0.037061 lr=0.000020 grad_norm=1.223022
Epoch 81/100 Iteration 207/234: loss=0.039886 lr=0.000020 grad_norm=0.775068
Epoch 81/100 Iteration 208/234: loss=0.033201 lr=0.000020 grad_norm=1.337220
Epoch 81/100 Iteration 209/234: loss=0.040438 lr=0.000020 grad_norm=1.049478
Epoch 81/100 Iteration 210/234: loss=0.036052 lr=0.000020 grad_norm=1.113253
Epoch 81/100 Iteration 211/234: loss=0.039411 lr=0.000020 grad_norm=0.753553
Epoch 81/100 Iteration 212/234: loss=0.038599 lr=0.000020 grad_norm=0.579510
Epoch 81/100 Iteration 213/234: loss=0.033894 lr=0.000020 grad_norm=0.911135
Epoch 81/100 Iteration 214/234: loss=0.036334 lr=0.000020 grad_norm=0.441149
Epoch 81/100 Iteration 215/234: loss=0.039299 lr=0.000020 grad_norm=0.917078
Epoch 81/100 Iteration 216/234: loss=0.034066 lr=0.000020 grad_norm=0.730348
Epoch 81/100 Iteration 217/234: loss=0.032803 lr=0.000020 grad_norm=0.799351
Epoch 81/100 Iteration 218/234: loss=0.034579 lr=0.000020 grad_norm=0.712259
Epoch 81/100 Iteration 219/234: loss=0.038884 lr=0.000020 grad_norm=0.537028
Epoch 81/100 Iteration 220/234: loss=0.042343 lr=0.000020 grad_norm=0.912779
Epoch 81/100 Iteration 221/234: loss=0.036849 lr=0.000020 grad_norm=0.927194
Epoch 81/100 Iteration 222/234: loss=0.037815 lr=0.000020 grad_norm=0.487964
Epoch 81/100 Iteration 223/234: loss=0.037723 lr=0.000020 grad_norm=0.954811
Epoch 81/100 Iteration 224/234: loss=0.036765 lr=0.000020 grad_norm=1.418181
Epoch 81/100 Iteration 225/234: loss=0.038416 lr=0.000020 grad_norm=1.195550
Epoch 81/100 Iteration 226/234: loss=0.037789 lr=0.000020 grad_norm=0.785627
Epoch 81/100 Iteration 227/234: loss=0.036969 lr=0.000020 grad_norm=0.788411
Epoch 81/100 Iteration 228/234: loss=0.035031 lr=0.000020 grad_norm=1.046207
Epoch 81/100 Iteration 229/234: loss=0.037471 lr=0.000020 grad_norm=1.235979
Epoch 81/100 Iteration 230/234: loss=0.037102 lr=0.000020 grad_norm=0.717015
Epoch 81/100 Iteration 231/234: loss=0.038823 lr=0.000020 grad_norm=0.558273
Epoch 81/100 Iteration 232/234: loss=0.036860 lr=0.000020 grad_norm=0.912745
Epoch 81/100 Iteration 233/234: loss=0.034424 lr=0.000020 grad_norm=0.809562
Epoch 81/100 Iteration 234/234: loss=0.032162 lr=0.000020 grad_norm=0.610569
Epoch 81/100 finished. Avg Loss: 0.036740
Epoch 82/100 Iteration 1/234: loss=0.038575 lr=0.000020 grad_norm=0.887244
Epoch 82/100 Iteration 2/234: loss=0.036159 lr=0.000020 grad_norm=0.703647
Epoch 82/100 Iteration 3/234: loss=0.035627 lr=0.000020 grad_norm=1.617512
Epoch 82/100 Iteration 4/234: loss=0.033760 lr=0.000020 grad_norm=1.610561
Epoch 82/100 Iteration 5/234: loss=0.038973 lr=0.000020 grad_norm=0.557612
Epoch 82/100 Iteration 6/234: loss=0.036785 lr=0.000020 grad_norm=2.181008
Epoch 82/100 Iteration 7/234: loss=0.039139 lr=0.000020 grad_norm=2.072004
Epoch 82/100 Iteration 8/234: loss=0.036461 lr=0.000020 grad_norm=0.750624
Epoch 82/100 Iteration 9/234: loss=0.037016 lr=0.000020 grad_norm=1.669642
Epoch 82/100 Iteration 10/234: loss=0.041627 lr=0.000020 grad_norm=1.365181
Epoch 82/100 Iteration 11/234: loss=0.034367 lr=0.000020 grad_norm=0.924929
Epoch 82/100 Iteration 12/234: loss=0.032815 lr=0.000020 grad_norm=0.736367
Epoch 82/100 Iteration 13/234: loss=0.038408 lr=0.000020 grad_norm=0.942974
Epoch 82/100 Iteration 14/234: loss=0.039035 lr=0.000020 grad_norm=0.932083
Epoch 82/100 Iteration 15/234: loss=0.037915 lr=0.000020 grad_norm=0.492076
Epoch 82/100 Iteration 16/234: loss=0.040616 lr=0.000020 grad_norm=1.179816
Epoch 82/100 Iteration 17/234: loss=0.038081 lr=0.000020 grad_norm=0.555662
Epoch 82/100 Iteration 18/234: loss=0.037009 lr=0.000020 grad_norm=0.829688
Epoch 82/100 Iteration 19/234: loss=0.035131 lr=0.000020 grad_norm=0.965326
Epoch 82/100 Iteration 20/234: loss=0.033903 lr=0.000020 grad_norm=0.530354
Epoch 82/100 Iteration 21/234: loss=0.033018 lr=0.000020 grad_norm=0.659252
Epoch 82/100 Iteration 22/234: loss=0.037164 lr=0.000020 grad_norm=0.533964
Epoch 82/100 Iteration 23/234: loss=0.035413 lr=0.000020 grad_norm=0.688852
Epoch 82/100 Iteration 24/234: loss=0.038143 lr=0.000020 grad_norm=0.828618
Epoch 82/100 Iteration 25/234: loss=0.035646 lr=0.000020 grad_norm=0.433678
Epoch 82/100 Iteration 26/234: loss=0.038754 lr=0.000020 grad_norm=0.676216
Epoch 82/100 Iteration 27/234: loss=0.036367 lr=0.000020 grad_norm=0.878869
Epoch 82/100 Iteration 28/234: loss=0.038169 lr=0.000020 grad_norm=0.833357
Epoch 82/100 Iteration 29/234: loss=0.038631 lr=0.000020 grad_norm=0.657369
Epoch 82/100 Iteration 30/234: loss=0.038134 lr=0.000020 grad_norm=1.275898
Epoch 82/100 Iteration 31/234: loss=0.034915 lr=0.000020 grad_norm=1.354351
Epoch 82/100 Iteration 32/234: loss=0.038283 lr=0.000020 grad_norm=0.390726
Epoch 82/100 Iteration 33/234: loss=0.036023 lr=0.000020 grad_norm=1.204244
Epoch 82/100 Iteration 34/234: loss=0.034501 lr=0.000020 grad_norm=0.679333
Epoch 82/100 Iteration 35/234: loss=0.038061 lr=0.000020 grad_norm=1.046600
Epoch 82/100 Iteration 36/234: loss=0.039458 lr=0.000020 grad_norm=1.245854
Epoch 82/100 Iteration 37/234: loss=0.036978 lr=0.000020 grad_norm=0.560953
Epoch 82/100 Iteration 38/234: loss=0.041896 lr=0.000020 grad_norm=1.737056
Epoch 82/100 Iteration 39/234: loss=0.032594 lr=0.000020 grad_norm=1.413966
Epoch 82/100 Iteration 40/234: loss=0.036365 lr=0.000020 grad_norm=1.042170
Epoch 82/100 Iteration 41/234: loss=0.034174 lr=0.000020 grad_norm=1.538033
Epoch 82/100 Iteration 42/234: loss=0.035248 lr=0.000020 grad_norm=0.886238
Epoch 82/100 Iteration 43/234: loss=0.040075 lr=0.000020 grad_norm=1.472633
Epoch 82/100 Iteration 44/234: loss=0.034773 lr=0.000020 grad_norm=1.091270
Epoch 82/100 Iteration 45/234: loss=0.034696 lr=0.000020 grad_norm=0.980926
Epoch 82/100 Iteration 46/234: loss=0.035474 lr=0.000020 grad_norm=0.602799
Epoch 82/100 Iteration 47/234: loss=0.033772 lr=0.000020 grad_norm=0.998080
Epoch 82/100 Iteration 48/234: loss=0.038874 lr=0.000020 grad_norm=0.477859
Epoch 82/100 Iteration 49/234: loss=0.042214 lr=0.000020 grad_norm=0.987525
Epoch 82/100 Iteration 50/234: loss=0.038447 lr=0.000020 grad_norm=1.020343
Epoch 82/100 Iteration 51/234: loss=0.036158 lr=0.000020 grad_norm=0.504920
Epoch 82/100 Iteration 52/234: loss=0.030720 lr=0.000020 grad_norm=0.505198
Epoch 82/100 Iteration 53/234: loss=0.038331 lr=0.000020 grad_norm=0.613104
Epoch 82/100 Iteration 54/234: loss=0.034244 lr=0.000020 grad_norm=0.596496
Epoch 82/100 Iteration 55/234: loss=0.036443 lr=0.000020 grad_norm=0.465509
Epoch 82/100 Iteration 56/234: loss=0.040659 lr=0.000020 grad_norm=1.111409
Epoch 82/100 Iteration 57/234: loss=0.038180 lr=0.000020 grad_norm=1.521746
Epoch 82/100 Iteration 58/234: loss=0.035593 lr=0.000020 grad_norm=0.793373
Epoch 82/100 Iteration 59/234: loss=0.037296 lr=0.000020 grad_norm=0.865927
Epoch 82/100 Iteration 60/234: loss=0.033822 lr=0.000020 grad_norm=0.955649
Epoch 82/100 Iteration 61/234: loss=0.036664 lr=0.000020 grad_norm=0.406858
Epoch 82/100 Iteration 62/234: loss=0.038787 lr=0.000020 grad_norm=1.339571
Epoch 82/100 Iteration 63/234: loss=0.038967 lr=0.000020 grad_norm=1.737155
Epoch 82/100 Iteration 64/234: loss=0.038070 lr=0.000020 grad_norm=0.647436
Epoch 82/100 Iteration 65/234: loss=0.036120 lr=0.000020 grad_norm=0.969184
Epoch 82/100 Iteration 66/234: loss=0.037799 lr=0.000020 grad_norm=1.325701
Epoch 82/100 Iteration 67/234: loss=0.040421 lr=0.000020 grad_norm=1.056859
Epoch 82/100 Iteration 68/234: loss=0.033881 lr=0.000020 grad_norm=0.508802
Epoch 82/100 Iteration 69/234: loss=0.034586 lr=0.000020 grad_norm=1.094949
Epoch 82/100 Iteration 70/234: loss=0.036425 lr=0.000020 grad_norm=0.748680
Epoch 82/100 Iteration 71/234: loss=0.036860 lr=0.000020 grad_norm=0.622860
Epoch 82/100 Iteration 72/234: loss=0.037741 lr=0.000020 grad_norm=0.514544
Epoch 82/100 Iteration 73/234: loss=0.039279 lr=0.000020 grad_norm=0.453098
Epoch 82/100 Iteration 74/234: loss=0.029222 lr=0.000020 grad_norm=0.420611
Epoch 82/100 Iteration 75/234: loss=0.038884 lr=0.000020 grad_norm=0.607874
Epoch 82/100 Iteration 76/234: loss=0.036373 lr=0.000020 grad_norm=0.568693
Epoch 82/100 Iteration 77/234: loss=0.036936 lr=0.000020 grad_norm=0.599897
Epoch 82/100 Iteration 78/234: loss=0.038474 lr=0.000020 grad_norm=0.845896
Epoch 82/100 Iteration 79/234: loss=0.038781 lr=0.000020 grad_norm=0.658146
Epoch 82/100 Iteration 80/234: loss=0.039106 lr=0.000020 grad_norm=0.550554
Epoch 82/100 Iteration 81/234: loss=0.037195 lr=0.000020 grad_norm=0.750928
Epoch 82/100 Iteration 82/234: loss=0.039344 lr=0.000020 grad_norm=0.710715
Epoch 82/100 Iteration 83/234: loss=0.038081 lr=0.000020 grad_norm=0.727748
Epoch 82/100 Iteration 84/234: loss=0.034001 lr=0.000020 grad_norm=0.875447
Epoch 82/100 Iteration 85/234: loss=0.037241 lr=0.000020 grad_norm=0.591110
Epoch 82/100 Iteration 86/234: loss=0.035442 lr=0.000020 grad_norm=0.333238
Epoch 82/100 Iteration 87/234: loss=0.039006 lr=0.000020 grad_norm=0.776513
Epoch 82/100 Iteration 88/234: loss=0.040608 lr=0.000020 grad_norm=0.681626
Epoch 82/100 Iteration 89/234: loss=0.033531 lr=0.000020 grad_norm=0.499778
Epoch 82/100 Iteration 90/234: loss=0.037187 lr=0.000020 grad_norm=1.037385
Epoch 82/100 Iteration 91/234: loss=0.032941 lr=0.000020 grad_norm=0.517679
Epoch 82/100 Iteration 92/234: loss=0.034045 lr=0.000020 grad_norm=1.128053
Epoch 82/100 Iteration 93/234: loss=0.037098 lr=0.000020 grad_norm=1.220485
Epoch 82/100 Iteration 94/234: loss=0.036767 lr=0.000020 grad_norm=0.407984
Epoch 82/100 Iteration 95/234: loss=0.035564 lr=0.000020 grad_norm=1.205587
Epoch 82/100 Iteration 96/234: loss=0.039002 lr=0.000020 grad_norm=1.217132
Epoch 82/100 Iteration 97/234: loss=0.034606 lr=0.000020 grad_norm=0.800643
Epoch 82/100 Iteration 98/234: loss=0.038163 lr=0.000020 grad_norm=0.578008
Epoch 82/100 Iteration 99/234: loss=0.034983 lr=0.000020 grad_norm=0.723955
Epoch 82/100 Iteration 100/234: loss=0.033181 lr=0.000020 grad_norm=0.682137
Epoch 82/100 Iteration 101/234: loss=0.040162 lr=0.000020 grad_norm=0.601284
Epoch 82/100 Iteration 102/234: loss=0.035618 lr=0.000020 grad_norm=0.681292
Epoch 82/100 Iteration 103/234: loss=0.037813 lr=0.000020 grad_norm=0.561783
Epoch 82/100 Iteration 104/234: loss=0.037087 lr=0.000020 grad_norm=0.770941
Epoch 82/100 Iteration 105/234: loss=0.035179 lr=0.000020 grad_norm=0.799502
Epoch 82/100 Iteration 106/234: loss=0.038668 lr=0.000020 grad_norm=0.420213
Epoch 82/100 Iteration 107/234: loss=0.038579 lr=0.000020 grad_norm=1.165843
Epoch 82/100 Iteration 108/234: loss=0.035912 lr=0.000020 grad_norm=1.463927
Epoch 82/100 Iteration 109/234: loss=0.037143 lr=0.000020 grad_norm=0.525515
Epoch 82/100 Iteration 110/234: loss=0.036149 lr=0.000020 grad_norm=1.085018
Epoch 82/100 Iteration 111/234: loss=0.038669 lr=0.000020 grad_norm=1.065392
Epoch 82/100 Iteration 112/234: loss=0.037008 lr=0.000020 grad_norm=0.550043
Epoch 82/100 Iteration 113/234: loss=0.040969 lr=0.000020 grad_norm=1.145380
Epoch 82/100 Iteration 114/234: loss=0.037524 lr=0.000020 grad_norm=0.828180
Epoch 82/100 Iteration 115/234: loss=0.035731 lr=0.000020 grad_norm=0.551706
Epoch 82/100 Iteration 116/234: loss=0.038534 lr=0.000020 grad_norm=0.507871
Epoch 82/100 Iteration 117/234: loss=0.033817 lr=0.000020 grad_norm=0.497532
Epoch 82/100 Iteration 118/234: loss=0.036130 lr=0.000020 grad_norm=0.766774
Epoch 82/100 Iteration 119/234: loss=0.034897 lr=0.000020 grad_norm=0.479936
Epoch 82/100 Iteration 120/234: loss=0.034511 lr=0.000020 grad_norm=0.533807
Epoch 82/100 Iteration 121/234: loss=0.036506 lr=0.000020 grad_norm=0.667822
Epoch 82/100 Iteration 122/234: loss=0.037239 lr=0.000020 grad_norm=0.464967
Epoch 82/100 Iteration 123/234: loss=0.036195 lr=0.000020 grad_norm=0.508214
Epoch 82/100 Iteration 124/234: loss=0.032439 lr=0.000020 grad_norm=0.460361
Epoch 82/100 Iteration 125/234: loss=0.037474 lr=0.000020 grad_norm=0.864748
Epoch 82/100 Iteration 126/234: loss=0.038809 lr=0.000020 grad_norm=1.212039
Epoch 82/100 Iteration 127/234: loss=0.037426 lr=0.000020 grad_norm=1.057214
Epoch 82/100 Iteration 128/234: loss=0.036960 lr=0.000020 grad_norm=0.901895
Epoch 82/100 Iteration 129/234: loss=0.039211 lr=0.000020 grad_norm=0.593063
Epoch 82/100 Iteration 130/234: loss=0.034873 lr=0.000020 grad_norm=0.979901
Epoch 82/100 Iteration 131/234: loss=0.037463 lr=0.000020 grad_norm=1.221962
Epoch 82/100 Iteration 132/234: loss=0.035657 lr=0.000020 grad_norm=0.901647
Epoch 82/100 Iteration 133/234: loss=0.039088 lr=0.000020 grad_norm=0.769687
Epoch 82/100 Iteration 134/234: loss=0.034849 lr=0.000020 grad_norm=1.172354
Epoch 82/100 Iteration 135/234: loss=0.033724 lr=0.000020 grad_norm=0.780592
Epoch 82/100 Iteration 136/234: loss=0.040662 lr=0.000020 grad_norm=0.904996
Epoch 82/100 Iteration 137/234: loss=0.033631 lr=0.000020 grad_norm=0.887346
Epoch 82/100 Iteration 138/234: loss=0.035958 lr=0.000020 grad_norm=0.821985
Epoch 82/100 Iteration 139/234: loss=0.040735 lr=0.000020 grad_norm=1.055691
Epoch 82/100 Iteration 140/234: loss=0.037343 lr=0.000020 grad_norm=0.873819
Epoch 82/100 Iteration 141/234: loss=0.033303 lr=0.000020 grad_norm=0.519037
Epoch 82/100 Iteration 142/234: loss=0.036112 lr=0.000020 grad_norm=1.014800
Epoch 82/100 Iteration 143/234: loss=0.034020 lr=0.000020 grad_norm=1.249245
Epoch 82/100 Iteration 144/234: loss=0.036371 lr=0.000020 grad_norm=0.737978
Epoch 82/100 Iteration 145/234: loss=0.035767 lr=0.000020 grad_norm=0.645167
Epoch 82/100 Iteration 146/234: loss=0.035924 lr=0.000020 grad_norm=0.816769
Epoch 82/100 Iteration 147/234: loss=0.035823 lr=0.000020 grad_norm=0.355647
Epoch 82/100 Iteration 148/234: loss=0.036181 lr=0.000020 grad_norm=0.773620
Epoch 82/100 Iteration 149/234: loss=0.037556 lr=0.000020 grad_norm=0.780031
Epoch 82/100 Iteration 150/234: loss=0.035261 lr=0.000020 grad_norm=0.564132
Epoch 82/100 Iteration 151/234: loss=0.039631 lr=0.000020 grad_norm=0.682956
Epoch 82/100 Iteration 152/234: loss=0.036166 lr=0.000020 grad_norm=0.739008
Epoch 82/100 Iteration 153/234: loss=0.036878 lr=0.000020 grad_norm=0.661681
Epoch 82/100 Iteration 154/234: loss=0.033686 lr=0.000020 grad_norm=0.592859
Epoch 82/100 Iteration 155/234: loss=0.038773 lr=0.000020 grad_norm=0.621866
Epoch 82/100 Iteration 156/234: loss=0.036930 lr=0.000020 grad_norm=0.748152
Epoch 82/100 Iteration 157/234: loss=0.031163 lr=0.000020 grad_norm=0.635341
Epoch 82/100 Iteration 158/234: loss=0.037369 lr=0.000020 grad_norm=0.666888
Epoch 82/100 Iteration 159/234: loss=0.037304 lr=0.000020 grad_norm=0.643774
Epoch 82/100 Iteration 160/234: loss=0.033945 lr=0.000020 grad_norm=0.540960
Epoch 82/100 Iteration 161/234: loss=0.034485 lr=0.000020 grad_norm=0.583086
Epoch 82/100 Iteration 162/234: loss=0.034132 lr=0.000020 grad_norm=0.859338
Epoch 82/100 Iteration 163/234: loss=0.033141 lr=0.000020 grad_norm=0.771481
Epoch 82/100 Iteration 164/234: loss=0.036259 lr=0.000020 grad_norm=0.648741
Epoch 82/100 Iteration 165/234: loss=0.037912 lr=0.000020 grad_norm=1.364957
Epoch 82/100 Iteration 166/234: loss=0.035492 lr=0.000020 grad_norm=1.176044
Epoch 82/100 Iteration 167/234: loss=0.035746 lr=0.000020 grad_norm=0.742361
Epoch 82/100 Iteration 168/234: loss=0.036360 lr=0.000020 grad_norm=0.818773
Epoch 82/100 Iteration 169/234: loss=0.034166 lr=0.000020 grad_norm=0.706234
Epoch 82/100 Iteration 170/234: loss=0.035851 lr=0.000020 grad_norm=0.854432
Epoch 82/100 Iteration 171/234: loss=0.036020 lr=0.000020 grad_norm=0.841424
Epoch 82/100 Iteration 172/234: loss=0.038508 lr=0.000020 grad_norm=0.941503
Epoch 82/100 Iteration 173/234: loss=0.036737 lr=0.000020 grad_norm=1.604667
Epoch 82/100 Iteration 174/234: loss=0.031853 lr=0.000020 grad_norm=1.307595
Epoch 82/100 Iteration 175/234: loss=0.035765 lr=0.000020 grad_norm=0.671507
Epoch 82/100 Iteration 176/234: loss=0.036498 lr=0.000020 grad_norm=1.361344
Epoch 82/100 Iteration 177/234: loss=0.035063 lr=0.000020 grad_norm=0.890604
Epoch 82/100 Iteration 178/234: loss=0.039455 lr=0.000020 grad_norm=0.804794
Epoch 82/100 Iteration 179/234: loss=0.034679 lr=0.000020 grad_norm=0.973881
Epoch 82/100 Iteration 180/234: loss=0.036581 lr=0.000020 grad_norm=0.764720
Epoch 82/100 Iteration 181/234: loss=0.036930 lr=0.000020 grad_norm=0.712604
Epoch 82/100 Iteration 182/234: loss=0.036322 lr=0.000020 grad_norm=0.750526
Epoch 82/100 Iteration 183/234: loss=0.039208 lr=0.000020 grad_norm=0.862926
Epoch 82/100 Iteration 184/234: loss=0.036165 lr=0.000020 grad_norm=0.858804
Epoch 82/100 Iteration 185/234: loss=0.035054 lr=0.000020 grad_norm=0.674447
Epoch 82/100 Iteration 186/234: loss=0.039583 lr=0.000020 grad_norm=0.594177
Epoch 82/100 Iteration 187/234: loss=0.038981 lr=0.000020 grad_norm=0.619253
Epoch 82/100 Iteration 188/234: loss=0.038774 lr=0.000020 grad_norm=0.509919
Epoch 82/100 Iteration 189/234: loss=0.038997 lr=0.000020 grad_norm=0.585554
Epoch 82/100 Iteration 190/234: loss=0.037456 lr=0.000020 grad_norm=0.373607
Epoch 82/100 Iteration 191/234: loss=0.033090 lr=0.000020 grad_norm=0.584952
Epoch 82/100 Iteration 192/234: loss=0.038027 lr=0.000020 grad_norm=0.529988
Epoch 82/100 Iteration 193/234: loss=0.035580 lr=0.000020 grad_norm=0.477204
Epoch 82/100 Iteration 194/234: loss=0.037305 lr=0.000020 grad_norm=0.766249
Epoch 82/100 Iteration 195/234: loss=0.035159 lr=0.000020 grad_norm=0.738228
Epoch 82/100 Iteration 196/234: loss=0.038665 lr=0.000020 grad_norm=0.348124
Epoch 82/100 Iteration 197/234: loss=0.035537 lr=0.000020 grad_norm=0.573944
Epoch 82/100 Iteration 198/234: loss=0.031689 lr=0.000020 grad_norm=0.340719
Epoch 82/100 Iteration 199/234: loss=0.038423 lr=0.000020 grad_norm=0.774158
Epoch 82/100 Iteration 200/234: loss=0.035847 lr=0.000020 grad_norm=0.615827
Epoch 82/100 Iteration 201/234: loss=0.035764 lr=0.000020 grad_norm=0.541198
Epoch 82/100 Iteration 202/234: loss=0.036968 lr=0.000020 grad_norm=0.764539
Epoch 82/100 Iteration 203/234: loss=0.036820 lr=0.000020 grad_norm=0.500790
Epoch 82/100 Iteration 204/234: loss=0.034546 lr=0.000020 grad_norm=0.514734
Epoch 82/100 Iteration 205/234: loss=0.034612 lr=0.000020 grad_norm=0.656319
Epoch 82/100 Iteration 206/234: loss=0.032929 lr=0.000020 grad_norm=0.418828
Epoch 82/100 Iteration 207/234: loss=0.037487 lr=0.000020 grad_norm=0.445793
Epoch 82/100 Iteration 208/234: loss=0.038735 lr=0.000020 grad_norm=0.538413
Epoch 82/100 Iteration 209/234: loss=0.037820 lr=0.000020 grad_norm=0.567988
Epoch 82/100 Iteration 210/234: loss=0.035989 lr=0.000020 grad_norm=0.494927
Epoch 82/100 Iteration 211/234: loss=0.036222 lr=0.000020 grad_norm=0.624948
Epoch 82/100 Iteration 212/234: loss=0.034738 lr=0.000020 grad_norm=0.620967
Epoch 82/100 Iteration 213/234: loss=0.037221 lr=0.000020 grad_norm=1.083397
Epoch 82/100 Iteration 214/234: loss=0.034485 lr=0.000020 grad_norm=0.503802
Epoch 82/100 Iteration 215/234: loss=0.035095 lr=0.000020 grad_norm=0.868705
Epoch 82/100 Iteration 216/234: loss=0.032932 lr=0.000020 grad_norm=1.064546
Epoch 82/100 Iteration 217/234: loss=0.038908 lr=0.000020 grad_norm=0.500003
Epoch 82/100 Iteration 218/234: loss=0.035920 lr=0.000020 grad_norm=0.754773
Epoch 82/100 Iteration 219/234: loss=0.037271 lr=0.000020 grad_norm=0.445110
Epoch 82/100 Iteration 220/234: loss=0.040287 lr=0.000020 grad_norm=0.791640
Epoch 82/100 Iteration 221/234: loss=0.033672 lr=0.000020 grad_norm=0.878156
Epoch 82/100 Iteration 222/234: loss=0.038044 lr=0.000020 grad_norm=0.511003
Epoch 82/100 Iteration 223/234: loss=0.037012 lr=0.000020 grad_norm=0.758946
Epoch 82/100 Iteration 224/234: loss=0.038057 lr=0.000020 grad_norm=1.054573
Epoch 82/100 Iteration 225/234: loss=0.038120 lr=0.000020 grad_norm=0.774678
Epoch 82/100 Iteration 226/234: loss=0.036348 lr=0.000020 grad_norm=0.504320
Epoch 82/100 Iteration 227/234: loss=0.039107 lr=0.000020 grad_norm=0.476897
Epoch 82/100 Iteration 228/234: loss=0.032519 lr=0.000020 grad_norm=0.367897
Epoch 82/100 Iteration 229/234: loss=0.032648 lr=0.000020 grad_norm=0.427658
Epoch 82/100 Iteration 230/234: loss=0.034699 lr=0.000020 grad_norm=0.443319
Epoch 82/100 Iteration 231/234: loss=0.036576 lr=0.000020 grad_norm=0.421446
Epoch 82/100 Iteration 232/234: loss=0.035858 lr=0.000020 grad_norm=0.394557
Epoch 82/100 Iteration 233/234: loss=0.035856 lr=0.000020 grad_norm=0.390456
Epoch 82/100 Iteration 234/234: loss=0.035283 lr=0.000020 grad_norm=0.398153
Epoch 82/100 finished. Avg Loss: 0.036560
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 83/100 Iteration 1/234: loss=0.040214 lr=0.000020 grad_norm=0.792765
Epoch 83/100 Iteration 2/234: loss=0.036073 lr=0.000020 grad_norm=0.822833
Epoch 83/100 Iteration 3/234: loss=0.038537 lr=0.000020 grad_norm=0.625723
Epoch 83/100 Iteration 4/234: loss=0.040240 lr=0.000020 grad_norm=0.490901
Epoch 83/100 Iteration 5/234: loss=0.034550 lr=0.000020 grad_norm=0.680173
Epoch 83/100 Iteration 6/234: loss=0.039121 lr=0.000020 grad_norm=0.518496
Epoch 83/100 Iteration 7/234: loss=0.034902 lr=0.000020 grad_norm=0.759974
Epoch 83/100 Iteration 8/234: loss=0.034943 lr=0.000020 grad_norm=1.065787
Epoch 83/100 Iteration 9/234: loss=0.038576 lr=0.000020 grad_norm=0.965699
Epoch 83/100 Iteration 10/234: loss=0.033808 lr=0.000020 grad_norm=0.837118
Epoch 83/100 Iteration 11/234: loss=0.038738 lr=0.000020 grad_norm=0.906026
Epoch 83/100 Iteration 12/234: loss=0.033085 lr=0.000020 grad_norm=1.164543
Epoch 83/100 Iteration 13/234: loss=0.036575 lr=0.000020 grad_norm=0.804670
Epoch 83/100 Iteration 14/234: loss=0.031058 lr=0.000020 grad_norm=0.477907
Epoch 83/100 Iteration 15/234: loss=0.034773 lr=0.000020 grad_norm=1.024150
Epoch 83/100 Iteration 16/234: loss=0.036721 lr=0.000020 grad_norm=1.167527
Epoch 83/100 Iteration 17/234: loss=0.034561 lr=0.000020 grad_norm=1.130088
Epoch 83/100 Iteration 18/234: loss=0.036721 lr=0.000020 grad_norm=1.105919
Epoch 83/100 Iteration 19/234: loss=0.035443 lr=0.000020 grad_norm=0.725849
Epoch 83/100 Iteration 20/234: loss=0.038324 lr=0.000020 grad_norm=0.690464
Epoch 83/100 Iteration 21/234: loss=0.038214 lr=0.000020 grad_norm=1.187987
Epoch 83/100 Iteration 22/234: loss=0.035357 lr=0.000020 grad_norm=1.059003
Epoch 83/100 Iteration 23/234: loss=0.035996 lr=0.000020 grad_norm=1.055847
Epoch 83/100 Iteration 24/234: loss=0.036927 lr=0.000020 grad_norm=1.289169
Epoch 83/100 Iteration 25/234: loss=0.033983 lr=0.000020 grad_norm=0.649264
Epoch 83/100 Iteration 26/234: loss=0.034739 lr=0.000020 grad_norm=0.835766
Epoch 83/100 Iteration 27/234: loss=0.037152 lr=0.000020 grad_norm=1.189732
Epoch 83/100 Iteration 28/234: loss=0.034185 lr=0.000020 grad_norm=0.541932
Epoch 83/100 Iteration 29/234: loss=0.038845 lr=0.000020 grad_norm=0.857031
Epoch 83/100 Iteration 30/234: loss=0.035805 lr=0.000020 grad_norm=0.955577
Epoch 83/100 Iteration 31/234: loss=0.040096 lr=0.000020 grad_norm=0.488810
Epoch 83/100 Iteration 32/234: loss=0.036940 lr=0.000020 grad_norm=0.875567
Epoch 83/100 Iteration 33/234: loss=0.036770 lr=0.000020 grad_norm=0.736033
Epoch 83/100 Iteration 34/234: loss=0.036042 lr=0.000020 grad_norm=0.956143
Epoch 83/100 Iteration 35/234: loss=0.036105 lr=0.000020 grad_norm=0.971036
Epoch 83/100 Iteration 36/234: loss=0.037089 lr=0.000020 grad_norm=0.568989
Epoch 83/100 Iteration 37/234: loss=0.035717 lr=0.000020 grad_norm=0.739281
Epoch 83/100 Iteration 38/234: loss=0.040495 lr=0.000020 grad_norm=0.821907
Epoch 83/100 Iteration 39/234: loss=0.035888 lr=0.000020 grad_norm=0.356813
Epoch 83/100 Iteration 40/234: loss=0.035987 lr=0.000020 grad_norm=0.717481
Epoch 83/100 Iteration 41/234: loss=0.035409 lr=0.000020 grad_norm=0.594655
Epoch 83/100 Iteration 42/234: loss=0.036516 lr=0.000020 grad_norm=0.842064
Epoch 83/100 Iteration 43/234: loss=0.038083 lr=0.000020 grad_norm=1.238747
Epoch 83/100 Iteration 44/234: loss=0.037791 lr=0.000020 grad_norm=1.413724
Epoch 83/100 Iteration 45/234: loss=0.034576 lr=0.000020 grad_norm=0.723487
Epoch 83/100 Iteration 46/234: loss=0.042668 lr=0.000020 grad_norm=0.561213
Epoch 83/100 Iteration 47/234: loss=0.035523 lr=0.000020 grad_norm=1.130965
Epoch 83/100 Iteration 48/234: loss=0.033508 lr=0.000020 grad_norm=1.266877
Epoch 83/100 Iteration 49/234: loss=0.034471 lr=0.000020 grad_norm=0.515438
Epoch 83/100 Iteration 50/234: loss=0.036363 lr=0.000020 grad_norm=0.814901
Epoch 83/100 Iteration 51/234: loss=0.034464 lr=0.000020 grad_norm=1.002269
Epoch 83/100 Iteration 52/234: loss=0.036296 lr=0.000020 grad_norm=0.529728
Epoch 83/100 Iteration 53/234: loss=0.038211 lr=0.000020 grad_norm=0.855274
Epoch 83/100 Iteration 54/234: loss=0.039502 lr=0.000020 grad_norm=1.186910
Epoch 83/100 Iteration 55/234: loss=0.033537 lr=0.000020 grad_norm=0.704174
Epoch 83/100 Iteration 56/234: loss=0.036766 lr=0.000020 grad_norm=0.722099
Epoch 83/100 Iteration 57/234: loss=0.036955 lr=0.000020 grad_norm=0.720456
Epoch 83/100 Iteration 58/234: loss=0.033474 lr=0.000020 grad_norm=0.748461
Epoch 83/100 Iteration 59/234: loss=0.035138 lr=0.000020 grad_norm=0.487028
Epoch 83/100 Iteration 60/234: loss=0.038784 lr=0.000020 grad_norm=0.549990
Epoch 83/100 Iteration 61/234: loss=0.037332 lr=0.000020 grad_norm=0.776572
Epoch 83/100 Iteration 62/234: loss=0.036014 lr=0.000020 grad_norm=0.883568
Epoch 83/100 Iteration 63/234: loss=0.035011 lr=0.000020 grad_norm=0.524826
Epoch 83/100 Iteration 64/234: loss=0.036859 lr=0.000020 grad_norm=0.502699
Epoch 83/100 Iteration 65/234: loss=0.036605 lr=0.000020 grad_norm=0.539873
Epoch 83/100 Iteration 66/234: loss=0.036892 lr=0.000020 grad_norm=0.623100
Epoch 83/100 Iteration 67/234: loss=0.034071 lr=0.000020 grad_norm=0.460270
Epoch 83/100 Iteration 68/234: loss=0.035192 lr=0.000020 grad_norm=0.389791
Epoch 83/100 Iteration 69/234: loss=0.035801 lr=0.000020 grad_norm=0.462723
Epoch 83/100 Iteration 70/234: loss=0.032700 lr=0.000020 grad_norm=0.454864
Epoch 83/100 Iteration 71/234: loss=0.033767 lr=0.000020 grad_norm=0.511694
Epoch 83/100 Iteration 72/234: loss=0.035496 lr=0.000020 grad_norm=0.438293
Epoch 83/100 Iteration 73/234: loss=0.036780 lr=0.000020 grad_norm=0.446887
Epoch 83/100 Iteration 74/234: loss=0.036159 lr=0.000020 grad_norm=0.513644
Epoch 83/100 Iteration 75/234: loss=0.037047 lr=0.000020 grad_norm=0.428531
Epoch 83/100 Iteration 76/234: loss=0.038390 lr=0.000020 grad_norm=0.617983
Epoch 83/100 Iteration 77/234: loss=0.032794 lr=0.000020 grad_norm=0.744889
Epoch 83/100 Iteration 78/234: loss=0.036623 lr=0.000020 grad_norm=0.784535
Epoch 83/100 Iteration 79/234: loss=0.039245 lr=0.000020 grad_norm=0.595025
Epoch 83/100 Iteration 80/234: loss=0.037905 lr=0.000020 grad_norm=0.465059
Epoch 83/100 Iteration 81/234: loss=0.034141 lr=0.000020 grad_norm=0.625599
Epoch 83/100 Iteration 82/234: loss=0.033524 lr=0.000020 grad_norm=0.440256
Epoch 83/100 Iteration 83/234: loss=0.035794 lr=0.000020 grad_norm=0.393290
Epoch 83/100 Iteration 84/234: loss=0.034921 lr=0.000020 grad_norm=0.544310
Epoch 83/100 Iteration 85/234: loss=0.036247 lr=0.000020 grad_norm=0.456447
Epoch 83/100 Iteration 86/234: loss=0.037721 lr=0.000020 grad_norm=0.983155
Epoch 83/100 Iteration 87/234: loss=0.034769 lr=0.000020 grad_norm=1.493776
Epoch 83/100 Iteration 88/234: loss=0.034486 lr=0.000020 grad_norm=1.380444
Epoch 83/100 Iteration 89/234: loss=0.030839 lr=0.000020 grad_norm=0.549915
Epoch 83/100 Iteration 90/234: loss=0.035781 lr=0.000020 grad_norm=0.667739
Epoch 83/100 Iteration 91/234: loss=0.039462 lr=0.000020 grad_norm=0.944328
Epoch 83/100 Iteration 92/234: loss=0.040690 lr=0.000020 grad_norm=0.959869
Epoch 83/100 Iteration 93/234: loss=0.039547 lr=0.000020 grad_norm=1.229671
Epoch 83/100 Iteration 94/234: loss=0.034023 lr=0.000020 grad_norm=1.065223
Epoch 83/100 Iteration 95/234: loss=0.036441 lr=0.000020 grad_norm=0.395582
Epoch 83/100 Iteration 96/234: loss=0.035966 lr=0.000020 grad_norm=0.942946
Epoch 83/100 Iteration 97/234: loss=0.036113 lr=0.000020 grad_norm=1.325042
Epoch 83/100 Iteration 98/234: loss=0.038378 lr=0.000020 grad_norm=1.109456
Epoch 83/100 Iteration 99/234: loss=0.036931 lr=0.000020 grad_norm=0.443267
Epoch 83/100 Iteration 100/234: loss=0.038358 lr=0.000020 grad_norm=1.241093
Epoch 83/100 Iteration 101/234: loss=0.037595 lr=0.000020 grad_norm=1.757962
Epoch 83/100 Iteration 102/234: loss=0.036244 lr=0.000020 grad_norm=1.310553
Epoch 83/100 Iteration 103/234: loss=0.037511 lr=0.000020 grad_norm=0.659157
Epoch 83/100 Iteration 104/234: loss=0.037000 lr=0.000020 grad_norm=1.689559
Epoch 83/100 Iteration 105/234: loss=0.035370 lr=0.000020 grad_norm=1.137982
Epoch 83/100 Iteration 106/234: loss=0.036144 lr=0.000020 grad_norm=0.820618
Epoch 83/100 Iteration 107/234: loss=0.035600 lr=0.000020 grad_norm=1.682525
Epoch 83/100 Iteration 108/234: loss=0.036410 lr=0.000020 grad_norm=0.787023
Epoch 83/100 Iteration 109/234: loss=0.034863 lr=0.000020 grad_norm=1.009364
Epoch 83/100 Iteration 110/234: loss=0.036589 lr=0.000020 grad_norm=0.934234
Epoch 83/100 Iteration 111/234: loss=0.033405 lr=0.000020 grad_norm=0.679762
Epoch 83/100 Iteration 112/234: loss=0.037622 lr=0.000020 grad_norm=1.225064
Epoch 83/100 Iteration 113/234: loss=0.034167 lr=0.000020 grad_norm=1.313089
Epoch 83/100 Iteration 114/234: loss=0.034494 lr=0.000020 grad_norm=0.746104
Epoch 83/100 Iteration 115/234: loss=0.035864 lr=0.000020 grad_norm=0.670154
Epoch 83/100 Iteration 116/234: loss=0.038174 lr=0.000020 grad_norm=0.642199
Epoch 83/100 Iteration 117/234: loss=0.032293 lr=0.000020 grad_norm=0.500926
Epoch 83/100 Iteration 118/234: loss=0.038848 lr=0.000020 grad_norm=0.441614
Epoch 83/100 Iteration 119/234: loss=0.035558 lr=0.000020 grad_norm=0.724495
Epoch 83/100 Iteration 120/234: loss=0.033961 lr=0.000020 grad_norm=0.541045
Epoch 83/100 Iteration 121/234: loss=0.038724 lr=0.000020 grad_norm=0.545771
Epoch 83/100 Iteration 122/234: loss=0.038056 lr=0.000020 grad_norm=0.636639
Epoch 83/100 Iteration 123/234: loss=0.038131 lr=0.000020 grad_norm=0.513362
Epoch 83/100 Iteration 124/234: loss=0.036893 lr=0.000020 grad_norm=0.355239
Epoch 83/100 Iteration 125/234: loss=0.037775 lr=0.000020 grad_norm=0.530388
Epoch 83/100 Iteration 126/234: loss=0.033859 lr=0.000020 grad_norm=0.401396
Epoch 83/100 Iteration 127/234: loss=0.034752 lr=0.000020 grad_norm=0.331864
Epoch 83/100 Iteration 128/234: loss=0.030942 lr=0.000020 grad_norm=0.441940
Epoch 83/100 Iteration 129/234: loss=0.034693 lr=0.000020 grad_norm=0.578273
Epoch 83/100 Iteration 130/234: loss=0.038733 lr=0.000020 grad_norm=0.468827
Epoch 83/100 Iteration 131/234: loss=0.034739 lr=0.000020 grad_norm=0.478837
Epoch 83/100 Iteration 132/234: loss=0.037112 lr=0.000020 grad_norm=0.529160
Epoch 83/100 Iteration 133/234: loss=0.037493 lr=0.000020 grad_norm=0.477182
Epoch 83/100 Iteration 134/234: loss=0.034999 lr=0.000020 grad_norm=0.599514
Epoch 83/100 Iteration 135/234: loss=0.035013 lr=0.000020 grad_norm=0.606468
Epoch 83/100 Iteration 136/234: loss=0.038877 lr=0.000020 grad_norm=0.680992
Epoch 83/100 Iteration 137/234: loss=0.033950 lr=0.000020 grad_norm=0.855700
Epoch 83/100 Iteration 138/234: loss=0.034657 lr=0.000020 grad_norm=0.811222
Epoch 83/100 Iteration 139/234: loss=0.036895 lr=0.000020 grad_norm=0.838888
Epoch 83/100 Iteration 140/234: loss=0.039702 lr=0.000020 grad_norm=1.367660
Epoch 83/100 Iteration 141/234: loss=0.033560 lr=0.000020 grad_norm=1.415846
Epoch 83/100 Iteration 142/234: loss=0.036451 lr=0.000020 grad_norm=0.570722
Epoch 83/100 Iteration 143/234: loss=0.037149 lr=0.000020 grad_norm=1.447654
Epoch 83/100 Iteration 144/234: loss=0.041173 lr=0.000020 grad_norm=1.068017
Epoch 83/100 Iteration 145/234: loss=0.038577 lr=0.000020 grad_norm=0.686876
Epoch 83/100 Iteration 146/234: loss=0.040437 lr=0.000020 grad_norm=1.066943
Epoch 83/100 Iteration 147/234: loss=0.032681 lr=0.000020 grad_norm=0.801022
Epoch 83/100 Iteration 148/234: loss=0.037602 lr=0.000020 grad_norm=0.818652
Epoch 83/100 Iteration 149/234: loss=0.035362 lr=0.000020 grad_norm=1.092919
Epoch 83/100 Iteration 150/234: loss=0.036611 lr=0.000020 grad_norm=0.819545
Epoch 83/100 Iteration 151/234: loss=0.040569 lr=0.000020 grad_norm=0.914582
Epoch 83/100 Iteration 152/234: loss=0.035241 lr=0.000020 grad_norm=0.946853
Epoch 83/100 Iteration 153/234: loss=0.035046 lr=0.000020 grad_norm=0.432334
Epoch 83/100 Iteration 154/234: loss=0.032695 lr=0.000020 grad_norm=1.007007
Epoch 83/100 Iteration 155/234: loss=0.038552 lr=0.000020 grad_norm=0.703456
Epoch 83/100 Iteration 156/234: loss=0.036955 lr=0.000020 grad_norm=0.937594
Epoch 83/100 Iteration 157/234: loss=0.036705 lr=0.000020 grad_norm=0.762411
Epoch 83/100 Iteration 158/234: loss=0.035930 lr=0.000020 grad_norm=0.821971
Epoch 83/100 Iteration 159/234: loss=0.036764 lr=0.000020 grad_norm=0.759540
Epoch 83/100 Iteration 160/234: loss=0.037329 lr=0.000020 grad_norm=0.402805
Epoch 83/100 Iteration 161/234: loss=0.040321 lr=0.000020 grad_norm=0.754047
Epoch 83/100 Iteration 162/234: loss=0.033778 lr=0.000020 grad_norm=0.687007
Epoch 83/100 Iteration 163/234: loss=0.035075 lr=0.000020 grad_norm=0.470781
Epoch 83/100 Iteration 164/234: loss=0.037781 lr=0.000020 grad_norm=0.836735
Epoch 83/100 Iteration 165/234: loss=0.035955 lr=0.000020 grad_norm=1.286313
Epoch 83/100 Iteration 166/234: loss=0.037828 lr=0.000020 grad_norm=1.160873
Epoch 83/100 Iteration 167/234: loss=0.037201 lr=0.000020 grad_norm=0.503229
Epoch 83/100 Iteration 168/234: loss=0.037039 lr=0.000020 grad_norm=0.887160
Epoch 83/100 Iteration 169/234: loss=0.040800 lr=0.000020 grad_norm=1.102025
Epoch 83/100 Iteration 170/234: loss=0.034664 lr=0.000020 grad_norm=0.501257
Epoch 83/100 Iteration 171/234: loss=0.036798 lr=0.000020 grad_norm=0.781923
Epoch 83/100 Iteration 172/234: loss=0.036540 lr=0.000020 grad_norm=0.943815
Epoch 83/100 Iteration 173/234: loss=0.035835 lr=0.000020 grad_norm=0.387010
Epoch 83/100 Iteration 174/234: loss=0.035850 lr=0.000020 grad_norm=1.038925
Epoch 83/100 Iteration 175/234: loss=0.037461 lr=0.000020 grad_norm=1.255793
Epoch 83/100 Iteration 176/234: loss=0.036985 lr=0.000020 grad_norm=0.819300
Epoch 83/100 Iteration 177/234: loss=0.034234 lr=0.000020 grad_norm=0.399788
Epoch 83/100 Iteration 178/234: loss=0.037417 lr=0.000020 grad_norm=0.659099
Epoch 83/100 Iteration 179/234: loss=0.032995 lr=0.000020 grad_norm=0.898785
Epoch 83/100 Iteration 180/234: loss=0.037982 lr=0.000020 grad_norm=0.795501
Epoch 83/100 Iteration 181/234: loss=0.032945 lr=0.000020 grad_norm=0.449486
Epoch 83/100 Iteration 182/234: loss=0.037306 lr=0.000020 grad_norm=0.434002
Epoch 83/100 Iteration 183/234: loss=0.031206 lr=0.000020 grad_norm=0.512279
Epoch 83/100 Iteration 184/234: loss=0.037071 lr=0.000020 grad_norm=0.656166
Epoch 83/100 Iteration 185/234: loss=0.033966 lr=0.000020 grad_norm=0.401834
Epoch 83/100 Iteration 186/234: loss=0.038413 lr=0.000020 grad_norm=0.645336
Epoch 83/100 Iteration 187/234: loss=0.032918 lr=0.000020 grad_norm=0.399419
Epoch 83/100 Iteration 188/234: loss=0.035869 lr=0.000020 grad_norm=0.702510
Epoch 83/100 Iteration 189/234: loss=0.036779 lr=0.000020 grad_norm=1.122388
Epoch 83/100 Iteration 190/234: loss=0.032949 lr=0.000020 grad_norm=0.821625
Epoch 83/100 Iteration 191/234: loss=0.036792 lr=0.000020 grad_norm=0.560577
Epoch 83/100 Iteration 192/234: loss=0.035790 lr=0.000020 grad_norm=0.780119
Epoch 83/100 Iteration 193/234: loss=0.036704 lr=0.000020 grad_norm=0.678367
Epoch 83/100 Iteration 194/234: loss=0.037062 lr=0.000020 grad_norm=0.698005
Epoch 83/100 Iteration 195/234: loss=0.041481 lr=0.000020 grad_norm=1.376058
Epoch 83/100 Iteration 196/234: loss=0.035326 lr=0.000020 grad_norm=1.278603
Epoch 83/100 Iteration 197/234: loss=0.032571 lr=0.000020 grad_norm=0.692319
Epoch 83/100 Iteration 198/234: loss=0.035696 lr=0.000020 grad_norm=0.891091
Epoch 83/100 Iteration 199/234: loss=0.035096 lr=0.000020 grad_norm=0.735976
Epoch 83/100 Iteration 200/234: loss=0.033427 lr=0.000020 grad_norm=0.835182
Epoch 83/100 Iteration 201/234: loss=0.034570 lr=0.000020 grad_norm=0.456973
Epoch 83/100 Iteration 202/234: loss=0.037723 lr=0.000020 grad_norm=0.895873
Epoch 83/100 Iteration 203/234: loss=0.038860 lr=0.000020 grad_norm=1.088130
Epoch 83/100 Iteration 204/234: loss=0.039768 lr=0.000020 grad_norm=0.584590
Epoch 83/100 Iteration 205/234: loss=0.035775 lr=0.000020 grad_norm=0.769138
Epoch 83/100 Iteration 206/234: loss=0.037662 lr=0.000020 grad_norm=0.617801
Epoch 83/100 Iteration 207/234: loss=0.040986 lr=0.000020 grad_norm=0.627002
Epoch 83/100 Iteration 208/234: loss=0.036776 lr=0.000020 grad_norm=0.888708
Epoch 83/100 Iteration 209/234: loss=0.032515 lr=0.000020 grad_norm=0.441307
Epoch 83/100 Iteration 210/234: loss=0.032988 lr=0.000020 grad_norm=0.642154
Epoch 83/100 Iteration 211/234: loss=0.039000 lr=0.000020 grad_norm=0.590022
Epoch 83/100 Iteration 212/234: loss=0.036683 lr=0.000020 grad_norm=0.522169
Epoch 83/100 Iteration 213/234: loss=0.032859 lr=0.000020 grad_norm=0.726197
Epoch 83/100 Iteration 214/234: loss=0.037342 lr=0.000020 grad_norm=0.437940
Epoch 83/100 Iteration 215/234: loss=0.032770 lr=0.000020 grad_norm=0.871928
Epoch 83/100 Iteration 216/234: loss=0.036104 lr=0.000020 grad_norm=0.881209
Epoch 83/100 Iteration 217/234: loss=0.033399 lr=0.000020 grad_norm=0.448751
Epoch 83/100 Iteration 218/234: loss=0.038688 lr=0.000020 grad_norm=0.848224
Epoch 83/100 Iteration 219/234: loss=0.035716 lr=0.000020 grad_norm=0.993388
Epoch 83/100 Iteration 220/234: loss=0.036982 lr=0.000020 grad_norm=0.857299
Epoch 83/100 Iteration 221/234: loss=0.038595 lr=0.000020 grad_norm=0.735132
Epoch 83/100 Iteration 222/234: loss=0.034355 lr=0.000020 grad_norm=0.893064
Epoch 83/100 Iteration 223/234: loss=0.037356 lr=0.000020 grad_norm=0.926520
Epoch 83/100 Iteration 224/234: loss=0.034691 lr=0.000020 grad_norm=0.720123
Epoch 83/100 Iteration 225/234: loss=0.041267 lr=0.000020 grad_norm=0.808015
Epoch 83/100 Iteration 226/234: loss=0.037409 lr=0.000020 grad_norm=1.395182
Epoch 83/100 Iteration 227/234: loss=0.031719 lr=0.000020 grad_norm=1.085304
Epoch 83/100 Iteration 228/234: loss=0.037109 lr=0.000020 grad_norm=0.497109
Epoch 83/100 Iteration 229/234: loss=0.035453 lr=0.000020 grad_norm=1.164449
Epoch 83/100 Iteration 230/234: loss=0.037528 lr=0.000020 grad_norm=0.917126
Epoch 83/100 Iteration 231/234: loss=0.035014 lr=0.000020 grad_norm=0.401894
Epoch 83/100 Iteration 232/234: loss=0.036183 lr=0.000020 grad_norm=0.745295
Epoch 83/100 Iteration 233/234: loss=0.039528 lr=0.000020 grad_norm=1.170527
Epoch 83/100 Iteration 234/234: loss=0.034443 lr=0.000020 grad_norm=1.161678
Epoch 83/100 finished. Avg Loss: 0.036286
Epoch 84/100 Iteration 1/234: loss=0.038269 lr=0.000020 grad_norm=0.664937
Epoch 84/100 Iteration 2/234: loss=0.038747 lr=0.000020 grad_norm=0.738226
Epoch 84/100 Iteration 3/234: loss=0.034191 lr=0.000020 grad_norm=0.546590
Epoch 84/100 Iteration 4/234: loss=0.041809 lr=0.000020 grad_norm=0.571142
Epoch 84/100 Iteration 5/234: loss=0.034209 lr=0.000020 grad_norm=0.549961
Epoch 84/100 Iteration 6/234: loss=0.037577 lr=0.000020 grad_norm=0.671940
Epoch 84/100 Iteration 7/234: loss=0.033387 lr=0.000020 grad_norm=0.602938
Epoch 84/100 Iteration 8/234: loss=0.034053 lr=0.000020 grad_norm=0.442783
Epoch 84/100 Iteration 9/234: loss=0.039194 lr=0.000020 grad_norm=0.755883
Epoch 84/100 Iteration 10/234: loss=0.034829 lr=0.000020 grad_norm=0.860072
Epoch 84/100 Iteration 11/234: loss=0.035089 lr=0.000020 grad_norm=0.492921
Epoch 84/100 Iteration 12/234: loss=0.034084 lr=0.000020 grad_norm=0.345479
Epoch 84/100 Iteration 13/234: loss=0.034054 lr=0.000020 grad_norm=0.373074
Epoch 84/100 Iteration 14/234: loss=0.035472 lr=0.000020 grad_norm=0.358960
Epoch 84/100 Iteration 15/234: loss=0.038367 lr=0.000020 grad_norm=0.534252
Epoch 84/100 Iteration 16/234: loss=0.037143 lr=0.000020 grad_norm=0.557639
Epoch 84/100 Iteration 17/234: loss=0.036482 lr=0.000020 grad_norm=0.704918
Epoch 84/100 Iteration 18/234: loss=0.037591 lr=0.000020 grad_norm=1.046373
Epoch 84/100 Iteration 19/234: loss=0.037245 lr=0.000020 grad_norm=1.179278
Epoch 84/100 Iteration 20/234: loss=0.036888 lr=0.000020 grad_norm=0.794652
Epoch 84/100 Iteration 21/234: loss=0.035577 lr=0.000020 grad_norm=0.588037
Epoch 84/100 Iteration 22/234: loss=0.036877 lr=0.000020 grad_norm=0.811391
Epoch 84/100 Iteration 23/234: loss=0.037384 lr=0.000020 grad_norm=0.847769
Epoch 84/100 Iteration 24/234: loss=0.037011 lr=0.000020 grad_norm=0.806671
Epoch 84/100 Iteration 25/234: loss=0.034837 lr=0.000020 grad_norm=0.579578
Epoch 84/100 Iteration 26/234: loss=0.036446 lr=0.000020 grad_norm=0.525532
Epoch 84/100 Iteration 27/234: loss=0.033623 lr=0.000020 grad_norm=0.970155
Epoch 84/100 Iteration 28/234: loss=0.034710 lr=0.000020 grad_norm=0.885098
Epoch 84/100 Iteration 29/234: loss=0.035726 lr=0.000020 grad_norm=0.418847
Epoch 84/100 Iteration 30/234: loss=0.038392 lr=0.000020 grad_norm=0.687044
Epoch 84/100 Iteration 31/234: loss=0.034930 lr=0.000020 grad_norm=0.809846
Epoch 84/100 Iteration 32/234: loss=0.032875 lr=0.000020 grad_norm=0.551298
Epoch 84/100 Iteration 33/234: loss=0.038551 lr=0.000020 grad_norm=0.686532
Epoch 84/100 Iteration 34/234: loss=0.033496 lr=0.000020 grad_norm=0.724522
Epoch 84/100 Iteration 35/234: loss=0.032927 lr=0.000020 grad_norm=0.550011
Epoch 84/100 Iteration 36/234: loss=0.032310 lr=0.000020 grad_norm=0.498133
Epoch 84/100 Iteration 37/234: loss=0.035380 lr=0.000020 grad_norm=0.865663
Epoch 84/100 Iteration 38/234: loss=0.037788 lr=0.000020 grad_norm=0.958799
Epoch 84/100 Iteration 39/234: loss=0.035082 lr=0.000020 grad_norm=0.893173
Epoch 84/100 Iteration 40/234: loss=0.037838 lr=0.000020 grad_norm=0.921127
Epoch 84/100 Iteration 41/234: loss=0.037039 lr=0.000020 grad_norm=0.935899
Epoch 84/100 Iteration 42/234: loss=0.036929 lr=0.000020 grad_norm=1.009463
Epoch 84/100 Iteration 43/234: loss=0.035529 lr=0.000020 grad_norm=1.042222
Epoch 84/100 Iteration 44/234: loss=0.032920 lr=0.000020 grad_norm=0.745976
Epoch 84/100 Iteration 45/234: loss=0.038839 lr=0.000020 grad_norm=0.718314
Epoch 84/100 Iteration 46/234: loss=0.038288 lr=0.000020 grad_norm=0.998110
Epoch 84/100 Iteration 47/234: loss=0.037489 lr=0.000020 grad_norm=1.270519
Epoch 84/100 Iteration 48/234: loss=0.035815 lr=0.000020 grad_norm=1.106528
Epoch 84/100 Iteration 49/234: loss=0.035252 lr=0.000020 grad_norm=0.467273
Epoch 84/100 Iteration 50/234: loss=0.039027 lr=0.000020 grad_norm=0.632112
Epoch 84/100 Iteration 51/234: loss=0.034591 lr=0.000020 grad_norm=0.730591
Epoch 84/100 Iteration 52/234: loss=0.035720 lr=0.000020 grad_norm=0.552280
Epoch 84/100 Iteration 53/234: loss=0.030453 lr=0.000020 grad_norm=1.004301
Epoch 84/100 Iteration 54/234: loss=0.035574 lr=0.000020 grad_norm=0.926619
Epoch 84/100 Iteration 55/234: loss=0.035492 lr=0.000020 grad_norm=0.491945
Epoch 84/100 Iteration 56/234: loss=0.039398 lr=0.000020 grad_norm=1.048947
Epoch 84/100 Iteration 57/234: loss=0.036554 lr=0.000020 grad_norm=1.185808
Epoch 84/100 Iteration 58/234: loss=0.033438 lr=0.000020 grad_norm=0.645525
Epoch 84/100 Iteration 59/234: loss=0.034608 lr=0.000020 grad_norm=0.610180
Epoch 84/100 Iteration 60/234: loss=0.035802 lr=0.000020 grad_norm=0.998035
Epoch 84/100 Iteration 61/234: loss=0.033241 lr=0.000020 grad_norm=0.643648
Epoch 84/100 Iteration 62/234: loss=0.035752 lr=0.000020 grad_norm=0.540829
Epoch 84/100 Iteration 63/234: loss=0.034554 lr=0.000020 grad_norm=0.845382
Epoch 84/100 Iteration 64/234: loss=0.032444 lr=0.000020 grad_norm=0.578095
Epoch 84/100 Iteration 65/234: loss=0.031471 lr=0.000020 grad_norm=0.483043
Epoch 84/100 Iteration 66/234: loss=0.036300 lr=0.000020 grad_norm=0.523759
Epoch 84/100 Iteration 67/234: loss=0.038300 lr=0.000020 grad_norm=0.738327
Epoch 84/100 Iteration 68/234: loss=0.035394 lr=0.000020 grad_norm=0.730060
Epoch 84/100 Iteration 69/234: loss=0.034531 lr=0.000020 grad_norm=0.457545
Epoch 84/100 Iteration 70/234: loss=0.035982 lr=0.000020 grad_norm=0.667491
Epoch 84/100 Iteration 71/234: loss=0.033376 lr=0.000020 grad_norm=0.708390
Epoch 84/100 Iteration 72/234: loss=0.037690 lr=0.000020 grad_norm=0.481645
Epoch 84/100 Iteration 73/234: loss=0.034857 lr=0.000020 grad_norm=0.716518
Epoch 84/100 Iteration 74/234: loss=0.037658 lr=0.000020 grad_norm=1.263919
Epoch 84/100 Iteration 75/234: loss=0.034997 lr=0.000020 grad_norm=1.641030
Epoch 84/100 Iteration 76/234: loss=0.037317 lr=0.000020 grad_norm=1.489045
Epoch 84/100 Iteration 77/234: loss=0.033841 lr=0.000020 grad_norm=0.718016
Epoch 84/100 Iteration 78/234: loss=0.035498 lr=0.000020 grad_norm=0.644728
Epoch 84/100 Iteration 79/234: loss=0.036788 lr=0.000020 grad_norm=0.690929
Epoch 84/100 Iteration 80/234: loss=0.036884 lr=0.000020 grad_norm=0.435867
Epoch 84/100 Iteration 81/234: loss=0.032329 lr=0.000020 grad_norm=0.761567
Epoch 84/100 Iteration 82/234: loss=0.036727 lr=0.000020 grad_norm=0.853133
Epoch 84/100 Iteration 83/234: loss=0.036958 lr=0.000020 grad_norm=0.507776
Epoch 84/100 Iteration 84/234: loss=0.032651 lr=0.000020 grad_norm=0.900048
Epoch 84/100 Iteration 85/234: loss=0.035988 lr=0.000020 grad_norm=0.726565
Epoch 84/100 Iteration 86/234: loss=0.037215 lr=0.000020 grad_norm=0.430399
Epoch 84/100 Iteration 87/234: loss=0.037982 lr=0.000020 grad_norm=0.849081
Epoch 84/100 Iteration 88/234: loss=0.035880 lr=0.000020 grad_norm=0.808731
Epoch 84/100 Iteration 89/234: loss=0.034970 lr=0.000020 grad_norm=0.643838
Epoch 84/100 Iteration 90/234: loss=0.035908 lr=0.000020 grad_norm=0.559444
Epoch 84/100 Iteration 91/234: loss=0.037143 lr=0.000020 grad_norm=0.518324
Epoch 84/100 Iteration 92/234: loss=0.035013 lr=0.000020 grad_norm=0.662960
Epoch 84/100 Iteration 93/234: loss=0.035361 lr=0.000020 grad_norm=0.458529
Epoch 84/100 Iteration 94/234: loss=0.033576 lr=0.000020 grad_norm=0.658266
Epoch 84/100 Iteration 95/234: loss=0.037566 lr=0.000020 grad_norm=0.905267
Epoch 84/100 Iteration 96/234: loss=0.033344 lr=0.000020 grad_norm=0.617010
Epoch 84/100 Iteration 97/234: loss=0.032606 lr=0.000020 grad_norm=0.431277
Epoch 84/100 Iteration 98/234: loss=0.037930 lr=0.000020 grad_norm=0.547112
Epoch 84/100 Iteration 99/234: loss=0.034149 lr=0.000020 grad_norm=0.496206
Epoch 84/100 Iteration 100/234: loss=0.034595 lr=0.000020 grad_norm=0.493512
Epoch 84/100 Iteration 101/234: loss=0.035895 lr=0.000020 grad_norm=0.594984
Epoch 84/100 Iteration 102/234: loss=0.033656 lr=0.000020 grad_norm=0.495311
Epoch 84/100 Iteration 103/234: loss=0.036073 lr=0.000020 grad_norm=0.613698
Epoch 84/100 Iteration 104/234: loss=0.034967 lr=0.000020 grad_norm=0.758653
Epoch 84/100 Iteration 105/234: loss=0.039963 lr=0.000020 grad_norm=0.565135
Epoch 84/100 Iteration 106/234: loss=0.034029 lr=0.000020 grad_norm=1.099193
Epoch 84/100 Iteration 107/234: loss=0.039367 lr=0.000020 grad_norm=0.493683
Epoch 84/100 Iteration 108/234: loss=0.038456 lr=0.000020 grad_norm=1.095094
Epoch 84/100 Iteration 109/234: loss=0.040024 lr=0.000020 grad_norm=1.419417
Epoch 84/100 Iteration 110/234: loss=0.036401 lr=0.000020 grad_norm=0.821146
Epoch 84/100 Iteration 111/234: loss=0.035224 lr=0.000020 grad_norm=0.641573
Epoch 84/100 Iteration 112/234: loss=0.032757 lr=0.000020 grad_norm=0.668768
Epoch 84/100 Iteration 113/234: loss=0.037506 lr=0.000020 grad_norm=0.695173
Epoch 84/100 Iteration 114/234: loss=0.034286 lr=0.000020 grad_norm=1.281780
Epoch 84/100 Iteration 115/234: loss=0.035440 lr=0.000020 grad_norm=0.408940
Epoch 84/100 Iteration 116/234: loss=0.034263 lr=0.000020 grad_norm=1.424106
Epoch 84/100 Iteration 117/234: loss=0.032694 lr=0.000020 grad_norm=1.688454
Epoch 84/100 Iteration 118/234: loss=0.037746 lr=0.000020 grad_norm=0.552815
Epoch 84/100 Iteration 119/234: loss=0.038246 lr=0.000020 grad_norm=1.239965
Epoch 84/100 Iteration 120/234: loss=0.036833 lr=0.000020 grad_norm=0.880737
Epoch 84/100 Iteration 121/234: loss=0.033666 lr=0.000020 grad_norm=0.543859
Epoch 84/100 Iteration 122/234: loss=0.034911 lr=0.000020 grad_norm=1.048680
Epoch 84/100 Iteration 123/234: loss=0.032824 lr=0.000020 grad_norm=0.650908
Epoch 84/100 Iteration 124/234: loss=0.037708 lr=0.000020 grad_norm=1.094420
Epoch 84/100 Iteration 125/234: loss=0.032107 lr=0.000020 grad_norm=1.044198
Epoch 84/100 Iteration 126/234: loss=0.036524 lr=0.000020 grad_norm=0.537480
Epoch 84/100 Iteration 127/234: loss=0.035048 lr=0.000020 grad_norm=1.217043
Epoch 84/100 Iteration 128/234: loss=0.032673 lr=0.000020 grad_norm=0.872898
Epoch 84/100 Iteration 129/234: loss=0.037440 lr=0.000020 grad_norm=0.729317
Epoch 84/100 Iteration 130/234: loss=0.039569 lr=0.000020 grad_norm=1.325289
Epoch 84/100 Iteration 131/234: loss=0.035305 lr=0.000020 grad_norm=1.110556
Epoch 84/100 Iteration 132/234: loss=0.039568 lr=0.000020 grad_norm=0.933361
Epoch 84/100 Iteration 133/234: loss=0.032266 lr=0.000020 grad_norm=1.064356
Epoch 84/100 Iteration 134/234: loss=0.038377 lr=0.000020 grad_norm=0.615606
Epoch 84/100 Iteration 135/234: loss=0.035979 lr=0.000020 grad_norm=1.133005
Epoch 84/100 Iteration 136/234: loss=0.037867 lr=0.000020 grad_norm=0.833378
Epoch 84/100 Iteration 137/234: loss=0.036438 lr=0.000020 grad_norm=0.936152
Epoch 84/100 Iteration 138/234: loss=0.037808 lr=0.000020 grad_norm=1.170632
Epoch 84/100 Iteration 139/234: loss=0.036397 lr=0.000020 grad_norm=0.529603
Epoch 84/100 Iteration 140/234: loss=0.033015 lr=0.000020 grad_norm=1.015646
Epoch 84/100 Iteration 141/234: loss=0.030810 lr=0.000020 grad_norm=0.725198
Epoch 84/100 Iteration 142/234: loss=0.033873 lr=0.000020 grad_norm=0.736794
Epoch 84/100 Iteration 143/234: loss=0.035602 lr=0.000020 grad_norm=1.195630
Epoch 84/100 Iteration 144/234: loss=0.038037 lr=0.000020 grad_norm=0.650125
Epoch 84/100 Iteration 145/234: loss=0.036959 lr=0.000020 grad_norm=0.570606
Epoch 84/100 Iteration 146/234: loss=0.036276 lr=0.000020 grad_norm=0.653959
Epoch 84/100 Iteration 147/234: loss=0.034303 lr=0.000020 grad_norm=0.493740
Epoch 84/100 Iteration 148/234: loss=0.036044 lr=0.000020 grad_norm=0.602008
Epoch 84/100 Iteration 149/234: loss=0.032888 lr=0.000020 grad_norm=0.489882
Epoch 84/100 Iteration 150/234: loss=0.034836 lr=0.000020 grad_norm=0.584002
Epoch 84/100 Iteration 151/234: loss=0.037802 lr=0.000020 grad_norm=0.688818
Epoch 84/100 Iteration 152/234: loss=0.035539 lr=0.000020 grad_norm=0.677970
Epoch 84/100 Iteration 153/234: loss=0.033607 lr=0.000020 grad_norm=0.489886
Epoch 84/100 Iteration 154/234: loss=0.036097 lr=0.000020 grad_norm=0.788102
Epoch 84/100 Iteration 155/234: loss=0.036475 lr=0.000020 grad_norm=0.523780
Epoch 84/100 Iteration 156/234: loss=0.033312 lr=0.000020 grad_norm=0.752389
Epoch 84/100 Iteration 157/234: loss=0.036695 lr=0.000020 grad_norm=0.555429
Epoch 84/100 Iteration 158/234: loss=0.034966 lr=0.000020 grad_norm=0.935501
Epoch 84/100 Iteration 159/234: loss=0.038111 lr=0.000020 grad_norm=1.173120
Epoch 84/100 Iteration 160/234: loss=0.037085 lr=0.000020 grad_norm=0.798688
Epoch 84/100 Iteration 161/234: loss=0.032262 lr=0.000020 grad_norm=0.716838
Epoch 84/100 Iteration 162/234: loss=0.036336 lr=0.000020 grad_norm=1.322334
Epoch 84/100 Iteration 163/234: loss=0.033639 lr=0.000020 grad_norm=1.034672
Epoch 84/100 Iteration 164/234: loss=0.032934 lr=0.000020 grad_norm=0.561329
Epoch 84/100 Iteration 165/234: loss=0.038134 lr=0.000020 grad_norm=1.063874
Epoch 84/100 Iteration 166/234: loss=0.037048 lr=0.000020 grad_norm=1.014865
Epoch 84/100 Iteration 167/234: loss=0.034902 lr=0.000020 grad_norm=0.855069
Epoch 84/100 Iteration 168/234: loss=0.033951 lr=0.000020 grad_norm=0.882076
Epoch 84/100 Iteration 169/234: loss=0.035028 lr=0.000020 grad_norm=0.675050
Epoch 84/100 Iteration 170/234: loss=0.033944 lr=0.000020 grad_norm=1.023839
Epoch 84/100 Iteration 171/234: loss=0.035478 lr=0.000020 grad_norm=0.419546
Epoch 84/100 Iteration 172/234: loss=0.035962 lr=0.000020 grad_norm=1.099795
Epoch 84/100 Iteration 173/234: loss=0.035256 lr=0.000020 grad_norm=1.229568
Epoch 84/100 Iteration 174/234: loss=0.035933 lr=0.000020 grad_norm=0.534152
Epoch 84/100 Iteration 175/234: loss=0.033904 lr=0.000020 grad_norm=0.822297
Epoch 84/100 Iteration 176/234: loss=0.036152 lr=0.000020 grad_norm=0.482981
Epoch 84/100 Iteration 177/234: loss=0.033656 lr=0.000020 grad_norm=0.699278
Epoch 84/100 Iteration 178/234: loss=0.036099 lr=0.000020 grad_norm=0.687531
Epoch 84/100 Iteration 179/234: loss=0.034778 lr=0.000020 grad_norm=1.362510
Epoch 84/100 Iteration 180/234: loss=0.038251 lr=0.000020 grad_norm=1.541030
Epoch 84/100 Iteration 181/234: loss=0.035904 lr=0.000020 grad_norm=0.718129
Epoch 84/100 Iteration 182/234: loss=0.036422 lr=0.000020 grad_norm=0.918976
Epoch 84/100 Iteration 183/234: loss=0.036517 lr=0.000020 grad_norm=0.659737
Epoch 84/100 Iteration 184/234: loss=0.039699 lr=0.000020 grad_norm=0.629635
Epoch 84/100 Iteration 185/234: loss=0.032500 lr=0.000020 grad_norm=0.845174
Epoch 84/100 Iteration 186/234: loss=0.034606 lr=0.000020 grad_norm=0.426979
Epoch 84/100 Iteration 187/234: loss=0.037972 lr=0.000020 grad_norm=0.828824
Epoch 84/100 Iteration 188/234: loss=0.034697 lr=0.000020 grad_norm=0.635008
Epoch 84/100 Iteration 189/234: loss=0.032005 lr=0.000020 grad_norm=0.455615
Epoch 84/100 Iteration 190/234: loss=0.032312 lr=0.000020 grad_norm=0.520788
Epoch 84/100 Iteration 191/234: loss=0.035275 lr=0.000020 grad_norm=0.648261
Epoch 84/100 Iteration 192/234: loss=0.036767 lr=0.000020 grad_norm=0.772335
Epoch 84/100 Iteration 193/234: loss=0.038700 lr=0.000020 grad_norm=0.700758
Epoch 84/100 Iteration 194/234: loss=0.037095 lr=0.000020 grad_norm=0.725069
Epoch 84/100 Iteration 195/234: loss=0.035084 lr=0.000020 grad_norm=0.772638
Epoch 84/100 Iteration 196/234: loss=0.036283 lr=0.000020 grad_norm=0.861631
Epoch 84/100 Iteration 197/234: loss=0.033968 lr=0.000020 grad_norm=0.721287
Epoch 84/100 Iteration 198/234: loss=0.035477 lr=0.000020 grad_norm=0.666843
Epoch 84/100 Iteration 199/234: loss=0.037124 lr=0.000020 grad_norm=2.127120
Epoch 84/100 Iteration 200/234: loss=0.039315 lr=0.000020 grad_norm=2.027542
Epoch 84/100 Iteration 201/234: loss=0.038011 lr=0.000020 grad_norm=1.017853
Epoch 84/100 Iteration 202/234: loss=0.036593 lr=0.000020 grad_norm=1.023346
Epoch 84/100 Iteration 203/234: loss=0.037718 lr=0.000020 grad_norm=0.701783
Epoch 84/100 Iteration 204/234: loss=0.037255 lr=0.000020 grad_norm=0.937844
Epoch 84/100 Iteration 205/234: loss=0.037483 lr=0.000020 grad_norm=1.172567
Epoch 84/100 Iteration 206/234: loss=0.037345 lr=0.000020 grad_norm=0.724647
Epoch 84/100 Iteration 207/234: loss=0.037197 lr=0.000020 grad_norm=0.870073
Epoch 84/100 Iteration 208/234: loss=0.038193 lr=0.000020 grad_norm=0.526734
Epoch 84/100 Iteration 209/234: loss=0.033701 lr=0.000020 grad_norm=0.541373
Epoch 84/100 Iteration 210/234: loss=0.033051 lr=0.000020 grad_norm=0.696952
Epoch 84/100 Iteration 211/234: loss=0.032197 lr=0.000020 grad_norm=0.434696
Epoch 84/100 Iteration 212/234: loss=0.037847 lr=0.000020 grad_norm=0.671505
Epoch 84/100 Iteration 213/234: loss=0.039005 lr=0.000020 grad_norm=0.855189
Epoch 84/100 Iteration 214/234: loss=0.034863 lr=0.000020 grad_norm=1.118801
Epoch 84/100 Iteration 215/234: loss=0.037895 lr=0.000020 grad_norm=0.881559
Epoch 84/100 Iteration 216/234: loss=0.035062 lr=0.000020 grad_norm=0.451097
Epoch 84/100 Iteration 217/234: loss=0.037136 lr=0.000020 grad_norm=0.687625
Epoch 84/100 Iteration 218/234: loss=0.037867 lr=0.000020 grad_norm=1.121845
Epoch 84/100 Iteration 219/234: loss=0.038347 lr=0.000020 grad_norm=0.813896
Epoch 84/100 Iteration 220/234: loss=0.037435 lr=0.000020 grad_norm=0.607861
Epoch 84/100 Iteration 221/234: loss=0.033297 lr=0.000020 grad_norm=0.633095
Epoch 84/100 Iteration 222/234: loss=0.036982 lr=0.000020 grad_norm=0.453078
Epoch 84/100 Iteration 223/234: loss=0.034783 lr=0.000020 grad_norm=0.306006
Epoch 84/100 Iteration 224/234: loss=0.034854 lr=0.000020 grad_norm=0.315489
Epoch 84/100 Iteration 225/234: loss=0.033337 lr=0.000020 grad_norm=0.392754
Epoch 84/100 Iteration 226/234: loss=0.037855 lr=0.000020 grad_norm=0.401304
Epoch 84/100 Iteration 227/234: loss=0.036394 lr=0.000020 grad_norm=0.458154
Epoch 84/100 Iteration 228/234: loss=0.035822 lr=0.000020 grad_norm=0.525507
Epoch 84/100 Iteration 229/234: loss=0.032681 lr=0.000020 grad_norm=0.574008
Epoch 84/100 Iteration 230/234: loss=0.034860 lr=0.000020 grad_norm=0.560494
Epoch 84/100 Iteration 231/234: loss=0.033491 lr=0.000020 grad_norm=0.423399
Epoch 84/100 Iteration 232/234: loss=0.034167 lr=0.000020 grad_norm=0.833387
Epoch 84/100 Iteration 233/234: loss=0.036984 lr=0.000020 grad_norm=0.971162
Epoch 84/100 Iteration 234/234: loss=0.035166 lr=0.000020 grad_norm=0.732883
Epoch 84/100 finished. Avg Loss: 0.035764
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 85/100 Iteration 1/234: loss=0.036454 lr=0.000020 grad_norm=0.582526
Epoch 85/100 Iteration 2/234: loss=0.036362 lr=0.000020 grad_norm=0.778282
Epoch 85/100 Iteration 3/234: loss=0.033331 lr=0.000020 grad_norm=1.009573
Epoch 85/100 Iteration 4/234: loss=0.039067 lr=0.000020 grad_norm=0.823049
Epoch 85/100 Iteration 5/234: loss=0.037615 lr=0.000020 grad_norm=0.654778
Epoch 85/100 Iteration 6/234: loss=0.031989 lr=0.000020 grad_norm=0.480999
Epoch 85/100 Iteration 7/234: loss=0.039455 lr=0.000020 grad_norm=0.805038
Epoch 85/100 Iteration 8/234: loss=0.035377 lr=0.000020 grad_norm=1.088590
Epoch 85/100 Iteration 9/234: loss=0.041446 lr=0.000020 grad_norm=1.159138
Epoch 85/100 Iteration 10/234: loss=0.036041 lr=0.000020 grad_norm=0.884274
Epoch 85/100 Iteration 11/234: loss=0.035513 lr=0.000020 grad_norm=0.849534
Epoch 85/100 Iteration 12/234: loss=0.036580 lr=0.000020 grad_norm=0.648915
Epoch 85/100 Iteration 13/234: loss=0.035032 lr=0.000020 grad_norm=0.943163
Epoch 85/100 Iteration 14/234: loss=0.038091 lr=0.000020 grad_norm=0.554745
Epoch 85/100 Iteration 15/234: loss=0.034605 lr=0.000020 grad_norm=1.227208
Epoch 85/100 Iteration 16/234: loss=0.040110 lr=0.000020 grad_norm=1.266783
Epoch 85/100 Iteration 17/234: loss=0.033395 lr=0.000020 grad_norm=0.719133
Epoch 85/100 Iteration 18/234: loss=0.034379 lr=0.000020 grad_norm=1.019401
Epoch 85/100 Iteration 19/234: loss=0.041497 lr=0.000020 grad_norm=1.594744
Epoch 85/100 Iteration 20/234: loss=0.033898 lr=0.000020 grad_norm=1.604622
Epoch 85/100 Iteration 21/234: loss=0.037628 lr=0.000020 grad_norm=1.329189
Epoch 85/100 Iteration 22/234: loss=0.034993 lr=0.000020 grad_norm=0.570286
Epoch 85/100 Iteration 23/234: loss=0.036344 lr=0.000020 grad_norm=1.039337
Epoch 85/100 Iteration 24/234: loss=0.039028 lr=0.000020 grad_norm=1.304836
Epoch 85/100 Iteration 25/234: loss=0.037319 lr=0.000020 grad_norm=0.775258
Epoch 85/100 Iteration 26/234: loss=0.033829 lr=0.000020 grad_norm=0.824731
Epoch 85/100 Iteration 27/234: loss=0.034318 lr=0.000020 grad_norm=0.625460
Epoch 85/100 Iteration 28/234: loss=0.034720 lr=0.000020 grad_norm=0.712412
Epoch 85/100 Iteration 29/234: loss=0.033889 lr=0.000020 grad_norm=0.801841
Epoch 85/100 Iteration 30/234: loss=0.032394 lr=0.000020 grad_norm=0.523153
Epoch 85/100 Iteration 31/234: loss=0.036757 lr=0.000020 grad_norm=1.091330
Epoch 85/100 Iteration 32/234: loss=0.033353 lr=0.000020 grad_norm=0.840114
Epoch 85/100 Iteration 33/234: loss=0.035696 lr=0.000020 grad_norm=0.384306
Epoch 85/100 Iteration 34/234: loss=0.032595 lr=0.000020 grad_norm=0.752224
Epoch 85/100 Iteration 35/234: loss=0.039355 lr=0.000020 grad_norm=0.862722
Epoch 85/100 Iteration 36/234: loss=0.034657 lr=0.000020 grad_norm=0.595378
Epoch 85/100 Iteration 37/234: loss=0.031013 lr=0.000020 grad_norm=0.446674
Epoch 85/100 Iteration 38/234: loss=0.033910 lr=0.000020 grad_norm=0.612773
Epoch 85/100 Iteration 39/234: loss=0.034740 lr=0.000020 grad_norm=0.692326
Epoch 85/100 Iteration 40/234: loss=0.035012 lr=0.000020 grad_norm=0.526943
Epoch 85/100 Iteration 41/234: loss=0.037646 lr=0.000020 grad_norm=0.841892
Epoch 85/100 Iteration 42/234: loss=0.036883 lr=0.000020 grad_norm=0.889081
Epoch 85/100 Iteration 43/234: loss=0.036830 lr=0.000020 grad_norm=0.575776
Epoch 85/100 Iteration 44/234: loss=0.031573 lr=0.000020 grad_norm=0.619553
Epoch 85/100 Iteration 45/234: loss=0.031854 lr=0.000020 grad_norm=0.628039
Epoch 85/100 Iteration 46/234: loss=0.033909 lr=0.000020 grad_norm=0.531351
Epoch 85/100 Iteration 47/234: loss=0.040914 lr=0.000020 grad_norm=0.767753
Epoch 85/100 Iteration 48/234: loss=0.036953 lr=0.000020 grad_norm=1.226106
Epoch 85/100 Iteration 49/234: loss=0.036802 lr=0.000020 grad_norm=0.947641
Epoch 85/100 Iteration 50/234: loss=0.034062 lr=0.000020 grad_norm=0.522182
Epoch 85/100 Iteration 51/234: loss=0.036538 lr=0.000020 grad_norm=1.298588
Epoch 85/100 Iteration 52/234: loss=0.039331 lr=0.000020 grad_norm=1.322721
Epoch 85/100 Iteration 53/234: loss=0.033490 lr=0.000020 grad_norm=0.485606
Epoch 85/100 Iteration 54/234: loss=0.035032 lr=0.000020 grad_norm=1.021209
Epoch 85/100 Iteration 55/234: loss=0.035004 lr=0.000020 grad_norm=1.003406
Epoch 85/100 Iteration 56/234: loss=0.038209 lr=0.000020 grad_norm=0.630937
Epoch 85/100 Iteration 57/234: loss=0.033671 lr=0.000020 grad_norm=1.173675
Epoch 85/100 Iteration 58/234: loss=0.039422 lr=0.000020 grad_norm=1.266529
Epoch 85/100 Iteration 59/234: loss=0.035723 lr=0.000020 grad_norm=1.153881
Epoch 85/100 Iteration 60/234: loss=0.037973 lr=0.000020 grad_norm=0.759476
Epoch 85/100 Iteration 61/234: loss=0.033085 lr=0.000020 grad_norm=1.066020
Epoch 85/100 Iteration 62/234: loss=0.031127 lr=0.000020 grad_norm=0.745478
Epoch 85/100 Iteration 63/234: loss=0.034806 lr=0.000020 grad_norm=0.876763
Epoch 85/100 Iteration 64/234: loss=0.034940 lr=0.000020 grad_norm=1.445967
Epoch 85/100 Iteration 65/234: loss=0.033670 lr=0.000020 grad_norm=0.700860
Epoch 85/100 Iteration 66/234: loss=0.036297 lr=0.000020 grad_norm=0.997418
Epoch 85/100 Iteration 67/234: loss=0.039835 lr=0.000020 grad_norm=1.710870
Epoch 85/100 Iteration 68/234: loss=0.033298 lr=0.000020 grad_norm=1.108996
Epoch 85/100 Iteration 69/234: loss=0.034970 lr=0.000020 grad_norm=0.861696
Epoch 85/100 Iteration 70/234: loss=0.036495 lr=0.000020 grad_norm=1.404492
Epoch 85/100 Iteration 71/234: loss=0.037785 lr=0.000020 grad_norm=0.630303
Epoch 85/100 Iteration 72/234: loss=0.030839 lr=0.000020 grad_norm=1.150950
Epoch 85/100 Iteration 73/234: loss=0.036523 lr=0.000020 grad_norm=0.999817
Epoch 85/100 Iteration 74/234: loss=0.037209 lr=0.000020 grad_norm=0.875124
Epoch 85/100 Iteration 75/234: loss=0.037038 lr=0.000020 grad_norm=1.236923
Epoch 85/100 Iteration 76/234: loss=0.039537 lr=0.000020 grad_norm=0.787928
Epoch 85/100 Iteration 77/234: loss=0.039008 lr=0.000020 grad_norm=1.192784
Epoch 85/100 Iteration 78/234: loss=0.036868 lr=0.000020 grad_norm=1.336426
Epoch 85/100 Iteration 79/234: loss=0.035679 lr=0.000020 grad_norm=1.102797
Epoch 85/100 Iteration 80/234: loss=0.038542 lr=0.000020 grad_norm=0.557608
Epoch 85/100 Iteration 81/234: loss=0.030971 lr=0.000020 grad_norm=0.754365
Epoch 85/100 Iteration 82/234: loss=0.034768 lr=0.000020 grad_norm=0.565677
Epoch 85/100 Iteration 83/234: loss=0.030267 lr=0.000020 grad_norm=0.705366
Epoch 85/100 Iteration 84/234: loss=0.035850 lr=0.000020 grad_norm=0.540512
Epoch 85/100 Iteration 85/234: loss=0.035279 lr=0.000020 grad_norm=0.555950
Epoch 85/100 Iteration 86/234: loss=0.034117 lr=0.000020 grad_norm=0.796714
Epoch 85/100 Iteration 87/234: loss=0.041883 lr=0.000020 grad_norm=0.689991
Epoch 85/100 Iteration 88/234: loss=0.033627 lr=0.000020 grad_norm=0.397895
Epoch 85/100 Iteration 89/234: loss=0.031589 lr=0.000020 grad_norm=0.700912
Epoch 85/100 Iteration 90/234: loss=0.033488 lr=0.000020 grad_norm=0.899379
Epoch 85/100 Iteration 91/234: loss=0.034053 lr=0.000020 grad_norm=0.405514
Epoch 85/100 Iteration 92/234: loss=0.033390 lr=0.000020 grad_norm=0.968512
Epoch 85/100 Iteration 93/234: loss=0.036218 lr=0.000020 grad_norm=0.923893
Epoch 85/100 Iteration 94/234: loss=0.036866 lr=0.000020 grad_norm=0.449001
Epoch 85/100 Iteration 95/234: loss=0.034698 lr=0.000020 grad_norm=0.503346
Epoch 85/100 Iteration 96/234: loss=0.038985 lr=0.000020 grad_norm=0.952573
Epoch 85/100 Iteration 97/234: loss=0.033089 lr=0.000020 grad_norm=0.795745
Epoch 85/100 Iteration 98/234: loss=0.034217 lr=0.000020 grad_norm=0.347653
Epoch 85/100 Iteration 99/234: loss=0.035560 lr=0.000020 grad_norm=0.741148
Epoch 85/100 Iteration 100/234: loss=0.037543 lr=0.000020 grad_norm=1.188818
Epoch 85/100 Iteration 101/234: loss=0.033968 lr=0.000020 grad_norm=0.641296
Epoch 85/100 Iteration 102/234: loss=0.034375 lr=0.000020 grad_norm=0.819745
Epoch 85/100 Iteration 103/234: loss=0.032824 lr=0.000020 grad_norm=0.844387
Epoch 85/100 Iteration 104/234: loss=0.030887 lr=0.000020 grad_norm=0.478955
Epoch 85/100 Iteration 105/234: loss=0.034194 lr=0.000020 grad_norm=1.011248
Epoch 85/100 Iteration 106/234: loss=0.040455 lr=0.000020 grad_norm=0.799071
Epoch 85/100 Iteration 107/234: loss=0.035949 lr=0.000020 grad_norm=0.681051
Epoch 85/100 Iteration 108/234: loss=0.033598 lr=0.000020 grad_norm=0.749187
Epoch 85/100 Iteration 109/234: loss=0.031464 lr=0.000020 grad_norm=0.408032
Epoch 85/100 Iteration 110/234: loss=0.031736 lr=0.000020 grad_norm=0.843140
Epoch 85/100 Iteration 111/234: loss=0.036720 lr=0.000020 grad_norm=0.860678
Epoch 85/100 Iteration 112/234: loss=0.034407 lr=0.000020 grad_norm=0.444647
Epoch 85/100 Iteration 113/234: loss=0.037749 lr=0.000020 grad_norm=0.596011
Epoch 85/100 Iteration 114/234: loss=0.035893 lr=0.000020 grad_norm=0.489486
Epoch 85/100 Iteration 115/234: loss=0.033566 lr=0.000020 grad_norm=0.439543
Epoch 85/100 Iteration 116/234: loss=0.036546 lr=0.000020 grad_norm=0.531533
Epoch 85/100 Iteration 117/234: loss=0.031791 lr=0.000020 grad_norm=0.422667
Epoch 85/100 Iteration 118/234: loss=0.031384 lr=0.000020 grad_norm=0.592799
Epoch 85/100 Iteration 119/234: loss=0.034467 lr=0.000020 grad_norm=0.508212
Epoch 85/100 Iteration 120/234: loss=0.038063 lr=0.000020 grad_norm=0.728285
Epoch 85/100 Iteration 121/234: loss=0.035579 lr=0.000020 grad_norm=1.252734
Epoch 85/100 Iteration 122/234: loss=0.035163 lr=0.000020 grad_norm=0.713325
Epoch 85/100 Iteration 123/234: loss=0.031080 lr=0.000020 grad_norm=0.777317
Epoch 85/100 Iteration 124/234: loss=0.038289 lr=0.000020 grad_norm=1.395750
Epoch 85/100 Iteration 125/234: loss=0.036071 lr=0.000020 grad_norm=0.736342
Epoch 85/100 Iteration 126/234: loss=0.037843 lr=0.000020 grad_norm=1.389984
Epoch 85/100 Iteration 127/234: loss=0.034814 lr=0.000020 grad_norm=1.928303
Epoch 85/100 Iteration 128/234: loss=0.035640 lr=0.000020 grad_norm=0.727865
Epoch 85/100 Iteration 129/234: loss=0.035361 lr=0.000020 grad_norm=1.640677
Epoch 85/100 Iteration 130/234: loss=0.040518 lr=0.000020 grad_norm=1.601909
Epoch 85/100 Iteration 131/234: loss=0.040287 lr=0.000020 grad_norm=0.987547
Epoch 85/100 Iteration 132/234: loss=0.029238 lr=0.000020 grad_norm=1.090356
Epoch 85/100 Iteration 133/234: loss=0.033428 lr=0.000020 grad_norm=1.081317
Epoch 85/100 Iteration 134/234: loss=0.036369 lr=0.000020 grad_norm=1.794165
Epoch 85/100 Iteration 135/234: loss=0.037655 lr=0.000020 grad_norm=0.598866
Epoch 85/100 Iteration 136/234: loss=0.034155 lr=0.000020 grad_norm=1.419553
Epoch 85/100 Iteration 137/234: loss=0.035146 lr=0.000020 grad_norm=0.497336
Epoch 85/100 Iteration 138/234: loss=0.034916 lr=0.000020 grad_norm=1.395742
Epoch 85/100 Iteration 139/234: loss=0.034244 lr=0.000020 grad_norm=0.897100
Epoch 85/100 Iteration 140/234: loss=0.037237 lr=0.000020 grad_norm=0.880545
Epoch 85/100 Iteration 141/234: loss=0.036557 lr=0.000020 grad_norm=1.029368
Epoch 85/100 Iteration 142/234: loss=0.036846 lr=0.000020 grad_norm=0.661338
Epoch 85/100 Iteration 143/234: loss=0.037475 lr=0.000020 grad_norm=1.273014
Epoch 85/100 Iteration 144/234: loss=0.034503 lr=0.000020 grad_norm=0.779816
Epoch 85/100 Iteration 145/234: loss=0.032136 lr=0.000020 grad_norm=0.959711
Epoch 85/100 Iteration 146/234: loss=0.037831 lr=0.000020 grad_norm=1.046685
Epoch 85/100 Iteration 147/234: loss=0.034246 lr=0.000020 grad_norm=0.787224
Epoch 85/100 Iteration 148/234: loss=0.038550 lr=0.000020 grad_norm=0.833314
Epoch 85/100 Iteration 149/234: loss=0.033180 lr=0.000020 grad_norm=0.689952
Epoch 85/100 Iteration 150/234: loss=0.035105 lr=0.000020 grad_norm=1.142560
Epoch 85/100 Iteration 151/234: loss=0.043777 lr=0.000020 grad_norm=1.051266
Epoch 85/100 Iteration 152/234: loss=0.033471 lr=0.000020 grad_norm=1.127483
Epoch 85/100 Iteration 153/234: loss=0.041443 lr=0.000020 grad_norm=0.565244
Epoch 85/100 Iteration 154/234: loss=0.039024 lr=0.000020 grad_norm=1.669322
Epoch 85/100 Iteration 155/234: loss=0.035136 lr=0.000020 grad_norm=1.383291
Epoch 85/100 Iteration 156/234: loss=0.035563 lr=0.000020 grad_norm=0.790650
Epoch 85/100 Iteration 157/234: loss=0.034988 lr=0.000020 grad_norm=1.208665
Epoch 85/100 Iteration 158/234: loss=0.034874 lr=0.000020 grad_norm=1.062932
Epoch 85/100 Iteration 159/234: loss=0.033714 lr=0.000020 grad_norm=0.760239
Epoch 85/100 Iteration 160/234: loss=0.035349 lr=0.000020 grad_norm=0.941062
Epoch 85/100 Iteration 161/234: loss=0.036871 lr=0.000020 grad_norm=1.214500
Epoch 85/100 Iteration 162/234: loss=0.036523 lr=0.000020 grad_norm=0.548095
Epoch 85/100 Iteration 163/234: loss=0.036718 lr=0.000020 grad_norm=1.101525
Epoch 85/100 Iteration 164/234: loss=0.036263 lr=0.000020 grad_norm=0.916738
Epoch 85/100 Iteration 165/234: loss=0.036025 lr=0.000020 grad_norm=1.100219
Epoch 85/100 Iteration 166/234: loss=0.035887 lr=0.000020 grad_norm=0.427333
Epoch 85/100 Iteration 167/234: loss=0.036360 lr=0.000020 grad_norm=1.109346
Epoch 85/100 Iteration 168/234: loss=0.032630 lr=0.000020 grad_norm=0.754392
Epoch 85/100 Iteration 169/234: loss=0.034847 lr=0.000020 grad_norm=0.910973
Epoch 85/100 Iteration 170/234: loss=0.037896 lr=0.000020 grad_norm=1.356334
Epoch 85/100 Iteration 171/234: loss=0.031961 lr=0.000020 grad_norm=0.496016
Epoch 85/100 Iteration 172/234: loss=0.035620 lr=0.000020 grad_norm=0.721780
Epoch 85/100 Iteration 173/234: loss=0.039651 lr=0.000020 grad_norm=0.594744
Epoch 85/100 Iteration 174/234: loss=0.037508 lr=0.000020 grad_norm=0.542380
Epoch 85/100 Iteration 175/234: loss=0.034954 lr=0.000020 grad_norm=0.787081
Epoch 85/100 Iteration 176/234: loss=0.034089 lr=0.000020 grad_norm=0.634615
Epoch 85/100 Iteration 177/234: loss=0.032312 lr=0.000020 grad_norm=0.598985
Epoch 85/100 Iteration 178/234: loss=0.031216 lr=0.000020 grad_norm=0.622368
Epoch 85/100 Iteration 179/234: loss=0.035964 lr=0.000020 grad_norm=0.682703
Epoch 85/100 Iteration 180/234: loss=0.033897 lr=0.000020 grad_norm=0.708947
Epoch 85/100 Iteration 181/234: loss=0.031709 lr=0.000020 grad_norm=0.753833
Epoch 85/100 Iteration 182/234: loss=0.034926 lr=0.000020 grad_norm=0.413228
Epoch 85/100 Iteration 183/234: loss=0.032169 lr=0.000020 grad_norm=0.617117
Epoch 85/100 Iteration 184/234: loss=0.034656 lr=0.000020 grad_norm=0.465793
Epoch 85/100 Iteration 185/234: loss=0.037418 lr=0.000020 grad_norm=0.528457
Epoch 85/100 Iteration 186/234: loss=0.033945 lr=0.000020 grad_norm=0.673050
Epoch 85/100 Iteration 187/234: loss=0.035962 lr=0.000020 grad_norm=0.607859
Epoch 85/100 Iteration 188/234: loss=0.035070 lr=0.000020 grad_norm=0.546699
Epoch 85/100 Iteration 189/234: loss=0.037522 lr=0.000020 grad_norm=0.481464
Epoch 85/100 Iteration 190/234: loss=0.032122 lr=0.000020 grad_norm=0.409884
Epoch 85/100 Iteration 191/234: loss=0.036345 lr=0.000020 grad_norm=0.414311
Epoch 85/100 Iteration 192/234: loss=0.034855 lr=0.000020 grad_norm=0.433457
Epoch 85/100 Iteration 193/234: loss=0.035850 lr=0.000020 grad_norm=0.651737
Epoch 85/100 Iteration 194/234: loss=0.033075 lr=0.000020 grad_norm=0.376672
Epoch 85/100 Iteration 195/234: loss=0.033510 lr=0.000020 grad_norm=0.570172
Epoch 85/100 Iteration 196/234: loss=0.035747 lr=0.000020 grad_norm=0.430263
Epoch 85/100 Iteration 197/234: loss=0.035650 lr=0.000020 grad_norm=0.578752
Epoch 85/100 Iteration 198/234: loss=0.031090 lr=0.000020 grad_norm=0.587903
Epoch 85/100 Iteration 199/234: loss=0.034236 lr=0.000020 grad_norm=0.824324
Epoch 85/100 Iteration 200/234: loss=0.037642 lr=0.000020 grad_norm=1.046834
Epoch 85/100 Iteration 201/234: loss=0.037329 lr=0.000020 grad_norm=0.397216
Epoch 85/100 Iteration 202/234: loss=0.033970 lr=0.000020 grad_norm=0.903102
Epoch 85/100 Iteration 203/234: loss=0.034709 lr=0.000020 grad_norm=0.835106
Epoch 85/100 Iteration 204/234: loss=0.033137 lr=0.000020 grad_norm=0.476235
Epoch 85/100 Iteration 205/234: loss=0.035436 lr=0.000020 grad_norm=0.809060
Epoch 85/100 Iteration 206/234: loss=0.035185 lr=0.000020 grad_norm=0.438350
Epoch 85/100 Iteration 207/234: loss=0.036945 lr=0.000020 grad_norm=0.441584
Epoch 85/100 Iteration 208/234: loss=0.036499 lr=0.000020 grad_norm=0.489506
Epoch 85/100 Iteration 209/234: loss=0.034950 lr=0.000020 grad_norm=0.881290
Epoch 85/100 Iteration 210/234: loss=0.035059 lr=0.000020 grad_norm=0.747098
Epoch 85/100 Iteration 211/234: loss=0.037177 lr=0.000020 grad_norm=0.842045
Epoch 85/100 Iteration 212/234: loss=0.036292 lr=0.000020 grad_norm=1.132263
Epoch 85/100 Iteration 213/234: loss=0.036792 lr=0.000020 grad_norm=0.774614
Epoch 85/100 Iteration 214/234: loss=0.034320 lr=0.000020 grad_norm=0.669094
Epoch 85/100 Iteration 215/234: loss=0.034394 lr=0.000020 grad_norm=1.105678
Epoch 85/100 Iteration 216/234: loss=0.035342 lr=0.000020 grad_norm=0.800028
Epoch 85/100 Iteration 217/234: loss=0.035301 lr=0.000020 grad_norm=0.706010
Epoch 85/100 Iteration 218/234: loss=0.032297 lr=0.000020 grad_norm=0.673150
Epoch 85/100 Iteration 219/234: loss=0.037347 lr=0.000020 grad_norm=0.664570
Epoch 85/100 Iteration 220/234: loss=0.033190 lr=0.000020 grad_norm=0.812162
Epoch 85/100 Iteration 221/234: loss=0.038327 lr=0.000020 grad_norm=0.477829
Epoch 85/100 Iteration 222/234: loss=0.037090 lr=0.000020 grad_norm=1.500700
Epoch 85/100 Iteration 223/234: loss=0.038137 lr=0.000020 grad_norm=1.416075
Epoch 85/100 Iteration 224/234: loss=0.036210 lr=0.000020 grad_norm=0.782037
Epoch 85/100 Iteration 225/234: loss=0.037157 lr=0.000020 grad_norm=0.786550
Epoch 85/100 Iteration 226/234: loss=0.036946 lr=0.000020 grad_norm=0.526365
Epoch 85/100 Iteration 227/234: loss=0.038383 lr=0.000020 grad_norm=1.256128
Epoch 85/100 Iteration 228/234: loss=0.034620 lr=0.000020 grad_norm=1.032605
Epoch 85/100 Iteration 229/234: loss=0.039324 lr=0.000020 grad_norm=1.031169
Epoch 85/100 Iteration 230/234: loss=0.034267 lr=0.000020 grad_norm=1.350765
Epoch 85/100 Iteration 231/234: loss=0.036638 lr=0.000020 grad_norm=0.566546
Epoch 85/100 Iteration 232/234: loss=0.035037 lr=0.000020 grad_norm=0.919103
Epoch 85/100 Iteration 233/234: loss=0.034119 lr=0.000020 grad_norm=0.824178
Epoch 85/100 Iteration 234/234: loss=0.036711 lr=0.000020 grad_norm=0.513256
Epoch 85/100 finished. Avg Loss: 0.035529
Epoch 86/100 Iteration 1/234: loss=0.040532 lr=0.000020 grad_norm=1.056953
Epoch 86/100 Iteration 2/234: loss=0.034496 lr=0.000020 grad_norm=1.145505
Epoch 86/100 Iteration 3/234: loss=0.034700 lr=0.000020 grad_norm=0.445447
Epoch 86/100 Iteration 4/234: loss=0.037936 lr=0.000020 grad_norm=1.094280
Epoch 86/100 Iteration 5/234: loss=0.039356 lr=0.000020 grad_norm=1.759146
Epoch 86/100 Iteration 6/234: loss=0.033220 lr=0.000020 grad_norm=0.882154
Epoch 86/100 Iteration 7/234: loss=0.033679 lr=0.000020 grad_norm=0.900156
Epoch 86/100 Iteration 8/234: loss=0.038120 lr=0.000020 grad_norm=1.102165
Epoch 86/100 Iteration 9/234: loss=0.032914 lr=0.000020 grad_norm=0.708857
Epoch 86/100 Iteration 10/234: loss=0.036070 lr=0.000020 grad_norm=1.150892
Epoch 86/100 Iteration 11/234: loss=0.037081 lr=0.000020 grad_norm=1.092837
Epoch 86/100 Iteration 12/234: loss=0.033231 lr=0.000020 grad_norm=0.650254
Epoch 86/100 Iteration 13/234: loss=0.032523 lr=0.000020 grad_norm=1.028004
Epoch 86/100 Iteration 14/234: loss=0.033798 lr=0.000020 grad_norm=0.938013
Epoch 86/100 Iteration 15/234: loss=0.038596 lr=0.000020 grad_norm=0.503629
Epoch 86/100 Iteration 16/234: loss=0.035754 lr=0.000020 grad_norm=1.305855
Epoch 86/100 Iteration 17/234: loss=0.036432 lr=0.000020 grad_norm=1.542649
Epoch 86/100 Iteration 18/234: loss=0.032615 lr=0.000020 grad_norm=0.562810
Epoch 86/100 Iteration 19/234: loss=0.034891 lr=0.000020 grad_norm=1.102744
Epoch 86/100 Iteration 20/234: loss=0.034343 lr=0.000020 grad_norm=1.484357
Epoch 86/100 Iteration 21/234: loss=0.031106 lr=0.000020 grad_norm=0.638421
Epoch 86/100 Iteration 22/234: loss=0.035607 lr=0.000020 grad_norm=0.913899
Epoch 86/100 Iteration 23/234: loss=0.033572 lr=0.000020 grad_norm=0.944291
Epoch 86/100 Iteration 24/234: loss=0.033242 lr=0.000020 grad_norm=0.423018
Epoch 86/100 Iteration 25/234: loss=0.035382 lr=0.000020 grad_norm=1.247289
Epoch 86/100 Iteration 26/234: loss=0.035044 lr=0.000020 grad_norm=1.030039
Epoch 86/100 Iteration 27/234: loss=0.035442 lr=0.000020 grad_norm=0.461837
Epoch 86/100 Iteration 28/234: loss=0.036113 lr=0.000020 grad_norm=0.859119
Epoch 86/100 Iteration 29/234: loss=0.037195 lr=0.000020 grad_norm=0.631714
Epoch 86/100 Iteration 30/234: loss=0.035482 lr=0.000020 grad_norm=0.662517
Epoch 86/100 Iteration 31/234: loss=0.035292 lr=0.000020 grad_norm=0.675814
Epoch 86/100 Iteration 32/234: loss=0.031660 lr=0.000020 grad_norm=0.391317
Epoch 86/100 Iteration 33/234: loss=0.035201 lr=0.000020 grad_norm=0.767712
Epoch 86/100 Iteration 34/234: loss=0.037694 lr=0.000020 grad_norm=0.846188
Epoch 86/100 Iteration 35/234: loss=0.036986 lr=0.000020 grad_norm=0.497962
Epoch 86/100 Iteration 36/234: loss=0.036409 lr=0.000020 grad_norm=0.610225
Epoch 86/100 Iteration 37/234: loss=0.037643 lr=0.000020 grad_norm=0.974083
Epoch 86/100 Iteration 38/234: loss=0.040107 lr=0.000020 grad_norm=1.204714
Epoch 86/100 Iteration 39/234: loss=0.035745 lr=0.000020 grad_norm=0.326163
Epoch 86/100 Iteration 40/234: loss=0.038475 lr=0.000020 grad_norm=1.007783
Epoch 86/100 Iteration 41/234: loss=0.036502 lr=0.000020 grad_norm=1.107086
Epoch 86/100 Iteration 42/234: loss=0.035542 lr=0.000020 grad_norm=0.664760
Epoch 86/100 Iteration 43/234: loss=0.033043 lr=0.000020 grad_norm=0.443062
Epoch 86/100 Iteration 44/234: loss=0.033521 lr=0.000020 grad_norm=0.725378
Epoch 86/100 Iteration 45/234: loss=0.036281 lr=0.000020 grad_norm=0.959052
Epoch 86/100 Iteration 46/234: loss=0.033274 lr=0.000020 grad_norm=0.746957
Epoch 86/100 Iteration 47/234: loss=0.034704 lr=0.000020 grad_norm=0.555489
Epoch 86/100 Iteration 48/234: loss=0.035585 lr=0.000020 grad_norm=0.693566
Epoch 86/100 Iteration 49/234: loss=0.034878 lr=0.000020 grad_norm=0.695692
Epoch 86/100 Iteration 50/234: loss=0.039353 lr=0.000020 grad_norm=0.565294
Epoch 86/100 Iteration 51/234: loss=0.037286 lr=0.000020 grad_norm=0.541646
Epoch 86/100 Iteration 52/234: loss=0.036798 lr=0.000020 grad_norm=0.794652
Epoch 86/100 Iteration 53/234: loss=0.038643 lr=0.000020 grad_norm=0.672473
Epoch 86/100 Iteration 54/234: loss=0.035003 lr=0.000020 grad_norm=0.634152
Epoch 86/100 Iteration 55/234: loss=0.039325 lr=0.000020 grad_norm=0.538117
Epoch 86/100 Iteration 56/234: loss=0.038525 lr=0.000020 grad_norm=0.463477
Epoch 86/100 Iteration 57/234: loss=0.037231 lr=0.000020 grad_norm=0.579750
Epoch 86/100 Iteration 58/234: loss=0.034436 lr=0.000020 grad_norm=0.530282
Epoch 86/100 Iteration 59/234: loss=0.031375 lr=0.000020 grad_norm=0.579710
Epoch 86/100 Iteration 60/234: loss=0.034466 lr=0.000020 grad_norm=1.048316
Epoch 86/100 Iteration 61/234: loss=0.034827 lr=0.000020 grad_norm=0.817620
Epoch 86/100 Iteration 62/234: loss=0.035133 lr=0.000020 grad_norm=0.397260
Epoch 86/100 Iteration 63/234: loss=0.035654 lr=0.000020 grad_norm=1.029694
Epoch 86/100 Iteration 64/234: loss=0.032897 lr=0.000020 grad_norm=0.975209
Epoch 86/100 Iteration 65/234: loss=0.037462 lr=0.000020 grad_norm=0.506827
Epoch 86/100 Iteration 66/234: loss=0.032404 lr=0.000020 grad_norm=0.727244
Epoch 86/100 Iteration 67/234: loss=0.035814 lr=0.000020 grad_norm=0.755313
Epoch 86/100 Iteration 68/234: loss=0.033349 lr=0.000020 grad_norm=0.472087
Epoch 86/100 Iteration 69/234: loss=0.033942 lr=0.000020 grad_norm=0.838893
Epoch 86/100 Iteration 70/234: loss=0.036239 lr=0.000020 grad_norm=1.051005
Epoch 86/100 Iteration 71/234: loss=0.033658 lr=0.000020 grad_norm=0.498104
Epoch 86/100 Iteration 72/234: loss=0.037544 lr=0.000020 grad_norm=1.106182
Epoch 86/100 Iteration 73/234: loss=0.037290 lr=0.000020 grad_norm=1.378564
Epoch 86/100 Iteration 74/234: loss=0.035240 lr=0.000020 grad_norm=0.711184
Epoch 86/100 Iteration 75/234: loss=0.037159 lr=0.000020 grad_norm=1.073742
Epoch 86/100 Iteration 76/234: loss=0.034220 lr=0.000020 grad_norm=1.477941
Epoch 86/100 Iteration 77/234: loss=0.038994 lr=0.000020 grad_norm=0.765534
Epoch 86/100 Iteration 78/234: loss=0.035321 lr=0.000020 grad_norm=1.257636
Epoch 86/100 Iteration 79/234: loss=0.039500 lr=0.000020 grad_norm=1.847342
Epoch 86/100 Iteration 80/234: loss=0.039112 lr=0.000020 grad_norm=1.568367
Epoch 86/100 Iteration 81/234: loss=0.034309 lr=0.000020 grad_norm=1.006319
Epoch 86/100 Iteration 82/234: loss=0.035525 lr=0.000020 grad_norm=0.789084
Epoch 86/100 Iteration 83/234: loss=0.032701 lr=0.000020 grad_norm=0.475711
Epoch 86/100 Iteration 84/234: loss=0.035746 lr=0.000020 grad_norm=0.608895
Epoch 86/100 Iteration 85/234: loss=0.033784 lr=0.000020 grad_norm=0.605689
Epoch 86/100 Iteration 86/234: loss=0.035861 lr=0.000020 grad_norm=0.560208
Epoch 86/100 Iteration 87/234: loss=0.035683 lr=0.000020 grad_norm=0.809163
Epoch 86/100 Iteration 88/234: loss=0.033527 lr=0.000020 grad_norm=0.730203
Epoch 86/100 Iteration 89/234: loss=0.035095 lr=0.000020 grad_norm=0.443795
Epoch 86/100 Iteration 90/234: loss=0.032439 lr=0.000020 grad_norm=0.486357
Epoch 86/100 Iteration 91/234: loss=0.036228 lr=0.000020 grad_norm=0.572259
Epoch 86/100 Iteration 92/234: loss=0.034308 lr=0.000020 grad_norm=0.637171
Epoch 86/100 Iteration 93/234: loss=0.033973 lr=0.000020 grad_norm=0.750794
Epoch 86/100 Iteration 94/234: loss=0.037973 lr=0.000020 grad_norm=0.436901
Epoch 86/100 Iteration 95/234: loss=0.035166 lr=0.000020 grad_norm=1.097537
Epoch 86/100 Iteration 96/234: loss=0.036468 lr=0.000020 grad_norm=1.117427
Epoch 86/100 Iteration 97/234: loss=0.036488 lr=0.000020 grad_norm=0.428796
Epoch 86/100 Iteration 98/234: loss=0.034925 lr=0.000020 grad_norm=0.910698
Epoch 86/100 Iteration 99/234: loss=0.033604 lr=0.000020 grad_norm=0.714984
Epoch 86/100 Iteration 100/234: loss=0.036385 lr=0.000020 grad_norm=1.010886
Epoch 86/100 Iteration 101/234: loss=0.037091 lr=0.000020 grad_norm=1.597499
Epoch 86/100 Iteration 102/234: loss=0.038119 lr=0.000020 grad_norm=1.029118
Epoch 86/100 Iteration 103/234: loss=0.036290 lr=0.000020 grad_norm=0.937057
Epoch 86/100 Iteration 104/234: loss=0.037181 lr=0.000020 grad_norm=1.471300
Epoch 86/100 Iteration 105/234: loss=0.035893 lr=0.000020 grad_norm=1.104370
Epoch 86/100 Iteration 106/234: loss=0.036798 lr=0.000020 grad_norm=0.683875
Epoch 86/100 Iteration 107/234: loss=0.038689 lr=0.000020 grad_norm=0.848627
Epoch 86/100 Iteration 108/234: loss=0.036825 lr=0.000020 grad_norm=0.822628
Epoch 86/100 Iteration 109/234: loss=0.030510 lr=0.000020 grad_norm=0.584038
Epoch 86/100 Iteration 110/234: loss=0.036442 lr=0.000020 grad_norm=1.007324
Epoch 86/100 Iteration 111/234: loss=0.036793 lr=0.000020 grad_norm=0.954022
Epoch 86/100 Iteration 112/234: loss=0.034609 lr=0.000020 grad_norm=0.646227
Epoch 86/100 Iteration 113/234: loss=0.035554 lr=0.000020 grad_norm=0.749518
Epoch 86/100 Iteration 114/234: loss=0.036301 lr=0.000020 grad_norm=0.686412
Epoch 86/100 Iteration 115/234: loss=0.038109 lr=0.000020 grad_norm=0.585175
Epoch 86/100 Iteration 116/234: loss=0.032841 lr=0.000020 grad_norm=0.568050
Epoch 86/100 Iteration 117/234: loss=0.032721 lr=0.000020 grad_norm=0.550395
Epoch 86/100 Iteration 118/234: loss=0.034716 lr=0.000020 grad_norm=0.467043
Epoch 86/100 Iteration 119/234: loss=0.036488 lr=0.000020 grad_norm=0.724467
Epoch 86/100 Iteration 120/234: loss=0.036040 lr=0.000020 grad_norm=0.641079
Epoch 86/100 Iteration 121/234: loss=0.037035 lr=0.000020 grad_norm=0.458705
Epoch 86/100 Iteration 122/234: loss=0.034611 lr=0.000020 grad_norm=0.810420
Epoch 86/100 Iteration 123/234: loss=0.032966 lr=0.000020 grad_norm=0.732641
Epoch 86/100 Iteration 124/234: loss=0.037663 lr=0.000020 grad_norm=0.493663
Epoch 86/100 Iteration 125/234: loss=0.033212 lr=0.000020 grad_norm=0.584657
Epoch 86/100 Iteration 126/234: loss=0.036411 lr=0.000020 grad_norm=0.458306
Epoch 86/100 Iteration 127/234: loss=0.035019 lr=0.000020 grad_norm=0.469569
Epoch 86/100 Iteration 128/234: loss=0.032315 lr=0.000020 grad_norm=0.629841
Epoch 86/100 Iteration 129/234: loss=0.031527 lr=0.000020 grad_norm=0.414012
Epoch 86/100 Iteration 130/234: loss=0.038304 lr=0.000020 grad_norm=0.401161
Epoch 86/100 Iteration 131/234: loss=0.034938 lr=0.000020 grad_norm=0.726499
Epoch 86/100 Iteration 132/234: loss=0.032999 lr=0.000020 grad_norm=0.790628
Epoch 86/100 Iteration 133/234: loss=0.038320 lr=0.000020 grad_norm=0.650264
Epoch 86/100 Iteration 134/234: loss=0.036898 lr=0.000020 grad_norm=0.872990
Epoch 86/100 Iteration 135/234: loss=0.034221 lr=0.000020 grad_norm=0.544825
Epoch 86/100 Iteration 136/234: loss=0.035283 lr=0.000020 grad_norm=0.592720
Epoch 86/100 Iteration 137/234: loss=0.036252 lr=0.000020 grad_norm=1.417390
Epoch 86/100 Iteration 138/234: loss=0.038894 lr=0.000020 grad_norm=1.769384
Epoch 86/100 Iteration 139/234: loss=0.032701 lr=0.000020 grad_norm=0.937432
Epoch 86/100 Iteration 140/234: loss=0.034452 lr=0.000020 grad_norm=0.530008
Epoch 86/100 Iteration 141/234: loss=0.033480 lr=0.000020 grad_norm=0.581521
Epoch 86/100 Iteration 142/234: loss=0.039950 lr=0.000020 grad_norm=0.684455
Epoch 86/100 Iteration 143/234: loss=0.033169 lr=0.000020 grad_norm=0.978083
Epoch 86/100 Iteration 144/234: loss=0.037417 lr=0.000020 grad_norm=0.456942
Epoch 86/100 Iteration 145/234: loss=0.034552 lr=0.000020 grad_norm=0.930656
Epoch 86/100 Iteration 146/234: loss=0.032061 lr=0.000020 grad_norm=0.771419
Epoch 86/100 Iteration 147/234: loss=0.036467 lr=0.000020 grad_norm=0.850412
Epoch 86/100 Iteration 148/234: loss=0.039668 lr=0.000020 grad_norm=1.266988
Epoch 86/100 Iteration 149/234: loss=0.040208 lr=0.000020 grad_norm=0.508410
Epoch 86/100 Iteration 150/234: loss=0.032618 lr=0.000020 grad_norm=0.753153
Epoch 86/100 Iteration 151/234: loss=0.034557 lr=0.000020 grad_norm=0.871626
Epoch 86/100 Iteration 152/234: loss=0.030861 lr=0.000020 grad_norm=0.613475
Epoch 86/100 Iteration 153/234: loss=0.036495 lr=0.000020 grad_norm=0.694485
Epoch 86/100 Iteration 154/234: loss=0.036515 lr=0.000020 grad_norm=0.967155
Epoch 86/100 Iteration 155/234: loss=0.033239 lr=0.000020 grad_norm=0.646740
Epoch 86/100 Iteration 156/234: loss=0.035169 lr=0.000020 grad_norm=0.814384
Epoch 86/100 Iteration 157/234: loss=0.036646 lr=0.000020 grad_norm=0.990456
Epoch 86/100 Iteration 158/234: loss=0.035582 lr=0.000020 grad_norm=0.714197
Epoch 86/100 Iteration 159/234: loss=0.037502 lr=0.000020 grad_norm=1.180372
Epoch 86/100 Iteration 160/234: loss=0.037600 lr=0.000020 grad_norm=1.602476
Epoch 86/100 Iteration 161/234: loss=0.035346 lr=0.000020 grad_norm=1.273020
Epoch 86/100 Iteration 162/234: loss=0.033922 lr=0.000020 grad_norm=0.780435
Epoch 86/100 Iteration 163/234: loss=0.028849 lr=0.000020 grad_norm=0.701839
Epoch 86/100 Iteration 164/234: loss=0.034502 lr=0.000020 grad_norm=0.794344
Epoch 86/100 Iteration 165/234: loss=0.036775 lr=0.000020 grad_norm=0.796120
Epoch 86/100 Iteration 166/234: loss=0.032084 lr=0.000020 grad_norm=0.443769
Epoch 86/100 Iteration 167/234: loss=0.031416 lr=0.000020 grad_norm=0.559850
Epoch 86/100 Iteration 168/234: loss=0.033475 lr=0.000020 grad_norm=0.665555
Epoch 86/100 Iteration 169/234: loss=0.037313 lr=0.000020 grad_norm=0.487719
Epoch 86/100 Iteration 170/234: loss=0.035706 lr=0.000020 grad_norm=0.542627
Epoch 86/100 Iteration 171/234: loss=0.032963 lr=0.000020 grad_norm=0.634028
Epoch 86/100 Iteration 172/234: loss=0.037618 lr=0.000020 grad_norm=0.415327
Epoch 86/100 Iteration 173/234: loss=0.034697 lr=0.000020 grad_norm=0.718652
Epoch 86/100 Iteration 174/234: loss=0.039295 lr=0.000020 grad_norm=1.348232
Epoch 86/100 Iteration 175/234: loss=0.039608 lr=0.000020 grad_norm=1.459967
Epoch 86/100 Iteration 176/234: loss=0.034541 lr=0.000020 grad_norm=0.656043
Epoch 86/100 Iteration 177/234: loss=0.036093 lr=0.000020 grad_norm=0.878996
Epoch 86/100 Iteration 178/234: loss=0.034831 lr=0.000020 grad_norm=1.092930
Epoch 86/100 Iteration 179/234: loss=0.036008 lr=0.000020 grad_norm=0.730607
Epoch 86/100 Iteration 180/234: loss=0.035209 lr=0.000020 grad_norm=0.737118
Epoch 86/100 Iteration 181/234: loss=0.038223 lr=0.000020 grad_norm=1.132676
Epoch 86/100 Iteration 182/234: loss=0.036453 lr=0.000020 grad_norm=0.909268
Epoch 86/100 Iteration 183/234: loss=0.037420 lr=0.000020 grad_norm=0.409865
Epoch 86/100 Iteration 184/234: loss=0.037493 lr=0.000020 grad_norm=0.845013
Epoch 86/100 Iteration 185/234: loss=0.037817 lr=0.000020 grad_norm=0.846823
Epoch 86/100 Iteration 186/234: loss=0.032202 lr=0.000020 grad_norm=0.604286
Epoch 86/100 Iteration 187/234: loss=0.032805 lr=0.000020 grad_norm=0.794418
Epoch 86/100 Iteration 188/234: loss=0.033378 lr=0.000020 grad_norm=0.504570
Epoch 86/100 Iteration 189/234: loss=0.034846 lr=0.000020 grad_norm=0.627398
Epoch 86/100 Iteration 190/234: loss=0.032101 lr=0.000020 grad_norm=0.710370
Epoch 86/100 Iteration 191/234: loss=0.035680 lr=0.000020 grad_norm=0.513123
Epoch 86/100 Iteration 192/234: loss=0.034549 lr=0.000020 grad_norm=0.581808
Epoch 86/100 Iteration 193/234: loss=0.034102 lr=0.000020 grad_norm=0.565643
Epoch 86/100 Iteration 194/234: loss=0.035575 lr=0.000020 grad_norm=0.747873
Epoch 86/100 Iteration 195/234: loss=0.031881 lr=0.000020 grad_norm=0.837068
Epoch 86/100 Iteration 196/234: loss=0.033442 lr=0.000020 grad_norm=0.479098
Epoch 86/100 Iteration 197/234: loss=0.036818 lr=0.000020 grad_norm=0.703442
Epoch 86/100 Iteration 198/234: loss=0.035323 lr=0.000020 grad_norm=0.849159
Epoch 86/100 Iteration 199/234: loss=0.035561 lr=0.000020 grad_norm=0.429972
Epoch 86/100 Iteration 200/234: loss=0.038520 lr=0.000020 grad_norm=0.836062
Epoch 86/100 Iteration 201/234: loss=0.035663 lr=0.000020 grad_norm=0.975818
Epoch 86/100 Iteration 202/234: loss=0.035989 lr=0.000020 grad_norm=0.575524
Epoch 86/100 Iteration 203/234: loss=0.038084 lr=0.000020 grad_norm=0.423947
Epoch 86/100 Iteration 204/234: loss=0.034744 lr=0.000020 grad_norm=0.395343
Epoch 86/100 Iteration 205/234: loss=0.034487 lr=0.000020 grad_norm=0.538306
Epoch 86/100 Iteration 206/234: loss=0.036948 lr=0.000020 grad_norm=0.297764
Epoch 86/100 Iteration 207/234: loss=0.034659 lr=0.000020 grad_norm=0.446611
Epoch 86/100 Iteration 208/234: loss=0.034544 lr=0.000020 grad_norm=0.616573
Epoch 86/100 Iteration 209/234: loss=0.038395 lr=0.000020 grad_norm=0.562998
Epoch 86/100 Iteration 210/234: loss=0.032848 lr=0.000020 grad_norm=0.378725
Epoch 86/100 Iteration 211/234: loss=0.037468 lr=0.000020 grad_norm=0.512848
Epoch 86/100 Iteration 212/234: loss=0.034913 lr=0.000020 grad_norm=0.334840
Epoch 86/100 Iteration 213/234: loss=0.035892 lr=0.000020 grad_norm=0.609677
Epoch 86/100 Iteration 214/234: loss=0.036518 lr=0.000020 grad_norm=0.583304
Epoch 86/100 Iteration 215/234: loss=0.031593 lr=0.000020 grad_norm=0.652961
Epoch 86/100 Iteration 216/234: loss=0.035505 lr=0.000020 grad_norm=0.612751
Epoch 86/100 Iteration 217/234: loss=0.034493 lr=0.000020 grad_norm=0.718282
Epoch 86/100 Iteration 218/234: loss=0.034305 lr=0.000020 grad_norm=1.162134
Epoch 86/100 Iteration 219/234: loss=0.031935 lr=0.000020 grad_norm=0.992294
Epoch 86/100 Iteration 220/234: loss=0.037670 lr=0.000020 grad_norm=0.563297
Epoch 86/100 Iteration 221/234: loss=0.036496 lr=0.000020 grad_norm=0.860356
Epoch 86/100 Iteration 222/234: loss=0.038789 lr=0.000020 grad_norm=1.057699
Epoch 86/100 Iteration 223/234: loss=0.035434 lr=0.000020 grad_norm=0.900250
Epoch 86/100 Iteration 224/234: loss=0.037922 lr=0.000020 grad_norm=0.944334
Epoch 86/100 Iteration 225/234: loss=0.035603 lr=0.000020 grad_norm=1.385017
Epoch 86/100 Iteration 226/234: loss=0.038164 lr=0.000020 grad_norm=1.304638
Epoch 86/100 Iteration 227/234: loss=0.035786 lr=0.000020 grad_norm=0.577194
Epoch 86/100 Iteration 228/234: loss=0.032099 lr=0.000020 grad_norm=0.821546
Epoch 86/100 Iteration 229/234: loss=0.035406 lr=0.000020 grad_norm=0.705255
Epoch 86/100 Iteration 230/234: loss=0.035124 lr=0.000020 grad_norm=0.955524
Epoch 86/100 Iteration 231/234: loss=0.033748 lr=0.000020 grad_norm=0.904094
Epoch 86/100 Iteration 232/234: loss=0.037767 lr=0.000020 grad_norm=0.777590
Epoch 86/100 Iteration 233/234: loss=0.033375 lr=0.000020 grad_norm=0.970307
Epoch 86/100 Iteration 234/234: loss=0.034819 lr=0.000020 grad_norm=0.918124
Epoch 86/100 finished. Avg Loss: 0.035482
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 87/100 Iteration 1/234: loss=0.031391 lr=0.000020 grad_norm=0.744242
Epoch 87/100 Iteration 2/234: loss=0.035762 lr=0.000020 grad_norm=0.526445
Epoch 87/100 Iteration 3/234: loss=0.037070 lr=0.000020 grad_norm=0.765088
Epoch 87/100 Iteration 4/234: loss=0.036291 lr=0.000020 grad_norm=0.741688
Epoch 87/100 Iteration 5/234: loss=0.037448 lr=0.000020 grad_norm=0.418968
Epoch 87/100 Iteration 6/234: loss=0.035167 lr=0.000020 grad_norm=0.749453
Epoch 87/100 Iteration 7/234: loss=0.034324 lr=0.000020 grad_norm=0.644662
Epoch 87/100 Iteration 8/234: loss=0.038102 lr=0.000020 grad_norm=0.895665
Epoch 87/100 Iteration 9/234: loss=0.035606 lr=0.000020 grad_norm=1.019673
Epoch 87/100 Iteration 10/234: loss=0.036814 lr=0.000020 grad_norm=0.639799
Epoch 87/100 Iteration 11/234: loss=0.032252 lr=0.000020 grad_norm=0.828822
Epoch 87/100 Iteration 12/234: loss=0.032077 lr=0.000020 grad_norm=0.862063
Epoch 87/100 Iteration 13/234: loss=0.036888 lr=0.000020 grad_norm=0.826968
Epoch 87/100 Iteration 14/234: loss=0.031522 lr=0.000020 grad_norm=1.029334
Epoch 87/100 Iteration 15/234: loss=0.034219 lr=0.000020 grad_norm=0.550464
Epoch 87/100 Iteration 16/234: loss=0.032431 lr=0.000020 grad_norm=0.837927
Epoch 87/100 Iteration 17/234: loss=0.036450 lr=0.000020 grad_norm=1.126357
Epoch 87/100 Iteration 18/234: loss=0.030954 lr=0.000020 grad_norm=0.521287
Epoch 87/100 Iteration 19/234: loss=0.036738 lr=0.000020 grad_norm=0.837408
Epoch 87/100 Iteration 20/234: loss=0.033891 lr=0.000020 grad_norm=0.559123
Epoch 87/100 Iteration 21/234: loss=0.038576 lr=0.000020 grad_norm=1.032853
Epoch 87/100 Iteration 22/234: loss=0.036410 lr=0.000020 grad_norm=1.563880
Epoch 87/100 Iteration 23/234: loss=0.036215 lr=0.000020 grad_norm=0.746429
Epoch 87/100 Iteration 24/234: loss=0.034389 lr=0.000020 grad_norm=1.095340
Epoch 87/100 Iteration 25/234: loss=0.036762 lr=0.000020 grad_norm=1.554621
Epoch 87/100 Iteration 26/234: loss=0.034470 lr=0.000020 grad_norm=1.157868
Epoch 87/100 Iteration 27/234: loss=0.037716 lr=0.000020 grad_norm=0.769378
Epoch 87/100 Iteration 28/234: loss=0.034002 lr=0.000020 grad_norm=0.941684
Epoch 87/100 Iteration 29/234: loss=0.036068 lr=0.000020 grad_norm=0.874622
Epoch 87/100 Iteration 30/234: loss=0.033675 lr=0.000020 grad_norm=0.752345
Epoch 87/100 Iteration 31/234: loss=0.037600 lr=0.000020 grad_norm=0.625759
Epoch 87/100 Iteration 32/234: loss=0.036683 lr=0.000020 grad_norm=0.797218
Epoch 87/100 Iteration 33/234: loss=0.034863 lr=0.000020 grad_norm=1.345771
Epoch 87/100 Iteration 34/234: loss=0.037073 lr=0.000020 grad_norm=1.067551
Epoch 87/100 Iteration 35/234: loss=0.033588 lr=0.000020 grad_norm=0.501615
Epoch 87/100 Iteration 36/234: loss=0.036827 lr=0.000020 grad_norm=1.388581
Epoch 87/100 Iteration 37/234: loss=0.037652 lr=0.000020 grad_norm=1.074103
Epoch 87/100 Iteration 38/234: loss=0.041044 lr=0.000020 grad_norm=0.756199
Epoch 87/100 Iteration 39/234: loss=0.031107 lr=0.000020 grad_norm=0.896470
Epoch 87/100 Iteration 40/234: loss=0.034356 lr=0.000020 grad_norm=0.778131
Epoch 87/100 Iteration 41/234: loss=0.036186 lr=0.000020 grad_norm=0.861668
Epoch 87/100 Iteration 42/234: loss=0.040063 lr=0.000020 grad_norm=0.887486
Epoch 87/100 Iteration 43/234: loss=0.033432 lr=0.000020 grad_norm=0.869757
Epoch 87/100 Iteration 44/234: loss=0.034531 lr=0.000020 grad_norm=0.949193
Epoch 87/100 Iteration 45/234: loss=0.035241 lr=0.000020 grad_norm=1.617308
Epoch 87/100 Iteration 46/234: loss=0.036753 lr=0.000020 grad_norm=0.980336
Epoch 87/100 Iteration 47/234: loss=0.036171 lr=0.000020 grad_norm=1.066546
Epoch 87/100 Iteration 48/234: loss=0.034472 lr=0.000020 grad_norm=1.278533
Epoch 87/100 Iteration 49/234: loss=0.034434 lr=0.000020 grad_norm=1.023406
Epoch 87/100 Iteration 50/234: loss=0.037976 lr=0.000020 grad_norm=1.131144
Epoch 87/100 Iteration 51/234: loss=0.033217 lr=0.000020 grad_norm=0.869023
Epoch 87/100 Iteration 52/234: loss=0.037515 lr=0.000020 grad_norm=0.810234
Epoch 87/100 Iteration 53/234: loss=0.032791 lr=0.000020 grad_norm=0.785964
Epoch 87/100 Iteration 54/234: loss=0.038695 lr=0.000020 grad_norm=0.794452
Epoch 87/100 Iteration 55/234: loss=0.035866 lr=0.000020 grad_norm=0.710543
Epoch 87/100 Iteration 56/234: loss=0.032348 lr=0.000020 grad_norm=0.636246
Epoch 87/100 Iteration 57/234: loss=0.033787 lr=0.000020 grad_norm=0.620416
Epoch 87/100 Iteration 58/234: loss=0.033672 lr=0.000020 grad_norm=0.489214
Epoch 87/100 Iteration 59/234: loss=0.036327 lr=0.000020 grad_norm=0.671917
Epoch 87/100 Iteration 60/234: loss=0.035566 lr=0.000020 grad_norm=0.899649
Epoch 87/100 Iteration 61/234: loss=0.035092 lr=0.000020 grad_norm=0.867015
Epoch 87/100 Iteration 62/234: loss=0.034617 lr=0.000020 grad_norm=0.689370
Epoch 87/100 Iteration 63/234: loss=0.036078 lr=0.000020 grad_norm=0.780928
Epoch 87/100 Iteration 64/234: loss=0.035119 lr=0.000020 grad_norm=1.310388
Epoch 87/100 Iteration 65/234: loss=0.035556 lr=0.000020 grad_norm=0.911840
Epoch 87/100 Iteration 66/234: loss=0.032670 lr=0.000020 grad_norm=0.728553
Epoch 87/100 Iteration 67/234: loss=0.031683 lr=0.000020 grad_norm=0.707473
Epoch 87/100 Iteration 68/234: loss=0.036701 lr=0.000020 grad_norm=0.835536
Epoch 87/100 Iteration 69/234: loss=0.035502 lr=0.000020 grad_norm=1.138919
Epoch 87/100 Iteration 70/234: loss=0.033475 lr=0.000020 grad_norm=0.725656
Epoch 87/100 Iteration 71/234: loss=0.035549 lr=0.000020 grad_norm=0.928714
Epoch 87/100 Iteration 72/234: loss=0.036809 lr=0.000020 grad_norm=1.146845
Epoch 87/100 Iteration 73/234: loss=0.035434 lr=0.000020 grad_norm=0.789642
Epoch 87/100 Iteration 74/234: loss=0.040384 lr=0.000020 grad_norm=0.639637
Epoch 87/100 Iteration 75/234: loss=0.032352 lr=0.000020 grad_norm=1.086114
Epoch 87/100 Iteration 76/234: loss=0.036034 lr=0.000020 grad_norm=0.634846
Epoch 87/100 Iteration 77/234: loss=0.036726 lr=0.000020 grad_norm=0.658762
Epoch 87/100 Iteration 78/234: loss=0.036065 lr=0.000020 grad_norm=0.795478
Epoch 87/100 Iteration 79/234: loss=0.036106 lr=0.000020 grad_norm=0.484208
Epoch 87/100 Iteration 80/234: loss=0.033113 lr=0.000020 grad_norm=0.664191
Epoch 87/100 Iteration 81/234: loss=0.034140 lr=0.000020 grad_norm=0.742242
Epoch 87/100 Iteration 82/234: loss=0.033127 lr=0.000020 grad_norm=0.469522
Epoch 87/100 Iteration 83/234: loss=0.036029 lr=0.000020 grad_norm=0.865579
Epoch 87/100 Iteration 84/234: loss=0.036599 lr=0.000020 grad_norm=0.619914
Epoch 87/100 Iteration 85/234: loss=0.031856 lr=0.000020 grad_norm=0.812929
Epoch 87/100 Iteration 86/234: loss=0.035015 lr=0.000020 grad_norm=0.795898
Epoch 87/100 Iteration 87/234: loss=0.038050 lr=0.000020 grad_norm=0.450145
Epoch 87/100 Iteration 88/234: loss=0.034062 lr=0.000020 grad_norm=0.883976
Epoch 87/100 Iteration 89/234: loss=0.035280 lr=0.000020 grad_norm=0.910702
Epoch 87/100 Iteration 90/234: loss=0.037642 lr=0.000020 grad_norm=0.489967
Epoch 87/100 Iteration 91/234: loss=0.034510 lr=0.000020 grad_norm=0.951034
Epoch 87/100 Iteration 92/234: loss=0.036645 lr=0.000020 grad_norm=0.820227
Epoch 87/100 Iteration 93/234: loss=0.040735 lr=0.000020 grad_norm=0.590420
Epoch 87/100 Iteration 94/234: loss=0.034896 lr=0.000020 grad_norm=1.149558
Epoch 87/100 Iteration 95/234: loss=0.034558 lr=0.000020 grad_norm=1.302842
Epoch 87/100 Iteration 96/234: loss=0.038271 lr=0.000020 grad_norm=0.601493
Epoch 87/100 Iteration 97/234: loss=0.038193 lr=0.000020 grad_norm=1.341206
Epoch 87/100 Iteration 98/234: loss=0.034472 lr=0.000020 grad_norm=1.522975
Epoch 87/100 Iteration 99/234: loss=0.032941 lr=0.000020 grad_norm=0.601216
Epoch 87/100 Iteration 100/234: loss=0.038738 lr=0.000020 grad_norm=1.762858
Epoch 87/100 Iteration 101/234: loss=0.035916 lr=0.000020 grad_norm=1.132133
Epoch 87/100 Iteration 102/234: loss=0.037685 lr=0.000020 grad_norm=1.095277
Epoch 87/100 Iteration 103/234: loss=0.035973 lr=0.000020 grad_norm=1.412874
Epoch 87/100 Iteration 104/234: loss=0.032772 lr=0.000020 grad_norm=0.576608
Epoch 87/100 Iteration 105/234: loss=0.037637 lr=0.000020 grad_norm=1.464454
Epoch 87/100 Iteration 106/234: loss=0.035591 lr=0.000020 grad_norm=0.846381
Epoch 87/100 Iteration 107/234: loss=0.033962 lr=0.000020 grad_norm=1.226031
Epoch 87/100 Iteration 108/234: loss=0.036088 lr=0.000020 grad_norm=1.079536
Epoch 87/100 Iteration 109/234: loss=0.034592 lr=0.000020 grad_norm=0.863064
Epoch 87/100 Iteration 110/234: loss=0.036629 lr=0.000020 grad_norm=1.197907
Epoch 87/100 Iteration 111/234: loss=0.033067 lr=0.000020 grad_norm=0.625381
Epoch 87/100 Iteration 112/234: loss=0.035938 lr=0.000020 grad_norm=0.884164
Epoch 87/100 Iteration 113/234: loss=0.036296 lr=0.000020 grad_norm=1.045181
Epoch 87/100 Iteration 114/234: loss=0.034874 lr=0.000020 grad_norm=0.783235
Epoch 87/100 Iteration 115/234: loss=0.035928 lr=0.000020 grad_norm=0.857061
Epoch 87/100 Iteration 116/234: loss=0.033910 lr=0.000020 grad_norm=0.846712
Epoch 87/100 Iteration 117/234: loss=0.032213 lr=0.000020 grad_norm=0.615470
Epoch 87/100 Iteration 118/234: loss=0.035685 lr=0.000020 grad_norm=0.801789
Epoch 87/100 Iteration 119/234: loss=0.034378 lr=0.000020 grad_norm=0.495411
Epoch 87/100 Iteration 120/234: loss=0.033873 lr=0.000020 grad_norm=0.807246
Epoch 87/100 Iteration 121/234: loss=0.034857 lr=0.000020 grad_norm=0.446586
Epoch 87/100 Iteration 122/234: loss=0.033569 lr=0.000020 grad_norm=0.422962
Epoch 87/100 Iteration 123/234: loss=0.036198 lr=0.000020 grad_norm=0.553402
Epoch 87/100 Iteration 124/234: loss=0.030796 lr=0.000020 grad_norm=0.491884
Epoch 87/100 Iteration 125/234: loss=0.030409 lr=0.000020 grad_norm=0.379720
Epoch 87/100 Iteration 126/234: loss=0.033114 lr=0.000020 grad_norm=0.525313
Epoch 87/100 Iteration 127/234: loss=0.032915 lr=0.000020 grad_norm=0.643234
Epoch 87/100 Iteration 128/234: loss=0.038116 lr=0.000020 grad_norm=0.693788
Epoch 87/100 Iteration 129/234: loss=0.034278 lr=0.000020 grad_norm=0.411542
Epoch 87/100 Iteration 130/234: loss=0.035033 lr=0.000020 grad_norm=0.727433
Epoch 87/100 Iteration 131/234: loss=0.033928 lr=0.000020 grad_norm=0.706945
Epoch 87/100 Iteration 132/234: loss=0.034929 lr=0.000020 grad_norm=0.659952
Epoch 87/100 Iteration 133/234: loss=0.038596 lr=0.000020 grad_norm=1.054881
Epoch 87/100 Iteration 134/234: loss=0.033328 lr=0.000020 grad_norm=0.752728
Epoch 87/100 Iteration 135/234: loss=0.035626 lr=0.000020 grad_norm=0.753502
Epoch 87/100 Iteration 136/234: loss=0.035176 lr=0.000020 grad_norm=0.491095
Epoch 87/100 Iteration 137/234: loss=0.033715 lr=0.000020 grad_norm=0.690071
Epoch 87/100 Iteration 138/234: loss=0.041257 lr=0.000020 grad_norm=1.122761
Epoch 87/100 Iteration 139/234: loss=0.039101 lr=0.000020 grad_norm=0.580709
Epoch 87/100 Iteration 140/234: loss=0.033115 lr=0.000020 grad_norm=0.821210
Epoch 87/100 Iteration 141/234: loss=0.036713 lr=0.000020 grad_norm=0.637892
Epoch 87/100 Iteration 142/234: loss=0.034349 lr=0.000020 grad_norm=0.647487
Epoch 87/100 Iteration 143/234: loss=0.033063 lr=0.000020 grad_norm=0.759210
Epoch 87/100 Iteration 144/234: loss=0.033550 lr=0.000020 grad_norm=0.469904
Epoch 87/100 Iteration 145/234: loss=0.034647 lr=0.000020 grad_norm=0.575218
Epoch 87/100 Iteration 146/234: loss=0.033891 lr=0.000020 grad_norm=0.537912
Epoch 87/100 Iteration 147/234: loss=0.037196 lr=0.000020 grad_norm=1.074659
Epoch 87/100 Iteration 148/234: loss=0.036491 lr=0.000020 grad_norm=0.824644
Epoch 87/100 Iteration 149/234: loss=0.032501 lr=0.000020 grad_norm=0.533136
Epoch 87/100 Iteration 150/234: loss=0.035344 lr=0.000020 grad_norm=0.712795
Epoch 87/100 Iteration 151/234: loss=0.032585 lr=0.000020 grad_norm=0.405713
Epoch 87/100 Iteration 152/234: loss=0.036249 lr=0.000020 grad_norm=0.541107
Epoch 87/100 Iteration 153/234: loss=0.031682 lr=0.000020 grad_norm=0.341610
Epoch 87/100 Iteration 154/234: loss=0.039735 lr=0.000020 grad_norm=0.795895
Epoch 87/100 Iteration 155/234: loss=0.033145 lr=0.000020 grad_norm=0.883986
Epoch 87/100 Iteration 156/234: loss=0.033932 lr=0.000020 grad_norm=0.501159
Epoch 87/100 Iteration 157/234: loss=0.037114 lr=0.000020 grad_norm=1.068586
Epoch 87/100 Iteration 158/234: loss=0.035827 lr=0.000020 grad_norm=1.060864
Epoch 87/100 Iteration 159/234: loss=0.033263 lr=0.000020 grad_norm=0.822644
Epoch 87/100 Iteration 160/234: loss=0.031411 lr=0.000020 grad_norm=0.613500
Epoch 87/100 Iteration 161/234: loss=0.035340 lr=0.000020 grad_norm=0.664836
Epoch 87/100 Iteration 162/234: loss=0.034486 lr=0.000020 grad_norm=0.463800
Epoch 87/100 Iteration 163/234: loss=0.030001 lr=0.000020 grad_norm=0.433429
Epoch 87/100 Iteration 164/234: loss=0.038211 lr=0.000020 grad_norm=0.483421
Epoch 87/100 Iteration 165/234: loss=0.031208 lr=0.000020 grad_norm=0.455176
Epoch 87/100 Iteration 166/234: loss=0.039225 lr=0.000020 grad_norm=0.633662
Epoch 87/100 Iteration 167/234: loss=0.034003 lr=0.000020 grad_norm=0.355036
Epoch 87/100 Iteration 168/234: loss=0.030732 lr=0.000020 grad_norm=0.360396
Epoch 87/100 Iteration 169/234: loss=0.036722 lr=0.000020 grad_norm=0.378914
Epoch 87/100 Iteration 170/234: loss=0.034528 lr=0.000020 grad_norm=0.423865
Epoch 87/100 Iteration 171/234: loss=0.031345 lr=0.000020 grad_norm=0.563004
Epoch 87/100 Iteration 172/234: loss=0.034543 lr=0.000020 grad_norm=0.741031
Epoch 87/100 Iteration 173/234: loss=0.035193 lr=0.000020 grad_norm=0.822637
Epoch 87/100 Iteration 174/234: loss=0.034636 lr=0.000020 grad_norm=0.357733
Epoch 87/100 Iteration 175/234: loss=0.037772 lr=0.000020 grad_norm=0.662196
Epoch 87/100 Iteration 176/234: loss=0.035200 lr=0.000020 grad_norm=0.684760
Epoch 87/100 Iteration 177/234: loss=0.036057 lr=0.000020 grad_norm=0.462913
Epoch 87/100 Iteration 178/234: loss=0.033669 lr=0.000020 grad_norm=0.519472
Epoch 87/100 Iteration 179/234: loss=0.033657 lr=0.000020 grad_norm=0.495276
Epoch 87/100 Iteration 180/234: loss=0.035031 lr=0.000020 grad_norm=0.327486
Epoch 87/100 Iteration 181/234: loss=0.034879 lr=0.000020 grad_norm=0.589729
Epoch 87/100 Iteration 182/234: loss=0.032495 lr=0.000020 grad_norm=0.569811
Epoch 87/100 Iteration 183/234: loss=0.034234 lr=0.000020 grad_norm=0.540173
Epoch 87/100 Iteration 184/234: loss=0.029838 lr=0.000020 grad_norm=0.740450
Epoch 87/100 Iteration 185/234: loss=0.036236 lr=0.000020 grad_norm=0.448396
Epoch 87/100 Iteration 186/234: loss=0.035219 lr=0.000020 grad_norm=1.144655
Epoch 87/100 Iteration 187/234: loss=0.036003 lr=0.000020 grad_norm=1.000680
Epoch 87/100 Iteration 188/234: loss=0.034724 lr=0.000020 grad_norm=0.421166
Epoch 87/100 Iteration 189/234: loss=0.035995 lr=0.000020 grad_norm=0.888497
Epoch 87/100 Iteration 190/234: loss=0.034940 lr=0.000020 grad_norm=0.634492
Epoch 87/100 Iteration 191/234: loss=0.034759 lr=0.000020 grad_norm=0.533305
Epoch 87/100 Iteration 192/234: loss=0.035898 lr=0.000020 grad_norm=0.848313
Epoch 87/100 Iteration 193/234: loss=0.033806 lr=0.000020 grad_norm=0.558714
Epoch 87/100 Iteration 194/234: loss=0.036758 lr=0.000020 grad_norm=0.670928
Epoch 87/100 Iteration 195/234: loss=0.035250 lr=0.000020 grad_norm=0.905684
Epoch 87/100 Iteration 196/234: loss=0.035922 lr=0.000020 grad_norm=0.705368
Epoch 87/100 Iteration 197/234: loss=0.031868 lr=0.000020 grad_norm=0.980996
Epoch 87/100 Iteration 198/234: loss=0.037441 lr=0.000020 grad_norm=0.624841
Epoch 87/100 Iteration 199/234: loss=0.033240 lr=0.000020 grad_norm=0.917295
Epoch 87/100 Iteration 200/234: loss=0.033680 lr=0.000020 grad_norm=1.146206
Epoch 87/100 Iteration 201/234: loss=0.036525 lr=0.000020 grad_norm=0.419851
Epoch 87/100 Iteration 202/234: loss=0.032839 lr=0.000020 grad_norm=1.129722
Epoch 87/100 Iteration 203/234: loss=0.030959 lr=0.000020 grad_norm=0.459467
Epoch 87/100 Iteration 204/234: loss=0.035622 lr=0.000020 grad_norm=1.075784
Epoch 87/100 Iteration 205/234: loss=0.034900 lr=0.000020 grad_norm=1.695890
Epoch 87/100 Iteration 206/234: loss=0.036804 lr=0.000020 grad_norm=1.412150
Epoch 87/100 Iteration 207/234: loss=0.035262 lr=0.000020 grad_norm=0.611403
Epoch 87/100 Iteration 208/234: loss=0.035108 lr=0.000020 grad_norm=0.669580
Epoch 87/100 Iteration 209/234: loss=0.033996 lr=0.000020 grad_norm=0.688212
Epoch 87/100 Iteration 210/234: loss=0.037042 lr=0.000020 grad_norm=0.635119
Epoch 87/100 Iteration 211/234: loss=0.034929 lr=0.000020 grad_norm=0.703629
Epoch 87/100 Iteration 212/234: loss=0.032478 lr=0.000020 grad_norm=0.657120
Epoch 87/100 Iteration 213/234: loss=0.035228 lr=0.000020 grad_norm=0.988544
Epoch 87/100 Iteration 214/234: loss=0.038741 lr=0.000020 grad_norm=1.819071
Epoch 87/100 Iteration 215/234: loss=0.034801 lr=0.000020 grad_norm=1.331350
Epoch 87/100 Iteration 216/234: loss=0.036144 lr=0.000020 grad_norm=0.796070
Epoch 87/100 Iteration 217/234: loss=0.035523 lr=0.000020 grad_norm=1.292180
Epoch 87/100 Iteration 218/234: loss=0.035469 lr=0.000020 grad_norm=1.410324
Epoch 87/100 Iteration 219/234: loss=0.033979 lr=0.000020 grad_norm=0.900289
Epoch 87/100 Iteration 220/234: loss=0.035120 lr=0.000020 grad_norm=0.743529
Epoch 87/100 Iteration 221/234: loss=0.035690 lr=0.000020 grad_norm=1.065289
Epoch 87/100 Iteration 222/234: loss=0.036719 lr=0.000020 grad_norm=0.655576
Epoch 87/100 Iteration 223/234: loss=0.035809 lr=0.000020 grad_norm=0.696712
Epoch 87/100 Iteration 224/234: loss=0.034744 lr=0.000020 grad_norm=1.009672
Epoch 87/100 Iteration 225/234: loss=0.031976 lr=0.000020 grad_norm=0.548980
Epoch 87/100 Iteration 226/234: loss=0.032479 lr=0.000020 grad_norm=0.675475
Epoch 87/100 Iteration 227/234: loss=0.033766 lr=0.000020 grad_norm=0.914193
Epoch 87/100 Iteration 228/234: loss=0.036872 lr=0.000020 grad_norm=0.531789
Epoch 87/100 Iteration 229/234: loss=0.034790 lr=0.000020 grad_norm=1.047815
Epoch 87/100 Iteration 230/234: loss=0.038564 lr=0.000020 grad_norm=1.552475
Epoch 87/100 Iteration 231/234: loss=0.035163 lr=0.000020 grad_norm=1.157545
Epoch 87/100 Iteration 232/234: loss=0.034460 lr=0.000020 grad_norm=0.412887
Epoch 87/100 Iteration 233/234: loss=0.038110 lr=0.000020 grad_norm=0.951176
Epoch 87/100 Iteration 234/234: loss=0.033426 lr=0.000020 grad_norm=0.867433
Epoch 87/100 finished. Avg Loss: 0.035115
Epoch 88/100 Iteration 1/234: loss=0.033936 lr=0.000020 grad_norm=0.698619
Epoch 88/100 Iteration 2/234: loss=0.031289 lr=0.000020 grad_norm=1.261253
Epoch 88/100 Iteration 3/234: loss=0.038506 lr=0.000020 grad_norm=0.481322
Epoch 88/100 Iteration 4/234: loss=0.037271 lr=0.000020 grad_norm=1.844027
Epoch 88/100 Iteration 5/234: loss=0.032200 lr=0.000020 grad_norm=1.262556
Epoch 88/100 Iteration 6/234: loss=0.035510 lr=0.000020 grad_norm=1.602238
Epoch 88/100 Iteration 7/234: loss=0.032228 lr=0.000020 grad_norm=1.966310
Epoch 88/100 Iteration 8/234: loss=0.035399 lr=0.000020 grad_norm=1.112766
Epoch 88/100 Iteration 9/234: loss=0.033892 lr=0.000020 grad_norm=2.326534
Epoch 88/100 Iteration 10/234: loss=0.034118 lr=0.000020 grad_norm=0.773482
Epoch 88/100 Iteration 11/234: loss=0.034382 lr=0.000020 grad_norm=1.711542
Epoch 88/100 Iteration 12/234: loss=0.034074 lr=0.000020 grad_norm=0.932034
Epoch 88/100 Iteration 13/234: loss=0.033092 lr=0.000020 grad_norm=1.475508
Epoch 88/100 Iteration 14/234: loss=0.032666 lr=0.000020 grad_norm=0.674156
Epoch 88/100 Iteration 15/234: loss=0.034960 lr=0.000020 grad_norm=1.279566
Epoch 88/100 Iteration 16/234: loss=0.039193 lr=0.000020 grad_norm=0.793392
Epoch 88/100 Iteration 17/234: loss=0.035664 lr=0.000020 grad_norm=1.291293
Epoch 88/100 Iteration 18/234: loss=0.029783 lr=0.000020 grad_norm=0.544924
Epoch 88/100 Iteration 19/234: loss=0.033235 lr=0.000020 grad_norm=0.919924
Epoch 88/100 Iteration 20/234: loss=0.033703 lr=0.000020 grad_norm=0.798090
Epoch 88/100 Iteration 21/234: loss=0.034977 lr=0.000020 grad_norm=0.652185
Epoch 88/100 Iteration 22/234: loss=0.035658 lr=0.000020 grad_norm=0.983636
Epoch 88/100 Iteration 23/234: loss=0.035745 lr=0.000020 grad_norm=1.149164
Epoch 88/100 Iteration 24/234: loss=0.036184 lr=0.000020 grad_norm=0.697437
Epoch 88/100 Iteration 25/234: loss=0.039079 lr=0.000020 grad_norm=1.004691
Epoch 88/100 Iteration 26/234: loss=0.032594 lr=0.000020 grad_norm=0.641733
Epoch 88/100 Iteration 27/234: loss=0.033672 lr=0.000020 grad_norm=0.956375
Epoch 88/100 Iteration 28/234: loss=0.037689 lr=0.000020 grad_norm=0.919986
Epoch 88/100 Iteration 29/234: loss=0.037239 lr=0.000020 grad_norm=0.865569
Epoch 88/100 Iteration 30/234: loss=0.037571 lr=0.000020 grad_norm=1.000176
Epoch 88/100 Iteration 31/234: loss=0.037980 lr=0.000020 grad_norm=1.681164
Epoch 88/100 Iteration 32/234: loss=0.031292 lr=0.000020 grad_norm=0.721284
Epoch 88/100 Iteration 33/234: loss=0.039855 lr=0.000020 grad_norm=1.334510
Epoch 88/100 Iteration 34/234: loss=0.034606 lr=0.000020 grad_norm=1.252947
Epoch 88/100 Iteration 35/234: loss=0.035716 lr=0.000020 grad_norm=1.003962
Epoch 88/100 Iteration 36/234: loss=0.034409 lr=0.000020 grad_norm=1.021213
Epoch 88/100 Iteration 37/234: loss=0.037650 lr=0.000020 grad_norm=1.021992
Epoch 88/100 Iteration 38/234: loss=0.038344 lr=0.000020 grad_norm=1.402631
Epoch 88/100 Iteration 39/234: loss=0.037053 lr=0.000020 grad_norm=0.582474
Epoch 88/100 Iteration 40/234: loss=0.034075 lr=0.000020 grad_norm=0.933001
Epoch 88/100 Iteration 41/234: loss=0.037864 lr=0.000020 grad_norm=0.637654
Epoch 88/100 Iteration 42/234: loss=0.041808 lr=0.000020 grad_norm=1.289407
Epoch 88/100 Iteration 43/234: loss=0.038446 lr=0.000020 grad_norm=0.811518
Epoch 88/100 Iteration 44/234: loss=0.034972 lr=0.000020 grad_norm=0.895889
Epoch 88/100 Iteration 45/234: loss=0.032677 lr=0.000020 grad_norm=0.511152
Epoch 88/100 Iteration 46/234: loss=0.037014 lr=0.000020 grad_norm=0.855399
Epoch 88/100 Iteration 47/234: loss=0.031541 lr=0.000020 grad_norm=0.549317
Epoch 88/100 Iteration 48/234: loss=0.034266 lr=0.000020 grad_norm=0.745007
Epoch 88/100 Iteration 49/234: loss=0.039281 lr=0.000020 grad_norm=0.815859
Epoch 88/100 Iteration 50/234: loss=0.035906 lr=0.000020 grad_norm=0.828974
Epoch 88/100 Iteration 51/234: loss=0.032617 lr=0.000020 grad_norm=0.626798
Epoch 88/100 Iteration 52/234: loss=0.031933 lr=0.000020 grad_norm=0.852295
Epoch 88/100 Iteration 53/234: loss=0.033293 lr=0.000020 grad_norm=0.739044
Epoch 88/100 Iteration 54/234: loss=0.033744 lr=0.000020 grad_norm=0.897781
Epoch 88/100 Iteration 55/234: loss=0.036521 lr=0.000020 grad_norm=1.208950
Epoch 88/100 Iteration 56/234: loss=0.034444 lr=0.000020 grad_norm=0.685851
Epoch 88/100 Iteration 57/234: loss=0.035830 lr=0.000020 grad_norm=1.021816
Epoch 88/100 Iteration 58/234: loss=0.033591 lr=0.000020 grad_norm=0.639898
Epoch 88/100 Iteration 59/234: loss=0.034632 lr=0.000020 grad_norm=0.848910
Epoch 88/100 Iteration 60/234: loss=0.035768 lr=0.000020 grad_norm=0.925008
Epoch 88/100 Iteration 61/234: loss=0.035552 lr=0.000020 grad_norm=0.710412
Epoch 88/100 Iteration 62/234: loss=0.036606 lr=0.000020 grad_norm=0.826358
Epoch 88/100 Iteration 63/234: loss=0.034571 lr=0.000020 grad_norm=1.027223
Epoch 88/100 Iteration 64/234: loss=0.033057 lr=0.000020 grad_norm=0.560641
Epoch 88/100 Iteration 65/234: loss=0.036068 lr=0.000020 grad_norm=0.766003
Epoch 88/100 Iteration 66/234: loss=0.037934 lr=0.000020 grad_norm=1.173240
Epoch 88/100 Iteration 67/234: loss=0.037477 lr=0.000020 grad_norm=0.887742
Epoch 88/100 Iteration 68/234: loss=0.034144 lr=0.000020 grad_norm=0.668230
Epoch 88/100 Iteration 69/234: loss=0.029178 lr=0.000020 grad_norm=0.856650
Epoch 88/100 Iteration 70/234: loss=0.035648 lr=0.000020 grad_norm=0.710082
Epoch 88/100 Iteration 71/234: loss=0.037647 lr=0.000020 grad_norm=1.636973
Epoch 88/100 Iteration 72/234: loss=0.035647 lr=0.000020 grad_norm=0.785815
Epoch 88/100 Iteration 73/234: loss=0.030748 lr=0.000020 grad_norm=0.914181
Epoch 88/100 Iteration 74/234: loss=0.031522 lr=0.000020 grad_norm=0.467978
Epoch 88/100 Iteration 75/234: loss=0.035420 lr=0.000020 grad_norm=1.057196
Epoch 88/100 Iteration 76/234: loss=0.034750 lr=0.000020 grad_norm=0.531901
Epoch 88/100 Iteration 77/234: loss=0.036473 lr=0.000020 grad_norm=0.864692
Epoch 88/100 Iteration 78/234: loss=0.035375 lr=0.000020 grad_norm=0.940321
Epoch 88/100 Iteration 79/234: loss=0.035079 lr=0.000020 grad_norm=0.642661
Epoch 88/100 Iteration 80/234: loss=0.036071 lr=0.000020 grad_norm=1.422087
Epoch 88/100 Iteration 81/234: loss=0.038150 lr=0.000020 grad_norm=0.511623
Epoch 88/100 Iteration 82/234: loss=0.034336 lr=0.000020 grad_norm=0.811146
Epoch 88/100 Iteration 83/234: loss=0.037738 lr=0.000020 grad_norm=0.576958
Epoch 88/100 Iteration 84/234: loss=0.033387 lr=0.000020 grad_norm=0.697182
Epoch 88/100 Iteration 85/234: loss=0.038475 lr=0.000020 grad_norm=0.665054
Epoch 88/100 Iteration 86/234: loss=0.033834 lr=0.000020 grad_norm=0.460341
Epoch 88/100 Iteration 87/234: loss=0.032720 lr=0.000020 grad_norm=0.506488
Epoch 88/100 Iteration 88/234: loss=0.034717 lr=0.000020 grad_norm=0.325092
Epoch 88/100 Iteration 89/234: loss=0.036767 lr=0.000020 grad_norm=0.445321
Epoch 88/100 Iteration 90/234: loss=0.038413 lr=0.000020 grad_norm=0.397751
Epoch 88/100 Iteration 91/234: loss=0.031182 lr=0.000020 grad_norm=0.543756
Epoch 88/100 Iteration 92/234: loss=0.031611 lr=0.000020 grad_norm=0.421452
Epoch 88/100 Iteration 93/234: loss=0.035131 lr=0.000020 grad_norm=0.366822
Epoch 88/100 Iteration 94/234: loss=0.031392 lr=0.000020 grad_norm=0.451741
Epoch 88/100 Iteration 95/234: loss=0.036136 lr=0.000020 grad_norm=0.732596
Epoch 88/100 Iteration 96/234: loss=0.034561 lr=0.000020 grad_norm=0.448086
Epoch 88/100 Iteration 97/234: loss=0.033290 lr=0.000020 grad_norm=0.560488
Epoch 88/100 Iteration 98/234: loss=0.032145 lr=0.000020 grad_norm=0.624511
Epoch 88/100 Iteration 99/234: loss=0.034666 lr=0.000020 grad_norm=0.369599
Epoch 88/100 Iteration 100/234: loss=0.036856 lr=0.000020 grad_norm=0.699105
Epoch 88/100 Iteration 101/234: loss=0.035197 lr=0.000020 grad_norm=0.461649
Epoch 88/100 Iteration 102/234: loss=0.035924 lr=0.000020 grad_norm=0.544390
Epoch 88/100 Iteration 103/234: loss=0.036080 lr=0.000020 grad_norm=0.559429
Epoch 88/100 Iteration 104/234: loss=0.037398 lr=0.000020 grad_norm=0.974113
Epoch 88/100 Iteration 105/234: loss=0.037650 lr=0.000020 grad_norm=0.945451
Epoch 88/100 Iteration 106/234: loss=0.033936 lr=0.000020 grad_norm=0.536220
Epoch 88/100 Iteration 107/234: loss=0.034458 lr=0.000020 grad_norm=0.681701
Epoch 88/100 Iteration 108/234: loss=0.038141 lr=0.000020 grad_norm=0.433502
Epoch 88/100 Iteration 109/234: loss=0.035752 lr=0.000020 grad_norm=0.847435
Epoch 88/100 Iteration 110/234: loss=0.038067 lr=0.000020 grad_norm=0.742052
Epoch 88/100 Iteration 111/234: loss=0.037067 lr=0.000020 grad_norm=0.555291
Epoch 88/100 Iteration 112/234: loss=0.034550 lr=0.000020 grad_norm=0.538652
Epoch 88/100 Iteration 113/234: loss=0.035341 lr=0.000020 grad_norm=0.442069
Epoch 88/100 Iteration 114/234: loss=0.034440 lr=0.000020 grad_norm=0.480676
Epoch 88/100 Iteration 115/234: loss=0.033115 lr=0.000020 grad_norm=0.790217
Epoch 88/100 Iteration 116/234: loss=0.033764 lr=0.000020 grad_norm=0.597939
Epoch 88/100 Iteration 117/234: loss=0.035092 lr=0.000020 grad_norm=0.453016
Epoch 88/100 Iteration 118/234: loss=0.033196 lr=0.000020 grad_norm=0.597341
Epoch 88/100 Iteration 119/234: loss=0.037642 lr=0.000020 grad_norm=0.422380
Epoch 88/100 Iteration 120/234: loss=0.035025 lr=0.000020 grad_norm=0.638742
Epoch 88/100 Iteration 121/234: loss=0.034022 lr=0.000020 grad_norm=0.785238
Epoch 88/100 Iteration 122/234: loss=0.031979 lr=0.000020 grad_norm=0.315316
Epoch 88/100 Iteration 123/234: loss=0.032485 lr=0.000020 grad_norm=0.651077
Epoch 88/100 Iteration 124/234: loss=0.035909 lr=0.000020 grad_norm=0.843397
Epoch 88/100 Iteration 125/234: loss=0.034544 lr=0.000020 grad_norm=0.762986
Epoch 88/100 Iteration 126/234: loss=0.037467 lr=0.000020 grad_norm=0.497637
Epoch 88/100 Iteration 127/234: loss=0.035650 lr=0.000020 grad_norm=0.707489
Epoch 88/100 Iteration 128/234: loss=0.033321 lr=0.000020 grad_norm=0.515054
Epoch 88/100 Iteration 129/234: loss=0.033427 lr=0.000020 grad_norm=0.514961
Epoch 88/100 Iteration 130/234: loss=0.036839 lr=0.000020 grad_norm=0.577937
Epoch 88/100 Iteration 131/234: loss=0.035642 lr=0.000020 grad_norm=0.336663
Epoch 88/100 Iteration 132/234: loss=0.036663 lr=0.000020 grad_norm=0.709468
Epoch 88/100 Iteration 133/234: loss=0.036756 lr=0.000020 grad_norm=0.856786
Epoch 88/100 Iteration 134/234: loss=0.038103 lr=0.000020 grad_norm=0.878739
Epoch 88/100 Iteration 135/234: loss=0.038642 lr=0.000020 grad_norm=1.510614
Epoch 88/100 Iteration 136/234: loss=0.035089 lr=0.000020 grad_norm=1.826398
Epoch 88/100 Iteration 137/234: loss=0.038207 lr=0.000020 grad_norm=1.032802
Epoch 88/100 Iteration 138/234: loss=0.033874 lr=0.000020 grad_norm=1.176939
Epoch 88/100 Iteration 139/234: loss=0.041608 lr=0.000020 grad_norm=1.106874
Epoch 88/100 Iteration 140/234: loss=0.032045 lr=0.000020 grad_norm=1.198073
Epoch 88/100 Iteration 141/234: loss=0.036451 lr=0.000020 grad_norm=0.755243
Epoch 88/100 Iteration 142/234: loss=0.033021 lr=0.000020 grad_norm=0.771153
Epoch 88/100 Iteration 143/234: loss=0.031525 lr=0.000020 grad_norm=1.022479
Epoch 88/100 Iteration 144/234: loss=0.033133 lr=0.000020 grad_norm=0.486653
Epoch 88/100 Iteration 145/234: loss=0.035451 lr=0.000020 grad_norm=1.025465
Epoch 88/100 Iteration 146/234: loss=0.034900 lr=0.000020 grad_norm=0.713281
Epoch 88/100 Iteration 147/234: loss=0.036101 lr=0.000020 grad_norm=0.687771
Epoch 88/100 Iteration 148/234: loss=0.034092 lr=0.000020 grad_norm=0.850941
Epoch 88/100 Iteration 149/234: loss=0.039589 lr=0.000020 grad_norm=0.738355
Epoch 88/100 Iteration 150/234: loss=0.036438 lr=0.000020 grad_norm=0.502303
Epoch 88/100 Iteration 151/234: loss=0.033013 lr=0.000020 grad_norm=0.525797
Epoch 88/100 Iteration 152/234: loss=0.032318 lr=0.000020 grad_norm=0.410364
Epoch 88/100 Iteration 153/234: loss=0.036718 lr=0.000020 grad_norm=0.607470
Epoch 88/100 Iteration 154/234: loss=0.034441 lr=0.000020 grad_norm=0.654352
Epoch 88/100 Iteration 155/234: loss=0.033319 lr=0.000020 grad_norm=0.542115
Epoch 88/100 Iteration 156/234: loss=0.035658 lr=0.000020 grad_norm=0.553835
Epoch 88/100 Iteration 157/234: loss=0.033505 lr=0.000020 grad_norm=0.587974
Epoch 88/100 Iteration 158/234: loss=0.034254 lr=0.000020 grad_norm=0.622904
Epoch 88/100 Iteration 159/234: loss=0.031753 lr=0.000020 grad_norm=0.421597
Epoch 88/100 Iteration 160/234: loss=0.033273 lr=0.000020 grad_norm=0.474081
Epoch 88/100 Iteration 161/234: loss=0.036551 lr=0.000020 grad_norm=0.619951
Epoch 88/100 Iteration 162/234: loss=0.035163 lr=0.000020 grad_norm=0.462213
Epoch 88/100 Iteration 163/234: loss=0.032296 lr=0.000020 grad_norm=0.455462
Epoch 88/100 Iteration 164/234: loss=0.033525 lr=0.000020 grad_norm=0.726510
Epoch 88/100 Iteration 165/234: loss=0.035801 lr=0.000020 grad_norm=0.547566
Epoch 88/100 Iteration 166/234: loss=0.033200 lr=0.000020 grad_norm=1.179042
Epoch 88/100 Iteration 167/234: loss=0.036954 lr=0.000020 grad_norm=0.961048
Epoch 88/100 Iteration 168/234: loss=0.032887 lr=0.000020 grad_norm=0.404195
Epoch 88/100 Iteration 169/234: loss=0.036174 lr=0.000020 grad_norm=0.733234
Epoch 88/100 Iteration 170/234: loss=0.033816 lr=0.000020 grad_norm=0.996706
Epoch 88/100 Iteration 171/234: loss=0.033713 lr=0.000020 grad_norm=0.920934
Epoch 88/100 Iteration 172/234: loss=0.032653 lr=0.000020 grad_norm=0.566900
Epoch 88/100 Iteration 173/234: loss=0.034650 lr=0.000020 grad_norm=0.864215
Epoch 88/100 Iteration 174/234: loss=0.035488 lr=0.000020 grad_norm=0.782438
Epoch 88/100 Iteration 175/234: loss=0.035177 lr=0.000020 grad_norm=0.694451
Epoch 88/100 Iteration 176/234: loss=0.034135 lr=0.000020 grad_norm=0.625177
Epoch 88/100 Iteration 177/234: loss=0.037409 lr=0.000020 grad_norm=0.729488
Epoch 88/100 Iteration 178/234: loss=0.036732 lr=0.000020 grad_norm=0.785931
Epoch 88/100 Iteration 179/234: loss=0.031730 lr=0.000020 grad_norm=0.762810
Epoch 88/100 Iteration 180/234: loss=0.036486 lr=0.000020 grad_norm=0.732920
Epoch 88/100 Iteration 181/234: loss=0.037107 lr=0.000020 grad_norm=1.175418
Epoch 88/100 Iteration 182/234: loss=0.030177 lr=0.000020 grad_norm=0.732693
Epoch 88/100 Iteration 183/234: loss=0.034470 lr=0.000020 grad_norm=0.717077
Epoch 88/100 Iteration 184/234: loss=0.031964 lr=0.000020 grad_norm=0.631848
Epoch 88/100 Iteration 185/234: loss=0.036822 lr=0.000020 grad_norm=0.970501
Epoch 88/100 Iteration 186/234: loss=0.034228 lr=0.000020 grad_norm=1.083930
Epoch 88/100 Iteration 187/234: loss=0.033530 lr=0.000020 grad_norm=0.515955
Epoch 88/100 Iteration 188/234: loss=0.037807 lr=0.000020 grad_norm=1.792064
Epoch 88/100 Iteration 189/234: loss=0.037656 lr=0.000020 grad_norm=1.652830
Epoch 88/100 Iteration 190/234: loss=0.036674 lr=0.000020 grad_norm=0.682263
Epoch 88/100 Iteration 191/234: loss=0.035491 lr=0.000020 grad_norm=1.194795
Epoch 88/100 Iteration 192/234: loss=0.035425 lr=0.000020 grad_norm=0.683550
Epoch 88/100 Iteration 193/234: loss=0.033188 lr=0.000020 grad_norm=0.789104
Epoch 88/100 Iteration 194/234: loss=0.032578 lr=0.000020 grad_norm=0.696241
Epoch 88/100 Iteration 195/234: loss=0.035238 lr=0.000020 grad_norm=0.648821
Epoch 88/100 Iteration 196/234: loss=0.036101 lr=0.000020 grad_norm=1.016149
Epoch 88/100 Iteration 197/234: loss=0.036798 lr=0.000020 grad_norm=0.521048
Epoch 88/100 Iteration 198/234: loss=0.035371 lr=0.000020 grad_norm=0.560132
Epoch 88/100 Iteration 199/234: loss=0.033566 lr=0.000020 grad_norm=0.778480
Epoch 88/100 Iteration 200/234: loss=0.033452 lr=0.000020 grad_norm=0.884634
Epoch 88/100 Iteration 201/234: loss=0.034271 lr=0.000020 grad_norm=0.547247
Epoch 88/100 Iteration 202/234: loss=0.031055 lr=0.000020 grad_norm=1.041768
Epoch 88/100 Iteration 203/234: loss=0.036188 lr=0.000020 grad_norm=0.569051
Epoch 88/100 Iteration 204/234: loss=0.036343 lr=0.000020 grad_norm=0.947231
Epoch 88/100 Iteration 205/234: loss=0.035335 lr=0.000020 grad_norm=0.852365
Epoch 88/100 Iteration 206/234: loss=0.033228 lr=0.000020 grad_norm=0.593078
Epoch 88/100 Iteration 207/234: loss=0.031269 lr=0.000020 grad_norm=0.571641
Epoch 88/100 Iteration 208/234: loss=0.034277 lr=0.000020 grad_norm=0.605507
Epoch 88/100 Iteration 209/234: loss=0.034868 lr=0.000020 grad_norm=0.662149
Epoch 88/100 Iteration 210/234: loss=0.036608 lr=0.000020 grad_norm=0.625479
Epoch 88/100 Iteration 211/234: loss=0.036239 lr=0.000020 grad_norm=0.933607
Epoch 88/100 Iteration 212/234: loss=0.035260 lr=0.000020 grad_norm=0.794960
Epoch 88/100 Iteration 213/234: loss=0.033929 lr=0.000020 grad_norm=0.527237
Epoch 88/100 Iteration 214/234: loss=0.034688 lr=0.000020 grad_norm=0.758285
Epoch 88/100 Iteration 215/234: loss=0.041661 lr=0.000020 grad_norm=0.747003
Epoch 88/100 Iteration 216/234: loss=0.037172 lr=0.000020 grad_norm=0.385239
Epoch 88/100 Iteration 217/234: loss=0.035946 lr=0.000020 grad_norm=0.583262
Epoch 88/100 Iteration 218/234: loss=0.033703 lr=0.000020 grad_norm=0.733471
Epoch 88/100 Iteration 219/234: loss=0.035589 lr=0.000020 grad_norm=0.470772
Epoch 88/100 Iteration 220/234: loss=0.032865 lr=0.000020 grad_norm=0.663866
Epoch 88/100 Iteration 221/234: loss=0.034318 lr=0.000020 grad_norm=0.441247
Epoch 88/100 Iteration 222/234: loss=0.035085 lr=0.000020 grad_norm=0.573376
Epoch 88/100 Iteration 223/234: loss=0.031114 lr=0.000020 grad_norm=0.524100
Epoch 88/100 Iteration 224/234: loss=0.032620 lr=0.000020 grad_norm=0.420649
Epoch 88/100 Iteration 225/234: loss=0.036630 lr=0.000020 grad_norm=0.687146
Epoch 88/100 Iteration 226/234: loss=0.031888 lr=0.000020 grad_norm=0.498080
Epoch 88/100 Iteration 227/234: loss=0.034451 lr=0.000020 grad_norm=0.426730
Epoch 88/100 Iteration 228/234: loss=0.034071 lr=0.000020 grad_norm=0.468351
Epoch 88/100 Iteration 229/234: loss=0.033540 lr=0.000020 grad_norm=0.545593
Epoch 88/100 Iteration 230/234: loss=0.033195 lr=0.000020 grad_norm=0.581985
Epoch 88/100 Iteration 231/234: loss=0.036773 lr=0.000020 grad_norm=0.639978
Epoch 88/100 Iteration 232/234: loss=0.032108 lr=0.000020 grad_norm=0.430678
Epoch 88/100 Iteration 233/234: loss=0.033567 lr=0.000020 grad_norm=0.589012
Epoch 88/100 Iteration 234/234: loss=0.034505 lr=0.000020 grad_norm=0.729603
Epoch 88/100 finished. Avg Loss: 0.034986
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 89/100 Iteration 1/234: loss=0.029839 lr=0.000020 grad_norm=0.410179
Epoch 89/100 Iteration 2/234: loss=0.033885 lr=0.000020 grad_norm=0.776814
Epoch 89/100 Iteration 3/234: loss=0.033757 lr=0.000020 grad_norm=0.592734
Epoch 89/100 Iteration 4/234: loss=0.034328 lr=0.000020 grad_norm=0.605723
Epoch 89/100 Iteration 5/234: loss=0.033083 lr=0.000020 grad_norm=0.615042
Epoch 89/100 Iteration 6/234: loss=0.034435 lr=0.000020 grad_norm=0.510052
Epoch 89/100 Iteration 7/234: loss=0.038585 lr=0.000020 grad_norm=0.888295
Epoch 89/100 Iteration 8/234: loss=0.035095 lr=0.000020 grad_norm=0.987840
Epoch 89/100 Iteration 9/234: loss=0.034497 lr=0.000020 grad_norm=0.531908
Epoch 89/100 Iteration 10/234: loss=0.036488 lr=0.000020 grad_norm=0.523468
Epoch 89/100 Iteration 11/234: loss=0.033767 lr=0.000020 grad_norm=0.567804
Epoch 89/100 Iteration 12/234: loss=0.035748 lr=0.000020 grad_norm=0.476949
Epoch 89/100 Iteration 13/234: loss=0.033302 lr=0.000020 grad_norm=0.561580
Epoch 89/100 Iteration 14/234: loss=0.037545 lr=0.000020 grad_norm=0.709794
Epoch 89/100 Iteration 15/234: loss=0.035364 lr=0.000020 grad_norm=0.477187
Epoch 89/100 Iteration 16/234: loss=0.036266 lr=0.000020 grad_norm=0.483090
Epoch 89/100 Iteration 17/234: loss=0.036444 lr=0.000020 grad_norm=0.366052
Epoch 89/100 Iteration 18/234: loss=0.035712 lr=0.000020 grad_norm=0.472469
Epoch 89/100 Iteration 19/234: loss=0.031952 lr=0.000020 grad_norm=0.296988
Epoch 89/100 Iteration 20/234: loss=0.037260 lr=0.000020 grad_norm=0.459268
Epoch 89/100 Iteration 21/234: loss=0.035413 lr=0.000020 grad_norm=0.510845
Epoch 89/100 Iteration 22/234: loss=0.034836 lr=0.000020 grad_norm=0.508537
Epoch 89/100 Iteration 23/234: loss=0.036337 lr=0.000020 grad_norm=0.646551
Epoch 89/100 Iteration 24/234: loss=0.031974 lr=0.000020 grad_norm=0.726210
Epoch 89/100 Iteration 25/234: loss=0.032209 lr=0.000020 grad_norm=0.567049
Epoch 89/100 Iteration 26/234: loss=0.031709 lr=0.000020 grad_norm=0.275979
Epoch 89/100 Iteration 27/234: loss=0.034869 lr=0.000020 grad_norm=0.574218
Epoch 89/100 Iteration 28/234: loss=0.036640 lr=0.000020 grad_norm=0.874808
Epoch 89/100 Iteration 29/234: loss=0.036898 lr=0.000020 grad_norm=0.850995
Epoch 89/100 Iteration 30/234: loss=0.038219 lr=0.000020 grad_norm=0.440309
Epoch 89/100 Iteration 31/234: loss=0.034324 lr=0.000020 grad_norm=0.715300
Epoch 89/100 Iteration 32/234: loss=0.032394 lr=0.000020 grad_norm=1.223146
Epoch 89/100 Iteration 33/234: loss=0.036313 lr=0.000020 grad_norm=1.353798
Epoch 89/100 Iteration 34/234: loss=0.032067 lr=0.000020 grad_norm=0.898897
Epoch 89/100 Iteration 35/234: loss=0.035059 lr=0.000020 grad_norm=0.671123
Epoch 89/100 Iteration 36/234: loss=0.032851 lr=0.000020 grad_norm=1.221786
Epoch 89/100 Iteration 37/234: loss=0.030754 lr=0.000020 grad_norm=0.923180
Epoch 89/100 Iteration 38/234: loss=0.034646 lr=0.000020 grad_norm=0.692499
Epoch 89/100 Iteration 39/234: loss=0.038421 lr=0.000020 grad_norm=1.635097
Epoch 89/100 Iteration 40/234: loss=0.033777 lr=0.000020 grad_norm=1.779722
Epoch 89/100 Iteration 41/234: loss=0.037298 lr=0.000020 grad_norm=0.933215
Epoch 89/100 Iteration 42/234: loss=0.033058 lr=0.000020 grad_norm=0.787282
Epoch 89/100 Iteration 43/234: loss=0.033529 lr=0.000020 grad_norm=1.008099
Epoch 89/100 Iteration 44/234: loss=0.033714 lr=0.000020 grad_norm=0.713715
Epoch 89/100 Iteration 45/234: loss=0.037237 lr=0.000020 grad_norm=1.226727
Epoch 89/100 Iteration 46/234: loss=0.033168 lr=0.000020 grad_norm=1.369857
Epoch 89/100 Iteration 47/234: loss=0.034623 lr=0.000020 grad_norm=0.901104
Epoch 89/100 Iteration 48/234: loss=0.034661 lr=0.000020 grad_norm=1.042604
Epoch 89/100 Iteration 49/234: loss=0.036279 lr=0.000020 grad_norm=1.573207
Epoch 89/100 Iteration 50/234: loss=0.032273 lr=0.000020 grad_norm=1.037146
Epoch 89/100 Iteration 51/234: loss=0.033688 lr=0.000020 grad_norm=0.833943
Epoch 89/100 Iteration 52/234: loss=0.034386 lr=0.000020 grad_norm=1.650935
Epoch 89/100 Iteration 53/234: loss=0.036287 lr=0.000020 grad_norm=1.307681
Epoch 89/100 Iteration 54/234: loss=0.033425 lr=0.000020 grad_norm=0.410193
Epoch 89/100 Iteration 55/234: loss=0.034968 lr=0.000020 grad_norm=1.418932
Epoch 89/100 Iteration 56/234: loss=0.032280 lr=0.000020 grad_norm=1.145836
Epoch 89/100 Iteration 57/234: loss=0.037469 lr=0.000020 grad_norm=0.763066
Epoch 89/100 Iteration 58/234: loss=0.032636 lr=0.000020 grad_norm=1.306620
Epoch 89/100 Iteration 59/234: loss=0.038152 lr=0.000020 grad_norm=0.790951
Epoch 89/100 Iteration 60/234: loss=0.037441 lr=0.000020 grad_norm=0.980833
Epoch 89/100 Iteration 61/234: loss=0.034756 lr=0.000020 grad_norm=1.566636
Epoch 89/100 Iteration 62/234: loss=0.038021 lr=0.000020 grad_norm=0.686315
Epoch 89/100 Iteration 63/234: loss=0.039478 lr=0.000020 grad_norm=1.327886
Epoch 89/100 Iteration 64/234: loss=0.036935 lr=0.000020 grad_norm=1.853281
Epoch 89/100 Iteration 65/234: loss=0.037770 lr=0.000020 grad_norm=0.850153
Epoch 89/100 Iteration 66/234: loss=0.032313 lr=0.000020 grad_norm=0.964562
Epoch 89/100 Iteration 67/234: loss=0.037001 lr=0.000020 grad_norm=1.071307
Epoch 89/100 Iteration 68/234: loss=0.033390 lr=0.000020 grad_norm=0.502690
Epoch 89/100 Iteration 69/234: loss=0.033842 lr=0.000020 grad_norm=0.952540
Epoch 89/100 Iteration 70/234: loss=0.033837 lr=0.000020 grad_norm=0.526938
Epoch 89/100 Iteration 71/234: loss=0.036053 lr=0.000020 grad_norm=0.825441
Epoch 89/100 Iteration 72/234: loss=0.034037 lr=0.000020 grad_norm=0.649419
Epoch 89/100 Iteration 73/234: loss=0.037555 lr=0.000020 grad_norm=0.796212
Epoch 89/100 Iteration 74/234: loss=0.031520 lr=0.000020 grad_norm=0.964318
Epoch 89/100 Iteration 75/234: loss=0.035532 lr=0.000020 grad_norm=0.932051
Epoch 89/100 Iteration 76/234: loss=0.035293 lr=0.000020 grad_norm=0.730043
Epoch 89/100 Iteration 77/234: loss=0.036934 lr=0.000020 grad_norm=0.995881
Epoch 89/100 Iteration 78/234: loss=0.035788 lr=0.000020 grad_norm=0.705696
Epoch 89/100 Iteration 79/234: loss=0.033193 lr=0.000020 grad_norm=0.548774
Epoch 89/100 Iteration 80/234: loss=0.035054 lr=0.000020 grad_norm=0.735181
Epoch 89/100 Iteration 81/234: loss=0.037780 lr=0.000020 grad_norm=0.537539
Epoch 89/100 Iteration 82/234: loss=0.032804 lr=0.000020 grad_norm=0.645528
Epoch 89/100 Iteration 83/234: loss=0.035165 lr=0.000020 grad_norm=0.609627
Epoch 89/100 Iteration 84/234: loss=0.034269 lr=0.000020 grad_norm=0.443121
Epoch 89/100 Iteration 85/234: loss=0.034989 lr=0.000020 grad_norm=0.658246
Epoch 89/100 Iteration 86/234: loss=0.033867 lr=0.000020 grad_norm=0.731095
Epoch 89/100 Iteration 87/234: loss=0.034359 lr=0.000020 grad_norm=0.527256
Epoch 89/100 Iteration 88/234: loss=0.034481 lr=0.000020 grad_norm=0.953960
Epoch 89/100 Iteration 89/234: loss=0.034716 lr=0.000020 grad_norm=0.886746
Epoch 89/100 Iteration 90/234: loss=0.035903 lr=0.000020 grad_norm=0.689134
Epoch 89/100 Iteration 91/234: loss=0.034945 lr=0.000020 grad_norm=1.651861
Epoch 89/100 Iteration 92/234: loss=0.034209 lr=0.000020 grad_norm=1.569306
Epoch 89/100 Iteration 93/234: loss=0.035452 lr=0.000020 grad_norm=0.619103
Epoch 89/100 Iteration 94/234: loss=0.033154 lr=0.000020 grad_norm=1.057133
Epoch 89/100 Iteration 95/234: loss=0.035383 lr=0.000020 grad_norm=0.633435
Epoch 89/100 Iteration 96/234: loss=0.035238 lr=0.000020 grad_norm=0.891879
Epoch 89/100 Iteration 97/234: loss=0.034057 lr=0.000020 grad_norm=0.642605
Epoch 89/100 Iteration 98/234: loss=0.035620 lr=0.000020 grad_norm=0.823244
Epoch 89/100 Iteration 99/234: loss=0.036897 lr=0.000020 grad_norm=0.937644
Epoch 89/100 Iteration 100/234: loss=0.034426 lr=0.000020 grad_norm=0.425930
Epoch 89/100 Iteration 101/234: loss=0.036187 lr=0.000020 grad_norm=0.713845
Epoch 89/100 Iteration 102/234: loss=0.034572 lr=0.000020 grad_norm=0.672412
Epoch 89/100 Iteration 103/234: loss=0.038483 lr=0.000020 grad_norm=1.135422
Epoch 89/100 Iteration 104/234: loss=0.032659 lr=0.000020 grad_norm=1.109674
Epoch 89/100 Iteration 105/234: loss=0.031734 lr=0.000020 grad_norm=0.416515
Epoch 89/100 Iteration 106/234: loss=0.035491 lr=0.000020 grad_norm=1.139912
Epoch 89/100 Iteration 107/234: loss=0.036230 lr=0.000020 grad_norm=0.894624
Epoch 89/100 Iteration 108/234: loss=0.035908 lr=0.000020 grad_norm=0.903074
Epoch 89/100 Iteration 109/234: loss=0.033154 lr=0.000020 grad_norm=1.169806
Epoch 89/100 Iteration 110/234: loss=0.034075 lr=0.000020 grad_norm=0.529741
Epoch 89/100 Iteration 111/234: loss=0.034107 lr=0.000020 grad_norm=1.076451
Epoch 89/100 Iteration 112/234: loss=0.035546 lr=0.000020 grad_norm=0.706158
Epoch 89/100 Iteration 113/234: loss=0.035646 lr=0.000020 grad_norm=0.994186
Epoch 89/100 Iteration 114/234: loss=0.036384 lr=0.000020 grad_norm=1.523065
Epoch 89/100 Iteration 115/234: loss=0.035512 lr=0.000020 grad_norm=0.973643
Epoch 89/100 Iteration 116/234: loss=0.038217 lr=0.000020 grad_norm=0.943312
Epoch 89/100 Iteration 117/234: loss=0.035906 lr=0.000020 grad_norm=0.915973
Epoch 89/100 Iteration 118/234: loss=0.034394 lr=0.000020 grad_norm=0.560032
Epoch 89/100 Iteration 119/234: loss=0.035668 lr=0.000020 grad_norm=0.902522
Epoch 89/100 Iteration 120/234: loss=0.037948 lr=0.000020 grad_norm=0.515919
Epoch 89/100 Iteration 121/234: loss=0.035974 lr=0.000020 grad_norm=0.869940
Epoch 89/100 Iteration 122/234: loss=0.033242 lr=0.000020 grad_norm=0.521052
Epoch 89/100 Iteration 123/234: loss=0.034276 lr=0.000020 grad_norm=0.757076
Epoch 89/100 Iteration 124/234: loss=0.036643 lr=0.000020 grad_norm=0.593286
Epoch 89/100 Iteration 125/234: loss=0.037481 lr=0.000020 grad_norm=1.012150
Epoch 89/100 Iteration 126/234: loss=0.032544 lr=0.000020 grad_norm=1.121383
Epoch 89/100 Iteration 127/234: loss=0.034631 lr=0.000020 grad_norm=0.501185
Epoch 89/100 Iteration 128/234: loss=0.034254 lr=0.000020 grad_norm=0.735697
Epoch 89/100 Iteration 129/234: loss=0.034715 lr=0.000020 grad_norm=0.716771
Epoch 89/100 Iteration 130/234: loss=0.034120 lr=0.000020 grad_norm=0.519985
Epoch 89/100 Iteration 131/234: loss=0.036874 lr=0.000020 grad_norm=0.630075
Epoch 89/100 Iteration 132/234: loss=0.035376 lr=0.000020 grad_norm=0.590775
Epoch 89/100 Iteration 133/234: loss=0.037380 lr=0.000020 grad_norm=0.989809
Epoch 89/100 Iteration 134/234: loss=0.032319 lr=0.000020 grad_norm=0.735462
Epoch 89/100 Iteration 135/234: loss=0.034344 lr=0.000020 grad_norm=0.634453
Epoch 89/100 Iteration 136/234: loss=0.036223 lr=0.000020 grad_norm=1.093317
Epoch 89/100 Iteration 137/234: loss=0.034410 lr=0.000020 grad_norm=0.751704
Epoch 89/100 Iteration 138/234: loss=0.035833 lr=0.000020 grad_norm=0.523721
Epoch 89/100 Iteration 139/234: loss=0.034887 lr=0.000020 grad_norm=0.720429
Epoch 89/100 Iteration 140/234: loss=0.037879 lr=0.000020 grad_norm=0.578456
Epoch 89/100 Iteration 141/234: loss=0.035714 lr=0.000020 grad_norm=0.488917
Epoch 89/100 Iteration 142/234: loss=0.030140 lr=0.000020 grad_norm=0.625157
Epoch 89/100 Iteration 143/234: loss=0.034923 lr=0.000020 grad_norm=0.402846
Epoch 89/100 Iteration 144/234: loss=0.031389 lr=0.000020 grad_norm=0.435753
Epoch 89/100 Iteration 145/234: loss=0.035384 lr=0.000020 grad_norm=0.687480
Epoch 89/100 Iteration 146/234: loss=0.032560 lr=0.000020 grad_norm=0.822144
Epoch 89/100 Iteration 147/234: loss=0.033634 lr=0.000020 grad_norm=0.462030
Epoch 89/100 Iteration 148/234: loss=0.035745 lr=0.000020 grad_norm=0.878727
Epoch 89/100 Iteration 149/234: loss=0.036076 lr=0.000020 grad_norm=1.020875
Epoch 89/100 Iteration 150/234: loss=0.031325 lr=0.000020 grad_norm=0.513145
Epoch 89/100 Iteration 151/234: loss=0.033306 lr=0.000020 grad_norm=0.754682
Epoch 89/100 Iteration 152/234: loss=0.033176 lr=0.000020 grad_norm=0.757490
Epoch 89/100 Iteration 153/234: loss=0.038893 lr=0.000020 grad_norm=0.602243
Epoch 89/100 Iteration 154/234: loss=0.034147 lr=0.000020 grad_norm=0.593221
Epoch 89/100 Iteration 155/234: loss=0.035187 lr=0.000020 grad_norm=0.516065
Epoch 89/100 Iteration 156/234: loss=0.037126 lr=0.000020 grad_norm=0.802786
Epoch 89/100 Iteration 157/234: loss=0.036032 lr=0.000020 grad_norm=0.474809
Epoch 89/100 Iteration 158/234: loss=0.033586 lr=0.000020 grad_norm=0.633713
Epoch 89/100 Iteration 159/234: loss=0.031788 lr=0.000020 grad_norm=0.536612
Epoch 89/100 Iteration 160/234: loss=0.034972 lr=0.000020 grad_norm=0.615750
Epoch 89/100 Iteration 161/234: loss=0.036416 lr=0.000020 grad_norm=0.590087
Epoch 89/100 Iteration 162/234: loss=0.034808 lr=0.000020 grad_norm=0.509303
Epoch 89/100 Iteration 163/234: loss=0.031621 lr=0.000020 grad_norm=0.554703
Epoch 89/100 Iteration 164/234: loss=0.035131 lr=0.000020 grad_norm=0.479992
Epoch 89/100 Iteration 165/234: loss=0.032004 lr=0.000020 grad_norm=0.323378
Epoch 89/100 Iteration 166/234: loss=0.034957 lr=0.000020 grad_norm=0.539082
Epoch 89/100 Iteration 167/234: loss=0.033414 lr=0.000020 grad_norm=0.503171
Epoch 89/100 Iteration 168/234: loss=0.036023 lr=0.000020 grad_norm=0.660420
Epoch 89/100 Iteration 169/234: loss=0.035937 lr=0.000020 grad_norm=0.489595
Epoch 89/100 Iteration 170/234: loss=0.034087 lr=0.000020 grad_norm=0.539978
Epoch 89/100 Iteration 171/234: loss=0.039683 lr=0.000020 grad_norm=0.673961
Epoch 89/100 Iteration 172/234: loss=0.035212 lr=0.000020 grad_norm=0.563270
Epoch 89/100 Iteration 173/234: loss=0.037482 lr=0.000020 grad_norm=0.553623
Epoch 89/100 Iteration 174/234: loss=0.036203 lr=0.000020 grad_norm=0.658146
Epoch 89/100 Iteration 175/234: loss=0.035129 lr=0.000020 grad_norm=0.364303
Epoch 89/100 Iteration 176/234: loss=0.034485 lr=0.000020 grad_norm=0.644463
Epoch 89/100 Iteration 177/234: loss=0.036022 lr=0.000020 grad_norm=1.129477
Epoch 89/100 Iteration 178/234: loss=0.034343 lr=0.000020 grad_norm=1.314535
Epoch 89/100 Iteration 179/234: loss=0.038872 lr=0.000020 grad_norm=0.664389
Epoch 89/100 Iteration 180/234: loss=0.034553 lr=0.000020 grad_norm=0.933622
Epoch 89/100 Iteration 181/234: loss=0.033894 lr=0.000020 grad_norm=1.551385
Epoch 89/100 Iteration 182/234: loss=0.033937 lr=0.000020 grad_norm=0.912753
Epoch 89/100 Iteration 183/234: loss=0.032054 lr=0.000020 grad_norm=0.529558
Epoch 89/100 Iteration 184/234: loss=0.034273 lr=0.000020 grad_norm=0.728770
Epoch 89/100 Iteration 185/234: loss=0.038083 lr=0.000020 grad_norm=0.537424
Epoch 89/100 Iteration 186/234: loss=0.034849 lr=0.000020 grad_norm=0.775287
Epoch 89/100 Iteration 187/234: loss=0.034891 lr=0.000020 grad_norm=0.616389
Epoch 89/100 Iteration 188/234: loss=0.032483 lr=0.000020 grad_norm=0.664528
Epoch 89/100 Iteration 189/234: loss=0.033278 lr=0.000020 grad_norm=0.755787
Epoch 89/100 Iteration 190/234: loss=0.029410 lr=0.000020 grad_norm=0.630905
Epoch 89/100 Iteration 191/234: loss=0.032106 lr=0.000020 grad_norm=0.770089
Epoch 89/100 Iteration 192/234: loss=0.032637 lr=0.000020 grad_norm=0.628707
Epoch 89/100 Iteration 193/234: loss=0.028897 lr=0.000020 grad_norm=0.791234
Epoch 89/100 Iteration 194/234: loss=0.036591 lr=0.000020 grad_norm=0.495834
Epoch 89/100 Iteration 195/234: loss=0.034072 lr=0.000020 grad_norm=0.931330
Epoch 89/100 Iteration 196/234: loss=0.033612 lr=0.000020 grad_norm=0.466420
Epoch 89/100 Iteration 197/234: loss=0.034790 lr=0.000020 grad_norm=0.737858
Epoch 89/100 Iteration 198/234: loss=0.034034 lr=0.000020 grad_norm=0.633249
Epoch 89/100 Iteration 199/234: loss=0.033601 lr=0.000020 grad_norm=0.512416
Epoch 89/100 Iteration 200/234: loss=0.033606 lr=0.000020 grad_norm=0.555886
Epoch 89/100 Iteration 201/234: loss=0.037235 lr=0.000020 grad_norm=0.549381
Epoch 89/100 Iteration 202/234: loss=0.035099 lr=0.000020 grad_norm=1.231077
Epoch 89/100 Iteration 203/234: loss=0.040028 lr=0.000020 grad_norm=1.611542
Epoch 89/100 Iteration 204/234: loss=0.034256 lr=0.000020 grad_norm=0.775843
Epoch 89/100 Iteration 205/234: loss=0.035547 lr=0.000020 grad_norm=0.706188
Epoch 89/100 Iteration 206/234: loss=0.037368 lr=0.000020 grad_norm=1.116765
Epoch 89/100 Iteration 207/234: loss=0.035755 lr=0.000020 grad_norm=1.151254
Epoch 89/100 Iteration 208/234: loss=0.034456 lr=0.000020 grad_norm=0.779663
Epoch 89/100 Iteration 209/234: loss=0.030979 lr=0.000020 grad_norm=0.455093
Epoch 89/100 Iteration 210/234: loss=0.035477 lr=0.000020 grad_norm=0.791143
Epoch 89/100 Iteration 211/234: loss=0.035982 lr=0.000020 grad_norm=0.562168
Epoch 89/100 Iteration 212/234: loss=0.035427 lr=0.000020 grad_norm=0.599824
Epoch 89/100 Iteration 213/234: loss=0.039985 lr=0.000020 grad_norm=0.941648
Epoch 89/100 Iteration 214/234: loss=0.035097 lr=0.000020 grad_norm=0.556017
Epoch 89/100 Iteration 215/234: loss=0.036372 lr=0.000020 grad_norm=0.609418
Epoch 89/100 Iteration 216/234: loss=0.035724 lr=0.000020 grad_norm=0.891862
Epoch 89/100 Iteration 217/234: loss=0.032842 lr=0.000020 grad_norm=0.611838
Epoch 89/100 Iteration 218/234: loss=0.033771 lr=0.000020 grad_norm=0.560871
Epoch 89/100 Iteration 219/234: loss=0.037489 lr=0.000020 grad_norm=0.876468
Epoch 89/100 Iteration 220/234: loss=0.032730 lr=0.000020 grad_norm=0.911552
Epoch 89/100 Iteration 221/234: loss=0.030043 lr=0.000020 grad_norm=0.644817
Epoch 89/100 Iteration 222/234: loss=0.035870 lr=0.000020 grad_norm=1.139368
Epoch 89/100 Iteration 223/234: loss=0.033285 lr=0.000020 grad_norm=1.610918
Epoch 89/100 Iteration 224/234: loss=0.031466 lr=0.000020 grad_norm=0.513145
Epoch 89/100 Iteration 225/234: loss=0.033167 lr=0.000020 grad_norm=1.696818
Epoch 89/100 Iteration 226/234: loss=0.036033 lr=0.000020 grad_norm=1.363993
Epoch 89/100 Iteration 227/234: loss=0.035368 lr=0.000020 grad_norm=0.519549
Epoch 89/100 Iteration 228/234: loss=0.031957 lr=0.000020 grad_norm=1.218658
Epoch 89/100 Iteration 229/234: loss=0.033551 lr=0.000020 grad_norm=0.458665
Epoch 89/100 Iteration 230/234: loss=0.033402 lr=0.000020 grad_norm=1.057986
Epoch 89/100 Iteration 231/234: loss=0.038575 lr=0.000020 grad_norm=0.874832
Epoch 89/100 Iteration 232/234: loss=0.034210 lr=0.000020 grad_norm=0.670749
Epoch 89/100 Iteration 233/234: loss=0.038630 lr=0.000020 grad_norm=1.229868
Epoch 89/100 Iteration 234/234: loss=0.033081 lr=0.000020 grad_norm=1.209372
Epoch 89/100 finished. Avg Loss: 0.034845
Epoch 90/100 Iteration 1/234: loss=0.031836 lr=0.000020 grad_norm=0.641676
Epoch 90/100 Iteration 2/234: loss=0.034924 lr=0.000020 grad_norm=0.851710
Epoch 90/100 Iteration 3/234: loss=0.037589 lr=0.000020 grad_norm=1.093900
Epoch 90/100 Iteration 4/234: loss=0.034409 lr=0.000020 grad_norm=0.941744
Epoch 90/100 Iteration 5/234: loss=0.030814 lr=0.000020 grad_norm=0.597692
Epoch 90/100 Iteration 6/234: loss=0.037233 lr=0.000020 grad_norm=1.077172
Epoch 90/100 Iteration 7/234: loss=0.034210 lr=0.000020 grad_norm=0.944020
Epoch 90/100 Iteration 8/234: loss=0.037797 lr=0.000020 grad_norm=1.064301
Epoch 90/100 Iteration 9/234: loss=0.031796 lr=0.000020 grad_norm=1.000592
Epoch 90/100 Iteration 10/234: loss=0.035360 lr=0.000020 grad_norm=0.495693
Epoch 90/100 Iteration 11/234: loss=0.034860 lr=0.000020 grad_norm=1.070464
Epoch 90/100 Iteration 12/234: loss=0.038600 lr=0.000020 grad_norm=1.040736
Epoch 90/100 Iteration 13/234: loss=0.033500 lr=0.000020 grad_norm=0.684524
Epoch 90/100 Iteration 14/234: loss=0.034285 lr=0.000020 grad_norm=0.693044
Epoch 90/100 Iteration 15/234: loss=0.038422 lr=0.000020 grad_norm=1.101892
Epoch 90/100 Iteration 16/234: loss=0.033155 lr=0.000020 grad_norm=0.604474
Epoch 90/100 Iteration 17/234: loss=0.035218 lr=0.000020 grad_norm=0.913624
Epoch 90/100 Iteration 18/234: loss=0.033993 lr=0.000020 grad_norm=0.808938
Epoch 90/100 Iteration 19/234: loss=0.036636 lr=0.000020 grad_norm=0.916730
Epoch 90/100 Iteration 20/234: loss=0.033274 lr=0.000020 grad_norm=0.807221
Epoch 90/100 Iteration 21/234: loss=0.034448 lr=0.000020 grad_norm=0.674646
Epoch 90/100 Iteration 22/234: loss=0.033761 lr=0.000020 grad_norm=0.767242
Epoch 90/100 Iteration 23/234: loss=0.036009 lr=0.000020 grad_norm=0.539307
Epoch 90/100 Iteration 24/234: loss=0.035408 lr=0.000020 grad_norm=0.446435
Epoch 90/100 Iteration 25/234: loss=0.035242 lr=0.000020 grad_norm=0.585651
Epoch 90/100 Iteration 26/234: loss=0.035118 lr=0.000020 grad_norm=0.572699
Epoch 90/100 Iteration 27/234: loss=0.039321 lr=0.000020 grad_norm=0.483420
Epoch 90/100 Iteration 28/234: loss=0.032443 lr=0.000020 grad_norm=0.806690
Epoch 90/100 Iteration 29/234: loss=0.031271 lr=0.000020 grad_norm=0.652422
Epoch 90/100 Iteration 30/234: loss=0.038362 lr=0.000020 grad_norm=0.591417
Epoch 90/100 Iteration 31/234: loss=0.031406 lr=0.000020 grad_norm=0.581349
Epoch 90/100 Iteration 32/234: loss=0.036437 lr=0.000020 grad_norm=0.740936
Epoch 90/100 Iteration 33/234: loss=0.037994 lr=0.000020 grad_norm=0.667839
Epoch 90/100 Iteration 34/234: loss=0.034868 lr=0.000020 grad_norm=0.505795
Epoch 90/100 Iteration 35/234: loss=0.036525 lr=0.000020 grad_norm=0.564837
Epoch 90/100 Iteration 36/234: loss=0.037156 lr=0.000020 grad_norm=0.459070
Epoch 90/100 Iteration 37/234: loss=0.030847 lr=0.000020 grad_norm=0.556520
Epoch 90/100 Iteration 38/234: loss=0.033709 lr=0.000020 grad_norm=0.509268
Epoch 90/100 Iteration 39/234: loss=0.037764 lr=0.000020 grad_norm=0.590429
Epoch 90/100 Iteration 40/234: loss=0.038621 lr=0.000020 grad_norm=0.540915
Epoch 90/100 Iteration 41/234: loss=0.035347 lr=0.000020 grad_norm=0.601237
Epoch 90/100 Iteration 42/234: loss=0.032722 lr=0.000020 grad_norm=0.925180
Epoch 90/100 Iteration 43/234: loss=0.033670 lr=0.000020 grad_norm=1.061213
Epoch 90/100 Iteration 44/234: loss=0.036552 lr=0.000020 grad_norm=0.706040
Epoch 90/100 Iteration 45/234: loss=0.036234 lr=0.000020 grad_norm=0.967650
Epoch 90/100 Iteration 46/234: loss=0.035974 lr=0.000020 grad_norm=1.183015
Epoch 90/100 Iteration 47/234: loss=0.033063 lr=0.000020 grad_norm=0.635200
Epoch 90/100 Iteration 48/234: loss=0.034378 lr=0.000020 grad_norm=0.575931
Epoch 90/100 Iteration 49/234: loss=0.032053 lr=0.000020 grad_norm=0.721851
Epoch 90/100 Iteration 50/234: loss=0.035481 lr=0.000020 grad_norm=0.740698
Epoch 90/100 Iteration 51/234: loss=0.033795 lr=0.000020 grad_norm=0.543584
Epoch 90/100 Iteration 52/234: loss=0.032394 lr=0.000020 grad_norm=0.966007
Epoch 90/100 Iteration 53/234: loss=0.035685 lr=0.000020 grad_norm=0.681539
Epoch 90/100 Iteration 54/234: loss=0.032573 lr=0.000020 grad_norm=0.642962
Epoch 90/100 Iteration 55/234: loss=0.035023 lr=0.000020 grad_norm=0.628619
Epoch 90/100 Iteration 56/234: loss=0.034433 lr=0.000020 grad_norm=0.526811
Epoch 90/100 Iteration 57/234: loss=0.037344 lr=0.000020 grad_norm=0.857887
Epoch 90/100 Iteration 58/234: loss=0.037426 lr=0.000020 grad_norm=0.899331
Epoch 90/100 Iteration 59/234: loss=0.033525 lr=0.000020 grad_norm=1.099651
Epoch 90/100 Iteration 60/234: loss=0.034403 lr=0.000020 grad_norm=0.904826
Epoch 90/100 Iteration 61/234: loss=0.032581 lr=0.000020 grad_norm=0.494334
Epoch 90/100 Iteration 62/234: loss=0.034840 lr=0.000020 grad_norm=0.668313
Epoch 90/100 Iteration 63/234: loss=0.039106 lr=0.000020 grad_norm=0.617816
Epoch 90/100 Iteration 64/234: loss=0.038569 lr=0.000020 grad_norm=0.541481
Epoch 90/100 Iteration 65/234: loss=0.030209 lr=0.000020 grad_norm=0.465956
Epoch 90/100 Iteration 66/234: loss=0.034642 lr=0.000020 grad_norm=0.499645
Epoch 90/100 Iteration 67/234: loss=0.033420 lr=0.000020 grad_norm=0.423386
Epoch 90/100 Iteration 68/234: loss=0.034220 lr=0.000020 grad_norm=0.463958
Epoch 90/100 Iteration 69/234: loss=0.033928 lr=0.000020 grad_norm=0.707724
Epoch 90/100 Iteration 70/234: loss=0.031575 lr=0.000020 grad_norm=0.603179
Epoch 90/100 Iteration 71/234: loss=0.034137 lr=0.000020 grad_norm=0.542429
Epoch 90/100 Iteration 72/234: loss=0.034850 lr=0.000020 grad_norm=1.321866
Epoch 90/100 Iteration 73/234: loss=0.032985 lr=0.000020 grad_norm=1.587903
Epoch 90/100 Iteration 74/234: loss=0.031289 lr=0.000020 grad_norm=0.887487
Epoch 90/100 Iteration 75/234: loss=0.034989 lr=0.000020 grad_norm=0.559030
Epoch 90/100 Iteration 76/234: loss=0.035501 lr=0.000020 grad_norm=1.162345
Epoch 90/100 Iteration 77/234: loss=0.037090 lr=0.000020 grad_norm=0.935387
Epoch 90/100 Iteration 78/234: loss=0.034910 lr=0.000020 grad_norm=0.675318
Epoch 90/100 Iteration 79/234: loss=0.037702 lr=0.000020 grad_norm=0.781448
Epoch 90/100 Iteration 80/234: loss=0.030520 lr=0.000020 grad_norm=0.587636
Epoch 90/100 Iteration 81/234: loss=0.035481 lr=0.000020 grad_norm=0.636145
Epoch 90/100 Iteration 82/234: loss=0.033718 lr=0.000020 grad_norm=1.350623
Epoch 90/100 Iteration 83/234: loss=0.031549 lr=0.000020 grad_norm=1.000274
Epoch 90/100 Iteration 84/234: loss=0.034065 lr=0.000020 grad_norm=0.447991
Epoch 90/100 Iteration 85/234: loss=0.030333 lr=0.000020 grad_norm=0.888434
Epoch 90/100 Iteration 86/234: loss=0.033350 lr=0.000020 grad_norm=0.442807
Epoch 90/100 Iteration 87/234: loss=0.033655 lr=0.000020 grad_norm=0.777093
Epoch 90/100 Iteration 88/234: loss=0.034438 lr=0.000020 grad_norm=0.539570
Epoch 90/100 Iteration 89/234: loss=0.036646 lr=0.000020 grad_norm=0.793995
Epoch 90/100 Iteration 90/234: loss=0.035931 lr=0.000020 grad_norm=1.094396
Epoch 90/100 Iteration 91/234: loss=0.035437 lr=0.000020 grad_norm=0.850039
Epoch 90/100 Iteration 92/234: loss=0.036026 lr=0.000020 grad_norm=0.622346
Epoch 90/100 Iteration 93/234: loss=0.035737 lr=0.000020 grad_norm=0.554256
Epoch 90/100 Iteration 94/234: loss=0.032460 lr=0.000020 grad_norm=0.696527
Epoch 90/100 Iteration 95/234: loss=0.032881 lr=0.000020 grad_norm=0.572359
Epoch 90/100 Iteration 96/234: loss=0.035126 lr=0.000020 grad_norm=0.628806
Epoch 90/100 Iteration 97/234: loss=0.034972 lr=0.000020 grad_norm=0.953019
Epoch 90/100 Iteration 98/234: loss=0.033698 lr=0.000020 grad_norm=0.872768
Epoch 90/100 Iteration 99/234: loss=0.034029 lr=0.000020 grad_norm=0.556083
Epoch 90/100 Iteration 100/234: loss=0.036371 lr=0.000020 grad_norm=0.857218
Epoch 90/100 Iteration 101/234: loss=0.033012 lr=0.000020 grad_norm=0.782315
Epoch 90/100 Iteration 102/234: loss=0.033352 lr=0.000020 grad_norm=0.797138
Epoch 90/100 Iteration 103/234: loss=0.037485 lr=0.000020 grad_norm=0.867768
Epoch 90/100 Iteration 104/234: loss=0.031602 lr=0.000020 grad_norm=0.671380
Epoch 90/100 Iteration 105/234: loss=0.031942 lr=0.000020 grad_norm=0.748814
Epoch 90/100 Iteration 106/234: loss=0.036064 lr=0.000020 grad_norm=0.723048
Epoch 90/100 Iteration 107/234: loss=0.033063 lr=0.000020 grad_norm=0.571600
Epoch 90/100 Iteration 108/234: loss=0.034083 lr=0.000020 grad_norm=0.546417
Epoch 90/100 Iteration 109/234: loss=0.032042 lr=0.000020 grad_norm=0.447147
Epoch 90/100 Iteration 110/234: loss=0.037191 lr=0.000020 grad_norm=0.711269
Epoch 90/100 Iteration 111/234: loss=0.032599 lr=0.000020 grad_norm=0.436422
Epoch 90/100 Iteration 112/234: loss=0.033632 lr=0.000020 grad_norm=0.579547
Epoch 90/100 Iteration 113/234: loss=0.031057 lr=0.000020 grad_norm=0.512767
Epoch 90/100 Iteration 114/234: loss=0.029729 lr=0.000020 grad_norm=0.545690
Epoch 90/100 Iteration 115/234: loss=0.036444 lr=0.000020 grad_norm=0.657154
Epoch 90/100 Iteration 116/234: loss=0.035573 lr=0.000020 grad_norm=0.567011
Epoch 90/100 Iteration 117/234: loss=0.033551 lr=0.000020 grad_norm=0.552675
Epoch 90/100 Iteration 118/234: loss=0.033853 lr=0.000020 grad_norm=0.589234
Epoch 90/100 Iteration 119/234: loss=0.034017 lr=0.000020 grad_norm=0.466007
Epoch 90/100 Iteration 120/234: loss=0.039583 lr=0.000020 grad_norm=0.542719
Epoch 90/100 Iteration 121/234: loss=0.035382 lr=0.000020 grad_norm=0.824630
Epoch 90/100 Iteration 122/234: loss=0.033006 lr=0.000020 grad_norm=0.639063
Epoch 90/100 Iteration 123/234: loss=0.032063 lr=0.000020 grad_norm=0.508211
Epoch 90/100 Iteration 124/234: loss=0.035137 lr=0.000020 grad_norm=1.216523
Epoch 90/100 Iteration 125/234: loss=0.037640 lr=0.000020 grad_norm=1.196737
Epoch 90/100 Iteration 126/234: loss=0.031059 lr=0.000020 grad_norm=0.594076
Epoch 90/100 Iteration 127/234: loss=0.031089 lr=0.000020 grad_norm=0.673824
Epoch 90/100 Iteration 128/234: loss=0.035013 lr=0.000020 grad_norm=0.804318
Epoch 90/100 Iteration 129/234: loss=0.033667 lr=0.000020 grad_norm=0.745155
Epoch 90/100 Iteration 130/234: loss=0.030989 lr=0.000020 grad_norm=0.356356
Epoch 90/100 Iteration 131/234: loss=0.034945 lr=0.000020 grad_norm=0.751237
Epoch 90/100 Iteration 132/234: loss=0.034520 lr=0.000020 grad_norm=0.708795
Epoch 90/100 Iteration 133/234: loss=0.034352 lr=0.000020 grad_norm=0.478633
Epoch 90/100 Iteration 134/234: loss=0.035665 lr=0.000020 grad_norm=0.469523
Epoch 90/100 Iteration 135/234: loss=0.035742 lr=0.000020 grad_norm=0.695167
Epoch 90/100 Iteration 136/234: loss=0.035244 lr=0.000020 grad_norm=0.665133
Epoch 90/100 Iteration 137/234: loss=0.033437 lr=0.000020 grad_norm=0.410456
Epoch 90/100 Iteration 138/234: loss=0.033978 lr=0.000020 grad_norm=0.668509
Epoch 90/100 Iteration 139/234: loss=0.033125 lr=0.000020 grad_norm=0.945008
Epoch 90/100 Iteration 140/234: loss=0.033354 lr=0.000020 grad_norm=0.431797
Epoch 90/100 Iteration 141/234: loss=0.032770 lr=0.000020 grad_norm=0.690070
Epoch 90/100 Iteration 142/234: loss=0.037366 lr=0.000020 grad_norm=1.161593
Epoch 90/100 Iteration 143/234: loss=0.031412 lr=0.000020 grad_norm=0.879179
Epoch 90/100 Iteration 144/234: loss=0.036868 lr=0.000020 grad_norm=0.486384
Epoch 90/100 Iteration 145/234: loss=0.034357 lr=0.000020 grad_norm=0.612110
Epoch 90/100 Iteration 146/234: loss=0.036745 lr=0.000020 grad_norm=0.517097
Epoch 90/100 Iteration 147/234: loss=0.032309 lr=0.000020 grad_norm=0.490217
Epoch 90/100 Iteration 148/234: loss=0.037128 lr=0.000020 grad_norm=0.552774
Epoch 90/100 Iteration 149/234: loss=0.037441 lr=0.000020 grad_norm=0.628360
Epoch 90/100 Iteration 150/234: loss=0.031351 lr=0.000020 grad_norm=0.446607
Epoch 90/100 Iteration 151/234: loss=0.034649 lr=0.000020 grad_norm=0.462354
Epoch 90/100 Iteration 152/234: loss=0.036865 lr=0.000020 grad_norm=0.704030
Epoch 90/100 Iteration 153/234: loss=0.032870 lr=0.000020 grad_norm=0.521174
Epoch 90/100 Iteration 154/234: loss=0.038299 lr=0.000020 grad_norm=0.499486
Epoch 90/100 Iteration 155/234: loss=0.035990 lr=0.000020 grad_norm=0.533012
Epoch 90/100 Iteration 156/234: loss=0.039162 lr=0.000020 grad_norm=0.623173
Epoch 90/100 Iteration 157/234: loss=0.033231 lr=0.000020 grad_norm=0.757784
Epoch 90/100 Iteration 158/234: loss=0.034028 lr=0.000020 grad_norm=0.731355
Epoch 90/100 Iteration 159/234: loss=0.034806 lr=0.000020 grad_norm=0.366526
Epoch 90/100 Iteration 160/234: loss=0.033050 lr=0.000020 grad_norm=0.532578
Epoch 90/100 Iteration 161/234: loss=0.032402 lr=0.000020 grad_norm=0.615191
Epoch 90/100 Iteration 162/234: loss=0.034568 lr=0.000020 grad_norm=0.362704
Epoch 90/100 Iteration 163/234: loss=0.032617 lr=0.000020 grad_norm=0.623082
Epoch 90/100 Iteration 164/234: loss=0.030972 lr=0.000020 grad_norm=0.820975
Epoch 90/100 Iteration 165/234: loss=0.032346 lr=0.000020 grad_norm=0.626069
Epoch 90/100 Iteration 166/234: loss=0.032679 lr=0.000020 grad_norm=0.540832
Epoch 90/100 Iteration 167/234: loss=0.037383 lr=0.000020 grad_norm=1.252209
Epoch 90/100 Iteration 168/234: loss=0.033112 lr=0.000020 grad_norm=1.571776
Epoch 90/100 Iteration 169/234: loss=0.037544 lr=0.000020 grad_norm=0.931165
Epoch 90/100 Iteration 170/234: loss=0.033544 lr=0.000020 grad_norm=0.493923
Epoch 90/100 Iteration 171/234: loss=0.037393 lr=0.000020 grad_norm=1.289340
Epoch 90/100 Iteration 172/234: loss=0.035220 lr=0.000020 grad_norm=1.467252
Epoch 90/100 Iteration 173/234: loss=0.034932 lr=0.000020 grad_norm=1.098318
Epoch 90/100 Iteration 174/234: loss=0.033275 lr=0.000020 grad_norm=0.566261
Epoch 90/100 Iteration 175/234: loss=0.034636 lr=0.000020 grad_norm=0.895378
Epoch 90/100 Iteration 176/234: loss=0.035549 lr=0.000020 grad_norm=0.857097
Epoch 90/100 Iteration 177/234: loss=0.031673 lr=0.000020 grad_norm=0.423592
Epoch 90/100 Iteration 178/234: loss=0.032468 lr=0.000020 grad_norm=0.594743
Epoch 90/100 Iteration 179/234: loss=0.037165 lr=0.000020 grad_norm=0.486449
Epoch 90/100 Iteration 180/234: loss=0.035668 lr=0.000020 grad_norm=0.445491
Epoch 90/100 Iteration 181/234: loss=0.034066 lr=0.000020 grad_norm=0.548338
Epoch 90/100 Iteration 182/234: loss=0.032845 lr=0.000020 grad_norm=0.500484
Epoch 90/100 Iteration 183/234: loss=0.032234 lr=0.000020 grad_norm=0.463792
Epoch 90/100 Iteration 184/234: loss=0.032150 lr=0.000020 grad_norm=0.661198
Epoch 90/100 Iteration 185/234: loss=0.034702 lr=0.000020 grad_norm=0.656841
Epoch 90/100 Iteration 186/234: loss=0.035213 lr=0.000020 grad_norm=0.858504
Epoch 90/100 Iteration 187/234: loss=0.034736 lr=0.000020 grad_norm=1.336279
Epoch 90/100 Iteration 188/234: loss=0.034889 lr=0.000020 grad_norm=0.909775
Epoch 90/100 Iteration 189/234: loss=0.033949 lr=0.000020 grad_norm=0.352264
Epoch 90/100 Iteration 190/234: loss=0.032168 lr=0.000020 grad_norm=0.580010
Epoch 90/100 Iteration 191/234: loss=0.035236 lr=0.000020 grad_norm=0.738530
Epoch 90/100 Iteration 192/234: loss=0.036816 lr=0.000020 grad_norm=0.400923
Epoch 90/100 Iteration 193/234: loss=0.036007 lr=0.000020 grad_norm=0.892216
Epoch 90/100 Iteration 194/234: loss=0.035853 lr=0.000020 grad_norm=1.270757
Epoch 90/100 Iteration 195/234: loss=0.037013 lr=0.000020 grad_norm=0.795413
Epoch 90/100 Iteration 196/234: loss=0.035961 lr=0.000020 grad_norm=0.821708
Epoch 90/100 Iteration 197/234: loss=0.035383 lr=0.000020 grad_norm=1.142584
Epoch 90/100 Iteration 198/234: loss=0.030362 lr=0.000020 grad_norm=0.538392
Epoch 90/100 Iteration 199/234: loss=0.032182 lr=0.000020 grad_norm=0.920327
Epoch 90/100 Iteration 200/234: loss=0.035069 lr=0.000020 grad_norm=1.296899
Epoch 90/100 Iteration 201/234: loss=0.033367 lr=0.000020 grad_norm=0.753141
Epoch 90/100 Iteration 202/234: loss=0.035019 lr=0.000020 grad_norm=0.952124
Epoch 90/100 Iteration 203/234: loss=0.033744 lr=0.000020 grad_norm=0.908117
Epoch 90/100 Iteration 204/234: loss=0.036548 lr=0.000020 grad_norm=0.729726
Epoch 90/100 Iteration 205/234: loss=0.031225 lr=0.000020 grad_norm=0.913764
Epoch 90/100 Iteration 206/234: loss=0.033525 lr=0.000020 grad_norm=0.791105
Epoch 90/100 Iteration 207/234: loss=0.036260 lr=0.000020 grad_norm=0.835122
Epoch 90/100 Iteration 208/234: loss=0.034866 lr=0.000020 grad_norm=0.495416
Epoch 90/100 Iteration 209/234: loss=0.035117 lr=0.000020 grad_norm=0.851383
Epoch 90/100 Iteration 210/234: loss=0.033073 lr=0.000020 grad_norm=0.832843
Epoch 90/100 Iteration 211/234: loss=0.037014 lr=0.000020 grad_norm=0.677660
Epoch 90/100 Iteration 212/234: loss=0.030456 lr=0.000020 grad_norm=0.668839
Epoch 90/100 Iteration 213/234: loss=0.032564 lr=0.000020 grad_norm=0.649408
Epoch 90/100 Iteration 214/234: loss=0.035013 lr=0.000020 grad_norm=0.446274
Epoch 90/100 Iteration 215/234: loss=0.036245 lr=0.000020 grad_norm=0.777319
Epoch 90/100 Iteration 216/234: loss=0.034983 lr=0.000020 grad_norm=0.747648
Epoch 90/100 Iteration 217/234: loss=0.034516 lr=0.000020 grad_norm=0.464453
Epoch 90/100 Iteration 218/234: loss=0.033766 lr=0.000020 grad_norm=0.577752
Epoch 90/100 Iteration 219/234: loss=0.031606 lr=0.000020 grad_norm=0.561818
Epoch 90/100 Iteration 220/234: loss=0.038760 lr=0.000020 grad_norm=0.529822
Epoch 90/100 Iteration 221/234: loss=0.032171 lr=0.000020 grad_norm=0.636777
Epoch 90/100 Iteration 222/234: loss=0.032515 lr=0.000020 grad_norm=0.499729
Epoch 90/100 Iteration 223/234: loss=0.033263 lr=0.000020 grad_norm=0.906647
Epoch 90/100 Iteration 224/234: loss=0.034078 lr=0.000020 grad_norm=1.320534
Epoch 90/100 Iteration 225/234: loss=0.035389 lr=0.000020 grad_norm=0.675211
Epoch 90/100 Iteration 226/234: loss=0.031569 lr=0.000020 grad_norm=0.499067
Epoch 90/100 Iteration 227/234: loss=0.035034 lr=0.000020 grad_norm=0.795214
Epoch 90/100 Iteration 228/234: loss=0.034811 lr=0.000020 grad_norm=0.578587
Epoch 90/100 Iteration 229/234: loss=0.034056 lr=0.000020 grad_norm=0.492121
Epoch 90/100 Iteration 230/234: loss=0.031565 lr=0.000020 grad_norm=0.419706
Epoch 90/100 Iteration 231/234: loss=0.037302 lr=0.000020 grad_norm=0.476710
Epoch 90/100 Iteration 232/234: loss=0.034020 lr=0.000020 grad_norm=0.438875
Epoch 90/100 Iteration 233/234: loss=0.036754 lr=0.000020 grad_norm=0.726087
Epoch 90/100 Iteration 234/234: loss=0.035416 lr=0.000020 grad_norm=1.020897
Epoch 90/100 finished. Avg Loss: 0.034475
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 91/100 Iteration 1/234: loss=0.034040 lr=0.000020 grad_norm=0.848755
Epoch 91/100 Iteration 2/234: loss=0.034716 lr=0.000020 grad_norm=0.410231
Epoch 91/100 Iteration 3/234: loss=0.037806 lr=0.000020 grad_norm=1.241450
Epoch 91/100 Iteration 4/234: loss=0.036189 lr=0.000020 grad_norm=1.140899
Epoch 91/100 Iteration 5/234: loss=0.032681 lr=0.000020 grad_norm=0.500286
Epoch 91/100 Iteration 6/234: loss=0.035070 lr=0.000020 grad_norm=0.799107
Epoch 91/100 Iteration 7/234: loss=0.036347 lr=0.000020 grad_norm=0.594111
Epoch 91/100 Iteration 8/234: loss=0.034989 lr=0.000020 grad_norm=0.398328
Epoch 91/100 Iteration 9/234: loss=0.031454 lr=0.000020 grad_norm=0.504012
Epoch 91/100 Iteration 10/234: loss=0.036078 lr=0.000020 grad_norm=0.347528
Epoch 91/100 Iteration 11/234: loss=0.035558 lr=0.000020 grad_norm=0.536632
Epoch 91/100 Iteration 12/234: loss=0.033521 lr=0.000020 grad_norm=0.503111
Epoch 91/100 Iteration 13/234: loss=0.032401 lr=0.000020 grad_norm=0.492097
Epoch 91/100 Iteration 14/234: loss=0.036773 lr=0.000020 grad_norm=0.576749
Epoch 91/100 Iteration 15/234: loss=0.033277 lr=0.000020 grad_norm=0.512676
Epoch 91/100 Iteration 16/234: loss=0.036346 lr=0.000020 grad_norm=0.678857
Epoch 91/100 Iteration 17/234: loss=0.031407 lr=0.000020 grad_norm=0.915780
Epoch 91/100 Iteration 18/234: loss=0.033044 lr=0.000020 grad_norm=0.710626
Epoch 91/100 Iteration 19/234: loss=0.035788 lr=0.000020 grad_norm=0.477993
Epoch 91/100 Iteration 20/234: loss=0.031077 lr=0.000020 grad_norm=0.762196
Epoch 91/100 Iteration 21/234: loss=0.035119 lr=0.000020 grad_norm=0.985163
Epoch 91/100 Iteration 22/234: loss=0.036950 lr=0.000020 grad_norm=1.125943
Epoch 91/100 Iteration 23/234: loss=0.037839 lr=0.000020 grad_norm=0.784073
Epoch 91/100 Iteration 24/234: loss=0.037619 lr=0.000020 grad_norm=0.358966
Epoch 91/100 Iteration 25/234: loss=0.039498 lr=0.000020 grad_norm=0.567892
Epoch 91/100 Iteration 26/234: loss=0.037567 lr=0.000020 grad_norm=0.649149
Epoch 91/100 Iteration 27/234: loss=0.034968 lr=0.000020 grad_norm=0.693306
Epoch 91/100 Iteration 28/234: loss=0.035325 lr=0.000020 grad_norm=0.838807
Epoch 91/100 Iteration 29/234: loss=0.036488 lr=0.000020 grad_norm=0.947740
Epoch 91/100 Iteration 30/234: loss=0.036098 lr=0.000020 grad_norm=0.588269
Epoch 91/100 Iteration 31/234: loss=0.032571 lr=0.000020 grad_norm=0.587146
Epoch 91/100 Iteration 32/234: loss=0.037707 lr=0.000020 grad_norm=0.688745
Epoch 91/100 Iteration 33/234: loss=0.038041 lr=0.000020 grad_norm=0.803757
Epoch 91/100 Iteration 34/234: loss=0.035321 lr=0.000020 grad_norm=0.895768
Epoch 91/100 Iteration 35/234: loss=0.035583 lr=0.000020 grad_norm=1.029712
Epoch 91/100 Iteration 36/234: loss=0.036039 lr=0.000020 grad_norm=0.903000
Epoch 91/100 Iteration 37/234: loss=0.033895 lr=0.000020 grad_norm=0.544354
Epoch 91/100 Iteration 38/234: loss=0.033065 lr=0.000020 grad_norm=0.866592
Epoch 91/100 Iteration 39/234: loss=0.033924 lr=0.000020 grad_norm=0.631097
Epoch 91/100 Iteration 40/234: loss=0.034354 lr=0.000020 grad_norm=0.425224
Epoch 91/100 Iteration 41/234: loss=0.037027 lr=0.000020 grad_norm=0.786450
Epoch 91/100 Iteration 42/234: loss=0.031414 lr=0.000020 grad_norm=0.641287
Epoch 91/100 Iteration 43/234: loss=0.035865 lr=0.000020 grad_norm=0.529174
Epoch 91/100 Iteration 44/234: loss=0.035084 lr=0.000020 grad_norm=0.619795
Epoch 91/100 Iteration 45/234: loss=0.032766 lr=0.000020 grad_norm=1.086095
Epoch 91/100 Iteration 46/234: loss=0.035491 lr=0.000020 grad_norm=1.276102
Epoch 91/100 Iteration 47/234: loss=0.033465 lr=0.000020 grad_norm=0.845415
Epoch 91/100 Iteration 48/234: loss=0.032253 lr=0.000020 grad_norm=0.416200
Epoch 91/100 Iteration 49/234: loss=0.032279 lr=0.000020 grad_norm=0.814650
Epoch 91/100 Iteration 50/234: loss=0.038018 lr=0.000020 grad_norm=1.188990
Epoch 91/100 Iteration 51/234: loss=0.037832 lr=0.000020 grad_norm=1.123064
Epoch 91/100 Iteration 52/234: loss=0.040918 lr=0.000020 grad_norm=0.695607
Epoch 91/100 Iteration 53/234: loss=0.032403 lr=0.000020 grad_norm=0.542811
Epoch 91/100 Iteration 54/234: loss=0.031371 lr=0.000020 grad_norm=0.554369
Epoch 91/100 Iteration 55/234: loss=0.034584 lr=0.000020 grad_norm=0.593444
Epoch 91/100 Iteration 56/234: loss=0.036307 lr=0.000020 grad_norm=0.997375
Epoch 91/100 Iteration 57/234: loss=0.035834 lr=0.000020 grad_norm=0.808766
Epoch 91/100 Iteration 58/234: loss=0.032723 lr=0.000020 grad_norm=0.536521
Epoch 91/100 Iteration 59/234: loss=0.034457 lr=0.000020 grad_norm=1.166216
Epoch 91/100 Iteration 60/234: loss=0.036956 lr=0.000020 grad_norm=0.977567
Epoch 91/100 Iteration 61/234: loss=0.032172 lr=0.000020 grad_norm=0.580127
Epoch 91/100 Iteration 62/234: loss=0.035061 lr=0.000020 grad_norm=1.448863
Epoch 91/100 Iteration 63/234: loss=0.035602 lr=0.000020 grad_norm=0.955801
Epoch 91/100 Iteration 64/234: loss=0.034814 lr=0.000020 grad_norm=0.763493
Epoch 91/100 Iteration 65/234: loss=0.036775 lr=0.000020 grad_norm=1.462045
Epoch 91/100 Iteration 66/234: loss=0.030819 lr=0.000020 grad_norm=0.802907
Epoch 91/100 Iteration 67/234: loss=0.032330 lr=0.000020 grad_norm=0.896126
Epoch 91/100 Iteration 68/234: loss=0.034659 lr=0.000020 grad_norm=0.996239
Epoch 91/100 Iteration 69/234: loss=0.035190 lr=0.000020 grad_norm=0.681497
Epoch 91/100 Iteration 70/234: loss=0.035796 lr=0.000020 grad_norm=0.752399
Epoch 91/100 Iteration 71/234: loss=0.035267 lr=0.000020 grad_norm=0.836999
Epoch 91/100 Iteration 72/234: loss=0.033855 lr=0.000020 grad_norm=0.745314
Epoch 91/100 Iteration 73/234: loss=0.032435 lr=0.000020 grad_norm=0.838798
Epoch 91/100 Iteration 74/234: loss=0.035991 lr=0.000020 grad_norm=0.639158
Epoch 91/100 Iteration 75/234: loss=0.035703 lr=0.000020 grad_norm=0.556725
Epoch 91/100 Iteration 76/234: loss=0.036948 lr=0.000020 grad_norm=0.826290
Epoch 91/100 Iteration 77/234: loss=0.031406 lr=0.000020 grad_norm=0.685263
Epoch 91/100 Iteration 78/234: loss=0.035276 lr=0.000020 grad_norm=0.990284
Epoch 91/100 Iteration 79/234: loss=0.033662 lr=0.000020 grad_norm=0.809326
Epoch 91/100 Iteration 80/234: loss=0.031706 lr=0.000020 grad_norm=0.475950
Epoch 91/100 Iteration 81/234: loss=0.032167 lr=0.000020 grad_norm=0.797230
Epoch 91/100 Iteration 82/234: loss=0.036172 lr=0.000020 grad_norm=1.125666
Epoch 91/100 Iteration 83/234: loss=0.036156 lr=0.000020 grad_norm=0.529019
Epoch 91/100 Iteration 84/234: loss=0.033076 lr=0.000020 grad_norm=0.667072
Epoch 91/100 Iteration 85/234: loss=0.032074 lr=0.000020 grad_norm=0.553732
Epoch 91/100 Iteration 86/234: loss=0.035328 lr=0.000020 grad_norm=0.575126
Epoch 91/100 Iteration 87/234: loss=0.033731 lr=0.000020 grad_norm=0.885779
Epoch 91/100 Iteration 88/234: loss=0.034070 lr=0.000020 grad_norm=0.621679
Epoch 91/100 Iteration 89/234: loss=0.037366 lr=0.000020 grad_norm=0.690540
Epoch 91/100 Iteration 90/234: loss=0.032749 lr=0.000020 grad_norm=1.152832
Epoch 91/100 Iteration 91/234: loss=0.038919 lr=0.000020 grad_norm=0.759452
Epoch 91/100 Iteration 92/234: loss=0.035816 lr=0.000020 grad_norm=0.385018
Epoch 91/100 Iteration 93/234: loss=0.034357 lr=0.000020 grad_norm=0.595185
Epoch 91/100 Iteration 94/234: loss=0.035373 lr=0.000020 grad_norm=0.381220
Epoch 91/100 Iteration 95/234: loss=0.032500 lr=0.000020 grad_norm=0.515518
Epoch 91/100 Iteration 96/234: loss=0.034079 lr=0.000020 grad_norm=0.709543
Epoch 91/100 Iteration 97/234: loss=0.036435 lr=0.000020 grad_norm=0.401005
Epoch 91/100 Iteration 98/234: loss=0.034019 lr=0.000020 grad_norm=0.498916
Epoch 91/100 Iteration 99/234: loss=0.036671 lr=0.000020 grad_norm=0.416433
Epoch 91/100 Iteration 100/234: loss=0.035047 lr=0.000020 grad_norm=0.574551
Epoch 91/100 Iteration 101/234: loss=0.036965 lr=0.000020 grad_norm=0.720547
Epoch 91/100 Iteration 102/234: loss=0.035636 lr=0.000020 grad_norm=0.592533
Epoch 91/100 Iteration 103/234: loss=0.031989 lr=0.000020 grad_norm=0.434629
Epoch 91/100 Iteration 104/234: loss=0.033388 lr=0.000020 grad_norm=0.804396
Epoch 91/100 Iteration 105/234: loss=0.032223 lr=0.000020 grad_norm=0.747636
Epoch 91/100 Iteration 106/234: loss=0.032036 lr=0.000020 grad_norm=0.452906
Epoch 91/100 Iteration 107/234: loss=0.033895 lr=0.000020 grad_norm=0.395812
Epoch 91/100 Iteration 108/234: loss=0.034510 lr=0.000020 grad_norm=0.528791
Epoch 91/100 Iteration 109/234: loss=0.035108 lr=0.000020 grad_norm=0.598466
Epoch 91/100 Iteration 110/234: loss=0.033701 lr=0.000020 grad_norm=0.485511
Epoch 91/100 Iteration 111/234: loss=0.038561 lr=0.000020 grad_norm=0.477874
Epoch 91/100 Iteration 112/234: loss=0.030578 lr=0.000020 grad_norm=0.635329
Epoch 91/100 Iteration 113/234: loss=0.035698 lr=0.000020 grad_norm=0.801437
Epoch 91/100 Iteration 114/234: loss=0.033650 lr=0.000020 grad_norm=0.506618
Epoch 91/100 Iteration 115/234: loss=0.037473 lr=0.000020 grad_norm=0.813711
Epoch 91/100 Iteration 116/234: loss=0.035933 lr=0.000020 grad_norm=1.352464
Epoch 91/100 Iteration 117/234: loss=0.035425 lr=0.000020 grad_norm=1.028010
Epoch 91/100 Iteration 118/234: loss=0.035951 lr=0.000020 grad_norm=0.828011
Epoch 91/100 Iteration 119/234: loss=0.036467 lr=0.000020 grad_norm=1.135452
Epoch 91/100 Iteration 120/234: loss=0.033233 lr=0.000020 grad_norm=0.854517
Epoch 91/100 Iteration 121/234: loss=0.031817 lr=0.000020 grad_norm=0.880657
Epoch 91/100 Iteration 122/234: loss=0.035503 lr=0.000020 grad_norm=1.142194
Epoch 91/100 Iteration 123/234: loss=0.037767 lr=0.000020 grad_norm=1.312626
Epoch 91/100 Iteration 124/234: loss=0.034429 lr=0.000020 grad_norm=1.233851
Epoch 91/100 Iteration 125/234: loss=0.034986 lr=0.000020 grad_norm=1.027177
Epoch 91/100 Iteration 126/234: loss=0.031637 lr=0.000020 grad_norm=0.471922
Epoch 91/100 Iteration 127/234: loss=0.034958 lr=0.000020 grad_norm=1.021134
Epoch 91/100 Iteration 128/234: loss=0.034515 lr=0.000020 grad_norm=1.213846
Epoch 91/100 Iteration 129/234: loss=0.036206 lr=0.000020 grad_norm=0.681126
Epoch 91/100 Iteration 130/234: loss=0.034490 lr=0.000020 grad_norm=0.468856
Epoch 91/100 Iteration 131/234: loss=0.034780 lr=0.000020 grad_norm=0.993440
Epoch 91/100 Iteration 132/234: loss=0.036527 lr=0.000020 grad_norm=0.960666
Epoch 91/100 Iteration 133/234: loss=0.035568 lr=0.000020 grad_norm=0.592465
Epoch 91/100 Iteration 134/234: loss=0.032081 lr=0.000020 grad_norm=0.537351
Epoch 91/100 Iteration 135/234: loss=0.032761 lr=0.000020 grad_norm=0.920977
Epoch 91/100 Iteration 136/234: loss=0.033323 lr=0.000020 grad_norm=0.702014
Epoch 91/100 Iteration 137/234: loss=0.035453 lr=0.000020 grad_norm=0.624974
Epoch 91/100 Iteration 138/234: loss=0.030792 lr=0.000020 grad_norm=0.856145
Epoch 91/100 Iteration 139/234: loss=0.033893 lr=0.000020 grad_norm=0.549778
Epoch 91/100 Iteration 140/234: loss=0.037474 lr=0.000020 grad_norm=0.905280
Epoch 91/100 Iteration 141/234: loss=0.032078 lr=0.000020 grad_norm=0.567495
Epoch 91/100 Iteration 142/234: loss=0.035251 lr=0.000020 grad_norm=1.061981
Epoch 91/100 Iteration 143/234: loss=0.034503 lr=0.000020 grad_norm=1.377288
Epoch 91/100 Iteration 144/234: loss=0.032908 lr=0.000020 grad_norm=0.602999
Epoch 91/100 Iteration 145/234: loss=0.032603 lr=0.000020 grad_norm=0.804269
Epoch 91/100 Iteration 146/234: loss=0.033220 lr=0.000020 grad_norm=0.578091
Epoch 91/100 Iteration 147/234: loss=0.032907 lr=0.000020 grad_norm=0.756031
Epoch 91/100 Iteration 148/234: loss=0.032808 lr=0.000020 grad_norm=0.689205
Epoch 91/100 Iteration 149/234: loss=0.037526 lr=0.000020 grad_norm=0.555941
Epoch 91/100 Iteration 150/234: loss=0.037288 lr=0.000020 grad_norm=0.881079
Epoch 91/100 Iteration 151/234: loss=0.037754 lr=0.000020 grad_norm=0.861140
Epoch 91/100 Iteration 152/234: loss=0.035196 lr=0.000020 grad_norm=0.575681
Epoch 91/100 Iteration 153/234: loss=0.032701 lr=0.000020 grad_norm=1.095025
Epoch 91/100 Iteration 154/234: loss=0.034270 lr=0.000020 grad_norm=1.047822
Epoch 91/100 Iteration 155/234: loss=0.036081 lr=0.000020 grad_norm=0.447089
Epoch 91/100 Iteration 156/234: loss=0.033691 lr=0.000020 grad_norm=0.782607
Epoch 91/100 Iteration 157/234: loss=0.034404 lr=0.000020 grad_norm=0.892390
Epoch 91/100 Iteration 158/234: loss=0.033071 lr=0.000020 grad_norm=0.529348
Epoch 91/100 Iteration 159/234: loss=0.036680 lr=0.000020 grad_norm=0.406933
Epoch 91/100 Iteration 160/234: loss=0.034893 lr=0.000020 grad_norm=0.564591
Epoch 91/100 Iteration 161/234: loss=0.031403 lr=0.000020 grad_norm=0.417311
Epoch 91/100 Iteration 162/234: loss=0.034097 lr=0.000020 grad_norm=0.612865
Epoch 91/100 Iteration 163/234: loss=0.034498 lr=0.000020 grad_norm=0.532186
Epoch 91/100 Iteration 164/234: loss=0.035135 lr=0.000020 grad_norm=0.639927
Epoch 91/100 Iteration 165/234: loss=0.034870 lr=0.000020 grad_norm=0.498680
Epoch 91/100 Iteration 166/234: loss=0.034545 lr=0.000020 grad_norm=0.488752
Epoch 91/100 Iteration 167/234: loss=0.031932 lr=0.000020 grad_norm=0.481161
Epoch 91/100 Iteration 168/234: loss=0.032707 lr=0.000020 grad_norm=0.542884
Epoch 91/100 Iteration 169/234: loss=0.032507 lr=0.000020 grad_norm=0.718543
Epoch 91/100 Iteration 170/234: loss=0.037138 lr=0.000020 grad_norm=0.728744
Epoch 91/100 Iteration 171/234: loss=0.031275 lr=0.000020 grad_norm=0.452968
Epoch 91/100 Iteration 172/234: loss=0.033414 lr=0.000020 grad_norm=0.610738
Epoch 91/100 Iteration 173/234: loss=0.032601 lr=0.000020 grad_norm=0.816733
Epoch 91/100 Iteration 174/234: loss=0.033559 lr=0.000020 grad_norm=0.594410
Epoch 91/100 Iteration 175/234: loss=0.037252 lr=0.000020 grad_norm=1.245831
Epoch 91/100 Iteration 176/234: loss=0.034871 lr=0.000020 grad_norm=1.537583
Epoch 91/100 Iteration 177/234: loss=0.034350 lr=0.000020 grad_norm=0.826055
Epoch 91/100 Iteration 178/234: loss=0.037476 lr=0.000020 grad_norm=0.655691
Epoch 91/100 Iteration 179/234: loss=0.029944 lr=0.000020 grad_norm=0.917006
Epoch 91/100 Iteration 180/234: loss=0.035345 lr=0.000020 grad_norm=0.801616
Epoch 91/100 Iteration 181/234: loss=0.036401 lr=0.000020 grad_norm=0.576867
Epoch 91/100 Iteration 182/234: loss=0.035570 lr=0.000020 grad_norm=0.939422
Epoch 91/100 Iteration 183/234: loss=0.035533 lr=0.000020 grad_norm=1.234731
Epoch 91/100 Iteration 184/234: loss=0.035051 lr=0.000020 grad_norm=0.875330
Epoch 91/100 Iteration 185/234: loss=0.032529 lr=0.000020 grad_norm=1.433869
Epoch 91/100 Iteration 186/234: loss=0.035662 lr=0.000020 grad_norm=1.176648
Epoch 91/100 Iteration 187/234: loss=0.033740 lr=0.000020 grad_norm=0.607061
Epoch 91/100 Iteration 188/234: loss=0.036065 lr=0.000020 grad_norm=0.938382
Epoch 91/100 Iteration 189/234: loss=0.035240 lr=0.000020 grad_norm=1.571370
Epoch 91/100 Iteration 190/234: loss=0.031813 lr=0.000020 grad_norm=1.268661
Epoch 91/100 Iteration 191/234: loss=0.035917 lr=0.000020 grad_norm=0.826124
Epoch 91/100 Iteration 192/234: loss=0.033995 lr=0.000020 grad_norm=2.999784
Epoch 91/100 Iteration 193/234: loss=0.036978 lr=0.000020 grad_norm=3.082588
Epoch 91/100 Iteration 194/234: loss=0.032905 lr=0.000020 grad_norm=0.651875
Epoch 91/100 Iteration 195/234: loss=0.036165 lr=0.000020 grad_norm=2.270518
Epoch 91/100 Iteration 196/234: loss=0.032349 lr=0.000020 grad_norm=1.064467
Epoch 91/100 Iteration 197/234: loss=0.035813 lr=0.000020 grad_norm=1.550346
Epoch 91/100 Iteration 198/234: loss=0.035542 lr=0.000020 grad_norm=1.134935
Epoch 91/100 Iteration 199/234: loss=0.036110 lr=0.000020 grad_norm=1.576861
Epoch 91/100 Iteration 200/234: loss=0.034104 lr=0.000020 grad_norm=1.309426
Epoch 91/100 Iteration 201/234: loss=0.031944 lr=0.000020 grad_norm=0.819672
Epoch 91/100 Iteration 202/234: loss=0.036318 lr=0.000020 grad_norm=0.979017
Epoch 91/100 Iteration 203/234: loss=0.033084 lr=0.000020 grad_norm=0.473418
Epoch 91/100 Iteration 204/234: loss=0.034386 lr=0.000020 grad_norm=0.845025
Epoch 91/100 Iteration 205/234: loss=0.035621 lr=0.000020 grad_norm=0.652311
Epoch 91/100 Iteration 206/234: loss=0.032330 lr=0.000020 grad_norm=0.527376
Epoch 91/100 Iteration 207/234: loss=0.033674 lr=0.000020 grad_norm=0.868809
Epoch 91/100 Iteration 208/234: loss=0.035366 lr=0.000020 grad_norm=0.997292
Epoch 91/100 Iteration 209/234: loss=0.034241 lr=0.000020 grad_norm=0.820066
Epoch 91/100 Iteration 210/234: loss=0.037890 lr=0.000020 grad_norm=0.739532
Epoch 91/100 Iteration 211/234: loss=0.034937 lr=0.000020 grad_norm=1.099848
Epoch 91/100 Iteration 212/234: loss=0.032828 lr=0.000020 grad_norm=0.390697
Epoch 91/100 Iteration 213/234: loss=0.035053 lr=0.000020 grad_norm=1.257325
Epoch 91/100 Iteration 214/234: loss=0.037687 lr=0.000020 grad_norm=0.724840
Epoch 91/100 Iteration 215/234: loss=0.034742 lr=0.000020 grad_norm=0.924305
Epoch 91/100 Iteration 216/234: loss=0.036064 lr=0.000020 grad_norm=0.949661
Epoch 91/100 Iteration 217/234: loss=0.035343 lr=0.000020 grad_norm=0.891191
Epoch 91/100 Iteration 218/234: loss=0.033137 lr=0.000020 grad_norm=0.866558
Epoch 91/100 Iteration 219/234: loss=0.033684 lr=0.000020 grad_norm=0.806436
Epoch 91/100 Iteration 220/234: loss=0.031350 lr=0.000020 grad_norm=0.766517
Epoch 91/100 Iteration 221/234: loss=0.035911 lr=0.000020 grad_norm=0.428940
Epoch 91/100 Iteration 222/234: loss=0.032368 lr=0.000020 grad_norm=0.680318
Epoch 91/100 Iteration 223/234: loss=0.035607 lr=0.000020 grad_norm=0.671647
Epoch 91/100 Iteration 224/234: loss=0.031909 lr=0.000020 grad_norm=0.427025
Epoch 91/100 Iteration 225/234: loss=0.031211 lr=0.000020 grad_norm=0.603369
Epoch 91/100 Iteration 226/234: loss=0.031964 lr=0.000020 grad_norm=0.588431
Epoch 91/100 Iteration 227/234: loss=0.032537 lr=0.000020 grad_norm=0.383817
Epoch 91/100 Iteration 228/234: loss=0.036143 lr=0.000020 grad_norm=0.629863
Epoch 91/100 Iteration 229/234: loss=0.036135 lr=0.000020 grad_norm=0.600920
Epoch 91/100 Iteration 230/234: loss=0.033089 lr=0.000020 grad_norm=0.595326
Epoch 91/100 Iteration 231/234: loss=0.032447 lr=0.000020 grad_norm=0.676007
Epoch 91/100 Iteration 232/234: loss=0.035846 lr=0.000020 grad_norm=0.461673
Epoch 91/100 Iteration 233/234: loss=0.033526 lr=0.000020 grad_norm=0.674090
Epoch 91/100 Iteration 234/234: loss=0.033418 lr=0.000020 grad_norm=0.935158
Epoch 91/100 finished. Avg Loss: 0.034592
Epoch 92/100 Iteration 1/234: loss=0.030882 lr=0.000020 grad_norm=0.958637
Epoch 92/100 Iteration 2/234: loss=0.034615 lr=0.000020 grad_norm=0.920480
Epoch 92/100 Iteration 3/234: loss=0.031523 lr=0.000020 grad_norm=0.558073
Epoch 92/100 Iteration 4/234: loss=0.035872 lr=0.000020 grad_norm=0.635476
Epoch 92/100 Iteration 5/234: loss=0.033548 lr=0.000020 grad_norm=0.775948
Epoch 92/100 Iteration 6/234: loss=0.034664 lr=0.000020 grad_norm=1.062260
Epoch 92/100 Iteration 7/234: loss=0.034127 lr=0.000020 grad_norm=1.461407
Epoch 92/100 Iteration 8/234: loss=0.031664 lr=0.000020 grad_norm=0.755019
Epoch 92/100 Iteration 9/234: loss=0.036259 lr=0.000020 grad_norm=1.145749
Epoch 92/100 Iteration 10/234: loss=0.034149 lr=0.000020 grad_norm=1.122596
Epoch 92/100 Iteration 11/234: loss=0.035415 lr=0.000020 grad_norm=0.697785
Epoch 92/100 Iteration 12/234: loss=0.036458 lr=0.000020 grad_norm=0.990976
Epoch 92/100 Iteration 13/234: loss=0.033359 lr=0.000020 grad_norm=0.767358
Epoch 92/100 Iteration 14/234: loss=0.035386 lr=0.000020 grad_norm=0.797895
Epoch 92/100 Iteration 15/234: loss=0.033577 lr=0.000020 grad_norm=0.815106
Epoch 92/100 Iteration 16/234: loss=0.035619 lr=0.000020 grad_norm=0.807186
Epoch 92/100 Iteration 17/234: loss=0.032928 lr=0.000020 grad_norm=0.923246
Epoch 92/100 Iteration 18/234: loss=0.031873 lr=0.000020 grad_norm=0.458898
Epoch 92/100 Iteration 19/234: loss=0.035301 lr=0.000020 grad_norm=0.661563
Epoch 92/100 Iteration 20/234: loss=0.036645 lr=0.000020 grad_norm=0.683398
Epoch 92/100 Iteration 21/234: loss=0.034517 lr=0.000020 grad_norm=0.402941
Epoch 92/100 Iteration 22/234: loss=0.034718 lr=0.000020 grad_norm=0.588639
Epoch 92/100 Iteration 23/234: loss=0.033845 lr=0.000020 grad_norm=0.603368
Epoch 92/100 Iteration 24/234: loss=0.029283 lr=0.000020 grad_norm=0.343045
Epoch 92/100 Iteration 25/234: loss=0.034515 lr=0.000020 grad_norm=0.561662
Epoch 92/100 Iteration 26/234: loss=0.031754 lr=0.000020 grad_norm=0.548553
Epoch 92/100 Iteration 27/234: loss=0.036346 lr=0.000020 grad_norm=0.450856
Epoch 92/100 Iteration 28/234: loss=0.033154 lr=0.000020 grad_norm=0.505590
Epoch 92/100 Iteration 29/234: loss=0.034650 lr=0.000020 grad_norm=0.644455
Epoch 92/100 Iteration 30/234: loss=0.034638 lr=0.000020 grad_norm=0.369527
Epoch 92/100 Iteration 31/234: loss=0.034208 lr=0.000020 grad_norm=0.671356
Epoch 92/100 Iteration 32/234: loss=0.029680 lr=0.000020 grad_norm=0.658677
Epoch 92/100 Iteration 33/234: loss=0.032209 lr=0.000020 grad_norm=0.599252
Epoch 92/100 Iteration 34/234: loss=0.033080 lr=0.000020 grad_norm=0.678824
Epoch 92/100 Iteration 35/234: loss=0.034584 lr=0.000020 grad_norm=0.810274
Epoch 92/100 Iteration 36/234: loss=0.031731 lr=0.000020 grad_norm=0.821388
Epoch 92/100 Iteration 37/234: loss=0.035592 lr=0.000020 grad_norm=0.371428
Epoch 92/100 Iteration 38/234: loss=0.031860 lr=0.000020 grad_norm=0.466718
Epoch 92/100 Iteration 39/234: loss=0.034865 lr=0.000020 grad_norm=0.380687
Epoch 92/100 Iteration 40/234: loss=0.034307 lr=0.000020 grad_norm=0.366582
Epoch 92/100 Iteration 41/234: loss=0.034693 lr=0.000020 grad_norm=0.344607
Epoch 92/100 Iteration 42/234: loss=0.032541 lr=0.000020 grad_norm=0.378787
Epoch 92/100 Iteration 43/234: loss=0.033847 lr=0.000020 grad_norm=0.429585
Epoch 92/100 Iteration 44/234: loss=0.035437 lr=0.000020 grad_norm=0.520758
Epoch 92/100 Iteration 45/234: loss=0.035509 lr=0.000020 grad_norm=0.684909
Epoch 92/100 Iteration 46/234: loss=0.038844 lr=0.000020 grad_norm=1.255155
Epoch 92/100 Iteration 47/234: loss=0.034419 lr=0.000020 grad_norm=1.354630
Epoch 92/100 Iteration 48/234: loss=0.036244 lr=0.000020 grad_norm=0.746667
Epoch 92/100 Iteration 49/234: loss=0.039060 lr=0.000020 grad_norm=0.948313
Epoch 92/100 Iteration 50/234: loss=0.034408 lr=0.000020 grad_norm=1.345259
Epoch 92/100 Iteration 51/234: loss=0.032251 lr=0.000020 grad_norm=0.773011
Epoch 92/100 Iteration 52/234: loss=0.035664 lr=0.000020 grad_norm=1.081211
Epoch 92/100 Iteration 53/234: loss=0.034719 lr=0.000020 grad_norm=1.244790
Epoch 92/100 Iteration 54/234: loss=0.036251 lr=0.000020 grad_norm=0.730387
Epoch 92/100 Iteration 55/234: loss=0.034730 lr=0.000020 grad_norm=1.002193
Epoch 92/100 Iteration 56/234: loss=0.035440 lr=0.000020 grad_norm=0.891693
Epoch 92/100 Iteration 57/234: loss=0.034643 lr=0.000020 grad_norm=0.795023
Epoch 92/100 Iteration 58/234: loss=0.033129 lr=0.000020 grad_norm=0.569196
Epoch 92/100 Iteration 59/234: loss=0.034017 lr=0.000020 grad_norm=0.892169
Epoch 92/100 Iteration 60/234: loss=0.033724 lr=0.000020 grad_norm=0.875355
Epoch 92/100 Iteration 61/234: loss=0.036193 lr=0.000020 grad_norm=0.810025
Epoch 92/100 Iteration 62/234: loss=0.030941 lr=0.000020 grad_norm=0.957237
Epoch 92/100 Iteration 63/234: loss=0.032200 lr=0.000020 grad_norm=0.512992
Epoch 92/100 Iteration 64/234: loss=0.037587 lr=0.000020 grad_norm=0.857224
Epoch 92/100 Iteration 65/234: loss=0.034706 lr=0.000020 grad_norm=0.769454
Epoch 92/100 Iteration 66/234: loss=0.034502 lr=0.000020 grad_norm=0.530426
Epoch 92/100 Iteration 67/234: loss=0.031346 lr=0.000020 grad_norm=0.626716
Epoch 92/100 Iteration 68/234: loss=0.037162 lr=0.000020 grad_norm=0.950539
Epoch 92/100 Iteration 69/234: loss=0.035707 lr=0.000020 grad_norm=0.537557
Epoch 92/100 Iteration 70/234: loss=0.034526 lr=0.000020 grad_norm=0.653883
Epoch 92/100 Iteration 71/234: loss=0.035644 lr=0.000020 grad_norm=1.018316
Epoch 92/100 Iteration 72/234: loss=0.037223 lr=0.000020 grad_norm=0.654662
Epoch 92/100 Iteration 73/234: loss=0.033188 lr=0.000020 grad_norm=0.839666
Epoch 92/100 Iteration 74/234: loss=0.031029 lr=0.000020 grad_norm=0.721604
Epoch 92/100 Iteration 75/234: loss=0.034802 lr=0.000020 grad_norm=0.730231
Epoch 92/100 Iteration 76/234: loss=0.034664 lr=0.000020 grad_norm=1.236789
Epoch 92/100 Iteration 77/234: loss=0.036081 lr=0.000020 grad_norm=0.632288
Epoch 92/100 Iteration 78/234: loss=0.037313 lr=0.000020 grad_norm=0.968460
Epoch 92/100 Iteration 79/234: loss=0.033305 lr=0.000020 grad_norm=0.952608
Epoch 92/100 Iteration 80/234: loss=0.030767 lr=0.000020 grad_norm=0.730923
Epoch 92/100 Iteration 81/234: loss=0.032093 lr=0.000020 grad_norm=1.369930
Epoch 92/100 Iteration 82/234: loss=0.032227 lr=0.000020 grad_norm=0.760605
Epoch 92/100 Iteration 83/234: loss=0.034119 lr=0.000020 grad_norm=0.948343
Epoch 92/100 Iteration 84/234: loss=0.037564 lr=0.000020 grad_norm=1.392040
Epoch 92/100 Iteration 85/234: loss=0.035739 lr=0.000020 grad_norm=0.879933
Epoch 92/100 Iteration 86/234: loss=0.033502 lr=0.000020 grad_norm=0.729326
Epoch 92/100 Iteration 87/234: loss=0.032157 lr=0.000020 grad_norm=0.868993
Epoch 92/100 Iteration 88/234: loss=0.031313 lr=0.000020 grad_norm=0.569812
Epoch 92/100 Iteration 89/234: loss=0.032308 lr=0.000020 grad_norm=1.035153
Epoch 92/100 Iteration 90/234: loss=0.038002 lr=0.000020 grad_norm=0.787794
Epoch 92/100 Iteration 91/234: loss=0.031778 lr=0.000020 grad_norm=0.594934
Epoch 92/100 Iteration 92/234: loss=0.034250 lr=0.000020 grad_norm=0.601337
Epoch 92/100 Iteration 93/234: loss=0.033426 lr=0.000020 grad_norm=0.848383
Epoch 92/100 Iteration 94/234: loss=0.036790 lr=0.000020 grad_norm=0.797004
Epoch 92/100 Iteration 95/234: loss=0.031332 lr=0.000020 grad_norm=0.474002
Epoch 92/100 Iteration 96/234: loss=0.038246 lr=0.000020 grad_norm=0.627320
Epoch 92/100 Iteration 97/234: loss=0.034655 lr=0.000020 grad_norm=0.686321
Epoch 92/100 Iteration 98/234: loss=0.030358 lr=0.000020 grad_norm=0.392703
Epoch 92/100 Iteration 99/234: loss=0.038796 lr=0.000020 grad_norm=0.728128
Epoch 92/100 Iteration 100/234: loss=0.035593 lr=0.000020 grad_norm=0.954839
Epoch 92/100 Iteration 101/234: loss=0.039947 lr=0.000020 grad_norm=1.198017
Epoch 92/100 Iteration 102/234: loss=0.034313 lr=0.000020 grad_norm=1.364738
Epoch 92/100 Iteration 103/234: loss=0.032468 lr=0.000020 grad_norm=0.750664
Epoch 92/100 Iteration 104/234: loss=0.032752 lr=0.000020 grad_norm=0.716292
Epoch 92/100 Iteration 105/234: loss=0.038195 lr=0.000020 grad_norm=1.331238
Epoch 92/100 Iteration 106/234: loss=0.033665 lr=0.000020 grad_norm=1.104920
Epoch 92/100 Iteration 107/234: loss=0.034181 lr=0.000020 grad_norm=0.433083
Epoch 92/100 Iteration 108/234: loss=0.034796 lr=0.000020 grad_norm=1.069077
Epoch 92/100 Iteration 109/234: loss=0.033979 lr=0.000020 grad_norm=1.044942
Epoch 92/100 Iteration 110/234: loss=0.033395 lr=0.000020 grad_norm=0.541937
Epoch 92/100 Iteration 111/234: loss=0.034706 lr=0.000020 grad_norm=0.911118
Epoch 92/100 Iteration 112/234: loss=0.030857 lr=0.000020 grad_norm=0.860480
Epoch 92/100 Iteration 113/234: loss=0.037263 lr=0.000020 grad_norm=0.652083
Epoch 92/100 Iteration 114/234: loss=0.037082 lr=0.000020 grad_norm=1.375872
Epoch 92/100 Iteration 115/234: loss=0.036294 lr=0.000020 grad_norm=1.131506
Epoch 92/100 Iteration 116/234: loss=0.035315 lr=0.000020 grad_norm=0.410671
Epoch 92/100 Iteration 117/234: loss=0.032068 lr=0.000020 grad_norm=0.871639
Epoch 92/100 Iteration 118/234: loss=0.032868 lr=0.000020 grad_norm=0.588534
Epoch 92/100 Iteration 119/234: loss=0.029812 lr=0.000020 grad_norm=0.469835
Epoch 92/100 Iteration 120/234: loss=0.034984 lr=0.000020 grad_norm=0.919869
Epoch 92/100 Iteration 121/234: loss=0.032849 lr=0.000020 grad_norm=0.490525
Epoch 92/100 Iteration 122/234: loss=0.036229 lr=0.000020 grad_norm=0.662520
Epoch 92/100 Iteration 123/234: loss=0.032798 lr=0.000020 grad_norm=0.705580
Epoch 92/100 Iteration 124/234: loss=0.031580 lr=0.000020 grad_norm=0.340555
Epoch 92/100 Iteration 125/234: loss=0.032662 lr=0.000020 grad_norm=0.744659
Epoch 92/100 Iteration 126/234: loss=0.035540 lr=0.000020 grad_norm=1.138509
Epoch 92/100 Iteration 127/234: loss=0.038086 lr=0.000020 grad_norm=0.658830
Epoch 92/100 Iteration 128/234: loss=0.037473 lr=0.000020 grad_norm=0.932111
Epoch 92/100 Iteration 129/234: loss=0.032772 lr=0.000020 grad_norm=1.536431
Epoch 92/100 Iteration 130/234: loss=0.030971 lr=0.000020 grad_norm=0.909279
Epoch 92/100 Iteration 131/234: loss=0.035439 lr=0.000020 grad_norm=0.995674
Epoch 92/100 Iteration 132/234: loss=0.031455 lr=0.000020 grad_norm=1.476178
Epoch 92/100 Iteration 133/234: loss=0.033086 lr=0.000020 grad_norm=0.364700
Epoch 92/100 Iteration 134/234: loss=0.032816 lr=0.000020 grad_norm=0.968601
Epoch 92/100 Iteration 135/234: loss=0.034573 lr=0.000020 grad_norm=0.508510
Epoch 92/100 Iteration 136/234: loss=0.034526 lr=0.000020 grad_norm=1.132833
Epoch 92/100 Iteration 137/234: loss=0.032406 lr=0.000020 grad_norm=1.489295
Epoch 92/100 Iteration 138/234: loss=0.035450 lr=0.000020 grad_norm=0.534007
Epoch 92/100 Iteration 139/234: loss=0.035841 lr=0.000020 grad_norm=1.384316
Epoch 92/100 Iteration 140/234: loss=0.031489 lr=0.000020 grad_norm=1.301684
Epoch 92/100 Iteration 141/234: loss=0.030294 lr=0.000020 grad_norm=0.358868
Epoch 92/100 Iteration 142/234: loss=0.034508 lr=0.000020 grad_norm=1.068401
Epoch 92/100 Iteration 143/234: loss=0.033768 lr=0.000020 grad_norm=0.852901
Epoch 92/100 Iteration 144/234: loss=0.032840 lr=0.000020 grad_norm=0.641455
Epoch 92/100 Iteration 145/234: loss=0.036105 lr=0.000020 grad_norm=1.515747
Epoch 92/100 Iteration 146/234: loss=0.033798 lr=0.000020 grad_norm=1.175964
Epoch 92/100 Iteration 147/234: loss=0.030579 lr=0.000020 grad_norm=0.426068
Epoch 92/100 Iteration 148/234: loss=0.035082 lr=0.000020 grad_norm=1.054462
Epoch 92/100 Iteration 149/234: loss=0.035187 lr=0.000020 grad_norm=0.734358
Epoch 92/100 Iteration 150/234: loss=0.034427 lr=0.000020 grad_norm=0.381359
Epoch 92/100 Iteration 151/234: loss=0.035688 lr=0.000020 grad_norm=0.764783
Epoch 92/100 Iteration 152/234: loss=0.035012 lr=0.000020 grad_norm=0.758286
Epoch 92/100 Iteration 153/234: loss=0.035458 lr=0.000020 grad_norm=0.409403
Epoch 92/100 Iteration 154/234: loss=0.036926 lr=0.000020 grad_norm=0.753386
Epoch 92/100 Iteration 155/234: loss=0.035241 lr=0.000020 grad_norm=0.975516
Epoch 92/100 Iteration 156/234: loss=0.031953 lr=0.000020 grad_norm=0.619304
Epoch 92/100 Iteration 157/234: loss=0.034280 lr=0.000020 grad_norm=0.543650
Epoch 92/100 Iteration 158/234: loss=0.034253 lr=0.000020 grad_norm=0.443170
Epoch 92/100 Iteration 159/234: loss=0.032825 lr=0.000020 grad_norm=0.510296
Epoch 92/100 Iteration 160/234: loss=0.035401 lr=0.000020 grad_norm=0.546296
Epoch 92/100 Iteration 161/234: loss=0.033472 lr=0.000020 grad_norm=0.406157
Epoch 92/100 Iteration 162/234: loss=0.030400 lr=0.000020 grad_norm=0.608213
Epoch 92/100 Iteration 163/234: loss=0.032555 lr=0.000020 grad_norm=0.424811
Epoch 92/100 Iteration 164/234: loss=0.034416 lr=0.000020 grad_norm=0.404462
Epoch 92/100 Iteration 165/234: loss=0.031129 lr=0.000020 grad_norm=0.573401
Epoch 92/100 Iteration 166/234: loss=0.031962 lr=0.000020 grad_norm=0.397587
Epoch 92/100 Iteration 167/234: loss=0.035921 lr=0.000020 grad_norm=0.438230
Epoch 92/100 Iteration 168/234: loss=0.033020 lr=0.000020 grad_norm=0.771957
Epoch 92/100 Iteration 169/234: loss=0.035816 lr=0.000020 grad_norm=0.708594
Epoch 92/100 Iteration 170/234: loss=0.031100 lr=0.000020 grad_norm=0.336922
Epoch 92/100 Iteration 171/234: loss=0.037307 lr=0.000020 grad_norm=0.485756
Epoch 92/100 Iteration 172/234: loss=0.034095 lr=0.000020 grad_norm=0.614689
Epoch 92/100 Iteration 173/234: loss=0.032763 lr=0.000020 grad_norm=0.482497
Epoch 92/100 Iteration 174/234: loss=0.032283 lr=0.000020 grad_norm=0.434026
Epoch 92/100 Iteration 175/234: loss=0.033494 lr=0.000020 grad_norm=0.693204
Epoch 92/100 Iteration 176/234: loss=0.032918 lr=0.000020 grad_norm=0.650331
Epoch 92/100 Iteration 177/234: loss=0.034305 lr=0.000020 grad_norm=1.098883
Epoch 92/100 Iteration 178/234: loss=0.034188 lr=0.000020 grad_norm=1.098642
Epoch 92/100 Iteration 179/234: loss=0.035250 lr=0.000020 grad_norm=0.721515
Epoch 92/100 Iteration 180/234: loss=0.031221 lr=0.000020 grad_norm=1.289970
Epoch 92/100 Iteration 181/234: loss=0.033117 lr=0.000020 grad_norm=0.782757
Epoch 92/100 Iteration 182/234: loss=0.034465 lr=0.000020 grad_norm=0.866772
Epoch 92/100 Iteration 183/234: loss=0.036774 lr=0.000020 grad_norm=1.614574
Epoch 92/100 Iteration 184/234: loss=0.037938 lr=0.000020 grad_norm=1.265739
Epoch 92/100 Iteration 185/234: loss=0.036565 lr=0.000020 grad_norm=0.438398
Epoch 92/100 Iteration 186/234: loss=0.034526 lr=0.000020 grad_norm=0.985124
Epoch 92/100 Iteration 187/234: loss=0.033047 lr=0.000020 grad_norm=1.292452
Epoch 92/100 Iteration 188/234: loss=0.030904 lr=0.000020 grad_norm=0.665864
Epoch 92/100 Iteration 189/234: loss=0.036811 lr=0.000020 grad_norm=0.872452
Epoch 92/100 Iteration 190/234: loss=0.030614 lr=0.000020 grad_norm=1.039753
Epoch 92/100 Iteration 191/234: loss=0.032026 lr=0.000020 grad_norm=0.406058
Epoch 92/100 Iteration 192/234: loss=0.032141 lr=0.000020 grad_norm=0.910149
Epoch 92/100 Iteration 193/234: loss=0.033524 lr=0.000020 grad_norm=0.507988
Epoch 92/100 Iteration 194/234: loss=0.035816 lr=0.000020 grad_norm=0.967492
Epoch 92/100 Iteration 195/234: loss=0.034709 lr=0.000020 grad_norm=0.915485
Epoch 92/100 Iteration 196/234: loss=0.031840 lr=0.000020 grad_norm=0.664751
Epoch 92/100 Iteration 197/234: loss=0.030560 lr=0.000020 grad_norm=0.965858
Epoch 92/100 Iteration 198/234: loss=0.032542 lr=0.000020 grad_norm=0.577135
Epoch 92/100 Iteration 199/234: loss=0.033760 lr=0.000020 grad_norm=0.931817
Epoch 92/100 Iteration 200/234: loss=0.033638 lr=0.000020 grad_norm=0.764526
Epoch 92/100 Iteration 201/234: loss=0.036277 lr=0.000020 grad_norm=0.822953
Epoch 92/100 Iteration 202/234: loss=0.037413 lr=0.000020 grad_norm=1.153453
Epoch 92/100 Iteration 203/234: loss=0.032972 lr=0.000020 grad_norm=0.756173
Epoch 92/100 Iteration 204/234: loss=0.032529 lr=0.000020 grad_norm=0.953225
Epoch 92/100 Iteration 205/234: loss=0.035599 lr=0.000020 grad_norm=0.878870
Epoch 92/100 Iteration 206/234: loss=0.031272 lr=0.000020 grad_norm=0.567678
Epoch 92/100 Iteration 207/234: loss=0.034498 lr=0.000020 grad_norm=0.703394
Epoch 92/100 Iteration 208/234: loss=0.037904 lr=0.000020 grad_norm=0.774430
Epoch 92/100 Iteration 209/234: loss=0.036357 lr=0.000020 grad_norm=0.918289
Epoch 92/100 Iteration 210/234: loss=0.035067 lr=0.000020 grad_norm=0.537391
Epoch 92/100 Iteration 211/234: loss=0.035169 lr=0.000020 grad_norm=0.737161
Epoch 92/100 Iteration 212/234: loss=0.035081 lr=0.000020 grad_norm=0.702588
Epoch 92/100 Iteration 213/234: loss=0.032819 lr=0.000020 grad_norm=0.462319
Epoch 92/100 Iteration 214/234: loss=0.033766 lr=0.000020 grad_norm=0.803843
Epoch 92/100 Iteration 215/234: loss=0.032672 lr=0.000020 grad_norm=0.713332
Epoch 92/100 Iteration 216/234: loss=0.032693 lr=0.000020 grad_norm=0.882035
Epoch 92/100 Iteration 217/234: loss=0.034977 lr=0.000020 grad_norm=0.646204
Epoch 92/100 Iteration 218/234: loss=0.033007 lr=0.000020 grad_norm=0.605898
Epoch 92/100 Iteration 219/234: loss=0.034758 lr=0.000020 grad_norm=1.017683
Epoch 92/100 Iteration 220/234: loss=0.032696 lr=0.000020 grad_norm=0.593213
Epoch 92/100 Iteration 221/234: loss=0.032850 lr=0.000020 grad_norm=0.837543
Epoch 92/100 Iteration 222/234: loss=0.035727 lr=0.000020 grad_norm=0.896416
Epoch 92/100 Iteration 223/234: loss=0.032039 lr=0.000020 grad_norm=0.489103
Epoch 92/100 Iteration 224/234: loss=0.030307 lr=0.000020 grad_norm=0.618727
Epoch 92/100 Iteration 225/234: loss=0.033104 lr=0.000020 grad_norm=0.401286
Epoch 92/100 Iteration 226/234: loss=0.034563 lr=0.000020 grad_norm=0.761854
Epoch 92/100 Iteration 227/234: loss=0.032362 lr=0.000020 grad_norm=0.581339
Epoch 92/100 Iteration 228/234: loss=0.031268 lr=0.000020 grad_norm=0.590369
Epoch 92/100 Iteration 229/234: loss=0.035441 lr=0.000020 grad_norm=0.712861
Epoch 92/100 Iteration 230/234: loss=0.032660 lr=0.000020 grad_norm=0.616201
Epoch 92/100 Iteration 231/234: loss=0.033004 lr=0.000020 grad_norm=1.070709
Epoch 92/100 Iteration 232/234: loss=0.036374 lr=0.000020 grad_norm=0.780013
Epoch 92/100 Iteration 233/234: loss=0.036729 lr=0.000020 grad_norm=0.860643
Epoch 92/100 Iteration 234/234: loss=0.036956 lr=0.000020 grad_norm=0.900168
Epoch 92/100 finished. Avg Loss: 0.034105
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 93/100 Iteration 1/234: loss=0.031342 lr=0.000020 grad_norm=0.594097
Epoch 93/100 Iteration 2/234: loss=0.038021 lr=0.000020 grad_norm=1.080808
Epoch 93/100 Iteration 3/234: loss=0.036323 lr=0.000020 grad_norm=1.046893
Epoch 93/100 Iteration 4/234: loss=0.034919 lr=0.000020 grad_norm=0.663466
Epoch 93/100 Iteration 5/234: loss=0.036211 lr=0.000020 grad_norm=1.114897
Epoch 93/100 Iteration 6/234: loss=0.037503 lr=0.000020 grad_norm=1.012634
Epoch 93/100 Iteration 7/234: loss=0.038947 lr=0.000020 grad_norm=1.034883
Epoch 93/100 Iteration 8/234: loss=0.030576 lr=0.000020 grad_norm=0.915047
Epoch 93/100 Iteration 9/234: loss=0.035574 lr=0.000020 grad_norm=0.514657
Epoch 93/100 Iteration 10/234: loss=0.031795 lr=0.000020 grad_norm=1.095816
Epoch 93/100 Iteration 11/234: loss=0.034704 lr=0.000020 grad_norm=0.914614
Epoch 93/100 Iteration 12/234: loss=0.030332 lr=0.000020 grad_norm=0.515128
Epoch 93/100 Iteration 13/234: loss=0.032477 lr=0.000020 grad_norm=0.973366
Epoch 93/100 Iteration 14/234: loss=0.036703 lr=0.000020 grad_norm=0.816513
Epoch 93/100 Iteration 15/234: loss=0.033419 lr=0.000020 grad_norm=0.510670
Epoch 93/100 Iteration 16/234: loss=0.034809 lr=0.000020 grad_norm=0.847724
Epoch 93/100 Iteration 17/234: loss=0.031001 lr=0.000020 grad_norm=0.464744
Epoch 93/100 Iteration 18/234: loss=0.036312 lr=0.000020 grad_norm=0.746855
Epoch 93/100 Iteration 19/234: loss=0.034506 lr=0.000020 grad_norm=0.847402
Epoch 93/100 Iteration 20/234: loss=0.032856 lr=0.000020 grad_norm=0.401463
Epoch 93/100 Iteration 21/234: loss=0.034703 lr=0.000020 grad_norm=0.744787
Epoch 93/100 Iteration 22/234: loss=0.034601 lr=0.000020 grad_norm=0.838149
Epoch 93/100 Iteration 23/234: loss=0.031569 lr=0.000020 grad_norm=0.477731
Epoch 93/100 Iteration 24/234: loss=0.032736 lr=0.000020 grad_norm=0.497975
Epoch 93/100 Iteration 25/234: loss=0.036349 lr=0.000020 grad_norm=0.745991
Epoch 93/100 Iteration 26/234: loss=0.037195 lr=0.000020 grad_norm=0.816063
Epoch 93/100 Iteration 27/234: loss=0.032558 lr=0.000020 grad_norm=0.582142
Epoch 93/100 Iteration 28/234: loss=0.033652 lr=0.000020 grad_norm=0.460286
Epoch 93/100 Iteration 29/234: loss=0.032967 lr=0.000020 grad_norm=0.557675
Epoch 93/100 Iteration 30/234: loss=0.034578 lr=0.000020 grad_norm=0.677053
Epoch 93/100 Iteration 31/234: loss=0.032516 lr=0.000020 grad_norm=0.471474
Epoch 93/100 Iteration 32/234: loss=0.033926 lr=0.000020 grad_norm=0.895015
Epoch 93/100 Iteration 33/234: loss=0.032900 lr=0.000020 grad_norm=0.751219
Epoch 93/100 Iteration 34/234: loss=0.033414 lr=0.000020 grad_norm=0.440470
Epoch 93/100 Iteration 35/234: loss=0.033831 lr=0.000020 grad_norm=0.547466
Epoch 93/100 Iteration 36/234: loss=0.035906 lr=0.000020 grad_norm=0.413370
Epoch 93/100 Iteration 37/234: loss=0.038512 lr=0.000020 grad_norm=0.853489
Epoch 93/100 Iteration 38/234: loss=0.036050 lr=0.000020 grad_norm=0.608843
Epoch 93/100 Iteration 39/234: loss=0.031149 lr=0.000020 grad_norm=0.437862
Epoch 93/100 Iteration 40/234: loss=0.032187 lr=0.000020 grad_norm=0.558606
Epoch 93/100 Iteration 41/234: loss=0.034579 lr=0.000020 grad_norm=0.481359
Epoch 93/100 Iteration 42/234: loss=0.033175 lr=0.000020 grad_norm=0.593192
Epoch 93/100 Iteration 43/234: loss=0.033444 lr=0.000020 grad_norm=0.580336
Epoch 93/100 Iteration 44/234: loss=0.036069 lr=0.000020 grad_norm=0.502116
Epoch 93/100 Iteration 45/234: loss=0.034112 lr=0.000020 grad_norm=0.470163
Epoch 93/100 Iteration 46/234: loss=0.034536 lr=0.000020 grad_norm=0.489928
Epoch 93/100 Iteration 47/234: loss=0.032135 lr=0.000020 grad_norm=0.374878
Epoch 93/100 Iteration 48/234: loss=0.030059 lr=0.000020 grad_norm=0.348274
Epoch 93/100 Iteration 49/234: loss=0.032963 lr=0.000020 grad_norm=0.481262
Epoch 93/100 Iteration 50/234: loss=0.033973 lr=0.000020 grad_norm=0.332053
Epoch 93/100 Iteration 51/234: loss=0.038487 lr=0.000020 grad_norm=0.488448
Epoch 93/100 Iteration 52/234: loss=0.036503 lr=0.000020 grad_norm=0.738621
Epoch 93/100 Iteration 53/234: loss=0.031835 lr=0.000020 grad_norm=0.554194
Epoch 93/100 Iteration 54/234: loss=0.032539 lr=0.000020 grad_norm=0.435614
Epoch 93/100 Iteration 55/234: loss=0.032005 lr=0.000020 grad_norm=0.569297
Epoch 93/100 Iteration 56/234: loss=0.034798 lr=0.000020 grad_norm=0.506977
Epoch 93/100 Iteration 57/234: loss=0.032475 lr=0.000020 grad_norm=0.707567
Epoch 93/100 Iteration 58/234: loss=0.032552 lr=0.000020 grad_norm=0.603761
Epoch 93/100 Iteration 59/234: loss=0.037034 lr=0.000020 grad_norm=0.515579
Epoch 93/100 Iteration 60/234: loss=0.032909 lr=0.000020 grad_norm=0.723857
Epoch 93/100 Iteration 61/234: loss=0.030817 lr=0.000020 grad_norm=0.494675
Epoch 93/100 Iteration 62/234: loss=0.034418 lr=0.000020 grad_norm=0.560813
Epoch 93/100 Iteration 63/234: loss=0.031598 lr=0.000020 grad_norm=1.037090
Epoch 93/100 Iteration 64/234: loss=0.034939 lr=0.000020 grad_norm=1.359563
Epoch 93/100 Iteration 65/234: loss=0.032669 lr=0.000020 grad_norm=0.994771
Epoch 93/100 Iteration 66/234: loss=0.035595 lr=0.000020 grad_norm=0.536330
Epoch 93/100 Iteration 67/234: loss=0.034254 lr=0.000020 grad_norm=0.827854
Epoch 93/100 Iteration 68/234: loss=0.031035 lr=0.000020 grad_norm=0.501173
Epoch 93/100 Iteration 69/234: loss=0.032549 lr=0.000020 grad_norm=0.700454
Epoch 93/100 Iteration 70/234: loss=0.035308 lr=0.000020 grad_norm=0.885700
Epoch 93/100 Iteration 71/234: loss=0.034789 lr=0.000020 grad_norm=0.420123
Epoch 93/100 Iteration 72/234: loss=0.034391 lr=0.000020 grad_norm=0.600420
Epoch 93/100 Iteration 73/234: loss=0.029653 lr=0.000020 grad_norm=0.416172
Epoch 93/100 Iteration 74/234: loss=0.032919 lr=0.000020 grad_norm=0.555289
Epoch 93/100 Iteration 75/234: loss=0.035646 lr=0.000020 grad_norm=0.834955
Epoch 93/100 Iteration 76/234: loss=0.032769 lr=0.000020 grad_norm=0.789806
Epoch 93/100 Iteration 77/234: loss=0.034455 lr=0.000020 grad_norm=0.552600
Epoch 93/100 Iteration 78/234: loss=0.030932 lr=0.000020 grad_norm=0.450194
Epoch 93/100 Iteration 79/234: loss=0.033939 lr=0.000020 grad_norm=0.544316
Epoch 93/100 Iteration 80/234: loss=0.036902 lr=0.000020 grad_norm=0.780749
Epoch 93/100 Iteration 81/234: loss=0.032468 lr=0.000020 grad_norm=0.733006
Epoch 93/100 Iteration 82/234: loss=0.031794 lr=0.000020 grad_norm=0.356353
Epoch 93/100 Iteration 83/234: loss=0.034477 lr=0.000020 grad_norm=0.917442
Epoch 93/100 Iteration 84/234: loss=0.033290 lr=0.000020 grad_norm=0.707871
Epoch 93/100 Iteration 85/234: loss=0.035002 lr=0.000020 grad_norm=0.422605
Epoch 93/100 Iteration 86/234: loss=0.035533 lr=0.000020 grad_norm=1.144111
Epoch 93/100 Iteration 87/234: loss=0.036532 lr=0.000020 grad_norm=1.915593
Epoch 93/100 Iteration 88/234: loss=0.035580 lr=0.000020 grad_norm=1.452193
Epoch 93/100 Iteration 89/234: loss=0.032853 lr=0.000020 grad_norm=0.440265
Epoch 93/100 Iteration 90/234: loss=0.035487 lr=0.000020 grad_norm=1.214779
Epoch 93/100 Iteration 91/234: loss=0.034800 lr=0.000020 grad_norm=1.021505
Epoch 93/100 Iteration 92/234: loss=0.034947 lr=0.000020 grad_norm=0.620059
Epoch 93/100 Iteration 93/234: loss=0.033733 lr=0.000020 grad_norm=1.085901
Epoch 93/100 Iteration 94/234: loss=0.034574 lr=0.000020 grad_norm=0.513562
Epoch 93/100 Iteration 95/234: loss=0.034394 lr=0.000020 grad_norm=0.493198
Epoch 93/100 Iteration 96/234: loss=0.031642 lr=0.000020 grad_norm=0.416420
Epoch 93/100 Iteration 97/234: loss=0.031191 lr=0.000020 grad_norm=0.519523
Epoch 93/100 Iteration 98/234: loss=0.038159 lr=0.000020 grad_norm=0.473286
Epoch 93/100 Iteration 99/234: loss=0.031482 lr=0.000020 grad_norm=0.565112
Epoch 93/100 Iteration 100/234: loss=0.034287 lr=0.000020 grad_norm=0.826255
Epoch 93/100 Iteration 101/234: loss=0.033292 lr=0.000020 grad_norm=0.542354
Epoch 93/100 Iteration 102/234: loss=0.037160 lr=0.000020 grad_norm=0.507234
Epoch 93/100 Iteration 103/234: loss=0.032379 lr=0.000020 grad_norm=0.693823
Epoch 93/100 Iteration 104/234: loss=0.032472 lr=0.000020 grad_norm=0.632077
Epoch 93/100 Iteration 105/234: loss=0.035018 lr=0.000020 grad_norm=0.954885
Epoch 93/100 Iteration 106/234: loss=0.028457 lr=0.000020 grad_norm=0.631803
Epoch 93/100 Iteration 107/234: loss=0.034708 lr=0.000020 grad_norm=0.711522
Epoch 93/100 Iteration 108/234: loss=0.034470 lr=0.000020 grad_norm=1.021936
Epoch 93/100 Iteration 109/234: loss=0.035708 lr=0.000020 grad_norm=0.685713
Epoch 93/100 Iteration 110/234: loss=0.037292 lr=0.000020 grad_norm=0.930554
Epoch 93/100 Iteration 111/234: loss=0.033284 lr=0.000020 grad_norm=1.284559
Epoch 93/100 Iteration 112/234: loss=0.035241 lr=0.000020 grad_norm=0.544832
Epoch 93/100 Iteration 113/234: loss=0.033052 lr=0.000020 grad_norm=1.129834
Epoch 93/100 Iteration 114/234: loss=0.033773 lr=0.000020 grad_norm=1.298013
Epoch 93/100 Iteration 115/234: loss=0.035421 lr=0.000020 grad_norm=0.498367
Epoch 93/100 Iteration 116/234: loss=0.035867 lr=0.000020 grad_norm=1.172283
Epoch 93/100 Iteration 117/234: loss=0.034459 lr=0.000020 grad_norm=1.279790
Epoch 93/100 Iteration 118/234: loss=0.031418 lr=0.000020 grad_norm=0.485968
Epoch 93/100 Iteration 119/234: loss=0.032427 lr=0.000020 grad_norm=1.352964
Epoch 93/100 Iteration 120/234: loss=0.034999 lr=0.000020 grad_norm=1.117632
Epoch 93/100 Iteration 121/234: loss=0.033329 lr=0.000020 grad_norm=0.861696
Epoch 93/100 Iteration 122/234: loss=0.032227 lr=0.000020 grad_norm=1.100501
Epoch 93/100 Iteration 123/234: loss=0.035193 lr=0.000020 grad_norm=0.536150
Epoch 93/100 Iteration 124/234: loss=0.031606 lr=0.000020 grad_norm=1.133889
Epoch 93/100 Iteration 125/234: loss=0.032247 lr=0.000020 grad_norm=0.536598
Epoch 93/100 Iteration 126/234: loss=0.032895 lr=0.000020 grad_norm=1.171530
Epoch 93/100 Iteration 127/234: loss=0.031592 lr=0.000020 grad_norm=0.693647
Epoch 93/100 Iteration 128/234: loss=0.034637 lr=0.000020 grad_norm=0.906628
Epoch 93/100 Iteration 129/234: loss=0.035492 lr=0.000020 grad_norm=1.034655
Epoch 93/100 Iteration 130/234: loss=0.036525 lr=0.000020 grad_norm=0.525464
Epoch 93/100 Iteration 131/234: loss=0.032865 lr=0.000020 grad_norm=0.542860
Epoch 93/100 Iteration 132/234: loss=0.033453 lr=0.000020 grad_norm=0.588847
Epoch 93/100 Iteration 133/234: loss=0.031003 lr=0.000020 grad_norm=0.519385
Epoch 93/100 Iteration 134/234: loss=0.034406 lr=0.000020 grad_norm=0.474667
Epoch 93/100 Iteration 135/234: loss=0.034414 lr=0.000020 grad_norm=0.780594
Epoch 93/100 Iteration 136/234: loss=0.039474 lr=0.000020 grad_norm=0.766432
Epoch 93/100 Iteration 137/234: loss=0.037003 lr=0.000020 grad_norm=0.423526
Epoch 93/100 Iteration 138/234: loss=0.037061 lr=0.000020 grad_norm=0.487363
Epoch 93/100 Iteration 139/234: loss=0.033504 lr=0.000020 grad_norm=0.608012
Epoch 93/100 Iteration 140/234: loss=0.033711 lr=0.000020 grad_norm=0.828833
Epoch 93/100 Iteration 141/234: loss=0.033747 lr=0.000020 grad_norm=0.398175
Epoch 93/100 Iteration 142/234: loss=0.032731 lr=0.000020 grad_norm=0.669816
Epoch 93/100 Iteration 143/234: loss=0.035722 lr=0.000020 grad_norm=0.714885
Epoch 93/100 Iteration 144/234: loss=0.035882 lr=0.000020 grad_norm=0.450776
Epoch 93/100 Iteration 145/234: loss=0.032595 lr=0.000020 grad_norm=0.594223
Epoch 93/100 Iteration 146/234: loss=0.036815 lr=0.000020 grad_norm=1.007030
Epoch 93/100 Iteration 147/234: loss=0.034247 lr=0.000020 grad_norm=1.039769
Epoch 93/100 Iteration 148/234: loss=0.034757 lr=0.000020 grad_norm=0.870775
Epoch 93/100 Iteration 149/234: loss=0.032396 lr=0.000020 grad_norm=0.825768
Epoch 93/100 Iteration 150/234: loss=0.032654 lr=0.000020 grad_norm=0.601106
Epoch 93/100 Iteration 151/234: loss=0.036063 lr=0.000020 grad_norm=0.877934
Epoch 93/100 Iteration 152/234: loss=0.037563 lr=0.000020 grad_norm=1.117009
Epoch 93/100 Iteration 153/234: loss=0.029472 lr=0.000020 grad_norm=0.773325
Epoch 93/100 Iteration 154/234: loss=0.035238 lr=0.000020 grad_norm=0.657393
Epoch 93/100 Iteration 155/234: loss=0.035849 lr=0.000020 grad_norm=1.364695
Epoch 93/100 Iteration 156/234: loss=0.034553 lr=0.000020 grad_norm=1.383619
Epoch 93/100 Iteration 157/234: loss=0.033868 lr=0.000020 grad_norm=0.931725
Epoch 93/100 Iteration 158/234: loss=0.036720 lr=0.000020 grad_norm=0.978448
Epoch 93/100 Iteration 159/234: loss=0.033504 lr=0.000020 grad_norm=1.037858
Epoch 93/100 Iteration 160/234: loss=0.033605 lr=0.000020 grad_norm=0.918534
Epoch 93/100 Iteration 161/234: loss=0.034667 lr=0.000020 grad_norm=0.881261
Epoch 93/100 Iteration 162/234: loss=0.035977 lr=0.000020 grad_norm=1.104642
Epoch 93/100 Iteration 163/234: loss=0.034804 lr=0.000020 grad_norm=0.805020
Epoch 93/100 Iteration 164/234: loss=0.034272 lr=0.000020 grad_norm=0.664523
Epoch 93/100 Iteration 165/234: loss=0.035730 lr=0.000020 grad_norm=1.442572
Epoch 93/100 Iteration 166/234: loss=0.035357 lr=0.000020 grad_norm=1.260353
Epoch 93/100 Iteration 167/234: loss=0.037116 lr=0.000020 grad_norm=0.974221
Epoch 93/100 Iteration 168/234: loss=0.035986 lr=0.000020 grad_norm=1.257990
Epoch 93/100 Iteration 169/234: loss=0.031276 lr=0.000020 grad_norm=0.774001
Epoch 93/100 Iteration 170/234: loss=0.033135 lr=0.000020 grad_norm=0.689713
Epoch 93/100 Iteration 171/234: loss=0.035217 lr=0.000020 grad_norm=0.894810
Epoch 93/100 Iteration 172/234: loss=0.037901 lr=0.000020 grad_norm=0.570008
Epoch 93/100 Iteration 173/234: loss=0.035714 lr=0.000020 grad_norm=0.592621
Epoch 93/100 Iteration 174/234: loss=0.034486 lr=0.000020 grad_norm=0.743519
Epoch 93/100 Iteration 175/234: loss=0.034003 lr=0.000020 grad_norm=0.512395
Epoch 93/100 Iteration 176/234: loss=0.033205 lr=0.000020 grad_norm=0.659590
Epoch 93/100 Iteration 177/234: loss=0.032434 lr=0.000020 grad_norm=0.971639
Epoch 93/100 Iteration 178/234: loss=0.035634 lr=0.000020 grad_norm=0.913037
Epoch 93/100 Iteration 179/234: loss=0.035166 lr=0.000020 grad_norm=0.629781
Epoch 93/100 Iteration 180/234: loss=0.034595 lr=0.000020 grad_norm=0.611005
Epoch 93/100 Iteration 181/234: loss=0.035360 lr=0.000020 grad_norm=0.441079
Epoch 93/100 Iteration 182/234: loss=0.036913 lr=0.000020 grad_norm=0.373161
Epoch 93/100 Iteration 183/234: loss=0.035241 lr=0.000020 grad_norm=0.495140
Epoch 93/100 Iteration 184/234: loss=0.031495 lr=0.000020 grad_norm=0.367333
Epoch 93/100 Iteration 185/234: loss=0.032227 lr=0.000020 grad_norm=0.402797
Epoch 93/100 Iteration 186/234: loss=0.033511 lr=0.000020 grad_norm=0.558348
Epoch 93/100 Iteration 187/234: loss=0.030941 lr=0.000020 grad_norm=0.764741
Epoch 93/100 Iteration 188/234: loss=0.035105 lr=0.000020 grad_norm=0.422140
Epoch 93/100 Iteration 189/234: loss=0.033719 lr=0.000020 grad_norm=0.521890
Epoch 93/100 Iteration 190/234: loss=0.031937 lr=0.000020 grad_norm=0.375334
Epoch 93/100 Iteration 191/234: loss=0.035350 lr=0.000020 grad_norm=0.445094
Epoch 93/100 Iteration 192/234: loss=0.035931 lr=0.000020 grad_norm=0.552677
Epoch 93/100 Iteration 193/234: loss=0.033159 lr=0.000020 grad_norm=0.499696
Epoch 93/100 Iteration 194/234: loss=0.033106 lr=0.000020 grad_norm=0.761234
Epoch 93/100 Iteration 195/234: loss=0.032800 lr=0.000020 grad_norm=0.460619
Epoch 93/100 Iteration 196/234: loss=0.030971 lr=0.000020 grad_norm=0.565222
Epoch 93/100 Iteration 197/234: loss=0.035431 lr=0.000020 grad_norm=0.935453
Epoch 93/100 Iteration 198/234: loss=0.035917 lr=0.000020 grad_norm=0.657922
Epoch 93/100 Iteration 199/234: loss=0.033600 lr=0.000020 grad_norm=0.485093
Epoch 93/100 Iteration 200/234: loss=0.034702 lr=0.000020 grad_norm=0.834840
Epoch 93/100 Iteration 201/234: loss=0.033536 lr=0.000020 grad_norm=0.691496
Epoch 93/100 Iteration 202/234: loss=0.034768 lr=0.000020 grad_norm=0.489265
Epoch 93/100 Iteration 203/234: loss=0.030739 lr=0.000020 grad_norm=0.398692
Epoch 93/100 Iteration 204/234: loss=0.033443 lr=0.000020 grad_norm=0.438624
Epoch 93/100 Iteration 205/234: loss=0.035960 lr=0.000020 grad_norm=0.583168
Epoch 93/100 Iteration 206/234: loss=0.033897 lr=0.000020 grad_norm=0.397140
Epoch 93/100 Iteration 207/234: loss=0.032663 lr=0.000020 grad_norm=0.469644
Epoch 93/100 Iteration 208/234: loss=0.031882 lr=0.000020 grad_norm=0.822674
Epoch 93/100 Iteration 209/234: loss=0.032903 lr=0.000020 grad_norm=1.011610
Epoch 93/100 Iteration 210/234: loss=0.035507 lr=0.000020 grad_norm=0.627458
Epoch 93/100 Iteration 211/234: loss=0.031562 lr=0.000020 grad_norm=0.496801
Epoch 93/100 Iteration 212/234: loss=0.034154 lr=0.000020 grad_norm=0.669112
Epoch 93/100 Iteration 213/234: loss=0.034135 lr=0.000020 grad_norm=0.802241
Epoch 93/100 Iteration 214/234: loss=0.037712 lr=0.000020 grad_norm=0.851646
Epoch 93/100 Iteration 215/234: loss=0.035813 lr=0.000020 grad_norm=1.048788
Epoch 93/100 Iteration 216/234: loss=0.032415 lr=0.000020 grad_norm=0.611356
Epoch 93/100 Iteration 217/234: loss=0.035972 lr=0.000020 grad_norm=0.591515
Epoch 93/100 Iteration 218/234: loss=0.030305 lr=0.000020 grad_norm=0.734180
Epoch 93/100 Iteration 219/234: loss=0.033760 lr=0.000020 grad_norm=0.525139
Epoch 93/100 Iteration 220/234: loss=0.032650 lr=0.000020 grad_norm=0.652159
Epoch 93/100 Iteration 221/234: loss=0.033137 lr=0.000020 grad_norm=0.746171
Epoch 93/100 Iteration 222/234: loss=0.035002 lr=0.000020 grad_norm=0.818335
Epoch 93/100 Iteration 223/234: loss=0.034831 lr=0.000020 grad_norm=0.819389
Epoch 93/100 Iteration 224/234: loss=0.035497 lr=0.000020 grad_norm=0.800955
Epoch 93/100 Iteration 225/234: loss=0.034057 lr=0.000020 grad_norm=0.921980
Epoch 93/100 Iteration 226/234: loss=0.032589 lr=0.000020 grad_norm=0.413852
Epoch 93/100 Iteration 227/234: loss=0.034491 lr=0.000020 grad_norm=0.822286
Epoch 93/100 Iteration 228/234: loss=0.034480 lr=0.000020 grad_norm=0.995484
Epoch 93/100 Iteration 229/234: loss=0.031996 lr=0.000020 grad_norm=1.026761
Epoch 93/100 Iteration 230/234: loss=0.034119 lr=0.000020 grad_norm=0.694425
Epoch 93/100 Iteration 231/234: loss=0.034138 lr=0.000020 grad_norm=0.624316
Epoch 93/100 Iteration 232/234: loss=0.029397 lr=0.000020 grad_norm=1.035322
Epoch 93/100 Iteration 233/234: loss=0.033505 lr=0.000020 grad_norm=0.664621
Epoch 93/100 Iteration 234/234: loss=0.032654 lr=0.000020 grad_norm=0.675769
Epoch 93/100 finished. Avg Loss: 0.034047
Epoch 94/100 Iteration 1/234: loss=0.033041 lr=0.000020 grad_norm=1.095859
Epoch 94/100 Iteration 2/234: loss=0.033602 lr=0.000020 grad_norm=0.657715
Epoch 94/100 Iteration 3/234: loss=0.034459 lr=0.000020 grad_norm=0.644168
Epoch 94/100 Iteration 4/234: loss=0.031967 lr=0.000020 grad_norm=0.651384
Epoch 94/100 Iteration 5/234: loss=0.034640 lr=0.000020 grad_norm=0.487497
Epoch 94/100 Iteration 6/234: loss=0.033064 lr=0.000020 grad_norm=0.397524
Epoch 94/100 Iteration 7/234: loss=0.033403 lr=0.000020 grad_norm=0.666476
Epoch 94/100 Iteration 8/234: loss=0.035849 lr=0.000020 grad_norm=0.711851
Epoch 94/100 Iteration 9/234: loss=0.036264 lr=0.000020 grad_norm=0.463285
Epoch 94/100 Iteration 10/234: loss=0.035589 lr=0.000020 grad_norm=0.829739
Epoch 94/100 Iteration 11/234: loss=0.035501 lr=0.000020 grad_norm=0.931521
Epoch 94/100 Iteration 12/234: loss=0.035575 lr=0.000020 grad_norm=0.686252
Epoch 94/100 Iteration 13/234: loss=0.032920 lr=0.000020 grad_norm=0.463213
Epoch 94/100 Iteration 14/234: loss=0.036355 lr=0.000020 grad_norm=0.987701
Epoch 94/100 Iteration 15/234: loss=0.033581 lr=0.000020 grad_norm=0.623936
Epoch 94/100 Iteration 16/234: loss=0.034186 lr=0.000020 grad_norm=0.771945
Epoch 94/100 Iteration 17/234: loss=0.034501 lr=0.000020 grad_norm=1.514341
Epoch 94/100 Iteration 18/234: loss=0.034290 lr=0.000020 grad_norm=1.233859
Epoch 94/100 Iteration 19/234: loss=0.033637 lr=0.000020 grad_norm=0.649691
Epoch 94/100 Iteration 20/234: loss=0.035295 lr=0.000020 grad_norm=1.817823
Epoch 94/100 Iteration 21/234: loss=0.036355 lr=0.000020 grad_norm=1.811818
Epoch 94/100 Iteration 22/234: loss=0.032772 lr=0.000020 grad_norm=0.583715
Epoch 94/100 Iteration 23/234: loss=0.036091 lr=0.000020 grad_norm=1.557938
Epoch 94/100 Iteration 24/234: loss=0.033492 lr=0.000020 grad_norm=1.231402
Epoch 94/100 Iteration 25/234: loss=0.032834 lr=0.000020 grad_norm=0.772181
Epoch 94/100 Iteration 26/234: loss=0.034435 lr=0.000020 grad_norm=0.799465
Epoch 94/100 Iteration 27/234: loss=0.035319 lr=0.000020 grad_norm=0.938553
Epoch 94/100 Iteration 28/234: loss=0.033202 lr=0.000020 grad_norm=0.767973
Epoch 94/100 Iteration 29/234: loss=0.036960 lr=0.000020 grad_norm=1.092602
Epoch 94/100 Iteration 30/234: loss=0.036076 lr=0.000020 grad_norm=0.690663
Epoch 94/100 Iteration 31/234: loss=0.034213 lr=0.000020 grad_norm=0.698386
Epoch 94/100 Iteration 32/234: loss=0.034692 lr=0.000020 grad_norm=0.644548
Epoch 94/100 Iteration 33/234: loss=0.030864 lr=0.000020 grad_norm=0.507654
Epoch 94/100 Iteration 34/234: loss=0.032544 lr=0.000020 grad_norm=0.691689
Epoch 94/100 Iteration 35/234: loss=0.032952 lr=0.000020 grad_norm=0.749908
Epoch 94/100 Iteration 36/234: loss=0.030634 lr=0.000020 grad_norm=0.467802
Epoch 94/100 Iteration 37/234: loss=0.035914 lr=0.000020 grad_norm=0.693307
Epoch 94/100 Iteration 38/234: loss=0.028766 lr=0.000020 grad_norm=0.618414
Epoch 94/100 Iteration 39/234: loss=0.031558 lr=0.000020 grad_norm=0.377194
Epoch 94/100 Iteration 40/234: loss=0.037097 lr=0.000020 grad_norm=0.655254
Epoch 94/100 Iteration 41/234: loss=0.035192 lr=0.000020 grad_norm=0.934361
Epoch 94/100 Iteration 42/234: loss=0.030954 lr=0.000020 grad_norm=0.788682
Epoch 94/100 Iteration 43/234: loss=0.031323 lr=0.000020 grad_norm=0.527212
Epoch 94/100 Iteration 44/234: loss=0.035844 lr=0.000020 grad_norm=1.054428
Epoch 94/100 Iteration 45/234: loss=0.031348 lr=0.000020 grad_norm=0.945207
Epoch 94/100 Iteration 46/234: loss=0.031715 lr=0.000020 grad_norm=0.679337
Epoch 94/100 Iteration 47/234: loss=0.035510 lr=0.000020 grad_norm=0.827427
Epoch 94/100 Iteration 48/234: loss=0.033091 lr=0.000020 grad_norm=0.757349
Epoch 94/100 Iteration 49/234: loss=0.036001 lr=0.000020 grad_norm=0.776223
Epoch 94/100 Iteration 50/234: loss=0.032646 lr=0.000020 grad_norm=0.788587
Epoch 94/100 Iteration 51/234: loss=0.036132 lr=0.000020 grad_norm=0.712675
Epoch 94/100 Iteration 52/234: loss=0.032234 lr=0.000020 grad_norm=0.496187
Epoch 94/100 Iteration 53/234: loss=0.032712 lr=0.000020 grad_norm=0.770506
Epoch 94/100 Iteration 54/234: loss=0.034408 lr=0.000020 grad_norm=0.762159
Epoch 94/100 Iteration 55/234: loss=0.035423 lr=0.000020 grad_norm=0.426775
Epoch 94/100 Iteration 56/234: loss=0.034962 lr=0.000020 grad_norm=0.739649
Epoch 94/100 Iteration 57/234: loss=0.033949 lr=0.000020 grad_norm=0.674227
Epoch 94/100 Iteration 58/234: loss=0.034095 lr=0.000020 grad_norm=0.434957
Epoch 94/100 Iteration 59/234: loss=0.031981 lr=0.000020 grad_norm=0.801470
Epoch 94/100 Iteration 60/234: loss=0.029851 lr=0.000020 grad_norm=0.614140
Epoch 94/100 Iteration 61/234: loss=0.031409 lr=0.000020 grad_norm=0.448510
Epoch 94/100 Iteration 62/234: loss=0.034762 lr=0.000020 grad_norm=0.596178
Epoch 94/100 Iteration 63/234: loss=0.034462 lr=0.000020 grad_norm=0.678073
Epoch 94/100 Iteration 64/234: loss=0.035929 lr=0.000020 grad_norm=0.307405
Epoch 94/100 Iteration 65/234: loss=0.034420 lr=0.000020 grad_norm=0.697117
Epoch 94/100 Iteration 66/234: loss=0.031920 lr=0.000020 grad_norm=0.842651
Epoch 94/100 Iteration 67/234: loss=0.032009 lr=0.000020 grad_norm=0.413102
Epoch 94/100 Iteration 68/234: loss=0.038202 lr=0.000020 grad_norm=1.361301
Epoch 94/100 Iteration 69/234: loss=0.032568 lr=0.000020 grad_norm=1.683381
Epoch 94/100 Iteration 70/234: loss=0.034388 lr=0.000020 grad_norm=0.898475
Epoch 94/100 Iteration 71/234: loss=0.033154 lr=0.000020 grad_norm=0.601608
Epoch 94/100 Iteration 72/234: loss=0.033073 lr=0.000020 grad_norm=1.454636
Epoch 94/100 Iteration 73/234: loss=0.034601 lr=0.000020 grad_norm=1.094527
Epoch 94/100 Iteration 74/234: loss=0.031230 lr=0.000020 grad_norm=0.663247
Epoch 94/100 Iteration 75/234: loss=0.035377 lr=0.000020 grad_norm=1.138448
Epoch 94/100 Iteration 76/234: loss=0.034139 lr=0.000020 grad_norm=0.621845
Epoch 94/100 Iteration 77/234: loss=0.033945 lr=0.000020 grad_norm=1.076529
Epoch 94/100 Iteration 78/234: loss=0.036365 lr=0.000020 grad_norm=0.610936
Epoch 94/100 Iteration 79/234: loss=0.030373 lr=0.000020 grad_norm=1.087980
Epoch 94/100 Iteration 80/234: loss=0.033674 lr=0.000020 grad_norm=0.926636
Epoch 94/100 Iteration 81/234: loss=0.034600 lr=0.000020 grad_norm=0.803084
Epoch 94/100 Iteration 82/234: loss=0.034217 lr=0.000020 grad_norm=0.869672
Epoch 94/100 Iteration 83/234: loss=0.034692 lr=0.000020 grad_norm=0.556287
Epoch 94/100 Iteration 84/234: loss=0.034958 lr=0.000020 grad_norm=0.838513
Epoch 94/100 Iteration 85/234: loss=0.036738 lr=0.000020 grad_norm=0.669631
Epoch 94/100 Iteration 86/234: loss=0.033092 lr=0.000020 grad_norm=0.357603
Epoch 94/100 Iteration 87/234: loss=0.035392 lr=0.000020 grad_norm=0.740164
Epoch 94/100 Iteration 88/234: loss=0.035165 lr=0.000020 grad_norm=0.763197
Epoch 94/100 Iteration 89/234: loss=0.034893 lr=0.000020 grad_norm=0.455580
Epoch 94/100 Iteration 90/234: loss=0.032913 lr=0.000020 grad_norm=0.635722
Epoch 94/100 Iteration 91/234: loss=0.036723 lr=0.000020 grad_norm=0.930495
Epoch 94/100 Iteration 92/234: loss=0.033921 lr=0.000020 grad_norm=0.513080
Epoch 94/100 Iteration 93/234: loss=0.033983 lr=0.000020 grad_norm=0.530058
Epoch 94/100 Iteration 94/234: loss=0.032493 lr=0.000020 grad_norm=0.672632
Epoch 94/100 Iteration 95/234: loss=0.032116 lr=0.000020 grad_norm=0.426278
Epoch 94/100 Iteration 96/234: loss=0.034876 lr=0.000020 grad_norm=0.688627
Epoch 94/100 Iteration 97/234: loss=0.030808 lr=0.000020 grad_norm=0.403105
Epoch 94/100 Iteration 98/234: loss=0.036830 lr=0.000020 grad_norm=0.727236
Epoch 94/100 Iteration 99/234: loss=0.032220 lr=0.000020 grad_norm=0.699658
Epoch 94/100 Iteration 100/234: loss=0.034899 lr=0.000020 grad_norm=0.688989
Epoch 94/100 Iteration 101/234: loss=0.033169 lr=0.000020 grad_norm=0.569147
Epoch 94/100 Iteration 102/234: loss=0.033573 lr=0.000020 grad_norm=0.775184
Epoch 94/100 Iteration 103/234: loss=0.033833 lr=0.000020 grad_norm=0.914809
Epoch 94/100 Iteration 104/234: loss=0.036519 lr=0.000020 grad_norm=0.866651
Epoch 94/100 Iteration 105/234: loss=0.034196 lr=0.000020 grad_norm=0.744355
Epoch 94/100 Iteration 106/234: loss=0.031493 lr=0.000020 grad_norm=0.814917
Epoch 94/100 Iteration 107/234: loss=0.033510 lr=0.000020 grad_norm=0.875572
Epoch 94/100 Iteration 108/234: loss=0.030810 lr=0.000020 grad_norm=0.487542
Epoch 94/100 Iteration 109/234: loss=0.034518 lr=0.000020 grad_norm=0.789092
Epoch 94/100 Iteration 110/234: loss=0.034379 lr=0.000020 grad_norm=0.962829
Epoch 94/100 Iteration 111/234: loss=0.036985 lr=0.000020 grad_norm=0.692139
Epoch 94/100 Iteration 112/234: loss=0.034598 lr=0.000020 grad_norm=0.707870
Epoch 94/100 Iteration 113/234: loss=0.034161 lr=0.000020 grad_norm=1.168865
Epoch 94/100 Iteration 114/234: loss=0.033267 lr=0.000020 grad_norm=0.796449
Epoch 94/100 Iteration 115/234: loss=0.036166 lr=0.000020 grad_norm=0.848105
Epoch 94/100 Iteration 116/234: loss=0.036269 lr=0.000020 grad_norm=1.470865
Epoch 94/100 Iteration 117/234: loss=0.031094 lr=0.000020 grad_norm=0.608634
Epoch 94/100 Iteration 118/234: loss=0.033203 lr=0.000020 grad_norm=1.017304
Epoch 94/100 Iteration 119/234: loss=0.034736 lr=0.000020 grad_norm=0.925850
Epoch 94/100 Iteration 120/234: loss=0.034974 lr=0.000020 grad_norm=0.970920
Epoch 94/100 Iteration 121/234: loss=0.031163 lr=0.000020 grad_norm=0.895651
Epoch 94/100 Iteration 122/234: loss=0.036566 lr=0.000020 grad_norm=1.118165
Epoch 94/100 Iteration 123/234: loss=0.031750 lr=0.000020 grad_norm=1.316025
Epoch 94/100 Iteration 124/234: loss=0.030461 lr=0.000020 grad_norm=0.686435
Epoch 94/100 Iteration 125/234: loss=0.034040 lr=0.000020 grad_norm=1.335353
Epoch 94/100 Iteration 126/234: loss=0.035954 lr=0.000020 grad_norm=0.510484
Epoch 94/100 Iteration 127/234: loss=0.034804 lr=0.000020 grad_norm=1.278583
Epoch 94/100 Iteration 128/234: loss=0.032467 lr=0.000020 grad_norm=0.884099
Epoch 94/100 Iteration 129/234: loss=0.036855 lr=0.000020 grad_norm=1.043898
Epoch 94/100 Iteration 130/234: loss=0.029385 lr=0.000020 grad_norm=0.942350
Epoch 94/100 Iteration 131/234: loss=0.033115 lr=0.000020 grad_norm=0.860428
Epoch 94/100 Iteration 132/234: loss=0.034786 lr=0.000020 grad_norm=1.268264
Epoch 94/100 Iteration 133/234: loss=0.037041 lr=0.000020 grad_norm=0.885714
Epoch 94/100 Iteration 134/234: loss=0.032633 lr=0.000020 grad_norm=1.110666
Epoch 94/100 Iteration 135/234: loss=0.032598 lr=0.000020 grad_norm=0.444949
Epoch 94/100 Iteration 136/234: loss=0.034584 lr=0.000020 grad_norm=1.218736
Epoch 94/100 Iteration 137/234: loss=0.031546 lr=0.000020 grad_norm=0.747578
Epoch 94/100 Iteration 138/234: loss=0.036236 lr=0.000020 grad_norm=0.740326
Epoch 94/100 Iteration 139/234: loss=0.032904 lr=0.000020 grad_norm=0.604257
Epoch 94/100 Iteration 140/234: loss=0.033366 lr=0.000020 grad_norm=0.909998
Epoch 94/100 Iteration 141/234: loss=0.031931 lr=0.000020 grad_norm=0.574548
Epoch 94/100 Iteration 142/234: loss=0.032286 lr=0.000020 grad_norm=0.962671
Epoch 94/100 Iteration 143/234: loss=0.034250 lr=0.000020 grad_norm=1.030673
Epoch 94/100 Iteration 144/234: loss=0.031490 lr=0.000020 grad_norm=0.866589
Epoch 94/100 Iteration 145/234: loss=0.031524 lr=0.000020 grad_norm=0.764194
Epoch 94/100 Iteration 146/234: loss=0.036141 lr=0.000020 grad_norm=0.790843
Epoch 94/100 Iteration 147/234: loss=0.033904 lr=0.000020 grad_norm=1.011936
Epoch 94/100 Iteration 148/234: loss=0.035912 lr=0.000020 grad_norm=0.448191
Epoch 94/100 Iteration 149/234: loss=0.034581 lr=0.000020 grad_norm=1.076432
Epoch 94/100 Iteration 150/234: loss=0.035852 lr=0.000020 grad_norm=0.803938
Epoch 94/100 Iteration 151/234: loss=0.035590 lr=0.000020 grad_norm=0.994598
Epoch 94/100 Iteration 152/234: loss=0.033334 lr=0.000020 grad_norm=0.769102
Epoch 94/100 Iteration 153/234: loss=0.032034 lr=0.000020 grad_norm=0.562852
Epoch 94/100 Iteration 154/234: loss=0.037472 lr=0.000020 grad_norm=0.916782
Epoch 94/100 Iteration 155/234: loss=0.032047 lr=0.000020 grad_norm=0.641479
Epoch 94/100 Iteration 156/234: loss=0.035622 lr=0.000020 grad_norm=0.702257
Epoch 94/100 Iteration 157/234: loss=0.035610 lr=0.000020 grad_norm=0.765994
Epoch 94/100 Iteration 158/234: loss=0.032453 lr=0.000020 grad_norm=0.580919
Epoch 94/100 Iteration 159/234: loss=0.033135 lr=0.000020 grad_norm=0.522590
Epoch 94/100 Iteration 160/234: loss=0.031651 lr=0.000020 grad_norm=0.560635
Epoch 94/100 Iteration 161/234: loss=0.032655 lr=0.000020 grad_norm=0.679828
Epoch 94/100 Iteration 162/234: loss=0.036845 lr=0.000020 grad_norm=0.403200
Epoch 94/100 Iteration 163/234: loss=0.032465 lr=0.000020 grad_norm=0.779486
Epoch 94/100 Iteration 164/234: loss=0.034685 lr=0.000020 grad_norm=0.658720
Epoch 94/100 Iteration 165/234: loss=0.030386 lr=0.000020 grad_norm=0.609926
Epoch 94/100 Iteration 166/234: loss=0.034013 lr=0.000020 grad_norm=0.906580
Epoch 94/100 Iteration 167/234: loss=0.037146 lr=0.000020 grad_norm=0.853040
Epoch 94/100 Iteration 168/234: loss=0.033548 lr=0.000020 grad_norm=0.855987
Epoch 94/100 Iteration 169/234: loss=0.031911 lr=0.000020 grad_norm=0.633963
Epoch 94/100 Iteration 170/234: loss=0.037213 lr=0.000020 grad_norm=0.643738
Epoch 94/100 Iteration 171/234: loss=0.033837 lr=0.000020 grad_norm=1.133102
Epoch 94/100 Iteration 172/234: loss=0.037866 lr=0.000020 grad_norm=0.823112
Epoch 94/100 Iteration 173/234: loss=0.036783 lr=0.000020 grad_norm=0.429797
Epoch 94/100 Iteration 174/234: loss=0.035160 lr=0.000020 grad_norm=0.661771
Epoch 94/100 Iteration 175/234: loss=0.036489 lr=0.000020 grad_norm=0.778789
Epoch 94/100 Iteration 176/234: loss=0.032944 lr=0.000020 grad_norm=0.580518
Epoch 94/100 Iteration 177/234: loss=0.034856 lr=0.000020 grad_norm=0.680883
Epoch 94/100 Iteration 178/234: loss=0.032347 lr=0.000020 grad_norm=0.707108
Epoch 94/100 Iteration 179/234: loss=0.036748 lr=0.000020 grad_norm=0.743659
Epoch 94/100 Iteration 180/234: loss=0.033308 lr=0.000020 grad_norm=0.820857
Epoch 94/100 Iteration 181/234: loss=0.034276 lr=0.000020 grad_norm=0.484469
Epoch 94/100 Iteration 182/234: loss=0.032371 lr=0.000020 grad_norm=0.613641
Epoch 94/100 Iteration 183/234: loss=0.037002 lr=0.000020 grad_norm=0.639826
Epoch 94/100 Iteration 184/234: loss=0.034785 lr=0.000020 grad_norm=0.575827
Epoch 94/100 Iteration 185/234: loss=0.034790 lr=0.000020 grad_norm=0.790101
Epoch 94/100 Iteration 186/234: loss=0.035133 lr=0.000020 grad_norm=0.861858
Epoch 94/100 Iteration 187/234: loss=0.032286 lr=0.000020 grad_norm=0.757515
Epoch 94/100 Iteration 188/234: loss=0.032141 lr=0.000020 grad_norm=0.346159
Epoch 94/100 Iteration 189/234: loss=0.034731 lr=0.000020 grad_norm=0.605193
Epoch 94/100 Iteration 190/234: loss=0.032911 lr=0.000020 grad_norm=0.765052
Epoch 94/100 Iteration 191/234: loss=0.034278 lr=0.000020 grad_norm=0.702122
Epoch 94/100 Iteration 192/234: loss=0.036595 lr=0.000020 grad_norm=0.632574
Epoch 94/100 Iteration 193/234: loss=0.038584 lr=0.000020 grad_norm=0.689924
Epoch 94/100 Iteration 194/234: loss=0.035918 lr=0.000020 grad_norm=0.701713
Epoch 94/100 Iteration 195/234: loss=0.034311 lr=0.000020 grad_norm=0.488697
Epoch 94/100 Iteration 196/234: loss=0.034998 lr=0.000020 grad_norm=0.496957
Epoch 94/100 Iteration 197/234: loss=0.031959 lr=0.000020 grad_norm=0.463501
Epoch 94/100 Iteration 198/234: loss=0.035221 lr=0.000020 grad_norm=0.480918
Epoch 94/100 Iteration 199/234: loss=0.032772 lr=0.000020 grad_norm=0.495262
Epoch 94/100 Iteration 200/234: loss=0.035717 lr=0.000020 grad_norm=0.432409
Epoch 94/100 Iteration 201/234: loss=0.034690 lr=0.000020 grad_norm=0.851662
Epoch 94/100 Iteration 202/234: loss=0.035987 lr=0.000020 grad_norm=1.056863
Epoch 94/100 Iteration 203/234: loss=0.032532 lr=0.000020 grad_norm=0.538230
Epoch 94/100 Iteration 204/234: loss=0.033011 lr=0.000020 grad_norm=0.531983
Epoch 94/100 Iteration 205/234: loss=0.036630 lr=0.000020 grad_norm=0.540362
Epoch 94/100 Iteration 206/234: loss=0.034134 lr=0.000020 grad_norm=0.430296
Epoch 94/100 Iteration 207/234: loss=0.032718 lr=0.000020 grad_norm=0.782556
Epoch 94/100 Iteration 208/234: loss=0.031635 lr=0.000020 grad_norm=0.637548
Epoch 94/100 Iteration 209/234: loss=0.033026 lr=0.000020 grad_norm=0.886144
Epoch 94/100 Iteration 210/234: loss=0.029787 lr=0.000020 grad_norm=0.725930
Epoch 94/100 Iteration 211/234: loss=0.032049 lr=0.000020 grad_norm=0.763006
Epoch 94/100 Iteration 212/234: loss=0.034095 lr=0.000020 grad_norm=1.368411
Epoch 94/100 Iteration 213/234: loss=0.033996 lr=0.000020 grad_norm=0.785802
Epoch 94/100 Iteration 214/234: loss=0.033742 lr=0.000020 grad_norm=0.731781
Epoch 94/100 Iteration 215/234: loss=0.034721 lr=0.000020 grad_norm=1.226624
Epoch 94/100 Iteration 216/234: loss=0.035472 lr=0.000020 grad_norm=0.858636
Epoch 94/100 Iteration 217/234: loss=0.031710 lr=0.000020 grad_norm=0.963884
Epoch 94/100 Iteration 218/234: loss=0.035394 lr=0.000020 grad_norm=1.071110
Epoch 94/100 Iteration 219/234: loss=0.034015 lr=0.000020 grad_norm=0.775315
Epoch 94/100 Iteration 220/234: loss=0.030844 lr=0.000020 grad_norm=0.802663
Epoch 94/100 Iteration 221/234: loss=0.036167 lr=0.000020 grad_norm=0.741281
Epoch 94/100 Iteration 222/234: loss=0.031331 lr=0.000020 grad_norm=0.849853
Epoch 94/100 Iteration 223/234: loss=0.033211 lr=0.000020 grad_norm=0.945696
Epoch 94/100 Iteration 224/234: loss=0.035787 lr=0.000020 grad_norm=0.482157
Epoch 94/100 Iteration 225/234: loss=0.031688 lr=0.000020 grad_norm=0.919313
Epoch 94/100 Iteration 226/234: loss=0.036595 lr=0.000020 grad_norm=0.748946
Epoch 94/100 Iteration 227/234: loss=0.034501 lr=0.000020 grad_norm=0.482183
Epoch 94/100 Iteration 228/234: loss=0.031071 lr=0.000020 grad_norm=0.770176
Epoch 94/100 Iteration 229/234: loss=0.036698 lr=0.000020 grad_norm=0.984763
Epoch 94/100 Iteration 230/234: loss=0.031618 lr=0.000020 grad_norm=0.880046
Epoch 94/100 Iteration 231/234: loss=0.032035 lr=0.000020 grad_norm=0.627102
Epoch 94/100 Iteration 232/234: loss=0.031485 lr=0.000020 grad_norm=0.539612
Epoch 94/100 Iteration 233/234: loss=0.033835 lr=0.000020 grad_norm=0.585379
Epoch 94/100 Iteration 234/234: loss=0.030907 lr=0.000020 grad_norm=0.480012
Epoch 94/100 finished. Avg Loss: 0.033953
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 95/100 Iteration 1/234: loss=0.035901 lr=0.000020 grad_norm=0.549567
Epoch 95/100 Iteration 2/234: loss=0.032720 lr=0.000020 grad_norm=0.434319
Epoch 95/100 Iteration 3/234: loss=0.035720 lr=0.000020 grad_norm=0.529984
Epoch 95/100 Iteration 4/234: loss=0.030493 lr=0.000020 grad_norm=0.366861
Epoch 95/100 Iteration 5/234: loss=0.031986 lr=0.000020 grad_norm=0.430206
Epoch 95/100 Iteration 6/234: loss=0.032829 lr=0.000020 grad_norm=0.521638
Epoch 95/100 Iteration 7/234: loss=0.032678 lr=0.000020 grad_norm=0.535195
Epoch 95/100 Iteration 8/234: loss=0.033155 lr=0.000020 grad_norm=0.542954
Epoch 95/100 Iteration 9/234: loss=0.032479 lr=0.000020 grad_norm=0.584757
Epoch 95/100 Iteration 10/234: loss=0.032561 lr=0.000020 grad_norm=0.423532
Epoch 95/100 Iteration 11/234: loss=0.033991 lr=0.000020 grad_norm=0.640372
Epoch 95/100 Iteration 12/234: loss=0.034012 lr=0.000020 grad_norm=0.694516
Epoch 95/100 Iteration 13/234: loss=0.037037 lr=0.000020 grad_norm=0.571526
Epoch 95/100 Iteration 14/234: loss=0.031285 lr=0.000020 grad_norm=0.514237
Epoch 95/100 Iteration 15/234: loss=0.033355 lr=0.000020 grad_norm=0.637971
Epoch 95/100 Iteration 16/234: loss=0.036933 lr=0.000020 grad_norm=0.676117
Epoch 95/100 Iteration 17/234: loss=0.037927 lr=0.000020 grad_norm=0.836942
Epoch 95/100 Iteration 18/234: loss=0.031723 lr=0.000020 grad_norm=0.965512
Epoch 95/100 Iteration 19/234: loss=0.032907 lr=0.000020 grad_norm=0.682095
Epoch 95/100 Iteration 20/234: loss=0.035331 lr=0.000020 grad_norm=0.607920
Epoch 95/100 Iteration 21/234: loss=0.033780 lr=0.000020 grad_norm=0.941869
Epoch 95/100 Iteration 22/234: loss=0.032866 lr=0.000020 grad_norm=0.670049
Epoch 95/100 Iteration 23/234: loss=0.032699 lr=0.000020 grad_norm=0.436709
Epoch 95/100 Iteration 24/234: loss=0.030102 lr=0.000020 grad_norm=0.634381
Epoch 95/100 Iteration 25/234: loss=0.033849 lr=0.000020 grad_norm=0.614821
Epoch 95/100 Iteration 26/234: loss=0.033607 lr=0.000020 grad_norm=0.539803
Epoch 95/100 Iteration 27/234: loss=0.034336 lr=0.000020 grad_norm=0.442747
Epoch 95/100 Iteration 28/234: loss=0.041236 lr=0.000020 grad_norm=0.714742
Epoch 95/100 Iteration 29/234: loss=0.036006 lr=0.000020 grad_norm=0.814840
Epoch 95/100 Iteration 30/234: loss=0.031729 lr=0.000020 grad_norm=0.399320
Epoch 95/100 Iteration 31/234: loss=0.035278 lr=0.000020 grad_norm=0.811577
Epoch 95/100 Iteration 32/234: loss=0.038745 lr=0.000020 grad_norm=1.248925
Epoch 95/100 Iteration 33/234: loss=0.032839 lr=0.000020 grad_norm=1.188608
Epoch 95/100 Iteration 34/234: loss=0.034738 lr=0.000020 grad_norm=0.745583
Epoch 95/100 Iteration 35/234: loss=0.033338 lr=0.000020 grad_norm=0.812575
Epoch 95/100 Iteration 36/234: loss=0.035249 lr=0.000020 grad_norm=0.701830
Epoch 95/100 Iteration 37/234: loss=0.031150 lr=0.000020 grad_norm=0.340062
Epoch 95/100 Iteration 38/234: loss=0.034151 lr=0.000020 grad_norm=0.720882
Epoch 95/100 Iteration 39/234: loss=0.033919 lr=0.000020 grad_norm=0.978037
Epoch 95/100 Iteration 40/234: loss=0.032175 lr=0.000020 grad_norm=1.116157
Epoch 95/100 Iteration 41/234: loss=0.032978 lr=0.000020 grad_norm=0.681279
Epoch 95/100 Iteration 42/234: loss=0.036683 lr=0.000020 grad_norm=0.766982
Epoch 95/100 Iteration 43/234: loss=0.034805 lr=0.000020 grad_norm=1.519894
Epoch 95/100 Iteration 44/234: loss=0.040115 lr=0.000020 grad_norm=1.730996
Epoch 95/100 Iteration 45/234: loss=0.033562 lr=0.000020 grad_norm=1.050348
Epoch 95/100 Iteration 46/234: loss=0.032650 lr=0.000020 grad_norm=0.690761
Epoch 95/100 Iteration 47/234: loss=0.032663 lr=0.000020 grad_norm=0.718069
Epoch 95/100 Iteration 48/234: loss=0.032051 lr=0.000020 grad_norm=0.582932
Epoch 95/100 Iteration 49/234: loss=0.032902 lr=0.000020 grad_norm=0.482596
Epoch 95/100 Iteration 50/234: loss=0.030237 lr=0.000020 grad_norm=0.350753
Epoch 95/100 Iteration 51/234: loss=0.031803 lr=0.000020 grad_norm=0.473314
Epoch 95/100 Iteration 52/234: loss=0.033394 lr=0.000020 grad_norm=0.628499
Epoch 95/100 Iteration 53/234: loss=0.036217 lr=0.000020 grad_norm=0.637378
Epoch 95/100 Iteration 54/234: loss=0.030756 lr=0.000020 grad_norm=0.483473
Epoch 95/100 Iteration 55/234: loss=0.031633 lr=0.000020 grad_norm=0.601613
Epoch 95/100 Iteration 56/234: loss=0.032530 lr=0.000020 grad_norm=0.449194
Epoch 95/100 Iteration 57/234: loss=0.035219 lr=0.000020 grad_norm=0.604688
Epoch 95/100 Iteration 58/234: loss=0.032983 lr=0.000020 grad_norm=0.542860
Epoch 95/100 Iteration 59/234: loss=0.033287 lr=0.000020 grad_norm=0.589566
Epoch 95/100 Iteration 60/234: loss=0.037366 lr=0.000020 grad_norm=0.523098
Epoch 95/100 Iteration 61/234: loss=0.035411 lr=0.000020 grad_norm=0.383495
Epoch 95/100 Iteration 62/234: loss=0.032329 lr=0.000020 grad_norm=0.536544
Epoch 95/100 Iteration 63/234: loss=0.035536 lr=0.000020 grad_norm=0.722845
Epoch 95/100 Iteration 64/234: loss=0.034788 lr=0.000020 grad_norm=0.580262
Epoch 95/100 Iteration 65/234: loss=0.035663 lr=0.000020 grad_norm=0.465596
Epoch 95/100 Iteration 66/234: loss=0.033782 lr=0.000020 grad_norm=0.489483
Epoch 95/100 Iteration 67/234: loss=0.031990 lr=0.000020 grad_norm=0.464998
Epoch 95/100 Iteration 68/234: loss=0.033403 lr=0.000020 grad_norm=0.543978
Epoch 95/100 Iteration 69/234: loss=0.034782 lr=0.000020 grad_norm=0.699141
Epoch 95/100 Iteration 70/234: loss=0.035859 lr=0.000020 grad_norm=0.506857
Epoch 95/100 Iteration 71/234: loss=0.037213 lr=0.000020 grad_norm=0.578701
Epoch 95/100 Iteration 72/234: loss=0.029962 lr=0.000020 grad_norm=0.699861
Epoch 95/100 Iteration 73/234: loss=0.030070 lr=0.000020 grad_norm=0.362959
Epoch 95/100 Iteration 74/234: loss=0.032718 lr=0.000020 grad_norm=0.763291
Epoch 95/100 Iteration 75/234: loss=0.033850 lr=0.000020 grad_norm=1.275766
Epoch 95/100 Iteration 76/234: loss=0.033331 lr=0.000020 grad_norm=1.208301
Epoch 95/100 Iteration 77/234: loss=0.031662 lr=0.000020 grad_norm=0.946440
Epoch 95/100 Iteration 78/234: loss=0.033290 lr=0.000020 grad_norm=0.583312
Epoch 95/100 Iteration 79/234: loss=0.033774 lr=0.000020 grad_norm=0.511483
Epoch 95/100 Iteration 80/234: loss=0.032749 lr=0.000020 grad_norm=0.445696
Epoch 95/100 Iteration 81/234: loss=0.034329 lr=0.000020 grad_norm=0.596049
Epoch 95/100 Iteration 82/234: loss=0.032330 lr=0.000020 grad_norm=0.735662
Epoch 95/100 Iteration 83/234: loss=0.034661 lr=0.000020 grad_norm=0.668314
Epoch 95/100 Iteration 84/234: loss=0.034406 lr=0.000020 grad_norm=0.549274
Epoch 95/100 Iteration 85/234: loss=0.030944 lr=0.000020 grad_norm=0.701553
Epoch 95/100 Iteration 86/234: loss=0.033617 lr=0.000020 grad_norm=0.519178
Epoch 95/100 Iteration 87/234: loss=0.035930 lr=0.000020 grad_norm=0.493539
Epoch 95/100 Iteration 88/234: loss=0.032659 lr=0.000020 grad_norm=0.455285
Epoch 95/100 Iteration 89/234: loss=0.035466 lr=0.000020 grad_norm=0.606878
Epoch 95/100 Iteration 90/234: loss=0.034771 lr=0.000020 grad_norm=0.670993
Epoch 95/100 Iteration 91/234: loss=0.036758 lr=0.000020 grad_norm=0.860167
Epoch 95/100 Iteration 92/234: loss=0.034327 lr=0.000020 grad_norm=1.005163
Epoch 95/100 Iteration 93/234: loss=0.034079 lr=0.000020 grad_norm=0.875222
Epoch 95/100 Iteration 94/234: loss=0.033720 lr=0.000020 grad_norm=0.705069
Epoch 95/100 Iteration 95/234: loss=0.030465 lr=0.000020 grad_norm=0.575057
Epoch 95/100 Iteration 96/234: loss=0.033828 lr=0.000020 grad_norm=0.905657
Epoch 95/100 Iteration 97/234: loss=0.031915 lr=0.000020 grad_norm=1.247295
Epoch 95/100 Iteration 98/234: loss=0.036902 lr=0.000020 grad_norm=1.060797
Epoch 95/100 Iteration 99/234: loss=0.034537 lr=0.000020 grad_norm=0.984824
Epoch 95/100 Iteration 100/234: loss=0.035413 lr=0.000020 grad_norm=1.476845
Epoch 95/100 Iteration 101/234: loss=0.030620 lr=0.000020 grad_norm=0.892530
Epoch 95/100 Iteration 102/234: loss=0.035142 lr=0.000020 grad_norm=1.243085
Epoch 95/100 Iteration 103/234: loss=0.035153 lr=0.000020 grad_norm=2.038276
Epoch 95/100 Iteration 104/234: loss=0.034085 lr=0.000020 grad_norm=1.154570
Epoch 95/100 Iteration 105/234: loss=0.031066 lr=0.000020 grad_norm=0.902742
Epoch 95/100 Iteration 106/234: loss=0.034483 lr=0.000020 grad_norm=1.334960
Epoch 95/100 Iteration 107/234: loss=0.031697 lr=0.000020 grad_norm=0.582619
Epoch 95/100 Iteration 108/234: loss=0.033558 lr=0.000020 grad_norm=0.834652
Epoch 95/100 Iteration 109/234: loss=0.031098 lr=0.000020 grad_norm=1.008043
Epoch 95/100 Iteration 110/234: loss=0.030659 lr=0.000020 grad_norm=0.481358
Epoch 95/100 Iteration 111/234: loss=0.036906 lr=0.000020 grad_norm=0.853698
Epoch 95/100 Iteration 112/234: loss=0.035040 lr=0.000020 grad_norm=1.531855
Epoch 95/100 Iteration 113/234: loss=0.036746 lr=0.000020 grad_norm=1.677675
Epoch 95/100 Iteration 114/234: loss=0.033910 lr=0.000020 grad_norm=0.698528
Epoch 95/100 Iteration 115/234: loss=0.031862 lr=0.000020 grad_norm=0.728447
Epoch 95/100 Iteration 116/234: loss=0.035233 lr=0.000020 grad_norm=0.855045
Epoch 95/100 Iteration 117/234: loss=0.034276 lr=0.000020 grad_norm=0.699972
Epoch 95/100 Iteration 118/234: loss=0.034121 lr=0.000020 grad_norm=1.514146
Epoch 95/100 Iteration 119/234: loss=0.031774 lr=0.000020 grad_norm=0.942768
Epoch 95/100 Iteration 120/234: loss=0.034662 lr=0.000020 grad_norm=0.836566
Epoch 95/100 Iteration 121/234: loss=0.035193 lr=0.000020 grad_norm=1.167568
Epoch 95/100 Iteration 122/234: loss=0.035504 lr=0.000020 grad_norm=0.911738
Epoch 95/100 Iteration 123/234: loss=0.032240 lr=0.000020 grad_norm=0.639383
Epoch 95/100 Iteration 124/234: loss=0.031553 lr=0.000020 grad_norm=0.866389
Epoch 95/100 Iteration 125/234: loss=0.034605 lr=0.000020 grad_norm=1.237110
Epoch 95/100 Iteration 126/234: loss=0.033401 lr=0.000020 grad_norm=0.986427
Epoch 95/100 Iteration 127/234: loss=0.032441 lr=0.000020 grad_norm=0.379466
Epoch 95/100 Iteration 128/234: loss=0.034308 lr=0.000020 grad_norm=0.634678
Epoch 95/100 Iteration 129/234: loss=0.036613 lr=0.000020 grad_norm=0.963601
Epoch 95/100 Iteration 130/234: loss=0.034783 lr=0.000020 grad_norm=0.976337
Epoch 95/100 Iteration 131/234: loss=0.032217 lr=0.000020 grad_norm=0.466536
Epoch 95/100 Iteration 132/234: loss=0.031380 lr=0.000020 grad_norm=0.750389
Epoch 95/100 Iteration 133/234: loss=0.028721 lr=0.000020 grad_norm=0.703469
Epoch 95/100 Iteration 134/234: loss=0.035387 lr=0.000020 grad_norm=0.738279
Epoch 95/100 Iteration 135/234: loss=0.032945 lr=0.000020 grad_norm=1.168197
Epoch 95/100 Iteration 136/234: loss=0.029833 lr=0.000020 grad_norm=0.522843
Epoch 95/100 Iteration 137/234: loss=0.035082 lr=0.000020 grad_norm=0.936287
Epoch 95/100 Iteration 138/234: loss=0.034919 lr=0.000020 grad_norm=1.383961
Epoch 95/100 Iteration 139/234: loss=0.032070 lr=0.000020 grad_norm=0.673445
Epoch 95/100 Iteration 140/234: loss=0.029819 lr=0.000020 grad_norm=0.864901
Epoch 95/100 Iteration 141/234: loss=0.031381 lr=0.000020 grad_norm=1.035934
Epoch 95/100 Iteration 142/234: loss=0.032178 lr=0.000020 grad_norm=0.477765
Epoch 95/100 Iteration 143/234: loss=0.036948 lr=0.000020 grad_norm=0.842699
Epoch 95/100 Iteration 144/234: loss=0.034274 lr=0.000020 grad_norm=0.815340
Epoch 95/100 Iteration 145/234: loss=0.032899 lr=0.000020 grad_norm=0.511620
Epoch 95/100 Iteration 146/234: loss=0.034608 lr=0.000020 grad_norm=0.664416
Epoch 95/100 Iteration 147/234: loss=0.033646 lr=0.000020 grad_norm=0.781869
Epoch 95/100 Iteration 148/234: loss=0.032359 lr=0.000020 grad_norm=0.667489
Epoch 95/100 Iteration 149/234: loss=0.035711 lr=0.000020 grad_norm=0.560946
Epoch 95/100 Iteration 150/234: loss=0.035906 lr=0.000020 grad_norm=0.541650
Epoch 95/100 Iteration 151/234: loss=0.034877 lr=0.000020 grad_norm=0.492020
Epoch 95/100 Iteration 152/234: loss=0.032192 lr=0.000020 grad_norm=0.534028
Epoch 95/100 Iteration 153/234: loss=0.034333 lr=0.000020 grad_norm=1.002066
Epoch 95/100 Iteration 154/234: loss=0.033959 lr=0.000020 grad_norm=0.854254
Epoch 95/100 Iteration 155/234: loss=0.034573 lr=0.000020 grad_norm=0.675589
Epoch 95/100 Iteration 156/234: loss=0.032938 lr=0.000020 grad_norm=1.601021
Epoch 95/100 Iteration 157/234: loss=0.036105 lr=0.000020 grad_norm=1.249679
Epoch 95/100 Iteration 158/234: loss=0.032090 lr=0.000020 grad_norm=0.557680
Epoch 95/100 Iteration 159/234: loss=0.030893 lr=0.000020 grad_norm=0.576631
Epoch 95/100 Iteration 160/234: loss=0.034128 lr=0.000020 grad_norm=0.699486
Epoch 95/100 Iteration 161/234: loss=0.031959 lr=0.000020 grad_norm=0.532634
Epoch 95/100 Iteration 162/234: loss=0.031034 lr=0.000020 grad_norm=0.536499
Epoch 95/100 Iteration 163/234: loss=0.032697 lr=0.000020 grad_norm=0.749955
Epoch 95/100 Iteration 164/234: loss=0.031849 lr=0.000020 grad_norm=1.121408
Epoch 95/100 Iteration 165/234: loss=0.032608 lr=0.000020 grad_norm=1.044315
Epoch 95/100 Iteration 166/234: loss=0.034957 lr=0.000020 grad_norm=0.591236
Epoch 95/100 Iteration 167/234: loss=0.033669 lr=0.000020 grad_norm=1.029714
Epoch 95/100 Iteration 168/234: loss=0.033699 lr=0.000020 grad_norm=1.398762
Epoch 95/100 Iteration 169/234: loss=0.028285 lr=0.000020 grad_norm=0.689904
Epoch 95/100 Iteration 170/234: loss=0.036252 lr=0.000020 grad_norm=1.158783
Epoch 95/100 Iteration 171/234: loss=0.034790 lr=0.000020 grad_norm=1.963114
Epoch 95/100 Iteration 172/234: loss=0.034391 lr=0.000020 grad_norm=1.602068
Epoch 95/100 Iteration 173/234: loss=0.030905 lr=0.000020 grad_norm=0.535964
Epoch 95/100 Iteration 174/234: loss=0.033048 lr=0.000020 grad_norm=1.340359
Epoch 95/100 Iteration 175/234: loss=0.035017 lr=0.000020 grad_norm=1.430064
Epoch 95/100 Iteration 176/234: loss=0.034965 lr=0.000020 grad_norm=0.839706
Epoch 95/100 Iteration 177/234: loss=0.032588 lr=0.000020 grad_norm=0.915311
Epoch 95/100 Iteration 178/234: loss=0.033706 lr=0.000020 grad_norm=0.821204
Epoch 95/100 Iteration 179/234: loss=0.032405 lr=0.000020 grad_norm=0.642229
Epoch 95/100 Iteration 180/234: loss=0.032843 lr=0.000020 grad_norm=0.652460
Epoch 95/100 Iteration 181/234: loss=0.038091 lr=0.000020 grad_norm=0.805791
Epoch 95/100 Iteration 182/234: loss=0.031201 lr=0.000020 grad_norm=0.681972
Epoch 95/100 Iteration 183/234: loss=0.035578 lr=0.000020 grad_norm=0.771719
Epoch 95/100 Iteration 184/234: loss=0.031617 lr=0.000020 grad_norm=0.876890
Epoch 95/100 Iteration 185/234: loss=0.029869 lr=0.000020 grad_norm=0.412018
Epoch 95/100 Iteration 186/234: loss=0.034546 lr=0.000020 grad_norm=0.892041
Epoch 95/100 Iteration 187/234: loss=0.033548 lr=0.000020 grad_norm=0.670396
Epoch 95/100 Iteration 188/234: loss=0.034229 lr=0.000020 grad_norm=0.542612
Epoch 95/100 Iteration 189/234: loss=0.034059 lr=0.000020 grad_norm=0.811508
Epoch 95/100 Iteration 190/234: loss=0.034839 lr=0.000020 grad_norm=0.504997
Epoch 95/100 Iteration 191/234: loss=0.034460 lr=0.000020 grad_norm=0.589286
Epoch 95/100 Iteration 192/234: loss=0.033880 lr=0.000020 grad_norm=0.654766
Epoch 95/100 Iteration 193/234: loss=0.037248 lr=0.000020 grad_norm=0.515522
Epoch 95/100 Iteration 194/234: loss=0.037407 lr=0.000020 grad_norm=0.894617
Epoch 95/100 Iteration 195/234: loss=0.031954 lr=0.000020 grad_norm=0.825688
Epoch 95/100 Iteration 196/234: loss=0.031388 lr=0.000020 grad_norm=0.440317
Epoch 95/100 Iteration 197/234: loss=0.036574 lr=0.000020 grad_norm=0.762690
Epoch 95/100 Iteration 198/234: loss=0.035376 lr=0.000020 grad_norm=0.939227
Epoch 95/100 Iteration 199/234: loss=0.034287 lr=0.000020 grad_norm=0.719077
Epoch 95/100 Iteration 200/234: loss=0.033357 lr=0.000020 grad_norm=0.487901
Epoch 95/100 Iteration 201/234: loss=0.033081 lr=0.000020 grad_norm=0.447502
Epoch 95/100 Iteration 202/234: loss=0.035574 lr=0.000020 grad_norm=0.459990
Epoch 95/100 Iteration 203/234: loss=0.035991 lr=0.000020 grad_norm=0.569984
Epoch 95/100 Iteration 204/234: loss=0.034540 lr=0.000020 grad_norm=0.623954
Epoch 95/100 Iteration 205/234: loss=0.032437 lr=0.000020 grad_norm=0.473956
Epoch 95/100 Iteration 206/234: loss=0.034524 lr=0.000020 grad_norm=0.725782
Epoch 95/100 Iteration 207/234: loss=0.033426 lr=0.000020 grad_norm=0.625005
Epoch 95/100 Iteration 208/234: loss=0.033598 lr=0.000020 grad_norm=0.560919
Epoch 95/100 Iteration 209/234: loss=0.032401 lr=0.000020 grad_norm=0.598331
Epoch 95/100 Iteration 210/234: loss=0.033205 lr=0.000020 grad_norm=0.604065
Epoch 95/100 Iteration 211/234: loss=0.032456 lr=0.000020 grad_norm=0.671631
Epoch 95/100 Iteration 212/234: loss=0.034326 lr=0.000020 grad_norm=0.451653
Epoch 95/100 Iteration 213/234: loss=0.033922 lr=0.000020 grad_norm=0.752496
Epoch 95/100 Iteration 214/234: loss=0.032777 lr=0.000020 grad_norm=0.871436
Epoch 95/100 Iteration 215/234: loss=0.032763 lr=0.000020 grad_norm=0.528994
Epoch 95/100 Iteration 216/234: loss=0.035462 lr=0.000020 grad_norm=0.851823
Epoch 95/100 Iteration 217/234: loss=0.032213 lr=0.000020 grad_norm=1.136719
Epoch 95/100 Iteration 218/234: loss=0.032448 lr=0.000020 grad_norm=0.514891
Epoch 95/100 Iteration 219/234: loss=0.034887 lr=0.000020 grad_norm=0.767126
Epoch 95/100 Iteration 220/234: loss=0.035999 lr=0.000020 grad_norm=1.151075
Epoch 95/100 Iteration 221/234: loss=0.032554 lr=0.000020 grad_norm=0.865702
Epoch 95/100 Iteration 222/234: loss=0.036912 lr=0.000020 grad_norm=0.528261
Epoch 95/100 Iteration 223/234: loss=0.032512 lr=0.000020 grad_norm=1.215620
Epoch 95/100 Iteration 224/234: loss=0.030846 lr=0.000020 grad_norm=1.102205
Epoch 95/100 Iteration 225/234: loss=0.035656 lr=0.000020 grad_norm=0.642975
Epoch 95/100 Iteration 226/234: loss=0.030989 lr=0.000020 grad_norm=1.104500
Epoch 95/100 Iteration 227/234: loss=0.031481 lr=0.000020 grad_norm=0.386719
Epoch 95/100 Iteration 228/234: loss=0.036483 lr=0.000020 grad_norm=1.356997
Epoch 95/100 Iteration 229/234: loss=0.033219 lr=0.000020 grad_norm=1.814314
Epoch 95/100 Iteration 230/234: loss=0.035478 lr=0.000020 grad_norm=0.548970
Epoch 95/100 Iteration 231/234: loss=0.038018 lr=0.000020 grad_norm=1.500627
Epoch 95/100 Iteration 232/234: loss=0.033262 lr=0.000020 grad_norm=1.253175
Epoch 95/100 Iteration 233/234: loss=0.033909 lr=0.000020 grad_norm=0.496754
Epoch 95/100 Iteration 234/234: loss=0.031808 lr=0.000020 grad_norm=0.949661
Epoch 95/100 finished. Avg Loss: 0.033715
Epoch 96/100 Iteration 1/234: loss=0.032949 lr=0.000020 grad_norm=0.542711
Epoch 96/100 Iteration 2/234: loss=0.034460 lr=0.000020 grad_norm=1.383519
Epoch 96/100 Iteration 3/234: loss=0.033848 lr=0.000020 grad_norm=0.560022
Epoch 96/100 Iteration 4/234: loss=0.032591 lr=0.000020 grad_norm=1.113875
Epoch 96/100 Iteration 5/234: loss=0.032909 lr=0.000020 grad_norm=0.740149
Epoch 96/100 Iteration 6/234: loss=0.033112 lr=0.000020 grad_norm=0.818626
Epoch 96/100 Iteration 7/234: loss=0.031894 lr=0.000020 grad_norm=1.268431
Epoch 96/100 Iteration 8/234: loss=0.033129 lr=0.000020 grad_norm=0.662402
Epoch 96/100 Iteration 9/234: loss=0.030601 lr=0.000020 grad_norm=0.707605
Epoch 96/100 Iteration 10/234: loss=0.032460 lr=0.000020 grad_norm=0.579544
Epoch 96/100 Iteration 11/234: loss=0.034240 lr=0.000020 grad_norm=0.815598
Epoch 96/100 Iteration 12/234: loss=0.029856 lr=0.000020 grad_norm=0.504717
Epoch 96/100 Iteration 13/234: loss=0.033801 lr=0.000020 grad_norm=0.687986
Epoch 96/100 Iteration 14/234: loss=0.031945 lr=0.000020 grad_norm=0.718382
Epoch 96/100 Iteration 15/234: loss=0.035544 lr=0.000020 grad_norm=0.707528
Epoch 96/100 Iteration 16/234: loss=0.032663 lr=0.000020 grad_norm=0.668445
Epoch 96/100 Iteration 17/234: loss=0.034134 lr=0.000020 grad_norm=0.668299
Epoch 96/100 Iteration 18/234: loss=0.036024 lr=0.000020 grad_norm=1.055882
Epoch 96/100 Iteration 19/234: loss=0.032070 lr=0.000020 grad_norm=0.782479
Epoch 96/100 Iteration 20/234: loss=0.035375 lr=0.000020 grad_norm=0.624983
Epoch 96/100 Iteration 21/234: loss=0.035707 lr=0.000020 grad_norm=0.975228
Epoch 96/100 Iteration 22/234: loss=0.034780 lr=0.000020 grad_norm=0.485211
Epoch 96/100 Iteration 23/234: loss=0.035695 lr=0.000020 grad_norm=0.548395
Epoch 96/100 Iteration 24/234: loss=0.033431 lr=0.000020 grad_norm=0.519281
Epoch 96/100 Iteration 25/234: loss=0.035056 lr=0.000020 grad_norm=0.585654
Epoch 96/100 Iteration 26/234: loss=0.032573 lr=0.000020 grad_norm=0.705356
Epoch 96/100 Iteration 27/234: loss=0.034866 lr=0.000020 grad_norm=0.538454
Epoch 96/100 Iteration 28/234: loss=0.033918 lr=0.000020 grad_norm=0.659849
Epoch 96/100 Iteration 29/234: loss=0.030364 lr=0.000020 grad_norm=0.649954
Epoch 96/100 Iteration 30/234: loss=0.034155 lr=0.000020 grad_norm=0.519011
Epoch 96/100 Iteration 31/234: loss=0.036551 lr=0.000020 grad_norm=1.090976
Epoch 96/100 Iteration 32/234: loss=0.032163 lr=0.000020 grad_norm=1.075325
Epoch 96/100 Iteration 33/234: loss=0.034913 lr=0.000020 grad_norm=0.822061
Epoch 96/100 Iteration 34/234: loss=0.031875 lr=0.000020 grad_norm=0.751061
Epoch 96/100 Iteration 35/234: loss=0.032545 lr=0.000020 grad_norm=0.834919
Epoch 96/100 Iteration 36/234: loss=0.034519 lr=0.000020 grad_norm=0.976880
Epoch 96/100 Iteration 37/234: loss=0.031987 lr=0.000020 grad_norm=0.492298
Epoch 96/100 Iteration 38/234: loss=0.033140 lr=0.000020 grad_norm=0.933922
Epoch 96/100 Iteration 39/234: loss=0.035290 lr=0.000020 grad_norm=1.257006
Epoch 96/100 Iteration 40/234: loss=0.031011 lr=0.000020 grad_norm=0.723344
Epoch 96/100 Iteration 41/234: loss=0.032429 lr=0.000020 grad_norm=0.584944
Epoch 96/100 Iteration 42/234: loss=0.033903 lr=0.000020 grad_norm=1.088209
Epoch 96/100 Iteration 43/234: loss=0.031061 lr=0.000020 grad_norm=0.734670
Epoch 96/100 Iteration 44/234: loss=0.030511 lr=0.000020 grad_norm=0.501902
Epoch 96/100 Iteration 45/234: loss=0.035274 lr=0.000020 grad_norm=0.824774
Epoch 96/100 Iteration 46/234: loss=0.033123 lr=0.000020 grad_norm=1.026646
Epoch 96/100 Iteration 47/234: loss=0.036446 lr=0.000020 grad_norm=0.668612
Epoch 96/100 Iteration 48/234: loss=0.034303 lr=0.000020 grad_norm=0.860457
Epoch 96/100 Iteration 49/234: loss=0.031986 lr=0.000020 grad_norm=1.078971
Epoch 96/100 Iteration 50/234: loss=0.033895 lr=0.000020 grad_norm=0.721459
Epoch 96/100 Iteration 51/234: loss=0.037238 lr=0.000020 grad_norm=0.406903
Epoch 96/100 Iteration 52/234: loss=0.034285 lr=0.000020 grad_norm=0.481342
Epoch 96/100 Iteration 53/234: loss=0.034469 lr=0.000020 grad_norm=0.618604
Epoch 96/100 Iteration 54/234: loss=0.030744 lr=0.000020 grad_norm=0.469520
Epoch 96/100 Iteration 55/234: loss=0.033442 lr=0.000020 grad_norm=0.362897
Epoch 96/100 Iteration 56/234: loss=0.035422 lr=0.000020 grad_norm=0.548167
Epoch 96/100 Iteration 57/234: loss=0.031003 lr=0.000020 grad_norm=0.460274
Epoch 96/100 Iteration 58/234: loss=0.032415 lr=0.000020 grad_norm=0.375392
Epoch 96/100 Iteration 59/234: loss=0.034932 lr=0.000020 grad_norm=0.508495
Epoch 96/100 Iteration 60/234: loss=0.032674 lr=0.000020 grad_norm=0.526968
Epoch 96/100 Iteration 61/234: loss=0.032655 lr=0.000020 grad_norm=0.519161
Epoch 96/100 Iteration 62/234: loss=0.029678 lr=0.000020 grad_norm=0.515715
Epoch 96/100 Iteration 63/234: loss=0.031148 lr=0.000020 grad_norm=0.444890
Epoch 96/100 Iteration 64/234: loss=0.033829 lr=0.000020 grad_norm=0.410531
Epoch 96/100 Iteration 65/234: loss=0.034961 lr=0.000020 grad_norm=0.426225
Epoch 96/100 Iteration 66/234: loss=0.031955 lr=0.000020 grad_norm=0.450030
Epoch 96/100 Iteration 67/234: loss=0.032544 lr=0.000020 grad_norm=0.375049
Epoch 96/100 Iteration 68/234: loss=0.034525 lr=0.000020 grad_norm=0.516545
Epoch 96/100 Iteration 69/234: loss=0.033152 lr=0.000020 grad_norm=0.541638
Epoch 96/100 Iteration 70/234: loss=0.031640 lr=0.000020 grad_norm=0.413242
Epoch 96/100 Iteration 71/234: loss=0.032695 lr=0.000020 grad_norm=0.542940
Epoch 96/100 Iteration 72/234: loss=0.033923 lr=0.000020 grad_norm=0.483301
Epoch 96/100 Iteration 73/234: loss=0.031043 lr=0.000020 grad_norm=0.725703
Epoch 96/100 Iteration 74/234: loss=0.032123 lr=0.000020 grad_norm=0.808389
Epoch 96/100 Iteration 75/234: loss=0.034514 lr=0.000020 grad_norm=0.492956
Epoch 96/100 Iteration 76/234: loss=0.032686 lr=0.000020 grad_norm=0.842720
Epoch 96/100 Iteration 77/234: loss=0.030753 lr=0.000020 grad_norm=0.679804
Epoch 96/100 Iteration 78/234: loss=0.034779 lr=0.000020 grad_norm=0.447329
Epoch 96/100 Iteration 79/234: loss=0.033615 lr=0.000020 grad_norm=0.901761
Epoch 96/100 Iteration 80/234: loss=0.030878 lr=0.000020 grad_norm=0.672933
Epoch 96/100 Iteration 81/234: loss=0.031278 lr=0.000020 grad_norm=0.517421
Epoch 96/100 Iteration 82/234: loss=0.034624 lr=0.000020 grad_norm=0.726453
Epoch 96/100 Iteration 83/234: loss=0.034598 lr=0.000020 grad_norm=0.627690
Epoch 96/100 Iteration 84/234: loss=0.032163 lr=0.000020 grad_norm=0.524828
Epoch 96/100 Iteration 85/234: loss=0.032936 lr=0.000020 grad_norm=0.494248
Epoch 96/100 Iteration 86/234: loss=0.035611 lr=0.000020 grad_norm=0.438517
Epoch 96/100 Iteration 87/234: loss=0.034260 lr=0.000020 grad_norm=0.529967
Epoch 96/100 Iteration 88/234: loss=0.031293 lr=0.000020 grad_norm=0.319900
Epoch 96/100 Iteration 89/234: loss=0.035288 lr=0.000020 grad_norm=0.539986
Epoch 96/100 Iteration 90/234: loss=0.034076 lr=0.000020 grad_norm=0.796129
Epoch 96/100 Iteration 91/234: loss=0.033501 lr=0.000020 grad_norm=0.960153
Epoch 96/100 Iteration 92/234: loss=0.032732 lr=0.000020 grad_norm=0.746726
Epoch 96/100 Iteration 93/234: loss=0.034172 lr=0.000020 grad_norm=0.506481
Epoch 96/100 Iteration 94/234: loss=0.032818 lr=0.000020 grad_norm=1.081391
Epoch 96/100 Iteration 95/234: loss=0.031379 lr=0.000020 grad_norm=1.027646
Epoch 96/100 Iteration 96/234: loss=0.035047 lr=0.000020 grad_norm=0.893655
Epoch 96/100 Iteration 97/234: loss=0.034665 lr=0.000020 grad_norm=0.671878
Epoch 96/100 Iteration 98/234: loss=0.028943 lr=0.000020 grad_norm=0.696124
Epoch 96/100 Iteration 99/234: loss=0.034612 lr=0.000020 grad_norm=0.845335
Epoch 96/100 Iteration 100/234: loss=0.032567 lr=0.000020 grad_norm=0.713515
Epoch 96/100 Iteration 101/234: loss=0.035083 lr=0.000020 grad_norm=0.634143
Epoch 96/100 Iteration 102/234: loss=0.033579 lr=0.000020 grad_norm=0.368583
Epoch 96/100 Iteration 103/234: loss=0.032998 lr=0.000020 grad_norm=0.553419
Epoch 96/100 Iteration 104/234: loss=0.033729 lr=0.000020 grad_norm=0.630594
Epoch 96/100 Iteration 105/234: loss=0.033166 lr=0.000020 grad_norm=0.459109
Epoch 96/100 Iteration 106/234: loss=0.033587 lr=0.000020 grad_norm=0.605644
Epoch 96/100 Iteration 107/234: loss=0.035456 lr=0.000020 grad_norm=0.874901
Epoch 96/100 Iteration 108/234: loss=0.032618 lr=0.000020 grad_norm=0.679334
Epoch 96/100 Iteration 109/234: loss=0.037251 lr=0.000020 grad_norm=0.482483
Epoch 96/100 Iteration 110/234: loss=0.032012 lr=0.000020 grad_norm=0.620101
Epoch 96/100 Iteration 111/234: loss=0.033437 lr=0.000020 grad_norm=0.902909
Epoch 96/100 Iteration 112/234: loss=0.033293 lr=0.000020 grad_norm=0.598911
Epoch 96/100 Iteration 113/234: loss=0.032742 lr=0.000020 grad_norm=0.545048
Epoch 96/100 Iteration 114/234: loss=0.035719 lr=0.000020 grad_norm=0.921299
Epoch 96/100 Iteration 115/234: loss=0.033591 lr=0.000020 grad_norm=0.726391
Epoch 96/100 Iteration 116/234: loss=0.031252 lr=0.000020 grad_norm=0.505317
Epoch 96/100 Iteration 117/234: loss=0.035100 lr=0.000020 grad_norm=0.522122
Epoch 96/100 Iteration 118/234: loss=0.030335 lr=0.000020 grad_norm=0.443262
Epoch 96/100 Iteration 119/234: loss=0.031980 lr=0.000020 grad_norm=0.495129
Epoch 96/100 Iteration 120/234: loss=0.034067 lr=0.000020 grad_norm=0.667018
Epoch 96/100 Iteration 121/234: loss=0.035870 lr=0.000020 grad_norm=0.642062
Epoch 96/100 Iteration 122/234: loss=0.032670 lr=0.000020 grad_norm=0.629452
Epoch 96/100 Iteration 123/234: loss=0.030381 lr=0.000020 grad_norm=0.421949
Epoch 96/100 Iteration 124/234: loss=0.030956 lr=0.000020 grad_norm=0.504158
Epoch 96/100 Iteration 125/234: loss=0.031781 lr=0.000020 grad_norm=0.687394
Epoch 96/100 Iteration 126/234: loss=0.029267 lr=0.000020 grad_norm=0.442931
Epoch 96/100 Iteration 127/234: loss=0.033810 lr=0.000020 grad_norm=0.620138
Epoch 96/100 Iteration 128/234: loss=0.034726 lr=0.000020 grad_norm=0.913238
Epoch 96/100 Iteration 129/234: loss=0.036478 lr=0.000020 grad_norm=0.956465
Epoch 96/100 Iteration 130/234: loss=0.034103 lr=0.000020 grad_norm=0.638356
Epoch 96/100 Iteration 131/234: loss=0.034327 lr=0.000020 grad_norm=0.865475
Epoch 96/100 Iteration 132/234: loss=0.032417 lr=0.000020 grad_norm=1.238035
Epoch 96/100 Iteration 133/234: loss=0.032652 lr=0.000020 grad_norm=0.646768
Epoch 96/100 Iteration 134/234: loss=0.029371 lr=0.000020 grad_norm=0.584396
Epoch 96/100 Iteration 135/234: loss=0.033978 lr=0.000020 grad_norm=0.615609
Epoch 96/100 Iteration 136/234: loss=0.030604 lr=0.000020 grad_norm=0.567615
Epoch 96/100 Iteration 137/234: loss=0.034628 lr=0.000020 grad_norm=0.806981
Epoch 96/100 Iteration 138/234: loss=0.032109 lr=0.000020 grad_norm=0.595232
Epoch 96/100 Iteration 139/234: loss=0.031919 lr=0.000020 grad_norm=0.483171
Epoch 96/100 Iteration 140/234: loss=0.032688 lr=0.000020 grad_norm=0.845746
Epoch 96/100 Iteration 141/234: loss=0.033677 lr=0.000020 grad_norm=1.038283
Epoch 96/100 Iteration 142/234: loss=0.031898 lr=0.000020 grad_norm=0.912297
Epoch 96/100 Iteration 143/234: loss=0.031753 lr=0.000020 grad_norm=0.598472
Epoch 96/100 Iteration 144/234: loss=0.033697 lr=0.000020 grad_norm=1.139821
Epoch 96/100 Iteration 145/234: loss=0.033176 lr=0.000020 grad_norm=1.329860
Epoch 96/100 Iteration 146/234: loss=0.032737 lr=0.000020 grad_norm=1.332597
Epoch 96/100 Iteration 147/234: loss=0.036060 lr=0.000020 grad_norm=1.262166
Epoch 96/100 Iteration 148/234: loss=0.034862 lr=0.000020 grad_norm=1.071093
Epoch 96/100 Iteration 149/234: loss=0.032877 lr=0.000020 grad_norm=1.361049
Epoch 96/100 Iteration 150/234: loss=0.032959 lr=0.000020 grad_norm=1.357069
Epoch 96/100 Iteration 151/234: loss=0.033062 lr=0.000020 grad_norm=0.829994
Epoch 96/100 Iteration 152/234: loss=0.034496 lr=0.000020 grad_norm=1.121185
Epoch 96/100 Iteration 153/234: loss=0.032166 lr=0.000020 grad_norm=1.174901
Epoch 96/100 Iteration 154/234: loss=0.033974 lr=0.000020 grad_norm=0.636786
Epoch 96/100 Iteration 155/234: loss=0.038498 lr=0.000020 grad_norm=1.107180
Epoch 96/100 Iteration 156/234: loss=0.032503 lr=0.000020 grad_norm=1.394321
Epoch 96/100 Iteration 157/234: loss=0.035815 lr=0.000020 grad_norm=0.768023
Epoch 96/100 Iteration 158/234: loss=0.031148 lr=0.000020 grad_norm=1.239108
Epoch 96/100 Iteration 159/234: loss=0.033689 lr=0.000020 grad_norm=1.077744
Epoch 96/100 Iteration 160/234: loss=0.031829 lr=0.000020 grad_norm=0.516963
Epoch 96/100 Iteration 161/234: loss=0.032313 lr=0.000020 grad_norm=0.552733
Epoch 96/100 Iteration 162/234: loss=0.032248 lr=0.000020 grad_norm=0.792034
Epoch 96/100 Iteration 163/234: loss=0.032476 lr=0.000020 grad_norm=0.495140
Epoch 96/100 Iteration 164/234: loss=0.033743 lr=0.000020 grad_norm=0.613742
Epoch 96/100 Iteration 165/234: loss=0.035253 lr=0.000020 grad_norm=1.059534
Epoch 96/100 Iteration 166/234: loss=0.034009 lr=0.000020 grad_norm=0.990490
Epoch 96/100 Iteration 167/234: loss=0.035254 lr=0.000020 grad_norm=0.502103
Epoch 96/100 Iteration 168/234: loss=0.037289 lr=0.000020 grad_norm=1.484277
Epoch 96/100 Iteration 169/234: loss=0.034386 lr=0.000020 grad_norm=1.469572
Epoch 96/100 Iteration 170/234: loss=0.034578 lr=0.000020 grad_norm=0.694020
Epoch 96/100 Iteration 171/234: loss=0.030625 lr=0.000020 grad_norm=0.650119
Epoch 96/100 Iteration 172/234: loss=0.032823 lr=0.000020 grad_norm=1.194971
Epoch 96/100 Iteration 173/234: loss=0.031877 lr=0.000020 grad_norm=0.864642
Epoch 96/100 Iteration 174/234: loss=0.032419 lr=0.000020 grad_norm=0.643454
Epoch 96/100 Iteration 175/234: loss=0.034766 lr=0.000020 grad_norm=1.498431
Epoch 96/100 Iteration 176/234: loss=0.033479 lr=0.000020 grad_norm=1.389155
Epoch 96/100 Iteration 177/234: loss=0.032299 lr=0.000020 grad_norm=0.605257
Epoch 96/100 Iteration 178/234: loss=0.032988 lr=0.000020 grad_norm=0.945059
Epoch 96/100 Iteration 179/234: loss=0.030624 lr=0.000020 grad_norm=0.743304
Epoch 96/100 Iteration 180/234: loss=0.033764 lr=0.000020 grad_norm=0.815238
Epoch 96/100 Iteration 181/234: loss=0.034677 lr=0.000020 grad_norm=1.311011
Epoch 96/100 Iteration 182/234: loss=0.036093 lr=0.000020 grad_norm=1.523692
Epoch 96/100 Iteration 183/234: loss=0.031433 lr=0.000020 grad_norm=0.568973
Epoch 96/100 Iteration 184/234: loss=0.031501 lr=0.000020 grad_norm=1.280078
Epoch 96/100 Iteration 185/234: loss=0.035532 lr=0.000020 grad_norm=1.229910
Epoch 96/100 Iteration 186/234: loss=0.034786 lr=0.000020 grad_norm=0.541393
Epoch 96/100 Iteration 187/234: loss=0.030406 lr=0.000020 grad_norm=0.675739
Epoch 96/100 Iteration 188/234: loss=0.030717 lr=0.000020 grad_norm=0.460157
Epoch 96/100 Iteration 189/234: loss=0.030511 lr=0.000020 grad_norm=0.750578
Epoch 96/100 Iteration 190/234: loss=0.034760 lr=0.000020 grad_norm=0.839663
Epoch 96/100 Iteration 191/234: loss=0.033974 lr=0.000020 grad_norm=0.550664
Epoch 96/100 Iteration 192/234: loss=0.034222 lr=0.000020 grad_norm=0.746385
Epoch 96/100 Iteration 193/234: loss=0.033711 lr=0.000020 grad_norm=0.911232
Epoch 96/100 Iteration 194/234: loss=0.031911 lr=0.000020 grad_norm=0.379142
Epoch 96/100 Iteration 195/234: loss=0.033243 lr=0.000020 grad_norm=0.962938
Epoch 96/100 Iteration 196/234: loss=0.030917 lr=0.000020 grad_norm=0.627636
Epoch 96/100 Iteration 197/234: loss=0.032316 lr=0.000020 grad_norm=0.525030
Epoch 96/100 Iteration 198/234: loss=0.033682 lr=0.000020 grad_norm=0.485101
Epoch 96/100 Iteration 199/234: loss=0.030769 lr=0.000020 grad_norm=0.612072
Epoch 96/100 Iteration 200/234: loss=0.034554 lr=0.000020 grad_norm=0.752251
Epoch 96/100 Iteration 201/234: loss=0.031024 lr=0.000020 grad_norm=0.522452
Epoch 96/100 Iteration 202/234: loss=0.032271 lr=0.000020 grad_norm=0.768632
Epoch 96/100 Iteration 203/234: loss=0.032571 lr=0.000020 grad_norm=0.640868
Epoch 96/100 Iteration 204/234: loss=0.035847 lr=0.000020 grad_norm=0.623497
Epoch 96/100 Iteration 205/234: loss=0.035819 lr=0.000020 grad_norm=0.900547
Epoch 96/100 Iteration 206/234: loss=0.036137 lr=0.000020 grad_norm=0.667841
Epoch 96/100 Iteration 207/234: loss=0.031858 lr=0.000020 grad_norm=0.773169
Epoch 96/100 Iteration 208/234: loss=0.029918 lr=0.000020 grad_norm=0.701458
Epoch 96/100 Iteration 209/234: loss=0.029947 lr=0.000020 grad_norm=0.504205
Epoch 96/100 Iteration 210/234: loss=0.031348 lr=0.000020 grad_norm=0.421083
Epoch 96/100 Iteration 211/234: loss=0.033651 lr=0.000020 grad_norm=0.381239
Epoch 96/100 Iteration 212/234: loss=0.032392 lr=0.000020 grad_norm=0.514907
Epoch 96/100 Iteration 213/234: loss=0.033044 lr=0.000020 grad_norm=0.676845
Epoch 96/100 Iteration 214/234: loss=0.032182 lr=0.000020 grad_norm=0.467610
Epoch 96/100 Iteration 215/234: loss=0.033318 lr=0.000020 grad_norm=0.524344
Epoch 96/100 Iteration 216/234: loss=0.032155 lr=0.000020 grad_norm=0.786943
Epoch 96/100 Iteration 217/234: loss=0.032972 lr=0.000020 grad_norm=0.476072
Epoch 96/100 Iteration 218/234: loss=0.030114 lr=0.000020 grad_norm=0.521430
Epoch 96/100 Iteration 219/234: loss=0.031568 lr=0.000020 grad_norm=0.823083
Epoch 96/100 Iteration 220/234: loss=0.031906 lr=0.000020 grad_norm=0.667654
Epoch 96/100 Iteration 221/234: loss=0.032939 lr=0.000020 grad_norm=0.488390
Epoch 96/100 Iteration 222/234: loss=0.035398 lr=0.000020 grad_norm=0.630740
Epoch 96/100 Iteration 223/234: loss=0.031956 lr=0.000020 grad_norm=0.980697
Epoch 96/100 Iteration 224/234: loss=0.031431 lr=0.000020 grad_norm=0.725037
Epoch 96/100 Iteration 225/234: loss=0.034019 lr=0.000020 grad_norm=0.434684
Epoch 96/100 Iteration 226/234: loss=0.030407 lr=0.000020 grad_norm=0.627897
Epoch 96/100 Iteration 227/234: loss=0.031474 lr=0.000020 grad_norm=0.658410
Epoch 96/100 Iteration 228/234: loss=0.035088 lr=0.000020 grad_norm=0.791941
Epoch 96/100 Iteration 229/234: loss=0.034423 lr=0.000020 grad_norm=1.101742
Epoch 96/100 Iteration 230/234: loss=0.033314 lr=0.000020 grad_norm=0.543257
Epoch 96/100 Iteration 231/234: loss=0.033010 lr=0.000020 grad_norm=1.044877
Epoch 96/100 Iteration 232/234: loss=0.032781 lr=0.000020 grad_norm=1.558953
Epoch 96/100 Iteration 233/234: loss=0.033836 lr=0.000020 grad_norm=0.952462
Epoch 96/100 Iteration 234/234: loss=0.037388 lr=0.000020 grad_norm=0.784646
Epoch 96/100 finished. Avg Loss: 0.033198
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 97/100 Iteration 1/234: loss=0.034475 lr=0.000020 grad_norm=0.974025
Epoch 97/100 Iteration 2/234: loss=0.032820 lr=0.000020 grad_norm=0.767069
Epoch 97/100 Iteration 3/234: loss=0.031974 lr=0.000020 grad_norm=0.633446
Epoch 97/100 Iteration 4/234: loss=0.036935 lr=0.000020 grad_norm=0.688963
Epoch 97/100 Iteration 5/234: loss=0.033677 lr=0.000020 grad_norm=0.748230
Epoch 97/100 Iteration 6/234: loss=0.033825 lr=0.000020 grad_norm=0.711001
Epoch 97/100 Iteration 7/234: loss=0.029273 lr=0.000020 grad_norm=0.502829
Epoch 97/100 Iteration 8/234: loss=0.032248 lr=0.000020 grad_norm=0.483400
Epoch 97/100 Iteration 9/234: loss=0.033168 lr=0.000020 grad_norm=0.630250
Epoch 97/100 Iteration 10/234: loss=0.032136 lr=0.000020 grad_norm=0.393133
Epoch 97/100 Iteration 11/234: loss=0.031105 lr=0.000020 grad_norm=0.438733
Epoch 97/100 Iteration 12/234: loss=0.035731 lr=0.000020 grad_norm=0.526416
Epoch 97/100 Iteration 13/234: loss=0.035805 lr=0.000020 grad_norm=0.694221
Epoch 97/100 Iteration 14/234: loss=0.034349 lr=0.000020 grad_norm=0.982431
Epoch 97/100 Iteration 15/234: loss=0.033071 lr=0.000020 grad_norm=1.048259
Epoch 97/100 Iteration 16/234: loss=0.036363 lr=0.000020 grad_norm=0.978918
Epoch 97/100 Iteration 17/234: loss=0.035376 lr=0.000020 grad_norm=0.508984
Epoch 97/100 Iteration 18/234: loss=0.031685 lr=0.000020 grad_norm=0.582990
Epoch 97/100 Iteration 19/234: loss=0.031641 lr=0.000020 grad_norm=0.777631
Epoch 97/100 Iteration 20/234: loss=0.032726 lr=0.000020 grad_norm=0.573876
Epoch 97/100 Iteration 21/234: loss=0.035974 lr=0.000020 grad_norm=0.776734
Epoch 97/100 Iteration 22/234: loss=0.031290 lr=0.000020 grad_norm=0.642332
Epoch 97/100 Iteration 23/234: loss=0.033564 lr=0.000020 grad_norm=0.506249
Epoch 97/100 Iteration 24/234: loss=0.035539 lr=0.000020 grad_norm=0.911665
Epoch 97/100 Iteration 25/234: loss=0.033866 lr=0.000020 grad_norm=0.388118
Epoch 97/100 Iteration 26/234: loss=0.034399 lr=0.000020 grad_norm=0.666499
Epoch 97/100 Iteration 27/234: loss=0.032589 lr=0.000020 grad_norm=0.548727
Epoch 97/100 Iteration 28/234: loss=0.031040 lr=0.000020 grad_norm=0.344326
Epoch 97/100 Iteration 29/234: loss=0.034248 lr=0.000020 grad_norm=0.505748
Epoch 97/100 Iteration 30/234: loss=0.032767 lr=0.000020 grad_norm=0.428237
Epoch 97/100 Iteration 31/234: loss=0.034203 lr=0.000020 grad_norm=0.540094
Epoch 97/100 Iteration 32/234: loss=0.034378 lr=0.000020 grad_norm=0.683811
Epoch 97/100 Iteration 33/234: loss=0.031621 lr=0.000020 grad_norm=0.530296
Epoch 97/100 Iteration 34/234: loss=0.032528 lr=0.000020 grad_norm=0.396014
Epoch 97/100 Iteration 35/234: loss=0.034103 lr=0.000020 grad_norm=0.816776
Epoch 97/100 Iteration 36/234: loss=0.033062 lr=0.000020 grad_norm=0.714570
Epoch 97/100 Iteration 37/234: loss=0.031274 lr=0.000020 grad_norm=0.492963
Epoch 97/100 Iteration 38/234: loss=0.033242 lr=0.000020 grad_norm=0.507132
Epoch 97/100 Iteration 39/234: loss=0.034685 lr=0.000020 grad_norm=0.512520
Epoch 97/100 Iteration 40/234: loss=0.029991 lr=0.000020 grad_norm=0.521838
Epoch 97/100 Iteration 41/234: loss=0.035555 lr=0.000020 grad_norm=0.460796
Epoch 97/100 Iteration 42/234: loss=0.033616 lr=0.000020 grad_norm=0.721924
Epoch 97/100 Iteration 43/234: loss=0.033966 lr=0.000020 grad_norm=0.793399
Epoch 97/100 Iteration 44/234: loss=0.035675 lr=0.000020 grad_norm=0.726071
Epoch 97/100 Iteration 45/234: loss=0.033449 lr=0.000020 grad_norm=1.494640
Epoch 97/100 Iteration 46/234: loss=0.035253 lr=0.000020 grad_norm=1.899980
Epoch 97/100 Iteration 47/234: loss=0.037129 lr=0.000020 grad_norm=1.197852
Epoch 97/100 Iteration 48/234: loss=0.030280 lr=0.000020 grad_norm=0.472244
Epoch 97/100 Iteration 49/234: loss=0.032630 lr=0.000020 grad_norm=0.886304
Epoch 97/100 Iteration 50/234: loss=0.036741 lr=0.000020 grad_norm=0.846665
Epoch 97/100 Iteration 51/234: loss=0.033668 lr=0.000020 grad_norm=0.855258
Epoch 97/100 Iteration 52/234: loss=0.032467 lr=0.000020 grad_norm=0.669157
Epoch 97/100 Iteration 53/234: loss=0.029708 lr=0.000020 grad_norm=0.719597
Epoch 97/100 Iteration 54/234: loss=0.032222 lr=0.000020 grad_norm=0.531270
Epoch 97/100 Iteration 55/234: loss=0.033021 lr=0.000020 grad_norm=0.377913
Epoch 97/100 Iteration 56/234: loss=0.028300 lr=0.000020 grad_norm=0.693918
Epoch 97/100 Iteration 57/234: loss=0.035962 lr=0.000020 grad_norm=0.439669
Epoch 97/100 Iteration 58/234: loss=0.032872 lr=0.000020 grad_norm=0.684189
Epoch 97/100 Iteration 59/234: loss=0.031858 lr=0.000020 grad_norm=0.745437
Epoch 97/100 Iteration 60/234: loss=0.035169 lr=0.000020 grad_norm=0.365735
Epoch 97/100 Iteration 61/234: loss=0.038106 lr=0.000020 grad_norm=0.652656
Epoch 97/100 Iteration 62/234: loss=0.031356 lr=0.000020 grad_norm=0.756110
Epoch 97/100 Iteration 63/234: loss=0.034031 lr=0.000020 grad_norm=0.605695
Epoch 97/100 Iteration 64/234: loss=0.034616 lr=0.000020 grad_norm=0.835764
Epoch 97/100 Iteration 65/234: loss=0.035517 lr=0.000020 grad_norm=0.463043
Epoch 97/100 Iteration 66/234: loss=0.032757 lr=0.000020 grad_norm=0.614120
Epoch 97/100 Iteration 67/234: loss=0.034492 lr=0.000020 grad_norm=0.612026
Epoch 97/100 Iteration 68/234: loss=0.035956 lr=0.000020 grad_norm=0.442051
Epoch 97/100 Iteration 69/234: loss=0.032172 lr=0.000020 grad_norm=0.380031
Epoch 97/100 Iteration 70/234: loss=0.030882 lr=0.000020 grad_norm=0.326523
Epoch 97/100 Iteration 71/234: loss=0.032411 lr=0.000020 grad_norm=0.478694
Epoch 97/100 Iteration 72/234: loss=0.032988 lr=0.000020 grad_norm=1.022156
Epoch 97/100 Iteration 73/234: loss=0.033416 lr=0.000020 grad_norm=1.001656
Epoch 97/100 Iteration 74/234: loss=0.032867 lr=0.000020 grad_norm=0.447351
Epoch 97/100 Iteration 75/234: loss=0.032766 lr=0.000020 grad_norm=0.760584
Epoch 97/100 Iteration 76/234: loss=0.031823 lr=0.000020 grad_norm=0.425103
Epoch 97/100 Iteration 77/234: loss=0.031399 lr=0.000020 grad_norm=1.032864
Epoch 97/100 Iteration 78/234: loss=0.033981 lr=0.000020 grad_norm=1.376232
Epoch 97/100 Iteration 79/234: loss=0.031189 lr=0.000020 grad_norm=0.665377
Epoch 97/100 Iteration 80/234: loss=0.032640 lr=0.000020 grad_norm=0.798364
Epoch 97/100 Iteration 81/234: loss=0.034666 lr=0.000020 grad_norm=1.434624
Epoch 97/100 Iteration 82/234: loss=0.033733 lr=0.000020 grad_norm=1.317399
Epoch 97/100 Iteration 83/234: loss=0.032879 lr=0.000020 grad_norm=0.711699
Epoch 97/100 Iteration 84/234: loss=0.035010 lr=0.000020 grad_norm=0.560631
Epoch 97/100 Iteration 85/234: loss=0.037043 lr=0.000020 grad_norm=1.087377
Epoch 97/100 Iteration 86/234: loss=0.033686 lr=0.000020 grad_norm=1.206952
Epoch 97/100 Iteration 87/234: loss=0.029633 lr=0.000020 grad_norm=0.624661
Epoch 97/100 Iteration 88/234: loss=0.035084 lr=0.000020 grad_norm=0.676276
Epoch 97/100 Iteration 89/234: loss=0.033602 lr=0.000020 grad_norm=0.943492
Epoch 97/100 Iteration 90/234: loss=0.028116 lr=0.000020 grad_norm=0.645936
Epoch 97/100 Iteration 91/234: loss=0.035288 lr=0.000020 grad_norm=0.667772
Epoch 97/100 Iteration 92/234: loss=0.031463 lr=0.000020 grad_norm=0.927987
Epoch 97/100 Iteration 93/234: loss=0.033818 lr=0.000020 grad_norm=0.714507
Epoch 97/100 Iteration 94/234: loss=0.036106 lr=0.000020 grad_norm=0.684766
Epoch 97/100 Iteration 95/234: loss=0.032256 lr=0.000020 grad_norm=0.889821
Epoch 97/100 Iteration 96/234: loss=0.036542 lr=0.000020 grad_norm=1.266975
Epoch 97/100 Iteration 97/234: loss=0.032894 lr=0.000020 grad_norm=0.828014
Epoch 97/100 Iteration 98/234: loss=0.035803 lr=0.000020 grad_norm=0.815238
Epoch 97/100 Iteration 99/234: loss=0.030884 lr=0.000020 grad_norm=0.871476
Epoch 97/100 Iteration 100/234: loss=0.032871 lr=0.000020 grad_norm=0.733660
Epoch 97/100 Iteration 101/234: loss=0.034393 lr=0.000020 grad_norm=0.795441
Epoch 97/100 Iteration 102/234: loss=0.035170 lr=0.000020 grad_norm=0.735631
Epoch 97/100 Iteration 103/234: loss=0.031706 lr=0.000020 grad_norm=0.773270
Epoch 97/100 Iteration 104/234: loss=0.034233 lr=0.000020 grad_norm=0.857155
Epoch 97/100 Iteration 105/234: loss=0.028730 lr=0.000020 grad_norm=0.718379
Epoch 97/100 Iteration 106/234: loss=0.031211 lr=0.000020 grad_norm=0.517564
Epoch 97/100 Iteration 107/234: loss=0.035193 lr=0.000020 grad_norm=0.895822
Epoch 97/100 Iteration 108/234: loss=0.033132 lr=0.000020 grad_norm=0.917700
Epoch 97/100 Iteration 109/234: loss=0.031074 lr=0.000020 grad_norm=0.475741
Epoch 97/100 Iteration 110/234: loss=0.032572 lr=0.000020 grad_norm=0.907646
Epoch 97/100 Iteration 111/234: loss=0.039401 lr=0.000020 grad_norm=0.685387
Epoch 97/100 Iteration 112/234: loss=0.032773 lr=0.000020 grad_norm=0.773768
Epoch 97/100 Iteration 113/234: loss=0.034943 lr=0.000020 grad_norm=1.077225
Epoch 97/100 Iteration 114/234: loss=0.037398 lr=0.000020 grad_norm=0.536848
Epoch 97/100 Iteration 115/234: loss=0.030905 lr=0.000020 grad_norm=0.860880
Epoch 97/100 Iteration 116/234: loss=0.030835 lr=0.000020 grad_norm=0.981608
Epoch 97/100 Iteration 117/234: loss=0.034195 lr=0.000020 grad_norm=0.452690
Epoch 97/100 Iteration 118/234: loss=0.032060 lr=0.000020 grad_norm=0.482409
Epoch 97/100 Iteration 119/234: loss=0.034872 lr=0.000020 grad_norm=0.425313
Epoch 97/100 Iteration 120/234: loss=0.030925 lr=0.000020 grad_norm=0.493525
Epoch 97/100 Iteration 121/234: loss=0.031432 lr=0.000020 grad_norm=0.409363
Epoch 97/100 Iteration 122/234: loss=0.031822 lr=0.000020 grad_norm=0.603746
Epoch 97/100 Iteration 123/234: loss=0.036180 lr=0.000020 grad_norm=0.566099
Epoch 97/100 Iteration 124/234: loss=0.033192 lr=0.000020 grad_norm=0.655092
Epoch 97/100 Iteration 125/234: loss=0.032932 lr=0.000020 grad_norm=0.632548
Epoch 97/100 Iteration 126/234: loss=0.032959 lr=0.000020 grad_norm=0.431123
Epoch 97/100 Iteration 127/234: loss=0.035039 lr=0.000020 grad_norm=0.469922
Epoch 97/100 Iteration 128/234: loss=0.033132 lr=0.000020 grad_norm=0.606058
Epoch 97/100 Iteration 129/234: loss=0.031472 lr=0.000020 grad_norm=0.615208
Epoch 97/100 Iteration 130/234: loss=0.032134 lr=0.000020 grad_norm=0.355014
Epoch 97/100 Iteration 131/234: loss=0.036206 lr=0.000020 grad_norm=0.555469
Epoch 97/100 Iteration 132/234: loss=0.032259 lr=0.000020 grad_norm=0.627544
Epoch 97/100 Iteration 133/234: loss=0.034466 lr=0.000020 grad_norm=0.566832
Epoch 97/100 Iteration 134/234: loss=0.033478 lr=0.000020 grad_norm=0.645134
Epoch 97/100 Iteration 135/234: loss=0.034985 lr=0.000020 grad_norm=0.623202
Epoch 97/100 Iteration 136/234: loss=0.032968 lr=0.000020 grad_norm=0.469839
Epoch 97/100 Iteration 137/234: loss=0.030703 lr=0.000020 grad_norm=0.536958
Epoch 97/100 Iteration 138/234: loss=0.032152 lr=0.000020 grad_norm=0.373115
Epoch 97/100 Iteration 139/234: loss=0.032274 lr=0.000020 grad_norm=0.479339
Epoch 97/100 Iteration 140/234: loss=0.034133 lr=0.000020 grad_norm=0.949381
Epoch 97/100 Iteration 141/234: loss=0.034183 lr=0.000020 grad_norm=1.092277
Epoch 97/100 Iteration 142/234: loss=0.033856 lr=0.000020 grad_norm=0.639137
Epoch 97/100 Iteration 143/234: loss=0.033167 lr=0.000020 grad_norm=0.607062
Epoch 97/100 Iteration 144/234: loss=0.033582 lr=0.000020 grad_norm=0.884626
Epoch 97/100 Iteration 145/234: loss=0.031969 lr=0.000020 grad_norm=0.415748
Epoch 97/100 Iteration 146/234: loss=0.035977 lr=0.000020 grad_norm=1.152855
Epoch 97/100 Iteration 147/234: loss=0.034138 lr=0.000020 grad_norm=1.028371
Epoch 97/100 Iteration 148/234: loss=0.029547 lr=0.000020 grad_norm=0.408320
Epoch 97/100 Iteration 149/234: loss=0.033336 lr=0.000020 grad_norm=1.061866
Epoch 97/100 Iteration 150/234: loss=0.034209 lr=0.000020 grad_norm=0.960636
Epoch 97/100 Iteration 151/234: loss=0.034682 lr=0.000020 grad_norm=0.603512
Epoch 97/100 Iteration 152/234: loss=0.030349 lr=0.000020 grad_norm=1.105919
Epoch 97/100 Iteration 153/234: loss=0.031850 lr=0.000020 grad_norm=0.547352
Epoch 97/100 Iteration 154/234: loss=0.033523 lr=0.000020 grad_norm=1.155943
Epoch 97/100 Iteration 155/234: loss=0.031823 lr=0.000020 grad_norm=1.251551
Epoch 97/100 Iteration 156/234: loss=0.036306 lr=0.000020 grad_norm=0.645782
Epoch 97/100 Iteration 157/234: loss=0.032433 lr=0.000020 grad_norm=1.030734
Epoch 97/100 Iteration 158/234: loss=0.034695 lr=0.000020 grad_norm=0.638299
Epoch 97/100 Iteration 159/234: loss=0.033575 lr=0.000020 grad_norm=1.163569
Epoch 97/100 Iteration 160/234: loss=0.031040 lr=0.000020 grad_norm=1.090798
Epoch 97/100 Iteration 161/234: loss=0.035071 lr=0.000020 grad_norm=0.570652
Epoch 97/100 Iteration 162/234: loss=0.034200 lr=0.000020 grad_norm=1.505115
Epoch 97/100 Iteration 163/234: loss=0.033367 lr=0.000020 grad_norm=0.733281
Epoch 97/100 Iteration 164/234: loss=0.035433 lr=0.000020 grad_norm=1.324320
Epoch 97/100 Iteration 165/234: loss=0.033486 lr=0.000020 grad_norm=1.480080
Epoch 97/100 Iteration 166/234: loss=0.032645 lr=0.000020 grad_norm=0.470302
Epoch 97/100 Iteration 167/234: loss=0.034179 lr=0.000020 grad_norm=1.273027
Epoch 97/100 Iteration 168/234: loss=0.035684 lr=0.000020 grad_norm=0.599168
Epoch 97/100 Iteration 169/234: loss=0.032571 lr=0.000020 grad_norm=0.775287
Epoch 97/100 Iteration 170/234: loss=0.033607 lr=0.000020 grad_norm=0.730482
Epoch 97/100 Iteration 171/234: loss=0.032094 lr=0.000020 grad_norm=0.844687
Epoch 97/100 Iteration 172/234: loss=0.035224 lr=0.000020 grad_norm=0.942777
Epoch 97/100 Iteration 173/234: loss=0.033734 lr=0.000020 grad_norm=0.657140
Epoch 97/100 Iteration 174/234: loss=0.032112 lr=0.000020 grad_norm=0.667889
Epoch 97/100 Iteration 175/234: loss=0.032673 lr=0.000020 grad_norm=0.536559
Epoch 97/100 Iteration 176/234: loss=0.033767 lr=0.000020 grad_norm=1.162500
Epoch 97/100 Iteration 177/234: loss=0.032071 lr=0.000020 grad_norm=0.926026
Epoch 97/100 Iteration 178/234: loss=0.033436 lr=0.000020 grad_norm=0.757108
Epoch 97/100 Iteration 179/234: loss=0.034033 lr=0.000020 grad_norm=0.670385
Epoch 97/100 Iteration 180/234: loss=0.034270 lr=0.000020 grad_norm=0.539596
Epoch 97/100 Iteration 181/234: loss=0.033812 lr=0.000020 grad_norm=0.803384
Epoch 97/100 Iteration 182/234: loss=0.030610 lr=0.000020 grad_norm=0.443241
Epoch 97/100 Iteration 183/234: loss=0.032448 lr=0.000020 grad_norm=0.459359
Epoch 97/100 Iteration 184/234: loss=0.034494 lr=0.000020 grad_norm=0.914697
Epoch 97/100 Iteration 185/234: loss=0.033019 lr=0.000020 grad_norm=0.906792
Epoch 97/100 Iteration 186/234: loss=0.033703 lr=0.000020 grad_norm=0.513447
Epoch 97/100 Iteration 187/234: loss=0.029254 lr=0.000020 grad_norm=0.471914
Epoch 97/100 Iteration 188/234: loss=0.033085 lr=0.000020 grad_norm=0.529065
Epoch 97/100 Iteration 189/234: loss=0.034500 lr=0.000020 grad_norm=0.978578
Epoch 97/100 Iteration 190/234: loss=0.033198 lr=0.000020 grad_norm=0.785387
Epoch 97/100 Iteration 191/234: loss=0.033684 lr=0.000020 grad_norm=0.605190
Epoch 97/100 Iteration 192/234: loss=0.032289 lr=0.000020 grad_norm=0.517281
Epoch 97/100 Iteration 193/234: loss=0.034138 lr=0.000020 grad_norm=0.730341
Epoch 97/100 Iteration 194/234: loss=0.031764 lr=0.000020 grad_norm=0.651694
Epoch 97/100 Iteration 195/234: loss=0.032135 lr=0.000020 grad_norm=0.370068
Epoch 97/100 Iteration 196/234: loss=0.032944 lr=0.000020 grad_norm=0.599470
Epoch 97/100 Iteration 197/234: loss=0.033740 lr=0.000020 grad_norm=1.100787
Epoch 97/100 Iteration 198/234: loss=0.030476 lr=0.000020 grad_norm=0.901794
Epoch 97/100 Iteration 199/234: loss=0.036917 lr=0.000020 grad_norm=0.500902
Epoch 97/100 Iteration 200/234: loss=0.033988 lr=0.000020 grad_norm=0.591984
Epoch 97/100 Iteration 201/234: loss=0.032526 lr=0.000020 grad_norm=0.725283
Epoch 97/100 Iteration 202/234: loss=0.034442 lr=0.000020 grad_norm=0.481183
Epoch 97/100 Iteration 203/234: loss=0.032476 lr=0.000020 grad_norm=0.525327
Epoch 97/100 Iteration 204/234: loss=0.036116 lr=0.000020 grad_norm=0.555814
Epoch 97/100 Iteration 205/234: loss=0.031925 lr=0.000020 grad_norm=0.478959
Epoch 97/100 Iteration 206/234: loss=0.033780 lr=0.000020 grad_norm=0.616163
Epoch 97/100 Iteration 207/234: loss=0.032742 lr=0.000020 grad_norm=0.936820
Epoch 97/100 Iteration 208/234: loss=0.032518 lr=0.000020 grad_norm=0.789945
Epoch 97/100 Iteration 209/234: loss=0.035422 lr=0.000020 grad_norm=0.820331
Epoch 97/100 Iteration 210/234: loss=0.034376 lr=0.000020 grad_norm=1.096256
Epoch 97/100 Iteration 211/234: loss=0.033838 lr=0.000020 grad_norm=0.779137
Epoch 97/100 Iteration 212/234: loss=0.034369 lr=0.000020 grad_norm=0.773283
Epoch 97/100 Iteration 213/234: loss=0.034139 lr=0.000020 grad_norm=1.055166
Epoch 97/100 Iteration 214/234: loss=0.031971 lr=0.000020 grad_norm=0.680322
Epoch 97/100 Iteration 215/234: loss=0.031764 lr=0.000020 grad_norm=1.448044
Epoch 97/100 Iteration 216/234: loss=0.034416 lr=0.000020 grad_norm=1.405264
Epoch 97/100 Iteration 217/234: loss=0.032836 lr=0.000020 grad_norm=0.730159
Epoch 97/100 Iteration 218/234: loss=0.033331 lr=0.000020 grad_norm=1.398326
Epoch 97/100 Iteration 219/234: loss=0.036367 lr=0.000020 grad_norm=1.153695
Epoch 97/100 Iteration 220/234: loss=0.030783 lr=0.000020 grad_norm=0.677140
Epoch 97/100 Iteration 221/234: loss=0.033874 lr=0.000020 grad_norm=0.825636
Epoch 97/100 Iteration 222/234: loss=0.037207 lr=0.000020 grad_norm=0.446955
Epoch 97/100 Iteration 223/234: loss=0.033989 lr=0.000020 grad_norm=0.968782
Epoch 97/100 Iteration 224/234: loss=0.034996 lr=0.000020 grad_norm=0.851921
Epoch 97/100 Iteration 225/234: loss=0.035488 lr=0.000020 grad_norm=0.709076
Epoch 97/100 Iteration 226/234: loss=0.031651 lr=0.000020 grad_norm=0.492858
Epoch 97/100 Iteration 227/234: loss=0.033273 lr=0.000020 grad_norm=0.608132
Epoch 97/100 Iteration 228/234: loss=0.031594 lr=0.000020 grad_norm=0.865367
Epoch 97/100 Iteration 229/234: loss=0.028178 lr=0.000020 grad_norm=0.472033
Epoch 97/100 Iteration 230/234: loss=0.030307 lr=0.000020 grad_norm=0.451985
Epoch 97/100 Iteration 231/234: loss=0.034345 lr=0.000020 grad_norm=0.669900
Epoch 97/100 Iteration 232/234: loss=0.037489 lr=0.000020 grad_norm=0.474231
Epoch 97/100 Iteration 233/234: loss=0.030017 lr=0.000020 grad_norm=0.365811
Epoch 97/100 Iteration 234/234: loss=0.031480 lr=0.000020 grad_norm=0.452097
Epoch 97/100 finished. Avg Loss: 0.033337
Epoch 98/100 Iteration 1/234: loss=0.029501 lr=0.000020 grad_norm=0.627463
Epoch 98/100 Iteration 2/234: loss=0.035284 lr=0.000020 grad_norm=0.451258
Epoch 98/100 Iteration 3/234: loss=0.031972 lr=0.000020 grad_norm=0.780899
Epoch 98/100 Iteration 4/234: loss=0.036278 lr=0.000020 grad_norm=0.641124
Epoch 98/100 Iteration 5/234: loss=0.035437 lr=0.000020 grad_norm=0.765871
Epoch 98/100 Iteration 6/234: loss=0.032612 lr=0.000020 grad_norm=0.986483
Epoch 98/100 Iteration 7/234: loss=0.030879 lr=0.000020 grad_norm=0.738683
Epoch 98/100 Iteration 8/234: loss=0.034092 lr=0.000020 grad_norm=0.533312
Epoch 98/100 Iteration 9/234: loss=0.036636 lr=0.000020 grad_norm=1.198035
Epoch 98/100 Iteration 10/234: loss=0.035808 lr=0.000020 grad_norm=1.213555
Epoch 98/100 Iteration 11/234: loss=0.033790 lr=0.000020 grad_norm=0.740748
Epoch 98/100 Iteration 12/234: loss=0.031478 lr=0.000020 grad_norm=0.854190
Epoch 98/100 Iteration 13/234: loss=0.033434 lr=0.000020 grad_norm=0.889029
Epoch 98/100 Iteration 14/234: loss=0.030837 lr=0.000020 grad_norm=0.985104
Epoch 98/100 Iteration 15/234: loss=0.032050 lr=0.000020 grad_norm=0.789632
Epoch 98/100 Iteration 16/234: loss=0.034858 lr=0.000020 grad_norm=1.071396
Epoch 98/100 Iteration 17/234: loss=0.032798 lr=0.000020 grad_norm=0.909117
Epoch 98/100 Iteration 18/234: loss=0.032509 lr=0.000020 grad_norm=0.542794
Epoch 98/100 Iteration 19/234: loss=0.034344 lr=0.000020 grad_norm=1.337291
Epoch 98/100 Iteration 20/234: loss=0.037142 lr=0.000020 grad_norm=1.424536
Epoch 98/100 Iteration 21/234: loss=0.030623 lr=0.000020 grad_norm=0.608684
Epoch 98/100 Iteration 22/234: loss=0.030790 lr=0.000020 grad_norm=0.636503
Epoch 98/100 Iteration 23/234: loss=0.036403 lr=0.000020 grad_norm=1.011106
Epoch 98/100 Iteration 24/234: loss=0.033459 lr=0.000020 grad_norm=0.851638
Epoch 98/100 Iteration 25/234: loss=0.033137 lr=0.000020 grad_norm=0.397108
Epoch 98/100 Iteration 26/234: loss=0.032100 lr=0.000020 grad_norm=1.028719
Epoch 98/100 Iteration 27/234: loss=0.029478 lr=0.000020 grad_norm=1.029794
Epoch 98/100 Iteration 28/234: loss=0.031201 lr=0.000020 grad_norm=0.594380
Epoch 98/100 Iteration 29/234: loss=0.036316 lr=0.000020 grad_norm=0.909301
Epoch 98/100 Iteration 30/234: loss=0.034808 lr=0.000020 grad_norm=1.090921
Epoch 98/100 Iteration 31/234: loss=0.033490 lr=0.000020 grad_norm=0.433742
Epoch 98/100 Iteration 32/234: loss=0.033570 lr=0.000020 grad_norm=1.045732
Epoch 98/100 Iteration 33/234: loss=0.031455 lr=0.000020 grad_norm=0.654566
Epoch 98/100 Iteration 34/234: loss=0.034803 lr=0.000020 grad_norm=0.826338
Epoch 98/100 Iteration 35/234: loss=0.033469 lr=0.000020 grad_norm=1.148703
Epoch 98/100 Iteration 36/234: loss=0.030424 lr=0.000020 grad_norm=0.622824
Epoch 98/100 Iteration 37/234: loss=0.034191 lr=0.000020 grad_norm=1.086512
Epoch 98/100 Iteration 38/234: loss=0.033870 lr=0.000020 grad_norm=1.993474
Epoch 98/100 Iteration 39/234: loss=0.033014 lr=0.000020 grad_norm=1.595514
Epoch 98/100 Iteration 40/234: loss=0.034731 lr=0.000020 grad_norm=0.660618
Epoch 98/100 Iteration 41/234: loss=0.031579 lr=0.000020 grad_norm=1.141549
Epoch 98/100 Iteration 42/234: loss=0.035068 lr=0.000020 grad_norm=0.947183
Epoch 98/100 Iteration 43/234: loss=0.031980 lr=0.000020 grad_norm=0.961463
Epoch 98/100 Iteration 44/234: loss=0.031417 lr=0.000020 grad_norm=0.819638
Epoch 98/100 Iteration 45/234: loss=0.033534 lr=0.000020 grad_norm=1.030783
Epoch 98/100 Iteration 46/234: loss=0.031248 lr=0.000020 grad_norm=1.080254
Epoch 98/100 Iteration 47/234: loss=0.030966 lr=0.000020 grad_norm=0.769306
Epoch 98/100 Iteration 48/234: loss=0.035164 lr=0.000020 grad_norm=0.751254
Epoch 98/100 Iteration 49/234: loss=0.034355 lr=0.000020 grad_norm=0.765989
Epoch 98/100 Iteration 50/234: loss=0.031186 lr=0.000020 grad_norm=0.630047
Epoch 98/100 Iteration 51/234: loss=0.033021 lr=0.000020 grad_norm=0.594308
Epoch 98/100 Iteration 52/234: loss=0.035199 lr=0.000020 grad_norm=1.037438
Epoch 98/100 Iteration 53/234: loss=0.033837 lr=0.000020 grad_norm=0.946380
Epoch 98/100 Iteration 54/234: loss=0.032422 lr=0.000020 grad_norm=0.765448
Epoch 98/100 Iteration 55/234: loss=0.031235 lr=0.000020 grad_norm=0.805435
Epoch 98/100 Iteration 56/234: loss=0.035648 lr=0.000020 grad_norm=0.738185
Epoch 98/100 Iteration 57/234: loss=0.033014 lr=0.000020 grad_norm=0.892195
Epoch 98/100 Iteration 58/234: loss=0.033973 lr=0.000020 grad_norm=0.742996
Epoch 98/100 Iteration 59/234: loss=0.033274 lr=0.000020 grad_norm=0.636453
Epoch 98/100 Iteration 60/234: loss=0.033188 lr=0.000020 grad_norm=1.042161
Epoch 98/100 Iteration 61/234: loss=0.033419 lr=0.000020 grad_norm=0.578698
Epoch 98/100 Iteration 62/234: loss=0.030728 lr=0.000020 grad_norm=0.470829
Epoch 98/100 Iteration 63/234: loss=0.031467 lr=0.000020 grad_norm=0.445771
Epoch 98/100 Iteration 64/234: loss=0.035152 lr=0.000020 grad_norm=0.606558
Epoch 98/100 Iteration 65/234: loss=0.034745 lr=0.000020 grad_norm=0.764553
Epoch 98/100 Iteration 66/234: loss=0.033410 lr=0.000020 grad_norm=0.542678
Epoch 98/100 Iteration 67/234: loss=0.031604 lr=0.000020 grad_norm=0.374275
Epoch 98/100 Iteration 68/234: loss=0.030920 lr=0.000020 grad_norm=0.600502
Epoch 98/100 Iteration 69/234: loss=0.034084 lr=0.000020 grad_norm=0.601579
Epoch 98/100 Iteration 70/234: loss=0.031872 lr=0.000020 grad_norm=0.380106
Epoch 98/100 Iteration 71/234: loss=0.033040 lr=0.000020 grad_norm=0.787649
Epoch 98/100 Iteration 72/234: loss=0.032643 lr=0.000020 grad_norm=0.645438
Epoch 98/100 Iteration 73/234: loss=0.033205 lr=0.000020 grad_norm=0.550263
Epoch 98/100 Iteration 74/234: loss=0.030821 lr=0.000020 grad_norm=0.999770
Epoch 98/100 Iteration 75/234: loss=0.030679 lr=0.000020 grad_norm=0.492818
Epoch 98/100 Iteration 76/234: loss=0.034921 lr=0.000020 grad_norm=0.873981
Epoch 98/100 Iteration 77/234: loss=0.038049 lr=0.000020 grad_norm=1.502703
Epoch 98/100 Iteration 78/234: loss=0.030220 lr=0.000020 grad_norm=1.207371
Epoch 98/100 Iteration 79/234: loss=0.031544 lr=0.000020 grad_norm=0.665086
Epoch 98/100 Iteration 80/234: loss=0.034123 lr=0.000020 grad_norm=1.720744
Epoch 98/100 Iteration 81/234: loss=0.033539 lr=0.000020 grad_norm=1.703459
Epoch 98/100 Iteration 82/234: loss=0.029751 lr=0.000020 grad_norm=0.553344
Epoch 98/100 Iteration 83/234: loss=0.033688 lr=0.000020 grad_norm=1.399815
Epoch 98/100 Iteration 84/234: loss=0.033642 lr=0.000020 grad_norm=1.685193
Epoch 98/100 Iteration 85/234: loss=0.034742 lr=0.000020 grad_norm=0.701571
Epoch 98/100 Iteration 86/234: loss=0.035725 lr=0.000020 grad_norm=0.802863
Epoch 98/100 Iteration 87/234: loss=0.031585 lr=0.000020 grad_norm=1.110894
Epoch 98/100 Iteration 88/234: loss=0.034971 lr=0.000020 grad_norm=0.459791
Epoch 98/100 Iteration 89/234: loss=0.034340 lr=0.000020 grad_norm=0.929266
Epoch 98/100 Iteration 90/234: loss=0.033628 lr=0.000020 grad_norm=0.766694
Epoch 98/100 Iteration 91/234: loss=0.035904 lr=0.000020 grad_norm=0.457856
Epoch 98/100 Iteration 92/234: loss=0.034500 lr=0.000020 grad_norm=0.618963
Epoch 98/100 Iteration 93/234: loss=0.032938 lr=0.000020 grad_norm=0.639273
Epoch 98/100 Iteration 94/234: loss=0.036745 lr=0.000020 grad_norm=0.584347
Epoch 98/100 Iteration 95/234: loss=0.034140 lr=0.000020 grad_norm=1.192313
Epoch 98/100 Iteration 96/234: loss=0.032758 lr=0.000020 grad_norm=0.771752
Epoch 98/100 Iteration 97/234: loss=0.033895 lr=0.000020 grad_norm=0.621336
Epoch 98/100 Iteration 98/234: loss=0.032914 lr=0.000020 grad_norm=0.824647
Epoch 98/100 Iteration 99/234: loss=0.032002 lr=0.000020 grad_norm=0.495580
Epoch 98/100 Iteration 100/234: loss=0.033439 lr=0.000020 grad_norm=0.731683
Epoch 98/100 Iteration 101/234: loss=0.032635 lr=0.000020 grad_norm=0.899155
Epoch 98/100 Iteration 102/234: loss=0.035721 lr=0.000020 grad_norm=0.582753
Epoch 98/100 Iteration 103/234: loss=0.034811 lr=0.000020 grad_norm=1.268173
Epoch 98/100 Iteration 104/234: loss=0.032829 lr=0.000020 grad_norm=1.467149
Epoch 98/100 Iteration 105/234: loss=0.033239 lr=0.000020 grad_norm=0.987597
Epoch 98/100 Iteration 106/234: loss=0.033929 lr=0.000020 grad_norm=0.860694
Epoch 98/100 Iteration 107/234: loss=0.031439 lr=0.000020 grad_norm=0.864365
Epoch 98/100 Iteration 108/234: loss=0.032572 lr=0.000020 grad_norm=0.696883
Epoch 98/100 Iteration 109/234: loss=0.033030 lr=0.000020 grad_norm=0.681880
Epoch 98/100 Iteration 110/234: loss=0.033945 lr=0.000020 grad_norm=0.705923
Epoch 98/100 Iteration 111/234: loss=0.032577 lr=0.000020 grad_norm=0.883913
Epoch 98/100 Iteration 112/234: loss=0.032285 lr=0.000020 grad_norm=0.609943
Epoch 98/100 Iteration 113/234: loss=0.030288 lr=0.000020 grad_norm=0.564797
Epoch 98/100 Iteration 114/234: loss=0.031153 lr=0.000020 grad_norm=0.754088
Epoch 98/100 Iteration 115/234: loss=0.034787 lr=0.000020 grad_norm=0.514505
Epoch 98/100 Iteration 116/234: loss=0.032915 lr=0.000020 grad_norm=0.858003
Epoch 98/100 Iteration 117/234: loss=0.035806 lr=0.000020 grad_norm=1.013887
Epoch 98/100 Iteration 118/234: loss=0.033818 lr=0.000020 grad_norm=0.453506
Epoch 98/100 Iteration 119/234: loss=0.032577 lr=0.000020 grad_norm=0.917964
Epoch 98/100 Iteration 120/234: loss=0.032241 lr=0.000020 grad_norm=0.485850
Epoch 98/100 Iteration 121/234: loss=0.033364 lr=0.000020 grad_norm=0.664843
Epoch 98/100 Iteration 122/234: loss=0.035013 lr=0.000020 grad_norm=0.789919
Epoch 98/100 Iteration 123/234: loss=0.036417 lr=0.000020 grad_norm=0.702745
Epoch 98/100 Iteration 124/234: loss=0.034644 lr=0.000020 grad_norm=0.706267
Epoch 98/100 Iteration 125/234: loss=0.030824 lr=0.000020 grad_norm=0.841606
Epoch 98/100 Iteration 126/234: loss=0.031512 lr=0.000020 grad_norm=0.552609
Epoch 98/100 Iteration 127/234: loss=0.033148 lr=0.000020 grad_norm=0.958804
Epoch 98/100 Iteration 128/234: loss=0.033136 lr=0.000020 grad_norm=1.325968
Epoch 98/100 Iteration 129/234: loss=0.030910 lr=0.000020 grad_norm=0.665827
Epoch 98/100 Iteration 130/234: loss=0.033371 lr=0.000020 grad_norm=1.072167
Epoch 98/100 Iteration 131/234: loss=0.032208 lr=0.000020 grad_norm=0.847654
Epoch 98/100 Iteration 132/234: loss=0.033960 lr=0.000020 grad_norm=0.769276
Epoch 98/100 Iteration 133/234: loss=0.029946 lr=0.000020 grad_norm=0.909418
Epoch 98/100 Iteration 134/234: loss=0.032931 lr=0.000020 grad_norm=0.669688
Epoch 98/100 Iteration 135/234: loss=0.031630 lr=0.000020 grad_norm=1.005602
Epoch 98/100 Iteration 136/234: loss=0.033466 lr=0.000020 grad_norm=0.481328
Epoch 98/100 Iteration 137/234: loss=0.035192 lr=0.000020 grad_norm=1.058697
Epoch 98/100 Iteration 138/234: loss=0.030185 lr=0.000020 grad_norm=0.486585
Epoch 98/100 Iteration 139/234: loss=0.032070 lr=0.000020 grad_norm=0.795977
Epoch 98/100 Iteration 140/234: loss=0.031683 lr=0.000020 grad_norm=0.963064
Epoch 98/100 Iteration 141/234: loss=0.032779 lr=0.000020 grad_norm=0.512839
Epoch 98/100 Iteration 142/234: loss=0.033815 lr=0.000020 grad_norm=0.735991
Epoch 98/100 Iteration 143/234: loss=0.031419 lr=0.000020 grad_norm=0.577876
Epoch 98/100 Iteration 144/234: loss=0.029992 lr=0.000020 grad_norm=0.325619
Epoch 98/100 Iteration 145/234: loss=0.031165 lr=0.000020 grad_norm=0.418334
Epoch 98/100 Iteration 146/234: loss=0.032051 lr=0.000020 grad_norm=0.387526
Epoch 98/100 Iteration 147/234: loss=0.035556 lr=0.000020 grad_norm=0.481854
Epoch 98/100 Iteration 148/234: loss=0.032812 lr=0.000020 grad_norm=0.667005
Epoch 98/100 Iteration 149/234: loss=0.036364 lr=0.000020 grad_norm=0.568005
Epoch 98/100 Iteration 150/234: loss=0.033731 lr=0.000020 grad_norm=0.314088
Epoch 98/100 Iteration 151/234: loss=0.033472 lr=0.000020 grad_norm=0.531045
Epoch 98/100 Iteration 152/234: loss=0.035341 lr=0.000020 grad_norm=0.601516
Epoch 98/100 Iteration 153/234: loss=0.032223 lr=0.000020 grad_norm=0.770360
Epoch 98/100 Iteration 154/234: loss=0.032925 lr=0.000020 grad_norm=1.205656
Epoch 98/100 Iteration 155/234: loss=0.030674 lr=0.000020 grad_norm=0.677033
Epoch 98/100 Iteration 156/234: loss=0.030821 lr=0.000020 grad_norm=0.792883
Epoch 98/100 Iteration 157/234: loss=0.032626 lr=0.000020 grad_norm=1.363120
Epoch 98/100 Iteration 158/234: loss=0.030238 lr=0.000020 grad_norm=0.620687
Epoch 98/100 Iteration 159/234: loss=0.037002 lr=0.000020 grad_norm=1.220113
Epoch 98/100 Iteration 160/234: loss=0.030148 lr=0.000020 grad_norm=1.542670
Epoch 98/100 Iteration 161/234: loss=0.033993 lr=0.000020 grad_norm=0.483693
Epoch 98/100 Iteration 162/234: loss=0.035618 lr=0.000020 grad_norm=1.478028
Epoch 98/100 Iteration 163/234: loss=0.034799 lr=0.000020 grad_norm=1.485415
Epoch 98/100 Iteration 164/234: loss=0.031680 lr=0.000020 grad_norm=0.989855
Epoch 98/100 Iteration 165/234: loss=0.035065 lr=0.000020 grad_norm=0.700776
Epoch 98/100 Iteration 166/234: loss=0.032464 lr=0.000020 grad_norm=1.118233
Epoch 98/100 Iteration 167/234: loss=0.033224 lr=0.000020 grad_norm=1.114096
Epoch 98/100 Iteration 168/234: loss=0.033485 lr=0.000020 grad_norm=1.114917
Epoch 98/100 Iteration 169/234: loss=0.034023 lr=0.000020 grad_norm=0.857191
Epoch 98/100 Iteration 170/234: loss=0.035066 lr=0.000020 grad_norm=0.671724
Epoch 98/100 Iteration 171/234: loss=0.034786 lr=0.000020 grad_norm=0.765005
Epoch 98/100 Iteration 172/234: loss=0.033111 lr=0.000020 grad_norm=0.452041
Epoch 98/100 Iteration 173/234: loss=0.030752 lr=0.000020 grad_norm=0.601491
Epoch 98/100 Iteration 174/234: loss=0.031343 lr=0.000020 grad_norm=0.492530
Epoch 98/100 Iteration 175/234: loss=0.032876 lr=0.000020 grad_norm=0.445004
Epoch 98/100 Iteration 176/234: loss=0.031611 lr=0.000020 grad_norm=0.623286
Epoch 98/100 Iteration 177/234: loss=0.030519 lr=0.000020 grad_norm=0.338839
Epoch 98/100 Iteration 178/234: loss=0.036943 lr=0.000020 grad_norm=1.050329
Epoch 98/100 Iteration 179/234: loss=0.032142 lr=0.000020 grad_norm=1.231795
Epoch 98/100 Iteration 180/234: loss=0.031007 lr=0.000020 grad_norm=0.702429
Epoch 98/100 Iteration 181/234: loss=0.032129 lr=0.000020 grad_norm=0.791057
Epoch 98/100 Iteration 182/234: loss=0.036529 lr=0.000020 grad_norm=0.793250
Epoch 98/100 Iteration 183/234: loss=0.030836 lr=0.000020 grad_norm=0.582242
Epoch 98/100 Iteration 184/234: loss=0.031195 lr=0.000020 grad_norm=0.695699
Epoch 98/100 Iteration 185/234: loss=0.031105 lr=0.000020 grad_norm=0.515202
Epoch 98/100 Iteration 186/234: loss=0.030395 lr=0.000020 grad_norm=0.541840
Epoch 98/100 Iteration 187/234: loss=0.037282 lr=0.000020 grad_norm=0.690617
Epoch 98/100 Iteration 188/234: loss=0.033309 lr=0.000020 grad_norm=0.624400
Epoch 98/100 Iteration 189/234: loss=0.036247 lr=0.000020 grad_norm=0.697089
Epoch 98/100 Iteration 190/234: loss=0.029809 lr=0.000020 grad_norm=0.629193
Epoch 98/100 Iteration 191/234: loss=0.032831 lr=0.000020 grad_norm=0.527660
Epoch 98/100 Iteration 192/234: loss=0.031346 lr=0.000020 grad_norm=0.487169
Epoch 98/100 Iteration 193/234: loss=0.031901 lr=0.000020 grad_norm=0.546060
Epoch 98/100 Iteration 194/234: loss=0.032567 lr=0.000020 grad_norm=0.721867
Epoch 98/100 Iteration 195/234: loss=0.035530 lr=0.000020 grad_norm=0.472130
Epoch 98/100 Iteration 196/234: loss=0.028630 lr=0.000020 grad_norm=0.420643
Epoch 98/100 Iteration 197/234: loss=0.030385 lr=0.000020 grad_norm=0.531222
Epoch 98/100 Iteration 198/234: loss=0.029764 lr=0.000020 grad_norm=0.432994
Epoch 98/100 Iteration 199/234: loss=0.030687 lr=0.000020 grad_norm=0.637645
Epoch 98/100 Iteration 200/234: loss=0.032943 lr=0.000020 grad_norm=0.515265
Epoch 98/100 Iteration 201/234: loss=0.034012 lr=0.000020 grad_norm=0.476074
Epoch 98/100 Iteration 202/234: loss=0.033131 lr=0.000020 grad_norm=0.348818
Epoch 98/100 Iteration 203/234: loss=0.032597 lr=0.000020 grad_norm=0.536320
Epoch 98/100 Iteration 204/234: loss=0.032357 lr=0.000020 grad_norm=0.414771
Epoch 98/100 Iteration 205/234: loss=0.034523 lr=0.000020 grad_norm=0.535671
Epoch 98/100 Iteration 206/234: loss=0.033526 lr=0.000020 grad_norm=0.551874
Epoch 98/100 Iteration 207/234: loss=0.031842 lr=0.000020 grad_norm=0.379426
Epoch 98/100 Iteration 208/234: loss=0.032435 lr=0.000020 grad_norm=0.434019
Epoch 98/100 Iteration 209/234: loss=0.036815 lr=0.000020 grad_norm=0.539012
Epoch 98/100 Iteration 210/234: loss=0.034432 lr=0.000020 grad_norm=0.835070
Epoch 98/100 Iteration 211/234: loss=0.032082 lr=0.000020 grad_norm=0.488403
Epoch 98/100 Iteration 212/234: loss=0.032062 lr=0.000020 grad_norm=0.647267
Epoch 98/100 Iteration 213/234: loss=0.032645 lr=0.000020 grad_norm=0.768789
Epoch 98/100 Iteration 214/234: loss=0.030768 lr=0.000020 grad_norm=0.453533
Epoch 98/100 Iteration 215/234: loss=0.033289 lr=0.000020 grad_norm=1.242866
Epoch 98/100 Iteration 216/234: loss=0.031289 lr=0.000020 grad_norm=1.121799
Epoch 98/100 Iteration 217/234: loss=0.030336 lr=0.000020 grad_norm=0.624685
Epoch 98/100 Iteration 218/234: loss=0.032057 lr=0.000020 grad_norm=0.989403
Epoch 98/100 Iteration 219/234: loss=0.033916 lr=0.000020 grad_norm=1.187713
Epoch 98/100 Iteration 220/234: loss=0.032868 lr=0.000020 grad_norm=0.472872
Epoch 98/100 Iteration 221/234: loss=0.033010 lr=0.000020 grad_norm=0.920962
Epoch 98/100 Iteration 222/234: loss=0.032548 lr=0.000020 grad_norm=0.694034
Epoch 98/100 Iteration 223/234: loss=0.033705 lr=0.000020 grad_norm=0.753698
Epoch 98/100 Iteration 224/234: loss=0.032708 lr=0.000020 grad_norm=0.952863
Epoch 98/100 Iteration 225/234: loss=0.032463 lr=0.000020 grad_norm=0.696022
Epoch 98/100 Iteration 226/234: loss=0.036827 lr=0.000020 grad_norm=1.257917
Epoch 98/100 Iteration 227/234: loss=0.033076 lr=0.000020 grad_norm=0.844633
Epoch 98/100 Iteration 228/234: loss=0.033697 lr=0.000020 grad_norm=1.024985
Epoch 98/100 Iteration 229/234: loss=0.031474 lr=0.000020 grad_norm=0.988049
Epoch 98/100 Iteration 230/234: loss=0.032966 lr=0.000020 grad_norm=0.573688
Epoch 98/100 Iteration 231/234: loss=0.032242 lr=0.000020 grad_norm=1.076861
Epoch 98/100 Iteration 232/234: loss=0.032282 lr=0.000020 grad_norm=0.643609
Epoch 98/100 Iteration 233/234: loss=0.032569 lr=0.000020 grad_norm=0.742787
Epoch 98/100 Iteration 234/234: loss=0.031643 lr=0.000020 grad_norm=0.688224
Epoch 98/100 finished. Avg Loss: 0.033023
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
Epoch 99/100 Iteration 1/234: loss=0.033775 lr=0.000020 grad_norm=0.534877
Epoch 99/100 Iteration 2/234: loss=0.032411 lr=0.000020 grad_norm=0.902233
Epoch 99/100 Iteration 3/234: loss=0.034105 lr=0.000020 grad_norm=0.610401
Epoch 99/100 Iteration 4/234: loss=0.031370 lr=0.000020 grad_norm=0.457733
Epoch 99/100 Iteration 5/234: loss=0.033697 lr=0.000020 grad_norm=0.731285
Epoch 99/100 Iteration 6/234: loss=0.031571 lr=0.000020 grad_norm=0.556882
Epoch 99/100 Iteration 7/234: loss=0.032596 lr=0.000020 grad_norm=0.588313
Epoch 99/100 Iteration 8/234: loss=0.031876 lr=0.000020 grad_norm=0.567159
Epoch 99/100 Iteration 9/234: loss=0.034029 lr=0.000020 grad_norm=0.733550
Epoch 99/100 Iteration 10/234: loss=0.033164 lr=0.000020 grad_norm=0.972272
Epoch 99/100 Iteration 11/234: loss=0.029582 lr=0.000020 grad_norm=0.426122
Epoch 99/100 Iteration 12/234: loss=0.034483 lr=0.000020 grad_norm=0.915766
Epoch 99/100 Iteration 13/234: loss=0.028493 lr=0.000020 grad_norm=0.753557
Epoch 99/100 Iteration 14/234: loss=0.033429 lr=0.000020 grad_norm=0.556736
Epoch 99/100 Iteration 15/234: loss=0.033769 lr=0.000020 grad_norm=0.835508
Epoch 99/100 Iteration 16/234: loss=0.035625 lr=0.000020 grad_norm=0.911569
Epoch 99/100 Iteration 17/234: loss=0.035849 lr=0.000020 grad_norm=1.096162
Epoch 99/100 Iteration 18/234: loss=0.034550 lr=0.000020 grad_norm=1.172785
Epoch 99/100 Iteration 19/234: loss=0.031506 lr=0.000020 grad_norm=0.794571
Epoch 99/100 Iteration 20/234: loss=0.032994 lr=0.000020 grad_norm=0.561507
Epoch 99/100 Iteration 21/234: loss=0.030676 lr=0.000020 grad_norm=0.857070
Epoch 99/100 Iteration 22/234: loss=0.032845 lr=0.000020 grad_norm=0.496852
Epoch 99/100 Iteration 23/234: loss=0.033472 lr=0.000020 grad_norm=0.896554
Epoch 99/100 Iteration 24/234: loss=0.034261 lr=0.000020 grad_norm=0.958923
Epoch 99/100 Iteration 25/234: loss=0.030919 lr=0.000020 grad_norm=0.694160
Epoch 99/100 Iteration 26/234: loss=0.031148 lr=0.000020 grad_norm=0.975842
Epoch 99/100 Iteration 27/234: loss=0.033076 lr=0.000020 grad_norm=1.052512
Epoch 99/100 Iteration 28/234: loss=0.029960 lr=0.000020 grad_norm=0.460361
Epoch 99/100 Iteration 29/234: loss=0.029953 lr=0.000020 grad_norm=0.671412
Epoch 99/100 Iteration 30/234: loss=0.029813 lr=0.000020 grad_norm=0.670812
Epoch 99/100 Iteration 31/234: loss=0.032179 lr=0.000020 grad_norm=0.500568
Epoch 99/100 Iteration 32/234: loss=0.032783 lr=0.000020 grad_norm=0.824319
Epoch 99/100 Iteration 33/234: loss=0.032821 lr=0.000020 grad_norm=0.653122
Epoch 99/100 Iteration 34/234: loss=0.028177 lr=0.000020 grad_norm=0.528051
Epoch 99/100 Iteration 35/234: loss=0.028705 lr=0.000020 grad_norm=0.464592
Epoch 99/100 Iteration 36/234: loss=0.033094 lr=0.000020 grad_norm=0.632277
Epoch 99/100 Iteration 37/234: loss=0.031204 lr=0.000020 grad_norm=0.810103
Epoch 99/100 Iteration 38/234: loss=0.032788 lr=0.000020 grad_norm=0.432858
Epoch 99/100 Iteration 39/234: loss=0.031807 lr=0.000020 grad_norm=0.739841
Epoch 99/100 Iteration 40/234: loss=0.032885 lr=0.000020 grad_norm=0.586571
Epoch 99/100 Iteration 41/234: loss=0.032993 lr=0.000020 grad_norm=0.546047
Epoch 99/100 Iteration 42/234: loss=0.034076 lr=0.000020 grad_norm=0.895760
Epoch 99/100 Iteration 43/234: loss=0.035812 lr=0.000020 grad_norm=0.900243
Epoch 99/100 Iteration 44/234: loss=0.030005 lr=0.000020 grad_norm=0.551634
Epoch 99/100 Iteration 45/234: loss=0.030655 lr=0.000020 grad_norm=0.724509
Epoch 99/100 Iteration 46/234: loss=0.033138 lr=0.000020 grad_norm=0.980428
Epoch 99/100 Iteration 47/234: loss=0.036878 lr=0.000020 grad_norm=0.601434
Epoch 99/100 Iteration 48/234: loss=0.032918 lr=0.000020 grad_norm=0.414976
Epoch 99/100 Iteration 49/234: loss=0.031914 lr=0.000020 grad_norm=0.587844
Epoch 99/100 Iteration 50/234: loss=0.030937 lr=0.000020 grad_norm=0.455981
Epoch 99/100 Iteration 51/234: loss=0.032745 lr=0.000020 grad_norm=0.905058
Epoch 99/100 Iteration 52/234: loss=0.032018 lr=0.000020 grad_norm=0.899705
Epoch 99/100 Iteration 53/234: loss=0.031215 lr=0.000020 grad_norm=0.477159
Epoch 99/100 Iteration 54/234: loss=0.033653 lr=0.000020 grad_norm=0.642994
Epoch 99/100 Iteration 55/234: loss=0.031856 lr=0.000020 grad_norm=0.994418
Epoch 99/100 Iteration 56/234: loss=0.028373 lr=0.000020 grad_norm=0.562481
Epoch 99/100 Iteration 57/234: loss=0.033176 lr=0.000020 grad_norm=0.546481
Epoch 99/100 Iteration 58/234: loss=0.033103 lr=0.000020 grad_norm=0.687237
Epoch 99/100 Iteration 59/234: loss=0.029719 lr=0.000020 grad_norm=0.400654
Epoch 99/100 Iteration 60/234: loss=0.033468 lr=0.000020 grad_norm=0.455625
Epoch 99/100 Iteration 61/234: loss=0.035499 lr=0.000020 grad_norm=0.800342
Epoch 99/100 Iteration 62/234: loss=0.035046 lr=0.000020 grad_norm=0.909855
Epoch 99/100 Iteration 63/234: loss=0.036405 lr=0.000020 grad_norm=1.276375
Epoch 99/100 Iteration 64/234: loss=0.032770 lr=0.000020 grad_norm=1.544635
Epoch 99/100 Iteration 65/234: loss=0.035129 lr=0.000020 grad_norm=0.871445
Epoch 99/100 Iteration 66/234: loss=0.033702 lr=0.000020 grad_norm=0.660446
Epoch 99/100 Iteration 67/234: loss=0.033420 lr=0.000020 grad_norm=1.016229
Epoch 99/100 Iteration 68/234: loss=0.035069 lr=0.000020 grad_norm=0.577525
Epoch 99/100 Iteration 69/234: loss=0.034708 lr=0.000020 grad_norm=1.536562
Epoch 99/100 Iteration 70/234: loss=0.031232 lr=0.000020 grad_norm=1.422666
Epoch 99/100 Iteration 71/234: loss=0.031917 lr=0.000020 grad_norm=0.699062
Epoch 99/100 Iteration 72/234: loss=0.030319 lr=0.000020 grad_norm=1.335838
Epoch 99/100 Iteration 73/234: loss=0.033356 lr=0.000020 grad_norm=0.695101
Epoch 99/100 Iteration 74/234: loss=0.034615 lr=0.000020 grad_norm=1.096745
Epoch 99/100 Iteration 75/234: loss=0.035261 lr=0.000020 grad_norm=0.795517
Epoch 99/100 Iteration 76/234: loss=0.031498 lr=0.000020 grad_norm=0.761835
Epoch 99/100 Iteration 77/234: loss=0.033991 lr=0.000020 grad_norm=1.052983
Epoch 99/100 Iteration 78/234: loss=0.031854 lr=0.000020 grad_norm=0.429962
Epoch 99/100 Iteration 79/234: loss=0.031459 lr=0.000020 grad_norm=0.813219
Epoch 99/100 Iteration 80/234: loss=0.034781 lr=0.000020 grad_norm=0.623815
Epoch 99/100 Iteration 81/234: loss=0.033656 lr=0.000020 grad_norm=0.921346
Epoch 99/100 Iteration 82/234: loss=0.032740 lr=0.000020 grad_norm=0.979647
Epoch 99/100 Iteration 83/234: loss=0.031627 lr=0.000020 grad_norm=0.535149
Epoch 99/100 Iteration 84/234: loss=0.031069 lr=0.000020 grad_norm=0.933404
Epoch 99/100 Iteration 85/234: loss=0.035162 lr=0.000020 grad_norm=0.415053
Epoch 99/100 Iteration 86/234: loss=0.030033 lr=0.000020 grad_norm=0.679564
Epoch 99/100 Iteration 87/234: loss=0.033172 lr=0.000020 grad_norm=0.550384
Epoch 99/100 Iteration 88/234: loss=0.028819 lr=0.000020 grad_norm=0.954026
Epoch 99/100 Iteration 89/234: loss=0.031628 lr=0.000020 grad_norm=0.449823
Epoch 99/100 Iteration 90/234: loss=0.031868 lr=0.000020 grad_norm=0.790504
Epoch 99/100 Iteration 91/234: loss=0.032963 lr=0.000020 grad_norm=0.710734
Epoch 99/100 Iteration 92/234: loss=0.031022 lr=0.000020 grad_norm=0.596793
Epoch 99/100 Iteration 93/234: loss=0.032616 lr=0.000020 grad_norm=0.705974
Epoch 99/100 Iteration 94/234: loss=0.031792 lr=0.000020 grad_norm=0.356181
Epoch 99/100 Iteration 95/234: loss=0.035100 lr=0.000020 grad_norm=0.662759
Epoch 99/100 Iteration 96/234: loss=0.031423 lr=0.000020 grad_norm=0.503305
Epoch 99/100 Iteration 97/234: loss=0.033985 lr=0.000020 grad_norm=0.358898
Epoch 99/100 Iteration 98/234: loss=0.033756 lr=0.000020 grad_norm=0.505001
Epoch 99/100 Iteration 99/234: loss=0.035495 lr=0.000020 grad_norm=0.584789
Epoch 99/100 Iteration 100/234: loss=0.032816 lr=0.000020 grad_norm=0.654384
Epoch 99/100 Iteration 101/234: loss=0.031933 lr=0.000020 grad_norm=0.575814
Epoch 99/100 Iteration 102/234: loss=0.034371 lr=0.000020 grad_norm=0.954599
Epoch 99/100 Iteration 103/234: loss=0.034279 lr=0.000020 grad_norm=1.022567
Epoch 99/100 Iteration 104/234: loss=0.034313 lr=0.000020 grad_norm=0.882773
Epoch 99/100 Iteration 105/234: loss=0.033816 lr=0.000020 grad_norm=0.651907
Epoch 99/100 Iteration 106/234: loss=0.032460 lr=0.000020 grad_norm=0.467872
Epoch 99/100 Iteration 107/234: loss=0.031619 lr=0.000020 grad_norm=0.623064
Epoch 99/100 Iteration 108/234: loss=0.032118 lr=0.000020 grad_norm=0.748201
Epoch 99/100 Iteration 109/234: loss=0.033697 lr=0.000020 grad_norm=0.531766
Epoch 99/100 Iteration 110/234: loss=0.033681 lr=0.000020 grad_norm=0.624994
Epoch 99/100 Iteration 111/234: loss=0.031319 lr=0.000020 grad_norm=0.661912
Epoch 99/100 Iteration 112/234: loss=0.037386 lr=0.000020 grad_norm=0.742386
Epoch 99/100 Iteration 113/234: loss=0.034966 lr=0.000020 grad_norm=0.709411
Epoch 99/100 Iteration 114/234: loss=0.034885 lr=0.000020 grad_norm=0.597345
Epoch 99/100 Iteration 115/234: loss=0.032924 lr=0.000020 grad_norm=0.658763
Epoch 99/100 Iteration 116/234: loss=0.032262 lr=0.000020 grad_norm=0.535162
Epoch 99/100 Iteration 117/234: loss=0.031372 lr=0.000020 grad_norm=0.541887
Epoch 99/100 Iteration 118/234: loss=0.035043 lr=0.000020 grad_norm=0.739581
Epoch 99/100 Iteration 119/234: loss=0.038538 lr=0.000020 grad_norm=0.795488
Epoch 99/100 Iteration 120/234: loss=0.031945 lr=0.000020 grad_norm=0.867387
Epoch 99/100 Iteration 121/234: loss=0.029047 lr=0.000020 grad_norm=0.417985
Epoch 99/100 Iteration 122/234: loss=0.032192 lr=0.000020 grad_norm=0.699871
Epoch 99/100 Iteration 123/234: loss=0.033412 lr=0.000020 grad_norm=0.734221
Epoch 99/100 Iteration 124/234: loss=0.033623 lr=0.000020 grad_norm=0.550742
Epoch 99/100 Iteration 125/234: loss=0.033605 lr=0.000020 grad_norm=0.567824
Epoch 99/100 Iteration 126/234: loss=0.035150 lr=0.000020 grad_norm=0.452224
Epoch 99/100 Iteration 127/234: loss=0.032116 lr=0.000020 grad_norm=0.420494
Epoch 99/100 Iteration 128/234: loss=0.031715 lr=0.000020 grad_norm=0.443101
Epoch 99/100 Iteration 129/234: loss=0.031099 lr=0.000020 grad_norm=0.418769
Epoch 99/100 Iteration 130/234: loss=0.033553 lr=0.000020 grad_norm=0.599472
Epoch 99/100 Iteration 131/234: loss=0.033400 lr=0.000020 grad_norm=0.866027
Epoch 99/100 Iteration 132/234: loss=0.036880 lr=0.000020 grad_norm=0.503888
Epoch 99/100 Iteration 133/234: loss=0.035775 lr=0.000020 grad_norm=0.592540
Epoch 99/100 Iteration 134/234: loss=0.035022 lr=0.000020 grad_norm=0.777396
Epoch 99/100 Iteration 135/234: loss=0.033265 lr=0.000020 grad_norm=0.538822
Epoch 99/100 Iteration 136/234: loss=0.034737 lr=0.000020 grad_norm=0.392698
Epoch 99/100 Iteration 137/234: loss=0.028722 lr=0.000020 grad_norm=0.421496
Epoch 99/100 Iteration 138/234: loss=0.033170 lr=0.000020 grad_norm=0.358629
Epoch 99/100 Iteration 139/234: loss=0.035285 lr=0.000020 grad_norm=0.853043
Epoch 99/100 Iteration 140/234: loss=0.032471 lr=0.000020 grad_norm=1.111504
Epoch 99/100 Iteration 141/234: loss=0.034280 lr=0.000020 grad_norm=0.667646
Epoch 99/100 Iteration 142/234: loss=0.032027 lr=0.000020 grad_norm=1.200638
Epoch 99/100 Iteration 143/234: loss=0.037029 lr=0.000020 grad_norm=1.237030
Epoch 99/100 Iteration 144/234: loss=0.030797 lr=0.000020 grad_norm=0.932565
Epoch 99/100 Iteration 145/234: loss=0.034801 lr=0.000020 grad_norm=0.927562
Epoch 99/100 Iteration 146/234: loss=0.032384 lr=0.000020 grad_norm=1.207427
Epoch 99/100 Iteration 147/234: loss=0.032560 lr=0.000020 grad_norm=1.370185
Epoch 99/100 Iteration 148/234: loss=0.034769 lr=0.000020 grad_norm=1.081327
Epoch 99/100 Iteration 149/234: loss=0.034541 lr=0.000020 grad_norm=0.528769
Epoch 99/100 Iteration 150/234: loss=0.032865 lr=0.000020 grad_norm=0.952836
Epoch 99/100 Iteration 151/234: loss=0.031737 lr=0.000020 grad_norm=1.222681
Epoch 99/100 Iteration 152/234: loss=0.031552 lr=0.000020 grad_norm=0.646439
Epoch 99/100 Iteration 153/234: loss=0.032093 lr=0.000020 grad_norm=0.890928
Epoch 99/100 Iteration 154/234: loss=0.032982 lr=0.000020 grad_norm=1.325487
Epoch 99/100 Iteration 155/234: loss=0.034260 lr=0.000020 grad_norm=0.811653
Epoch 99/100 Iteration 156/234: loss=0.033740 lr=0.000020 grad_norm=0.568159
Epoch 99/100 Iteration 157/234: loss=0.033162 lr=0.000020 grad_norm=1.071432
Epoch 99/100 Iteration 158/234: loss=0.030794 lr=0.000020 grad_norm=0.985363
Epoch 99/100 Iteration 159/234: loss=0.034144 lr=0.000020 grad_norm=0.649656
Epoch 99/100 Iteration 160/234: loss=0.035661 lr=0.000020 grad_norm=1.162927
Epoch 99/100 Iteration 161/234: loss=0.032304 lr=0.000020 grad_norm=1.186699
Epoch 99/100 Iteration 162/234: loss=0.032620 lr=0.000020 grad_norm=1.081155
Epoch 99/100 Iteration 163/234: loss=0.032233 lr=0.000020 grad_norm=1.321688
Epoch 99/100 Iteration 164/234: loss=0.032723 lr=0.000020 grad_norm=0.960416
Epoch 99/100 Iteration 165/234: loss=0.034481 lr=0.000020 grad_norm=1.619743
Epoch 99/100 Iteration 166/234: loss=0.034692 lr=0.000020 grad_norm=1.467855
Epoch 99/100 Iteration 167/234: loss=0.033216 lr=0.000020 grad_norm=0.886919
Epoch 99/100 Iteration 168/234: loss=0.028493 lr=0.000020 grad_norm=0.940563
Epoch 99/100 Iteration 169/234: loss=0.031592 lr=0.000020 grad_norm=0.936944
Epoch 99/100 Iteration 170/234: loss=0.030770 lr=0.000020 grad_norm=0.650778
Epoch 99/100 Iteration 171/234: loss=0.029591 lr=0.000020 grad_norm=0.885815
Epoch 99/100 Iteration 172/234: loss=0.035409 lr=0.000020 grad_norm=0.568022
Epoch 99/100 Iteration 173/234: loss=0.034183 lr=0.000020 grad_norm=1.013255
Epoch 99/100 Iteration 174/234: loss=0.030436 lr=0.000020 grad_norm=0.968990
Epoch 99/100 Iteration 175/234: loss=0.028765 lr=0.000020 grad_norm=0.624399
Epoch 99/100 Iteration 176/234: loss=0.031659 lr=0.000020 grad_norm=1.114322
Epoch 99/100 Iteration 177/234: loss=0.028771 lr=0.000020 grad_norm=0.596541
Epoch 99/100 Iteration 178/234: loss=0.036861 lr=0.000020 grad_norm=1.204704
Epoch 99/100 Iteration 179/234: loss=0.036888 lr=0.000020 grad_norm=1.156152
Epoch 99/100 Iteration 180/234: loss=0.031417 lr=0.000020 grad_norm=0.626425
Epoch 99/100 Iteration 181/234: loss=0.032666 lr=0.000020 grad_norm=0.716276
Epoch 99/100 Iteration 182/234: loss=0.034177 lr=0.000020 grad_norm=0.765195
Epoch 99/100 Iteration 183/234: loss=0.033205 lr=0.000020 grad_norm=0.771977
Epoch 99/100 Iteration 184/234: loss=0.034525 lr=0.000020 grad_norm=0.770864
Epoch 99/100 Iteration 185/234: loss=0.031179 lr=0.000020 grad_norm=0.793138
Epoch 99/100 Iteration 186/234: loss=0.031339 lr=0.000020 grad_norm=0.701515
Epoch 99/100 Iteration 187/234: loss=0.030360 lr=0.000020 grad_norm=0.901248
Epoch 99/100 Iteration 188/234: loss=0.034574 lr=0.000020 grad_norm=1.237710
Epoch 99/100 Iteration 189/234: loss=0.030613 lr=0.000020 grad_norm=0.902470
Epoch 99/100 Iteration 190/234: loss=0.032695 lr=0.000020 grad_norm=0.933928
Epoch 99/100 Iteration 191/234: loss=0.031225 lr=0.000020 grad_norm=1.500904
Epoch 99/100 Iteration 192/234: loss=0.034992 lr=0.000020 grad_norm=1.232682
Epoch 99/100 Iteration 193/234: loss=0.034944 lr=0.000020 grad_norm=1.181591
Epoch 99/100 Iteration 194/234: loss=0.033498 lr=0.000020 grad_norm=1.070502
Epoch 99/100 Iteration 195/234: loss=0.033968 lr=0.000020 grad_norm=1.295844
Epoch 99/100 Iteration 196/234: loss=0.033636 lr=0.000020 grad_norm=1.207250
Epoch 99/100 Iteration 197/234: loss=0.033412 lr=0.000020 grad_norm=0.601592
Epoch 99/100 Iteration 198/234: loss=0.031994 lr=0.000020 grad_norm=0.597656
Epoch 99/100 Iteration 199/234: loss=0.034615 lr=0.000020 grad_norm=0.549051
Epoch 99/100 Iteration 200/234: loss=0.031252 lr=0.000020 grad_norm=0.755663
Epoch 99/100 Iteration 201/234: loss=0.030686 lr=0.000020 grad_norm=1.026651
Epoch 99/100 Iteration 202/234: loss=0.032258 lr=0.000020 grad_norm=0.830005
Epoch 99/100 Iteration 203/234: loss=0.032266 lr=0.000020 grad_norm=0.499817
Epoch 99/100 Iteration 204/234: loss=0.031241 lr=0.000020 grad_norm=0.449581
Epoch 99/100 Iteration 205/234: loss=0.036715 lr=0.000020 grad_norm=0.641449
Epoch 99/100 Iteration 206/234: loss=0.033889 lr=0.000020 grad_norm=0.485080
Epoch 99/100 Iteration 207/234: loss=0.032551 lr=0.000020 grad_norm=0.406433
Epoch 99/100 Iteration 208/234: loss=0.030233 lr=0.000020 grad_norm=0.437013
Epoch 99/100 Iteration 209/234: loss=0.033086 lr=0.000020 grad_norm=0.484322
Epoch 99/100 Iteration 210/234: loss=0.036555 lr=0.000020 grad_norm=0.681650
Epoch 99/100 Iteration 211/234: loss=0.031335 lr=0.000020 grad_norm=0.627831
Epoch 99/100 Iteration 212/234: loss=0.034689 lr=0.000020 grad_norm=0.383079
Epoch 99/100 Iteration 213/234: loss=0.031905 lr=0.000020 grad_norm=0.642906
Epoch 99/100 Iteration 214/234: loss=0.033519 lr=0.000020 grad_norm=0.548568
Epoch 99/100 Iteration 215/234: loss=0.035598 lr=0.000020 grad_norm=0.776026
Epoch 99/100 Iteration 216/234: loss=0.032919 lr=0.000020 grad_norm=0.747141
Epoch 99/100 Iteration 217/234: loss=0.032087 lr=0.000020 grad_norm=0.444458
Epoch 99/100 Iteration 218/234: loss=0.034111 lr=0.000020 grad_norm=0.511084
Epoch 99/100 Iteration 219/234: loss=0.031777 lr=0.000020 grad_norm=0.821192
Epoch 99/100 Iteration 220/234: loss=0.033079 lr=0.000020 grad_norm=0.431550
Epoch 99/100 Iteration 221/234: loss=0.031925 lr=0.000020 grad_norm=0.607795
Epoch 99/100 Iteration 222/234: loss=0.031485 lr=0.000020 grad_norm=0.357248
Epoch 99/100 Iteration 223/234: loss=0.031489 lr=0.000020 grad_norm=0.716392
Epoch 99/100 Iteration 224/234: loss=0.032566 lr=0.000020 grad_norm=0.792642
Epoch 99/100 Iteration 225/234: loss=0.034198 lr=0.000020 grad_norm=0.536566
Epoch 99/100 Iteration 226/234: loss=0.032300 lr=0.000020 grad_norm=0.591951
Epoch 99/100 Iteration 227/234: loss=0.032154 lr=0.000020 grad_norm=0.585281
Epoch 99/100 Iteration 228/234: loss=0.032168 lr=0.000020 grad_norm=0.425278
Epoch 99/100 Iteration 229/234: loss=0.032824 lr=0.000020 grad_norm=0.425557
Epoch 99/100 Iteration 230/234: loss=0.032792 lr=0.000020 grad_norm=0.570868
Epoch 99/100 Iteration 231/234: loss=0.030992 lr=0.000020 grad_norm=0.688591
Epoch 99/100 Iteration 232/234: loss=0.030305 lr=0.000020 grad_norm=0.414024
Epoch 99/100 Iteration 233/234: loss=0.033374 lr=0.000020 grad_norm=0.659092
Epoch 99/100 Iteration 234/234: loss=0.036411 lr=0.000020 grad_norm=0.911150
Epoch 99/100 finished. Avg Loss: 0.032853
Epoch 100/100 Iteration 1/234: loss=0.034167 lr=0.000020 grad_norm=0.911020
Epoch 100/100 Iteration 2/234: loss=0.031655 lr=0.000020 grad_norm=0.751410
Epoch 100/100 Iteration 3/234: loss=0.032213 lr=0.000020 grad_norm=0.414658
Epoch 100/100 Iteration 4/234: loss=0.033567 lr=0.000020 grad_norm=0.546717
Epoch 100/100 Iteration 5/234: loss=0.032137 lr=0.000020 grad_norm=0.683486
Epoch 100/100 Iteration 6/234: loss=0.032673 lr=0.000020 grad_norm=0.430717
Epoch 100/100 Iteration 7/234: loss=0.034547 lr=0.000020 grad_norm=0.798385
Epoch 100/100 Iteration 8/234: loss=0.030129 lr=0.000020 grad_norm=0.801264
Epoch 100/100 Iteration 9/234: loss=0.030597 lr=0.000020 grad_norm=0.371361
Epoch 100/100 Iteration 10/234: loss=0.034791 lr=0.000020 grad_norm=0.982650
Epoch 100/100 Iteration 11/234: loss=0.030633 lr=0.000020 grad_norm=0.949681
Epoch 100/100 Iteration 12/234: loss=0.035357 lr=0.000020 grad_norm=0.760676
Epoch 100/100 Iteration 13/234: loss=0.031922 lr=0.000020 grad_norm=0.776201
Epoch 100/100 Iteration 14/234: loss=0.031928 lr=0.000020 grad_norm=0.457896
Epoch 100/100 Iteration 15/234: loss=0.033014 lr=0.000020 grad_norm=0.672178
Epoch 100/100 Iteration 16/234: loss=0.031684 lr=0.000020 grad_norm=0.553299
Epoch 100/100 Iteration 17/234: loss=0.032798 lr=0.000020 grad_norm=0.369062
Epoch 100/100 Iteration 18/234: loss=0.029730 lr=0.000020 grad_norm=0.613284
Epoch 100/100 Iteration 19/234: loss=0.031990 lr=0.000020 grad_norm=0.559342
Epoch 100/100 Iteration 20/234: loss=0.031074 lr=0.000020 grad_norm=0.350902
Epoch 100/100 Iteration 21/234: loss=0.035645 lr=0.000020 grad_norm=0.699630
Epoch 100/100 Iteration 22/234: loss=0.038858 lr=0.000020 grad_norm=1.359061
Epoch 100/100 Iteration 23/234: loss=0.032516 lr=0.000020 grad_norm=1.834360
Epoch 100/100 Iteration 24/234: loss=0.032252 lr=0.000020 grad_norm=1.396426
Epoch 100/100 Iteration 25/234: loss=0.034187 lr=0.000020 grad_norm=0.598660
Epoch 100/100 Iteration 26/234: loss=0.032293 lr=0.000020 grad_norm=1.949122
Epoch 100/100 Iteration 27/234: loss=0.033268 lr=0.000020 grad_norm=1.500029
Epoch 100/100 Iteration 28/234: loss=0.032198 lr=0.000020 grad_norm=0.701150
Epoch 100/100 Iteration 29/234: loss=0.031037 lr=0.000020 grad_norm=1.742606
Epoch 100/100 Iteration 30/234: loss=0.034143 lr=0.000020 grad_norm=1.032606
Epoch 100/100 Iteration 31/234: loss=0.033447 lr=0.000020 grad_norm=0.689888
Epoch 100/100 Iteration 32/234: loss=0.034111 lr=0.000020 grad_norm=1.025531
Epoch 100/100 Iteration 33/234: loss=0.030867 lr=0.000020 grad_norm=0.621571
Epoch 100/100 Iteration 34/234: loss=0.033708 lr=0.000020 grad_norm=0.875650
Epoch 100/100 Iteration 35/234: loss=0.032775 lr=0.000020 grad_norm=1.232137
Epoch 100/100 Iteration 36/234: loss=0.033376 lr=0.000020 grad_norm=0.676650
Epoch 100/100 Iteration 37/234: loss=0.032162 lr=0.000020 grad_norm=0.426199
Epoch 100/100 Iteration 38/234: loss=0.030267 lr=0.000020 grad_norm=0.619244
Epoch 100/100 Iteration 39/234: loss=0.032825 lr=0.000020 grad_norm=0.488772
Epoch 100/100 Iteration 40/234: loss=0.033867 lr=0.000020 grad_norm=0.667348
Epoch 100/100 Iteration 41/234: loss=0.032385 lr=0.000020 grad_norm=0.837966
Epoch 100/100 Iteration 42/234: loss=0.032190 lr=0.000020 grad_norm=0.413970
Epoch 100/100 Iteration 43/234: loss=0.033529 lr=0.000020 grad_norm=0.800514
Epoch 100/100 Iteration 44/234: loss=0.032453 lr=0.000020 grad_norm=0.678859
Epoch 100/100 Iteration 45/234: loss=0.030533 lr=0.000020 grad_norm=0.564733
Epoch 100/100 Iteration 46/234: loss=0.033640 lr=0.000020 grad_norm=0.581659
Epoch 100/100 Iteration 47/234: loss=0.030256 lr=0.000020 grad_norm=0.540779
Epoch 100/100 Iteration 48/234: loss=0.031590 lr=0.000020 grad_norm=0.517046
Epoch 100/100 Iteration 49/234: loss=0.035237 lr=0.000020 grad_norm=0.598281
Epoch 100/100 Iteration 50/234: loss=0.034927 lr=0.000020 grad_norm=0.611942
Epoch 100/100 Iteration 51/234: loss=0.034808 lr=0.000020 grad_norm=0.819016
Epoch 100/100 Iteration 52/234: loss=0.032333 lr=0.000020 grad_norm=0.785868
Epoch 100/100 Iteration 53/234: loss=0.032007 lr=0.000020 grad_norm=0.582025
Epoch 100/100 Iteration 54/234: loss=0.033028 lr=0.000020 grad_norm=0.859597
Epoch 100/100 Iteration 55/234: loss=0.032520 lr=0.000020 grad_norm=0.578637
Epoch 100/100 Iteration 56/234: loss=0.031942 lr=0.000020 grad_norm=0.383683
Epoch 100/100 Iteration 57/234: loss=0.033011 lr=0.000020 grad_norm=0.520589
Epoch 100/100 Iteration 58/234: loss=0.031449 lr=0.000020 grad_norm=0.408965
Epoch 100/100 Iteration 59/234: loss=0.032132 lr=0.000020 grad_norm=0.391998
Epoch 100/100 Iteration 60/234: loss=0.031131 lr=0.000020 grad_norm=0.680558
Epoch 100/100 Iteration 61/234: loss=0.033050 lr=0.000020 grad_norm=0.625737
Epoch 100/100 Iteration 62/234: loss=0.032051 lr=0.000020 grad_norm=0.350640
Epoch 100/100 Iteration 63/234: loss=0.034308 lr=0.000020 grad_norm=1.023645
Epoch 100/100 Iteration 64/234: loss=0.033881 lr=0.000020 grad_norm=1.199989
Epoch 100/100 Iteration 65/234: loss=0.032794 lr=0.000020 grad_norm=1.253848
Epoch 100/100 Iteration 66/234: loss=0.033705 lr=0.000020 grad_norm=1.058876
Epoch 100/100 Iteration 67/234: loss=0.032004 lr=0.000020 grad_norm=0.902543
Epoch 100/100 Iteration 68/234: loss=0.033868 lr=0.000020 grad_norm=1.053295
Epoch 100/100 Iteration 69/234: loss=0.032420 lr=0.000020 grad_norm=0.790325
Epoch 100/100 Iteration 70/234: loss=0.033576 lr=0.000020 grad_norm=0.456371
Epoch 100/100 Iteration 71/234: loss=0.030487 lr=0.000020 grad_norm=0.722916
Epoch 100/100 Iteration 72/234: loss=0.032840 lr=0.000020 grad_norm=0.599840
Epoch 100/100 Iteration 73/234: loss=0.035342 lr=0.000020 grad_norm=1.027653
Epoch 100/100 Iteration 74/234: loss=0.032660 lr=0.000020 grad_norm=1.080208
Epoch 100/100 Iteration 75/234: loss=0.031081 lr=0.000020 grad_norm=0.333675
Epoch 100/100 Iteration 76/234: loss=0.033866 lr=0.000020 grad_norm=0.920124
Epoch 100/100 Iteration 77/234: loss=0.034196 lr=0.000020 grad_norm=1.157944
Epoch 100/100 Iteration 78/234: loss=0.032407 lr=0.000020 grad_norm=0.674150
Epoch 100/100 Iteration 79/234: loss=0.033050 lr=0.000020 grad_norm=0.665593
Epoch 100/100 Iteration 80/234: loss=0.029751 lr=0.000020 grad_norm=0.746580
Epoch 100/100 Iteration 81/234: loss=0.031536 lr=0.000020 grad_norm=0.478996
Epoch 100/100 Iteration 82/234: loss=0.034984 lr=0.000020 grad_norm=0.894793
Epoch 100/100 Iteration 83/234: loss=0.033985 lr=0.000020 grad_norm=0.469020
Epoch 100/100 Iteration 84/234: loss=0.032783 lr=0.000020 grad_norm=0.461893
Epoch 100/100 Iteration 85/234: loss=0.032170 lr=0.000020 grad_norm=0.515555
Epoch 100/100 Iteration 86/234: loss=0.032906 lr=0.000020 grad_norm=0.541405
Epoch 100/100 Iteration 87/234: loss=0.032412 lr=0.000020 grad_norm=0.648780
Epoch 100/100 Iteration 88/234: loss=0.033835 lr=0.000020 grad_norm=0.593746
Epoch 100/100 Iteration 89/234: loss=0.031582 lr=0.000020 grad_norm=0.520535
Epoch 100/100 Iteration 90/234: loss=0.033264 lr=0.000020 grad_norm=0.649713
Epoch 100/100 Iteration 91/234: loss=0.033209 lr=0.000020 grad_norm=0.573057
Epoch 100/100 Iteration 92/234: loss=0.034753 lr=0.000020 grad_norm=0.414871
Epoch 100/100 Iteration 93/234: loss=0.032288 lr=0.000020 grad_norm=0.564462
Epoch 100/100 Iteration 94/234: loss=0.034241 lr=0.000020 grad_norm=0.639875
Epoch 100/100 Iteration 95/234: loss=0.030893 lr=0.000020 grad_norm=0.813939
Epoch 100/100 Iteration 96/234: loss=0.032680 lr=0.000020 grad_norm=0.538245
Epoch 100/100 Iteration 97/234: loss=0.031242 lr=0.000020 grad_norm=0.393826
Epoch 100/100 Iteration 98/234: loss=0.032264 lr=0.000020 grad_norm=0.354226
Epoch 100/100 Iteration 99/234: loss=0.033922 lr=0.000020 grad_norm=0.433943
Epoch 100/100 Iteration 100/234: loss=0.030336 lr=0.000020 grad_norm=0.621272
Epoch 100/100 Iteration 101/234: loss=0.034932 lr=0.000020 grad_norm=0.456868
Epoch 100/100 Iteration 102/234: loss=0.033344 lr=0.000020 grad_norm=0.475811
Epoch 100/100 Iteration 103/234: loss=0.030879 lr=0.000020 grad_norm=0.437214
Epoch 100/100 Iteration 104/234: loss=0.032602 lr=0.000020 grad_norm=0.413122
Epoch 100/100 Iteration 105/234: loss=0.033549 lr=0.000020 grad_norm=0.436177
Epoch 100/100 Iteration 106/234: loss=0.030817 lr=0.000020 grad_norm=0.635739
Epoch 100/100 Iteration 107/234: loss=0.032515 lr=0.000020 grad_norm=0.577024
Epoch 100/100 Iteration 108/234: loss=0.030553 lr=0.000020 grad_norm=0.383538
Epoch 100/100 Iteration 109/234: loss=0.035452 lr=0.000020 grad_norm=0.957207
Epoch 100/100 Iteration 110/234: loss=0.037136 lr=0.000020 grad_norm=1.379833
Epoch 100/100 Iteration 111/234: loss=0.033071 lr=0.000020 grad_norm=1.295324
Epoch 100/100 Iteration 112/234: loss=0.032656 lr=0.000020 grad_norm=0.489118
Epoch 100/100 Iteration 113/234: loss=0.033431 lr=0.000020 grad_norm=0.975151
Epoch 100/100 Iteration 114/234: loss=0.035093 lr=0.000020 grad_norm=1.095844
Epoch 100/100 Iteration 115/234: loss=0.031476 lr=0.000020 grad_norm=0.596142
Epoch 100/100 Iteration 116/234: loss=0.032570 lr=0.000020 grad_norm=0.584837
Epoch 100/100 Iteration 117/234: loss=0.031364 lr=0.000020 grad_norm=0.622904
Epoch 100/100 Iteration 118/234: loss=0.032299 lr=0.000020 grad_norm=0.615136
Epoch 100/100 Iteration 119/234: loss=0.031051 lr=0.000020 grad_norm=1.078836
Epoch 100/100 Iteration 120/234: loss=0.033485 lr=0.000020 grad_norm=1.039508
Epoch 100/100 Iteration 121/234: loss=0.032528 lr=0.000020 grad_norm=0.487504
Epoch 100/100 Iteration 122/234: loss=0.032441 lr=0.000020 grad_norm=0.757397
Epoch 100/100 Iteration 123/234: loss=0.033584 lr=0.000020 grad_norm=1.229925
Epoch 100/100 Iteration 124/234: loss=0.031564 lr=0.000020 grad_norm=1.066140
Epoch 100/100 Iteration 125/234: loss=0.035453 lr=0.000020 grad_norm=0.810013
Epoch 100/100 Iteration 126/234: loss=0.034812 lr=0.000020 grad_norm=1.503369
Epoch 100/100 Iteration 127/234: loss=0.034030 lr=0.000020 grad_norm=1.377468
Epoch 100/100 Iteration 128/234: loss=0.031333 lr=0.000020 grad_norm=0.719078
Epoch 100/100 Iteration 129/234: loss=0.034402 lr=0.000020 grad_norm=1.112212
Epoch 100/100 Iteration 130/234: loss=0.033583 lr=0.000020 grad_norm=0.860816
Epoch 100/100 Iteration 131/234: loss=0.029831 lr=0.000020 grad_norm=0.626506
Epoch 100/100 Iteration 132/234: loss=0.033980 lr=0.000020 grad_norm=1.368956
Epoch 100/100 Iteration 133/234: loss=0.032144 lr=0.000020 grad_norm=1.308048
Epoch 100/100 Iteration 134/234: loss=0.032546 lr=0.000020 grad_norm=0.828496
Epoch 100/100 Iteration 135/234: loss=0.030595 lr=0.000020 grad_norm=1.256476
Epoch 100/100 Iteration 136/234: loss=0.031657 lr=0.000020 grad_norm=0.634985
Epoch 100/100 Iteration 137/234: loss=0.030723 lr=0.000020 grad_norm=1.193627
Epoch 100/100 Iteration 138/234: loss=0.032826 lr=0.000020 grad_norm=0.839417
Epoch 100/100 Iteration 139/234: loss=0.031410 lr=0.000020 grad_norm=0.757663
Epoch 100/100 Iteration 140/234: loss=0.031655 lr=0.000020 grad_norm=0.832331
Epoch 100/100 Iteration 141/234: loss=0.031843 lr=0.000020 grad_norm=0.556136
Epoch 100/100 Iteration 142/234: loss=0.032399 lr=0.000020 grad_norm=0.770736
Epoch 100/100 Iteration 143/234: loss=0.034324 lr=0.000020 grad_norm=0.502644
Epoch 100/100 Iteration 144/234: loss=0.031652 lr=0.000020 grad_norm=0.662544
Epoch 100/100 Iteration 145/234: loss=0.031002 lr=0.000020 grad_norm=0.485863
Epoch 100/100 Iteration 146/234: loss=0.032682 lr=0.000020 grad_norm=0.434273
Epoch 100/100 Iteration 147/234: loss=0.033704 lr=0.000020 grad_norm=0.561667
Epoch 100/100 Iteration 148/234: loss=0.029261 lr=0.000020 grad_norm=0.476488
Epoch 100/100 Iteration 149/234: loss=0.031863 lr=0.000020 grad_norm=0.422912
Epoch 100/100 Iteration 150/234: loss=0.032762 lr=0.000020 grad_norm=0.491482
Epoch 100/100 Iteration 151/234: loss=0.031686 lr=0.000020 grad_norm=0.416560
Epoch 100/100 Iteration 152/234: loss=0.032525 lr=0.000020 grad_norm=0.699490
Epoch 100/100 Iteration 153/234: loss=0.029363 lr=0.000020 grad_norm=0.852525
Epoch 100/100 Iteration 154/234: loss=0.034413 lr=0.000020 grad_norm=0.373253
Epoch 100/100 Iteration 155/234: loss=0.032938 lr=0.000020 grad_norm=0.977483
Epoch 100/100 Iteration 156/234: loss=0.034609 lr=0.000020 grad_norm=1.313128
Epoch 100/100 Iteration 157/234: loss=0.034269 lr=0.000020 grad_norm=0.846737
Epoch 100/100 Iteration 158/234: loss=0.032852 lr=0.000020 grad_norm=0.532214
Epoch 100/100 Iteration 159/234: loss=0.031637 lr=0.000020 grad_norm=0.473007
Epoch 100/100 Iteration 160/234: loss=0.031560 lr=0.000020 grad_norm=0.417343
Epoch 100/100 Iteration 161/234: loss=0.031936 lr=0.000020 grad_norm=0.775823
Epoch 100/100 Iteration 162/234: loss=0.034171 lr=0.000020 grad_norm=0.932804
Epoch 100/100 Iteration 163/234: loss=0.034271 lr=0.000020 grad_norm=0.496932
Epoch 100/100 Iteration 164/234: loss=0.031650 lr=0.000020 grad_norm=0.569734
Epoch 100/100 Iteration 165/234: loss=0.033807 lr=0.000020 grad_norm=0.520964
Epoch 100/100 Iteration 166/234: loss=0.030056 lr=0.000020 grad_norm=0.436639
Epoch 100/100 Iteration 167/234: loss=0.032813 lr=0.000020 grad_norm=0.649703
Epoch 100/100 Iteration 168/234: loss=0.034977 lr=0.000020 grad_norm=0.493368
Epoch 100/100 Iteration 169/234: loss=0.032961 lr=0.000020 grad_norm=0.687857
Epoch 100/100 Iteration 170/234: loss=0.033515 lr=0.000020 grad_norm=0.874192
Epoch 100/100 Iteration 171/234: loss=0.033587 lr=0.000020 grad_norm=0.644879
Epoch 100/100 Iteration 172/234: loss=0.032737 lr=0.000020 grad_norm=0.671255
Epoch 100/100 Iteration 173/234: loss=0.036297 lr=0.000020 grad_norm=1.152932
Epoch 100/100 Iteration 174/234: loss=0.036445 lr=0.000020 grad_norm=1.313455
Epoch 100/100 Iteration 175/234: loss=0.032449 lr=0.000020 grad_norm=0.733244
Epoch 100/100 Iteration 176/234: loss=0.035785 lr=0.000020 grad_norm=0.736046
Epoch 100/100 Iteration 177/234: loss=0.030206 lr=0.000020 grad_norm=1.133806
Epoch 100/100 Iteration 178/234: loss=0.030828 lr=0.000020 grad_norm=0.636313
Epoch 100/100 Iteration 179/234: loss=0.031591 lr=0.000020 grad_norm=0.646536
Epoch 100/100 Iteration 180/234: loss=0.032787 lr=0.000020 grad_norm=0.649479
Epoch 100/100 Iteration 181/234: loss=0.032046 lr=0.000020 grad_norm=0.400843
Epoch 100/100 Iteration 182/234: loss=0.031692 lr=0.000020 grad_norm=0.634196
Epoch 100/100 Iteration 183/234: loss=0.033915 lr=0.000020 grad_norm=0.735083
Epoch 100/100 Iteration 184/234: loss=0.032541 lr=0.000020 grad_norm=0.316948
Epoch 100/100 Iteration 185/234: loss=0.032449 lr=0.000020 grad_norm=0.647637
Epoch 100/100 Iteration 186/234: loss=0.030816 lr=0.000020 grad_norm=0.657306
Epoch 100/100 Iteration 187/234: loss=0.032627 lr=0.000020 grad_norm=0.449509
Epoch 100/100 Iteration 188/234: loss=0.031848 lr=0.000020 grad_norm=0.800001
Epoch 100/100 Iteration 189/234: loss=0.032829 lr=0.000020 grad_norm=0.760068
Epoch 100/100 Iteration 190/234: loss=0.034374 lr=0.000020 grad_norm=0.521745
Epoch 100/100 Iteration 191/234: loss=0.039675 lr=0.000020 grad_norm=0.965170
Epoch 100/100 Iteration 192/234: loss=0.034325 lr=0.000020 grad_norm=0.866813
Epoch 100/100 Iteration 193/234: loss=0.032468 lr=0.000020 grad_norm=0.464429
Epoch 100/100 Iteration 194/234: loss=0.034818 lr=0.000020 grad_norm=0.585302
Epoch 100/100 Iteration 195/234: loss=0.030756 lr=0.000020 grad_norm=0.692017
Epoch 100/100 Iteration 196/234: loss=0.031617 lr=0.000020 grad_norm=0.356696
Epoch 100/100 Iteration 197/234: loss=0.033787 lr=0.000020 grad_norm=1.264170
Epoch 100/100 Iteration 198/234: loss=0.032397 lr=0.000020 grad_norm=1.880786
Epoch 100/100 Iteration 199/234: loss=0.036348 lr=0.000020 grad_norm=1.524378
Epoch 100/100 Iteration 200/234: loss=0.031905 lr=0.000020 grad_norm=0.616858
Epoch 100/100 Iteration 201/234: loss=0.030925 lr=0.000020 grad_norm=1.130017
Epoch 100/100 Iteration 202/234: loss=0.032666 lr=0.000020 grad_norm=0.812282
Epoch 100/100 Iteration 203/234: loss=0.035259 lr=0.000020 grad_norm=0.717550
Epoch 100/100 Iteration 204/234: loss=0.033604 lr=0.000020 grad_norm=1.151461
Epoch 100/100 Iteration 205/234: loss=0.034577 lr=0.000020 grad_norm=0.810222
Epoch 100/100 Iteration 206/234: loss=0.032205 lr=0.000020 grad_norm=0.892614
Epoch 100/100 Iteration 207/234: loss=0.030251 lr=0.000020 grad_norm=0.818087
Epoch 100/100 Iteration 208/234: loss=0.034063 lr=0.000020 grad_norm=0.515645
Epoch 100/100 Iteration 209/234: loss=0.033953 lr=0.000020 grad_norm=0.971200
Epoch 100/100 Iteration 210/234: loss=0.032153 lr=0.000020 grad_norm=0.810000
Epoch 100/100 Iteration 211/234: loss=0.032267 lr=0.000020 grad_norm=0.590101
Epoch 100/100 Iteration 212/234: loss=0.032442 lr=0.000020 grad_norm=0.931791
Epoch 100/100 Iteration 213/234: loss=0.036422 lr=0.000020 grad_norm=0.625409
Epoch 100/100 Iteration 214/234: loss=0.029800 lr=0.000020 grad_norm=0.981165
Epoch 100/100 Iteration 215/234: loss=0.031955 lr=0.000020 grad_norm=0.615211
Epoch 100/100 Iteration 216/234: loss=0.032445 lr=0.000020 grad_norm=0.821189
Epoch 100/100 Iteration 217/234: loss=0.030435 lr=0.000020 grad_norm=0.825157
Epoch 100/100 Iteration 218/234: loss=0.032549 lr=0.000020 grad_norm=0.710995
Epoch 100/100 Iteration 219/234: loss=0.033839 lr=0.000020 grad_norm=1.458699
Epoch 100/100 Iteration 220/234: loss=0.031413 lr=0.000020 grad_norm=0.575331
Epoch 100/100 Iteration 221/234: loss=0.032957 lr=0.000020 grad_norm=0.908565
Epoch 100/100 Iteration 222/234: loss=0.033683 lr=0.000020 grad_norm=0.780285
Epoch 100/100 Iteration 223/234: loss=0.034860 lr=0.000020 grad_norm=0.705058
Epoch 100/100 Iteration 224/234: loss=0.035478 lr=0.000020 grad_norm=0.966070
Epoch 100/100 Iteration 225/234: loss=0.033353 lr=0.000020 grad_norm=0.478753
Epoch 100/100 Iteration 226/234: loss=0.033260 lr=0.000020 grad_norm=0.850252
Epoch 100/100 Iteration 227/234: loss=0.032956 lr=0.000020 grad_norm=0.482318
Epoch 100/100 Iteration 228/234: loss=0.033505 lr=0.000020 grad_norm=0.770636
Epoch 100/100 Iteration 229/234: loss=0.032150 lr=0.000020 grad_norm=0.888953
Epoch 100/100 Iteration 230/234: loss=0.033383 lr=0.000020 grad_norm=0.533619
Epoch 100/100 Iteration 231/234: loss=0.032429 lr=0.000020 grad_norm=0.795001
Epoch 100/100 Iteration 232/234: loss=0.034070 lr=0.000020 grad_norm=0.484661
Epoch 100/100 Iteration 233/234: loss=0.035675 lr=0.000020 grad_norm=0.552401
Epoch 100/100 Iteration 234/234: loss=0.036388 lr=0.000020 grad_norm=0.633415
Epoch 100/100 finished. Avg Loss: 0.032839
/home/hoiming/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
